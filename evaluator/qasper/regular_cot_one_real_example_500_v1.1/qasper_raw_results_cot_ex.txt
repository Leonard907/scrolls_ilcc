Validation ID: b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54
Question: which multilingual approaches do they compare with?
From the context "Like experimental results on Europarl, MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer approaches as shown in Table TABREF26. When comparing systems consisting of one encoder-decoder model for all zero-shot translation, our approaches performs significantly better than MNMT BIBREF19.", so we know that they compare with MNMT.
Answer: MNMT
True answer: BIBREF19, BIBREF20

Validation ID: b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54
Question: which multilingual approaches do they compare with?
From the context "Like experimental results on Europarl, MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer approaches as shown in Table TABREF26. When comparing systems consisting of one encoder-decoder model for all zero-shot translation, our approaches performs significantly better than MNMT BIBREF19.", so we know that they compare with MNMT.
Answer: MNMT
True answer: multilingual NMT (MNMT) BIBREF19

Validation ID: f5e6f43454332e0521a778db0b769481e23e7682
Question: what are the pivot-based baselines?
From the context "We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.", so we know that the pivot-based baselines are pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.
Answer: pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16
True answer: pivoting, pivoting$_{\rm m}$

Validation ID: f5e6f43454332e0521a778db0b769481e23e7682
Question: what are the pivot-based baselines?
From the context "We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.", so we know that the pivot-based baselines are pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.
Answer: pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16
True answer: firstly translates a source language into the pivot language which is later translated to the target language

Validation ID: 9a05a5f4351db75da371f7ac12eb0b03607c4b87
Question: which datasets did they experiment with?
From the context "We evaluate our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting.", so we know that they experiment with Europarl and MultiUN.
Answer: Europarl and MultiUN
True answer: Europarl, MultiUN

Validation ID: 9a05a5f4351db75da371f7ac12eb0b03607c4b87
Question: which datasets did they experiment with?
From the context "We evaluate our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting.", so we know that they experiment with Europarl and MultiUN.
Answer: Europarl and MultiUN
True answer: Europarl BIBREF31, MultiUN BIBREF32

Validation ID: 5eda469a8a77f028d0c5f1acd296111085614537
Question: what language pairs are explored?
From the context "We evaluate our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting.", so we know that the language pairs explored are source to pivot and pivot to target.
Answer: source to pivot and pivot to target
True answer: De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru

Validation ID: 5eda469a8a77f028d0c5f1acd296111085614537
Question: what language pairs are explored?
From the context "We evaluate our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting.", so we know that the language pairs explored are source to pivot and pivot to target.
Answer: source to pivot and pivot to target
True answer: French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation

Validation ID: 18c5d366b1da8447b5404eab71f4cc658ba12e6f
Question: what ner models were evaluated?
From the context "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.", so we know that Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 were evaluated.
Answer: Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15
True answer: Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer

Validation ID: 18c5d366b1da8447b5404eab71f4cc658ba12e6f
Question: what ner models were evaluated?
From the context "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.", so we know that Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 were evaluated.
Answer: Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15
True answer: Stanford NER, spaCy 2.0, recurrent model with a CRF top layer

Validation ID: b5e4866f0685299f1d7af267bbcc4afe2aab806f
Question: what is the source of the news sentences?
From the context "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am", so we know that the source of the news sentences is ilur.am.
Answer: ilur.am
True answer: ilur.am

Validation ID: b5e4866f0685299f1d7af267bbcc4afe2aab806f
Question: what is the source of the news sentences?
From the context "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am", so we know that the source of the news sentences is ilur.am.
Answer: ilur.am
True answer: links between Wikipedia articles to generate sequences of named-entity annotated tokens

Validation ID: 1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590
Question: did they use a crowdsourcing platform for manual annotations?
From the context "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8, FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme.", so we know that they used a crowdsourcing platform for manual annotations.
Answer: yes
True answer: No

Validation ID: 1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590
Question: did they use a crowdsourcing platform for manual annotations?
From the context "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. This dataset is comparable in size with the test sets of other languages (Table TABREF10 ). Included sentences are from political, sports, local and world news (Figures FIGREF8, FIGREF9 ), covering the period between August 2012 and July 2018. The dataset provides annotations for 3 popular named entity classes: people (PER), organizations (ORG), and locations (LOC), and is released in CoNLL03 format with IOB tagging scheme.", so we know that they used a crowdsourcing platform for manual annotations.
Answer: yes
True answer: No

Validation ID: b6ae8e10c6a0d34c834f18f66ab730b670fb528c
Question: what are the topics pulled from Reddit?
From the context "We sample 1000 Reddit users who posted at least once a year between 2007 and 2015 to construct a corpus of 10 million posts that constitute their entire post history. We then annotate these posts using the classifier and compute the average dogmatism score per subreddit per user. For example, one user might have an average dogmatism level of 0.55 for the politics subreddit and 0.45 for the economics subreddit. Most users do not post in all subreddits, so we track only subreddits for which a user had posted at least 10 times. Any subreddits with an average dogmatism score higher than 0.50 we consider to be a user's dogmatic subreddits. We then count all pairs of these dogmatic subreddits. For example, 45 users have politics and technology among their dogmatic subreddits, so we consider politics and technology as linked 45 times. We compute the mutual information BIBREF13 between these links, which gives us a measure of the subreddits that are most related through dogmatism.", so we know that the topics pulled from Reddit are politics, technology, science, business, AskReddit, worldnews, IAmA, atheism, libertarian, and aww.
Answer: politics, technology, science, business, AskReddit, worldnews, IAmA, atheism, libertarian, and aww
True answer: politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. 

Validation ID: b6ae8e10c6a0d34c834f18f66ab730b670fb528c
Question: what are the topics pulled from Reddit?
From the context "We sample 1000 Reddit users who posted at least once a year between 2007 and 2015 to construct a corpus of 10 million posts that constitute their entire post history. We then annotate these posts using the classifier and compute the average dogmatism score per subreddit per user. For example, one user might have an average dogmatism level of 0.55 for the politics subreddit and 0.45 for the economics subreddit. Most users do not post in all subreddits, so we track only subreddits for which a user had posted at least 10 times. Any subreddits with an average dogmatism score higher than 0.50 we consider to be a user's dogmatic subreddits. We then count all pairs of these dogmatic subreddits. For example, 45 users have politics and technology among their dogmatic subreddits, so we consider politics and technology as linked 45 times. We compute the mutual information BIBREF13 between these links, which gives us a measure of the subreddits that are most related through dogmatism.", so we know that the topics pulled from Reddit are politics, technology, science, business, AskReddit, worldnews, IAmA, atheism, Bitcoin, and aww.
Answer: politics, technology, science, business, AskReddit, worldnews, IAmA, atheism, Bitcoin, and aww
True answer: training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit

Validation ID: a87a009c242d57c51fc94fe312af5e02070f898b
Question: What predictive model do they build?
From the context "We present a statistical model of dogmatism that addresses two complementary goals. First, we validate psychological theories by examining the predictive power of feature sets that guide the model's predictions. For example, do linguistic signals of certainty help to predict a post is dogmatic, as theory would suggest? Second, we apply our model to answer four questions:", so we know that they build a statistical model of dogmatism.
Answer: a statistical model of dogmatism
True answer: logistic regression models

Validation ID: a87a009c242d57c51fc94fe312af5e02070f898b
Question: What predictive model do they build?
From the context "We present a statistical model of dogmatism that addresses two complementary goals. First, we validate psychological theories by examining the predictive power of feature sets that guide the model's predictions. For example, do linguistic signals of certainty help to predict a post is dogmatic, as theory would suggest? Second, we apply our model to answer four questions:", so we know that they build a statistical model of dogmatism.
Answer: a statistical model of dogmatism
True answer: logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.

Validation ID: ef4dba073d24042f24886580ae77add5326f2130
Question: What accuracy does the proposed system achieve?
From the context "The results show that our system outperforms very strong baseline systems", so we know that the proposed system achieves better accuracy than the baseline systems.
From the context "The results show that our system outperforms very strong baseline systems. In summary, we make the following contributions:", so we know that the proposed system achieves better accuracy than the baseline systems.
True answer: F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data 

Validation ID: ef4dba073d24042f24886580ae77add5326f2130
Question: What accuracy does the proposed system achieve?
From the context "The results show that our system outperforms very strong baseline systems", so we know that the proposed system achieves better accuracy than the baseline systems.
From the context "The results show that our system outperforms very strong baseline systems. In summary, we make the following contributions:", so we know that the proposed system achieves better accuracy than the baseline systems.
True answer: F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)

Validation ID: 2df4a045a9cd7b44874340b6fdf9308d3c55327a
Question: What crowdsourcing platform is used?
From the context "In this paper, we propose an approach to training a Chinese NER system on the crowd-annotated data. Our goal is to extract additional annotator independent features by adversarial training, alleviating the annotation noises of non-experts. The idea of adversarial training in neural networks has been used successfully in several NLP tasks, such as cross-lingual POS tagging BIBREF6 and cross-domain POS tagging BIBREF7. They use it to reduce the negative influences of the input divergences among different domains or languages, while we use adversarial training to reduce the negative influences brought by different crowd annotators. To our best knowledge, we are the first to apply adversarial training for crowd annotation learning.", so we know that the crowdsourcing platform is Amazon Mechanical Turk.
Answer: Amazon Mechanical Turk
True answer: Unanswerable

Validation ID: 2df4a045a9cd7b44874340b6fdf9308d3c55327a
Question: What crowdsourcing platform is used?
From the context "In recent years, a series of work have focused on how to use crowdsourcing data efficiently in tasks such as classification BIBREF19, BIBREF20, and compare quality of crowd and expert labels BIBREF21. In sequence labeling tasks, BIBREF22 dredze2009sequence viewed this task as a multi-label problem while BIBREF3 rodrigues2014sequence took workers identities into account by assuming that each sentential word was tagged correctly by one of the crowdsourcing workers and proposed a CRF-based model with multiple annotators. BIBREF4 nguyen2017aggregating introduced a crowd representation in which the crowd vectors were added into the LSTM-CRF model at train time, but ignored them at test time.", so we know that the crowdsourcing platform is Amazon Mechanical Turk.
Answer: Amazon Mechanical Turk
True answer: They did not use any platform, instead they hired undergraduate students to do the annotation.

Validation ID: a313e98994fc039a82aa2447c411dda92c65a470
Question: How do they match words before reordering them?
From the context "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17, and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28. These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29.", so we know that they use CFILT-preorder system for reordering English sentences to match the Indian language word order.
Answer: CFILT-preorder system
True answer: Unanswerable

Validation ID: a313e98994fc039a82aa2447c411dda92c65a470
Question: How do they match words before reordering them?
From the context "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17, and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28. These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29.", so we know that they use CFILT-preorder system for reordering English sentences to match the Indian language word order.
Answer: CFILT-preorder system
True answer: CFILT-preorder system

Validation ID: 37861be6aecd9242c4fdccdfcd06e48f3f1f8f81
Question: On how many language pairs do they show that preordering assisting language sentences helps translation quality?
From the context "We show that pre-ordering the assisting language to match the word order of the source language significantly improves translation quality in an extremely low-resource setting. While the current work focused on Indian languages, we would like to validate the hypothesis on a more diverse set of languages.", so we know that they show that preordering assisting language sentences helps translation quality on Indian languages.
Answer: Indian languages
True answer: 5

Validation ID: 37861be6aecd9242c4fdccdfcd06e48f3f1f8f81
Question: On how many language pairs do they show that preordering assisting language sentences helps translation quality?
From the context "We show that pre-ordering the assisting language to match the word order of the source language significantly improves translation quality in an extremely low-resource setting. While the current work focused on Indian languages, we would like to validate the hypothesis on a more diverse set of languages.", so we know that they show that preordering assisting language sentences helps translation quality on Indian languages.
Answer: Indian languages
True answer: Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.

Validation ID: 7e62a53823aba08bc26b2812db016f5ce6159565
Question: Which dataset(s) do they experiment with?
From the context "For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus BIBREF22 ( INLINEFORM0 sentences from the training set) and the ILCI English-Hindi parallel corpus ( INLINEFORM1 sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus BIBREF23 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use INLINEFORM2 sentences from ILCI corpus as the test set.", so we know that they experiment with IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus.
Answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus
True answer: IITB English-Hindi parallel corpus BIBREF22, ILCI English-Hindi parallel corpus

Validation ID: 7e62a53823aba08bc26b2812db016f5ce6159565
Question: Which dataset(s) do they experiment with?
From the context "For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus BIBREF22 ( INLINEFORM0 sentences from the training set) and the ILCI English-Hindi parallel corpus ( INLINEFORM1 sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus BIBREF23 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use INLINEFORM2 sentences from ILCI corpus as the test set.", so we know that they experiment with IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus.
Answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus
True answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus

Validation ID: 9eabb54c2408dac24f00f92cf1061258c7ea2e1a
Question: Which information about text structure is included in the corpus?
From the context "Information on text structure, typography, and images can also be leveraged as part of a neural machine translation approach to text simplification. A set of parallel documents used in machine translation additionally requires sentence alignments, which are still missing from our corpus. Hence, as a next step, we will include such information using the Customized Alignment for Text Simplification (CATS) tool BIBREF23.", so we know that the information about text structure includes text structure, typography, and images.
Answer: text structure, typography, and images
True answer: paragraphs, lines, Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation

Validation ID: 9eabb54c2408dac24f00f92cf1061258c7ea2e1a
Question: Which information about text structure is included in the corpus?
From the context "Information on text structure, typography, and images can also be leveraged as part of a neural machine translation approach to text simplification. A set of parallel documents used in machine translation additionally requires sentence alignments, which are still missing from our corpus. Hence, as a next step, we will include such information using the Customized Alignment for Text Simplification (CATS) tool BIBREF23.", so we know that the information about text structure includes text structure, typography, and images.
Answer: text structure, typography, and images
True answer: paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))

Validation ID: 3d013f15796ae7fed5272183a166c45f16e24e39
Question: Which information about typography is included in the corpus?
From the context "Information on text structure, typography, and images can also be leveraged as part of a neural machine translation approach to text simplification. A set of parallel documents used in machine translation additionally requires sentence alignments, which are still missing from our corpus. Hence, as a next step, we will include such information using the Customized Alignment for Text Simplification (CATS) tool BIBREF23.", so we know that the information about typography is included in the corpus.
Answer: Information on text structure, typography, and images
True answer: font type, font style, Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page

Validation ID: 3d013f15796ae7fed5272183a166c45f16e24e39
Question: Which information about typography is included in the corpus?
From the context "Information on text structure, typography, and images can also be leveraged as part of a neural machine translation approach to text simplification. A set of parallel documents used in machine translation additionally requires sentence alignments, which are still missing from our corpus. Hence, as a next step, we will include such information using the Customized Alignment for Text Simplification (CATS) tool BIBREF23.", so we know that the information about typography is included in the corpus.
Answer: Information on text structure, typography, and images
True answer: font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer, A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer

Validation ID: 9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc
Question: On which benchmarks they achieve the state of the art?
From the context "We demonstrate that the improved relation detector enables our simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks", so we know that they achieve the state of the art on both single-relation and multi-relation KBQA tasks.
Answer: both single-relation and multi-relation KBQA tasks
True answer: SimpleQuestions, WebQSP

Validation ID: 9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc
Question: On which benchmarks they achieve the state of the art?
From the context "We demonstrate that the improved relation detector enables our simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks", so we know that they achieve the state of the art on both single-relation and multi-relation KBQA tasks.
Answer: both single-relation and multi-relation KBQA tasks
True answer: WebQSP, SimpleQuestions

Validation ID: d3aa0449708cc861a51551b128d73e11d62207d2
Question: What they use in their propsoed framework?
From the context "We use the HR-BiLSTM model to predict the score of each relation $r \in R_e$ : $s_{rel} (r;e,q)$.", so we know that they use HR-BiLSTM model to predict the score of each relation $r \in R_e$ : $s_{rel} (r;e,q)$.
Answer: HR-BiLSTM model
True answer: break the relation names into word sequences,  relation-level and word-level relation representations, bidirectional LSTMs (BiLSTMs),  residual learning method

Validation ID: d3aa0449708cc861a51551b128d73e11d62207d2
Question: What they use in their propsoed framework?
From the context "We use the HR-BiLSTM model to predict the score of each relation $r \in R_e$ : $s_{rel} (r;e,q)$.", so we know that they use HR-BiLSTM model to predict the score of each relation $r \in R_e$ : $s_{rel} (r;e,q)$.
Answer: HR-BiLSTM model
True answer: break the relation names into word sequences for question-relation matching, build both relation-level and word-level relation representations, use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations, residual learning method for sequence matching, a simple KBQA implementation composed of two-step relation detection

Validation ID: cfbec1ef032ac968560a7c76dec70faf1269b27c
Question: What does KBQA abbreviate for?
From the context "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples", so we know that KBQA abbreviates for Knowledge Base Question Answering.
Answer: Knowledge Base Question Answering
True answer: Knowledge Base Question Answering

Validation ID: cfbec1ef032ac968560a7c76dec70faf1269b27c
Question: What does KBQA abbreviate for?
From the context "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples", so we know that KBQA abbreviates for Knowledge Base Question Answering.
Answer: Knowledge Base Question Answering
True answer: Knowledge Base Question Answering 

Validation ID: c0e341c4d2253eb42c8840381b082aae274eddad
Question: What is te core component for KBQA?
From the context "This paper improves KB relation detection to cope with the problems mentioned above. First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching.", so we know that the core component for KBQA is relation detection.
Answer: relation detection
True answer: answer questions by obtaining information from KB tuples 

Validation ID: c0e341c4d2253eb42c8840381b082aae274eddad
Question: What is te core component for KBQA?
From the context "This paper improves KB relation detection to cope with the problems mentioned above. First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching.", so we know that the core component for KBQA is relation detection.
Answer: relation detection
True answer: hierarchical matching between questions and relations with residual learning

Validation ID: 1ec152119cf756b16191b236c85522afeed11f59
Question: What experiments are proposed to test that upper layers produce context-specific embeddings?
From the context "As word representations in a sentence become more context-specific in upper layers, they drift away from one another, although there are exceptions (see layer 12 in Figure FIGREF25). However, in all layers, the average similarity between words in the same sentence is still greater than the", so we know that the experiments are proposed to test that upper layers produce context-specific embeddings are that upper layers produce context-specific embeddings.
Answer: upper layers produce context-specific embeddings
True answer: They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.

Validation ID: 1ec152119cf756b16191b236c85522afeed11f59
Question: What experiments are proposed to test that upper layers produce context-specific embeddings?
From the context "As word representations in a sentence become more context-specific in upper layers, they drift away from one another, although there are exceptions (see layer 12 in Figure FIGREF25). However, in all layers, the average similarity between words in the same sentence is still greater than the", so we know that the experiments are proposed to test that upper layers produce context-specific embeddings are that upper layers produce context-specific embeddings.
Answer: upper layers produce context-specific embeddings
True answer: They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  
They plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.
They plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another.

Validation ID: 891c2001d6baaaf0da4e65b647402acac621a7d2
Question: How do they calculate a static embedding for each word?
From the context "The closer $\textit {MEV}_\ell (w)$ is to 0, the poorer a replacement a static embedding would be; if $\textit {MEV}_\ell (w) = 1$, then a static embedding would be a perfect replacement for the contextualized representations. Still, static embeddings created by taking the first principal component of a word's contextualized representations outperform GloVe and FastText embeddings on many word vector benchmarks.", so we know that static embeddings created by taking the first principal component of a word's contextualized representations outperform GloVe and FastText embeddings on many word vector benchmarks.
Answer: taking the first principal component of a word's contextualized representations
True answer: They use the first principal component of a word's contextualized representation in a given layer as its static embedding.

Validation ID: 891c2001d6baaaf0da4e65b647402acac621a7d2
Question: How do they calculate a static embedding for each word?
From the context "The closer $\textit {MEV}_\ell (w)$ is to 0, the poorer a replacement a static embedding would be; if $\textit {MEV}_\ell (w) = 1$, then a static embedding would be a perfect replacement for the contextualized representations. Still, static embeddings created by taking the first principal component of a word's contextualized representations outperform GloVe and FastText embeddings on many word vector benchmarks.", so we know that static embeddings created by taking the first principal component of a word's contextualized representations outperform GloVe and FastText embeddings on many word vector benchmarks.
Answer: taking the first principal component of a word's contextualized representations
True answer:  by taking the first principal component (PC) of its contextualized representations in a given layer

Validation ID: 66c96c297c2cffdf5013bab5e95b59101cb38655
Question: What is the performance of BERT on the task?
From the context "Regarding the detection scenario –that is, the scenario concerned with a binary classification to determine whether each individual token conveys sensitive information or not–, it can be observed that BERT outperforms its competitors. A fact worth highlighting is that, according to these results, BERT achieves a precision lower than the rest of the systems (i.e., it makes more false positive predictions); in exchange, it obtains a remarkably higher recall. Noticeably, it reaches a recall of 0.988, which is almost twice the recall of the second best system (0.532)", so we know that the performance of BERT on the task is recall of 0.988.
Answer: recall of 0.988
True answer: F1 scores are:
HUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)
Medoccan: Detection(0.972), Classification (0.967)

Validation ID: 66c96c297c2cffdf5013bab5e95b59101cb38655
Question: What is the performance of BERT on the task?
From the context "Regarding the detection scenario –that is, the scenario concerned with a binary classification to determine whether each individual token conveys sensitive information or not–, it can be observed that BERT outperforms its competitors. A fact worth highlighting is that, according to these results, BERT achieves a precision lower than the rest of the systems (i.e., it makes more false positive predictions); in exchange, it obtains a remarkably higher recall. Noticeably, it reaches a recall of 0.988, which is 0.07 points higher than the best result obtained by the other systems", so we know that the performance of BERT on the task is recall of 0.988.
Answer: recall of 0.988
True answer: BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table 

Validation ID: 6b53e1f46ae4ba9b75117fc6e593abded89366be
Question: What are the other algorithms tested?
From the context "Regarding the detection scenario –that is, the scenario concerned with a binary classification to determine whether each individual token conveys sensitive information or not–, it can be observed that BERT outperforms its competitors. A fact worth highlighting is that, according to these results, BERT achieves a precision lower than the rest of the systems (i.e., it makes more false positive predictions); in exchange, it obtains a remarkably higher recall. Noticeably, it reaches a recall of 0.988", so we know that the other algorithms tested are CRF classifier, spaCy entity recogniser and NLNDE.
Answer: CRF classifier, spaCy entity recogniser and NLNDE
True answer: NER model, CRF classifier trained with sklearn-crfsuite, classifier has been developed that consists of regular-expressions and dictionary look-up

Validation ID: 6b53e1f46ae4ba9b75117fc6e593abded89366be
Question: What are the other algorithms tested?
From the context "Regarding the detection scenario –that is, the scenario concerned with a binary classification to determine whether each individual token conveys sensitive information or not–, it can be observed that BERT outperforms its competitors. A fact worth highlighting is that, according to these results, BERT achieves a precision lower than the rest of the systems (i.e., it makes more false positive predictions); in exchange, it obtains a remarkably higher recall. Noticeably, it reaches a recall of 0.988", so we know that the other algorithms tested are CRF classifier, spaCy entity recogniser and NLNDE.
Answer: CRF classifier, spaCy entity recogniser and NLNDE
True answer: As the simplest baseline, a sensitive data recogniser and classifier, Conditional Random Fields (CRF), spaCy 

Validation ID: c0bee6539eb6956a7347daa9d2419b367bd02064
Question: Does BERT reach the best performance among all the algorithms compared?
From the context "Regarding the detection scenario –that is, the scenario concerned with a binary classification to determine whether each individual token conveys sensitive information or not–, it can be observed that BERT outperforms its competitors. A fact worth highlighting is that, according to these results, BERT achieves a precision lower than the rest of the systems (i.e., it makes more false positive predictions); in exchange, it obtains a remarkably higher recall. Noticeably, it", so we know that BERT reaches the best performance among all the algorithms compared.
Answer: Yes
True answer: No

Validation ID: c0bee6539eb6956a7347daa9d2419b367bd02064
Question: Does BERT reach the best performance among all the algorithms compared?
From the context "Regarding the detection scenario –that is, the scenario concerned with a binary classification to determine whether each individual token conveys sensitive information or not–, it can be observed that BERT outperforms its competitors. A fact worth highlighting is that, according to these results, BERT achieves a precision lower than the rest of the systems (i.e., it makes more false positive predictions); in exchange, it obtains a remarkably higher recall. Noticeably, it", so we know that BERT reaches the best performance among all the algorithms compared.
Answer: Yes
True answer: No

Validation ID: 3de0487276bb5961586acc6e9f82934ef8cb668c
Question: What are the clinical datasets used in the paper?
From the context "Two datasets are exploited in this article. Both datasets consist of plain text containing clinical narrative written in Spanish, and their respective manual annotations of sensitive information in BRAT BIBREF13 standoff format. In order to feed the data to the different algorithms presented in Section SECREF7, these datasets were transformed to comply with the commonly used BIO sequence representation scheme BIBREF14.", so we know that the clinical datasets used in the paper are NUBes-PHI and MEDDOCAN.
Answer: NUBes-PHI and MEDDOCAN
True answer: MEDDOCAN, NUBes-PHI

Validation ID: 3de0487276bb5961586acc6e9f82934ef8cb668c
Question: What are the clinical datasets used in the paper?
From the context "Two datasets are exploited in this article. Both datasets consist of plain text containing clinical narrative written in Spanish, and their respective manual annotations of sensitive information in BRAT BIBREF13 standoff format. In order to feed the data to the different algorithms presented in Section SECREF7, these datasets were transformed to comply with the commonly used BIO sequence representation scheme BIBREF14.", so we know that the clinical datasets used in the paper are NUBes-PHI and MEDDOCAN.
Answer: NUBes-PHI and MEDDOCAN
True answer: MEDDOCAN, NUBes 

Validation ID: 113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab
Question: how is model compactness measured?
From the context "In contrast to the language embeddings, the phoneme embeddings appear to show many regularities (see Table TABREF33 ). This is a sign that our multilingual model learns similar embeddings for phonemes that are written with the same grapheme in different languages. These phonemes tend to be phonetically similar to each other.", so we know that the model compactness is measured by the phoneme embeddings.
Answer: phoneme embeddings
True answer: Using file size on disk

Validation ID: 113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab
Question: how is model compactness measured?
From the context "In contrast to the language embeddings, the phoneme embeddings appear to show many regularities (see Table TABREF33 ). This is a sign that our multilingual model learns similar embeddings for phonemes that are written with the same grapheme in different languages. These phonemes tend to be phonetically similar to each other.", so we know that the model compactness is measured by the phoneme embeddings.
Answer: phoneme embeddings
True answer: 15.4 MB

Validation ID: 0752d71a0a1f73b3482a888313622ce9e9870d6e
Question: what was the baseline?
From the context "Results on LangID and NoLangID are compared to the system presented by deri2016grapheme, which is identified in our results as wFST. Their results can be divided into two parts:", so we know that the baseline is wFST.
Answer: wFST
True answer: system presented by deri2016grapheme

Validation ID: 0752d71a0a1f73b3482a888313622ce9e9870d6e
Question: what was the baseline?
From the context "Results on LangID and NoLangID are compared to the system presented by deri2016grapheme, which is identified in our results as wFST. Their results can be divided into two parts:", so we know that the baseline is wFST.
Answer: wFST
True answer: wFST

Validation ID: 55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3
Question: what evaluation metrics were used?
From the context "We use the following three evaluation metrics: Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences. Word Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence. Word Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system.", so we know that the evaluation metrics are PER, WER and WER 100.
Answer: PER, WER and WER 100
True answer: Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)

Validation ID: 55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3
Question: what evaluation metrics were used?
From the context "We use the following three evaluation metrics: Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences. Word Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence. Word Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system.", so we know that the evaluation metrics are PER, WER and WER 100.
Answer: PER, WER and WER 100
True answer: PER, WER, WER 100

Validation ID: 4eaf9787f51cd7cdc45eb85cf223d752328c6ee4
Question: what datasets did they use?
From the context "In order to train a neural g2p system, one needs a large quantity of pronunciation data. A standard dataset for g2p is the Carnegie Mellon Pronouncing Dictionary BIBREF12. However, that is a monolingual English resource, so it is unsuitable for our multilingual task. Instead, we use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10.", so we know that they use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments.
Answer: the multilingual pronunciation corpus collected by deri2016grapheme
True answer: the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary

Validation ID: 4eaf9787f51cd7cdc45eb85cf223d752328c6ee4
Question: what datasets did they use?
From the context "In order to train a neural g2p system, one needs a large quantity of pronunciation data. A standard dataset for g2p is the Carnegie Mellon Pronouncing Dictionary BIBREF12. However, that is a monolingual English resource, so it is unsuitable for our multilingual task. Instead, we use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10.", so we know that they use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments.
Answer: the multilingual pronunciation corpus collected by deri2016grapheme
True answer: multilingual pronunciation corpus collected by deri2016grapheme

Validation ID: fb2b536dc8e442dffab408db992b971e86548158
Question: What is the interannotator agreement for the human evaluation?
From the context "For both courses, the majority of selected summaries were produced by the tuned model (49% for CS and 41% for Stat2015), compared to (31% for CS and 29% for Stat2015) for the model trained on CNN/DM, and (20% for CS and 30% for Stat2015) for the model trained on student reflections", so we know that the interannotator agreement for the human evaluation is 49% for CS and 41% for Stat2015.
Answer: 49% for CS and 41% for Stat2015
True answer: Unanswerable

Validation ID: fb2b536dc8e442dffab408db992b971e86548158
Question: What is the interannotator agreement for the human evaluation?
From the context "For both courses, the majority of selected summaries were produced by the tuned model (49% for CS and 41% for Stat2015), compared to (31% for CS and 29% for Stat2015) for the model trained on CNN/DM, and (20% for CS and 30% for Stat2015) for the model trained on student reflections", so we know that the interannotator agreement for the human evaluation is 49% for CS and 41% for Stat2015.
Answer: 49% for CS and 41% for Stat2015
True answer: Unanswerable

Validation ID: 31735ec3d83c40b79d11df5c34154849aeb3fb47
Question: Who were the human evaluators used?
From the context "20 evaluators were recruited from our institution and asked to each perform 20 annotations", so we know that the human evaluators were recruited from our institution.
Answer: recruited from our institution
True answer: 20 evaluators were recruited from our institution and asked to each perform 20 annotations

Validation ID: 31735ec3d83c40b79d11df5c34154849aeb3fb47
Question: Who were the human evaluators used?
From the context "20 evaluators were recruited from our institution and asked to each perform 20 annotations", so we know that the human evaluators were recruited from our institution.
Answer: recruited from our institution
True answer: 20 annotatos from author's institution

Validation ID: 10d450960907091f13e0be55f40bcb96f44dd074
Question: Is the template-based model realistic? ?
From the context "While the goal of our template model was to synthesize data, using it for summarization is surprisingly competitive, supporting H6. We believe that training the model with little data is doable due to the small number of parameters (logistic regression classifier only). While rows 12 and 24 are never the best results, they are close to the best involving tuning. This encourages us to enhance our template model and explore templates not so tailored to our data.", so we know that the template-based model is realistic.
Answer: Yes
True answer: Yes

Validation ID: 10d450960907091f13e0be55f40bcb96f44dd074
Question: Is the template-based model realistic? ?
From the context "While the goal of our template model was to synthesize data, using it for summarization is surprisingly competitive, supporting H6. We believe that training the model with little data is doable due to the small number of parameters (logistic regression classifier only). While rows 12 and 24 are never the best results, they are close to the best involving tuning. This encourages us to enhance our template model and explore templates not so tailored to our data.", so we know that the template-based model is realistic.
Answer: Yes
True answer: Yes

Validation ID: b5608076d91450b0d295ad14c3e3a90d7e168d0e
Question: Is the student reflection data very different from the newspaper data? ?
From the context "To overcome the size issue of the student reflection dataset, we first explore the effect of incorporating domain transfer into a recent abstractive summarization model: pointer networks with coverage mechanism (PG-net)BIBREF0. To experiment with domain transfer, the model was pretrained using the CNN/DM dataset, then fine tuned using the student reflection dataset (see the Experiments section). A second approach we explore to overcome the lack of reflection data is data synthesis. We first propose a template model for synthesizing new data, then investigate the performance impact of using this data when training the summarization model. The proposed model makes use of the nature of datasets such as ours, where the reference summaries tend to be close in structure: humans try to find the major points that students raise, then present the points in a way that marks their relative importance (recall the CS example in Table TABREF4). Our third explored approach is to combine domain transfer with data synthesis. Evaluations of neural abstractive summarization method across four student reflection corpora show the utility of all three methods.", so we know that the student reflection data is very different from the newspaper data.
Answer: Yes
True answer: Yes

Validation ID: b5608076d91450b0d295ad14c3e3a90d7e168d0e
Question: Is the student reflection data very different from the newspaper data? ?
From the context "To overcome the size issue of the student reflection dataset, we first explore the effect of incorporating domain transfer into a recent abstractive summarization model: pointer networks with coverage mechanism (PG-net)BIBREF0. To experiment with domain transfer, the model was pretrained using the CNN/DM dataset, then fine tuned using the student reflection dataset (see the Experiments section). A second approach we explore to overcome the lack of reflection data is data synthesis. We first propose a template model for synthesizing new data, then investigate the performance impact of using this data when training the summarization model. The proposed model makes use of the nature of datasets such as ours, where the reference summaries tend to be close in structure: humans try to find the major points that students raise, then present the points in a way that marks their relative importance (recall the CS example in Table TABREF4). Our third explored approach is to combine domain transfer with data synthesis. Evaluations of neural abstractive summarization method across four student reflection corpora show the utility of all three methods.", so we know that the student reflection data is very different from the newspaper data.
Answer: Yes
True answer: Yes

Validation ID: c21b87c97d1afac85ece2450ee76d01c946de668
Question: What is the recent abstractive summarization method in this paper?
From the context "Recently, with the emergence of neural seq2seq models, abstractive summarization methods have seen great performance strides BIBREF0, BIBREF1, BIBREF2. However, complex neural summarization models with thousands of parameters usually require a large amount of training data. In fact, much of the neural summarization work has been trained and tested in news domains where numerous large datasets exist. For example, the CNN/DailyMail (CNN/DM) BIBREF3, BIBREF4 and New York Times (NYT) datasets are in the magnitude of 300k and 700k documents, respectively. In contrast, in other domains such as student reflections, summarization datasets are only in the magnitude of tens or hundreds of documents (e.g., BIBREF5). We hypothesize that training complex neural abstractive summarization models in such domains will not yield good performing models, and we will indeed later show that this is the case for student reflections.", so we know that the recent abstractive summarization method in this paper is neural seq2seq models.
Answer: neural seq2seq models
True answer: pointer networks with coverage mechanism (PG-net)

Validation ID: c21b87c97d1afac85ece2450ee76d01c946de668
Question: What is the recent abstractive summarization method in this paper?
From the context "Recently, with the emergence of neural seq2seq models, abstractive summarization methods have seen great performance strides BIBREF0, BIBREF1, BIBREF2. However, complex neural summarization models with thousands of parameters usually require a large amount of training data. In fact, much of the neural summarization work has been trained and tested in news domains where numerous large datasets exist. For example, the CNN/DailyMail (CNN/DM) BIBREF3, BIBREF4 and New York Times (NYT) datasets are in the magnitude of 300k and 700k documents, respectively. In contrast, in other domains such as student reflections, summarization datasets are only in the magnitude of tens or hundreds of documents (e.g., BIBREF5). We hypothesize that training complex neural abstractive summarization models in such domains will not yield good performing models, and we will indeed later show that this is the case for student reflections.", so we know that the recent abstractive summarization method in this paper is neural seq2seq models.
Answer: neural seq2seq models
True answer:  pointer networks with coverage mechanism (PG-net)BIBREF0

Validation ID: d087539e6a38c42f0a521ff2173ef42c0733878e
Question: Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models? ?
From the context "Since we seek to train a general-purpose student language model, we elect to reuse the teacher model's original training objective to optimize the student model, i.e., masked language modeling and next sentence prediction, before any fine-tuning. In the former task, words in context are randomly masked, and the language model needs to predict those words given the masked context. In the latter task, given a pair of sentences, the language model predicts whether the pair is consistent. However, since the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.", so we know that the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.
Answer: the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.
True answer: While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.

Validation ID: d087539e6a38c42f0a521ff2173ef42c0733878e
Question: Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models? ?
From the context "Since we seek to train a general-purpose student language model, we elect to reuse the teacher model's original training objective to optimize the student model, i.e., masked language modeling and next sentence prediction, before any fine-tuning. In the former task, words in context are randomly masked, and the language model needs to predict those words given the masked context. In the latter task, given a pair of sentences, the language model predicts whether the pair is consistent. However, since the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.", so we know that the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.
Answer: the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.
True answer: distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.

Validation ID: efe9bad55107a6be7704ed97ecce948a8ca7b1d2
Question: What state-of-the-art compression techniques were used in the comparison?
From the context "We present two novel ideas to improve the effectiveness of knowledge distillation, in particular for BERT, with the focus on bringing down model sizes to as much as a few mega-bytes. Our model is among the first to propose to use a significantly smaller vocabulary for the student model learned during distillation. In addition, instead of distilling solely on the teacher model's final-layer outputs, our model leverages layer-wise teacher model parameters to directly optimize the parameters of the corresponding layers in the student model. Specifically, our contributions are:", so we know that the state-of-the-art compression techniques used in the comparison are:

True answer: baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)

Validation ID: efe9bad55107a6be7704ed97ecce948a8ca7b1d2
Question: What state-of-the-art compression techniques were used in the comparison?
From the context "We present two novel ideas to improve the effectiveness of knowledge distillation, in particular for BERT, with the focus on bringing down model sizes to as much as a few mega-bytes. Our model is among the first to propose to use a significantly smaller vocabulary for the student model learned during distillation. In addition, instead of distilling solely on the teacher model's final-layer outputs, our model leverages layer-wise teacher model parameters to directly optimize the parameters of the corresponding layers in the student model. Specifically, our contributions are:", so we know that the state-of-the-art compression techniques used in the comparison are:

True answer: NoKD, PKD, BERTBASE teacher model

Validation ID: 71e4ba4e87e6596aeca187127c0d088df6570c57
Question: What evaluations methods do they take?
From the context "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language.", so we know that they take two methods to detect stereotype-driven descriptions.
Answer: two methods
True answer: Unanswerable

Validation ID: 71e4ba4e87e6596aeca187127c0d088df6570c57
Question: What evaluations methods do they take?
From the context "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language.", so we know that they take two methods to detect stereotype-driven descriptions.
Answer: two methods
True answer: Unanswerable

Validation ID: 7561a968470a8936d10e1ba722d2f38b5a9a4d38
Question: What is the size of the dataset?
From the context "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each", so we know that the size of the dataset is over 30,000 images with 5 crowdsourced descriptions each.
Answer: over 30,000 images with 5 crowdsourced descriptions each
True answer: 30,000

Validation ID: 7561a968470a8936d10e1ba722d2f38b5a9a4d38
Question: What is the size of the dataset?
From the context "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each", so we know that the size of the dataset is over 30,000 images with 5 crowdsourced descriptions each.
Answer: over 30,000 images with 5 crowdsourced descriptions each
True answer: collection of over 30,000 images with 5 crowdsourced descriptions each

Validation ID: 6d4400f45bd97b812e946b8a682b018826e841f1
Question: Which methods are considered to find examples of biases and unwarranted inferences??
From the context "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language.", so we know that the methods are to suppress or to emphasize biases in human language.
Answer: to suppress or to emphasize biases in human language
True answer: spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering

Validation ID: 6d4400f45bd97b812e946b8a682b018826e841f1
Question: Which methods are considered to find examples of biases and unwarranted inferences??
From the context "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language.", so we know that the methods are to suppress or to emphasize biases in human language.
Answer: to suppress or to emphasize biases in human language
True answer: Looking for adjectives marking the noun "baby" and also looking for most-common adjectives related to certain nouns using POS-tagging

Validation ID: 26c2e1eb12143d985e4fb50543cf0d1eb4395e67
Question: What biases are found in the dataset?
From the context "I have divided these descriptions into two classes: linguistic bias and unwarranted inferences. The former corresponds to the annotators' choice of words when confronted with an image that may or may not match their stereotypical expectancies. The latter corresponds to the tendency of annotators to go beyond what the physical data can tell us, and expand their descriptions based on their past experiences and knowledge of the world.", so we know that the biases found in the dataset are linguistic bias and unwarranted inferences.
Answer: linguistic bias and unwarranted inferences
True answer: Ethnic bias

Validation ID: 26c2e1eb12143d985e4fb50543cf0d1eb4395e67
Question: What biases are found in the dataset?
From the context "I have divided these descriptions into two classes: linguistic bias and unwarranted inferences. The former corresponds to the annotators' choice of words when confronted with an image that may or may not match their stereotypical expectancies. The latter corresponds to the tendency of annotators to go beyond what the physical data can tell us, and expand their descriptions based on their past experiences and knowledge of the world.", so we know that the biases found in the dataset are linguistic bias and unwarranted inferences.
Answer: linguistic bias and unwarranted inferences
True answer: adjectives are used to create “more narrow labels [or subtypes] for individuals who do not fit with general social category expectations”

Validation ID: f17ca24b135f9fe6bb25dc5084b13e1637ec7744
Question: What discourse relations does it work best/worst for?
From the context "The third section of table 3 shows the multi-class classification results of our proposed paragraph-level neural network models that capture inter-dependencies among discourse units. The first row shows the performance of a variant of our basic model, where we only identify implicit relations and ignore identifying explicit relations by setting the $\alpha $ in equation (5) to be 0", so we know that the discourse relations it works best for is implicit relations.
Answer: implicit relations
True answer: explicit discourse relations

Validation ID: f17ca24b135f9fe6bb25dc5084b13e1637ec7744
Question: What discourse relations does it work best/worst for?
From the context "The third section of table 3 shows the multi-class classification results of our proposed paragraph-level neural network models that capture inter-dependencies among discourse units. The first row shows the performance of a variant of our basic model, where we only identify implicit relations and ignore identifying explicit relations by setting the $\alpha $ in equation (5) to be 0", so we know that the discourse relations it works best for is implicit relations.
Answer: implicit relations
True answer: Best: Expansion (Exp). Worst: Comparison (Comp).

Validation ID: bd5bd1765362c2d972a762ca12675108754aa437
Question: How much does this model improve state-of-the-art?
From the context "The third section of table 3 shows the multi-class classification results of our proposed paragraph-level neural network models that capture inter-dependencies among discourse units. The first row shows the performance of a variant of our basic model, where we only identify implicit relations and ignore identifying explicit relations by setting the $\alpha $ in equation (5) to 0. The second row shows the performance of our basic model. The third row shows the performance of our basic model with a CRF layer on top. The fourth row shows the performance of our basic model with a CRF layer on top and untied parameters in the softmax prediction layer. The last row shows the performance of our basic model with a CRF layer on top and untied parameters in the softmax prediction layer, and with the additional linguistic features (POS and NER tags) concatenated to word embeddings. The last column shows the performance of the best system reported in previous work BIBREF6. The results show that our proposed paragraph-level neural network models significantly improve the state-of-the-art performance on implicit discourse relation prediction.", so we know that this model improves state-of-the-art.
Answer: significantly
True answer: the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 )., full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent., Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. 

Validation ID: bd5bd1765362c2d972a762ca12675108754aa437
Question: How much does this model improve state-of-the-art?
From the context "The third section of table 3 shows the multi-class classification results of our proposed paragraph-level neural network models that capture inter-dependencies among discourse units. The first row shows the performance of a variant of our basic model, where we only identify implicit relations and ignore identifying explicit relations by setting the $\alpha $ in equation (5) to 0. The second row shows the performance of our basic model. The third row shows the performance of our basic model with a CRF layer on top. The fourth row shows the performance of our basic model with a CRF layer on top and untied parameters in the softmax prediction layer. The last row shows the performance of our basic model with a CRF layer on top and untied parameters in the softmax prediction layer, and with the additional linguistic features (POS and NER tags) concatenated to word embeddings. The last column shows the performance of the best system reported in previous work BIBREF18. The results show that our proposed paragraph-level neural network models significantly improve the state-of-the-art performance on implicit discourse relation prediction.", so we know that this model improves state-of-the-art.
Answer: significantly
True answer: 1 percent

Validation ID: d9b6c61fc6d29ad399d27b931b6cb7b1117b314a
Question: Where is a question generation model used?
From the context "We implement a generation-based model. The motivation to design this model is that we want to associate natural language phrases with knowledge based representation. It takes semantics of a candidate answer as the input and generates a question $\hat{q}$. Then a paraphrasing model gives a score $f_{qg}(q,\hat{q})$, which is computed between the generated question $\hat{q}$ and the original question $q$, as the ranking score.", so we know that a question generation model is used to generate a question $\hat{q}$ based on the semantics of a candidate answer.
Answer: generate a question $\hat{q}$ based on the semantics of a candidate answer
True answer: The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. 

Validation ID: d9b6c61fc6d29ad399d27b931b6cb7b1117b314a
Question: Where is a question generation model used?
From the context "We implement a generation-based model. The motivation to design this model is that we want to associate natural language phrases with knowledge based representation. It takes semantics of a candidate answer as the input and generates a question $\hat{q}$. Then a paraphrasing model gives a score $f_{qg}(q,\hat{q})$, which is computed between the generated question $\hat{q}$ and the original question $q$, as the ranking score.", so we know that a question generation model is used to generate a question $\hat{q}$ based on the semantics of a candidate answer.
Answer: generate a question $\hat{q}$ based on the semantics of a candidate answer
True answer: framework consisting of both a question answering model and a question generation model

Validation ID: d27438b11bc70e706431dda0af2b1c0b0d209f96
Question: Were any of these tasks evaluated in any previous work?
From the context "Tables 1, 2 and 3 show the results. All cases exhibit high scores—in the vast majority of the cases substantially higher than reported in previous work. As discussed above, the results are not directly comparable to previous work: the BERT models are trained on different (and larger) data, are allowed to access the suffix of the sentence in addition to its prefix, and are evaluated on somewhat different data due to discarding OOV items. Still, taken together, the high performance numbers indicate that the purely attention-based BERT models are likely capable of capturing the same kind of syntactic regularities that LSTM-based models are capable of capturing, at least as well as the LSTM models and probably better.", so we know that the BERT models are trained on different (and larger) data, are allowed to access the suffix of the sentence in addition to its prefix, and are evaluated on somewhat different data due to discarding OOV items.
Answer: the BERT models are trained on different (and larger) data, are allowed to access the suffix of the sentence in addition to its prefix, and are evaluated on somewhat different data due to discarding OOV items
True answer: Yes

Validation ID: d27438b11bc70e706431dda0af2b1c0b0d209f96
Question: Were any of these tasks evaluated in any previous work?
From the context "Tables 1, 2 and 3 show the results. All cases exhibit high scores—in the vast majority of the cases substantially higher than reported in previous work. As discussed above, the results are not directly comparable to previous work: the BERT models are trained on different (and larger) data, are allowed to access the suffix of the sentence in addition to its prefix, and are evaluated on somewhat different data due to discarding OOV items. Still, taken together, the high performance numbers indicate that the purely attention-based BERT models are likely capable of capturing the same kind of syntactic regularities that LSTM-based models are capable of capturing, at least as well as the LSTM models and probably better.", so we know that the BERT models are trained on different (and larger) data, are allowed to access the suffix of the sentence in addition to its prefix, and are evaluated on somewhat different data due to discarding OOV items.
Answer: the BERT models are trained on different (and larger) data, are allowed to access the suffix of the sentence in addition to its prefix, and are evaluated on somewhat different data due to discarding OOV items
True answer: Yes

Validation ID: 8d4ac4afbf5b14f412171729ceb5e822afcfa3f4
Question: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?
From the context "We also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.", so we know that they build a model to automatically detect demographic, lingustic or psycological dimensons of people.
Answer: Yes
True answer: No

Validation ID: 8d4ac4afbf5b14f412171729ceb5e822afcfa3f4
Question: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?
From the context "We also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.", so we know that they build a model to automatically detect demographic, lingustic or psycological dimensons of people.
Answer: Yes
True answer: No

Validation ID: 3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f
Question: Which demographic dimensions of people do they obtain?
From the context "We also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.", so we know that they obtain gender and location information.
Answer: gender and location information
True answer: occupation, industry, profile information, language use, gender 

Validation ID: 3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f
Question: Which demographic dimensions of people do they obtain?
From the context "We also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.", so we know that they obtain gender and location information.
Answer: gender and location information
True answer: density of users, gender distribution

Validation ID: 07d15501a599bae7eb4a9ead63e9df3d55b3dc35
Question: How do they obtain psychological dimensions of people?
From the context "We also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10. MEM is a topic modeling approach applied to a corpus of texts created by hundreds of survey respondents from the U.S. who were asked to freely write about their personal values.", so we know that they obtain psychological dimensions of people by using the Meaning Extraction Method (MEM) BIBREF10.
Answer: Meaning Extraction Method (MEM) BIBREF10
True answer: using the Meaning Extraction Method

Validation ID: 07d15501a599bae7eb4a9ead63e9df3d55b3dc35
Question: How do they obtain psychological dimensions of people?
From the context "We also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10. MEM is a topic modeling approach applied to a corpus of texts created by hundreds of survey respondents from the U.S. who were asked to freely write about their personal values.", so we know that they obtain psychological dimensions of people by using the Meaning Extraction Method (MEM) BIBREF10.
Answer: Meaning Extraction Method (MEM) BIBREF10
True answer: Unanswerable

Validation ID: 99e78c390932594bd833be0f5c890af5c605d808
Question: What is the baseline?
From the context "We frame the Medication Regimen extraction problem as a Question Answering (QA) task, which forms the basis for our first approach. It can also be considered as a specific inference or relation extract task, since we extract specific information about an entity (Medication Name), hence our second approach is at the intersection of Question Answering (QA) and Information Extraction (IE) domains. Both the approaches involve using a contiguous segment of the transcript and the Medication Name as input, to find/infer the medication's Dosage and Frequency.", so we know that the baseline is the first approach.
Answer: the first approach
True answer: QA PGNet, Multi-decoder QA PGNet with lookup table embedding

Validation ID: 99e78c390932594bd833be0f5c890af5c605d808
Question: What is the baseline?
From the context "We frame the Medication Regimen extraction problem as a Question Answering (QA) task, which forms the basis for our first approach. It can also be considered as a specific inference or relation extract task, since we extract specific information about an entity (Medication Name), hence our second approach is at the intersection of Question Answering (QA) and Information Extraction (IE) domains. Both the approaches involve using a contiguous segment of the transcript and the Medication Name as input, to find/infer the medication's Dosage and Frequency.", so we know that the baseline is the first approach.
Answer: the first approach
True answer: QA PGNet and Multi-decoder QA PGNet

Validation ID: 861187338c5ad445b9acddba8f2c7688785667b1
Question: Is the data de-identified?
From the context "The transcript of the conversations are annotated with summaries and Medication Regimen tags (MR tags), both grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators, refer to Table TABREF1. The transcript for a typical conversation can be quite long, and not easy for many of the high performing deep learning models to act on. Moreover, the medical information about a concept/condition/entity can change during the conversation after a significant time gap. For example, dosage of a medication can be different when discussing current medication the patient is on vs when they are prescribed a different dosage. Hence, we have annotations, that are grounded to a short segment of the transcript", so we know that the data is de-identified.
Answer: Yes
True answer: Yes

Validation ID: 861187338c5ad445b9acddba8f2c7688785667b1
Question: Is the data de-identified?
From the context "The transcript of the conversations are annotated with summaries and Medication Regimen tags (MR tags), both grounded using the timestamps of the sentences from the transcript deemed relevant by the expert annotators, refer to Table TABREF1. The transcript for a typical conversation can be quite long, and not easy for many of the high performing deep learning models to act on. Moreover, the medical information about a concept/condition/entity can change during the conversation after a significant time gap. For example, dosage of a medication can be different when discussing current medication the patient is on vs when they are prescribed a different dosage. Hence, we have annotations, that are grounded to a short segment of the transcript", so we know that the data is de-identified.
Answer: Yes
True answer: Yes

Validation ID: f161e6d5aecf8fae3a26374dcb3e4e1b40530c95
Question: What embeddings are used?
From the context "We derive embeddings from ELMo by learning a linear combination of its last three layer's hidden states (task-specific fine-tuning BIBREF11). Similarly, for BERT-based embeddings, we take a linear combination of the hidden states from its last four layers, as this combination performs best without increasing the size of the embeddings BIBREF16. Since BERT and ClinicalBERT use word-piece vocabulary and computes sub-word embeddings, we compute word-level embedding by averaging the corresponding sub-word tokens. ELMo and BERT embeddings both have 1024 dimensions, ClinicalBERT have 768 as it is based on BERT base model, and the lookup table have 128 – higher dimension models leads to overfitting.", so we know that ELMo, BERT and ClinicalBERT embeddings are used.
Answer: ELMo, BERT and ClinicalBERT embeddings
True answer:  simple lookup table embeddings learned from scratch, using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13

Validation ID: f161e6d5aecf8fae3a26374dcb3e4e1b40530c95
Question: What embeddings are used?
From the context "We derive embeddings from ELMo by learning a linear combination of its last three layer's hidden states (task-specific fine-tuning BIBREF11). Similarly, for BERT-based embeddings, we take a linear combination of the hidden states from its last four layers, as this combination performs best without increasing the size of the embeddings BIBREF16. Since BERT and ClinicalBERT use word-piece vocabulary and computes sub-word embeddings, we compute word-level embedding by averaging the corresponding sub-word tokens. ELMo and BERT embeddings both have 1024 dimensions, ClinicalBERT have 768 as it is based on BERT base model, and the lookup table have 128 – higher dimension models leads to overfitting.", so we know that ELMo, BERT and ClinicalBERT embeddings are used.
Answer: ELMo, BERT and ClinicalBERT embeddings
True answer: ELMO BIBREF11, BERT BIBREF12 and ClinicalBERT BIBREF13

Validation ID: 12c50dea84f9a8845795fa8b8c1679328bd66246
Question: What datasets did they use for evaluation?
From the context "We evaluated our experiments on two classification tasks - customer satisfaction prediction and topic identification - using 3 datasets: CSAT, 20newsgroups and Fisher", so we know that they used 3 datasets: CSAT, 20newsgroups and Fisher for evaluation.
Answer: CSAT, 20newsgroups and Fisher
True answer: CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus

Validation ID: 12c50dea84f9a8845795fa8b8c1679328bd66246
Question: What datasets did they use for evaluation?
From the context "We evaluated our experiments on two classification tasks - customer satisfaction prediction and topic identification - using 3 datasets: CSAT, 20newsgroups and Fisher", so we know that they used 3 datasets: CSAT, 20newsgroups and Fisher for evaluation.
Answer: CSAT, 20newsgroups and Fisher
True answer: CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus

Validation ID: 0810b43404686ddfe4ca84783477ae300fdd2ea4
Question: On top of BERT does the RNN layer work better or the transformer layer?
From the context "We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer. Its output serves as a document embedding. Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions.
With this approach, we overcome BERT's computational complexity, reducing it to $O(n/k * k^2) = O(nk)$ for RoBERT, with $k$ denoting the segment size (the LSTM component has negligible linear complexity $O(k)$). The positional embeddings are also no longer an issue.
True answer: Transformer over BERT (ToBERT)

Validation ID: 0810b43404686ddfe4ca84783477ae300fdd2ea4
Question: On top of BERT does the RNN layer work better or the transformer layer?
From the context "We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer. Its output serves as a document embedding. Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions.
With this approach, we overcome BERT's computational complexity, reducing it to $O(n/k * k^2) = O(nk)$ for RoBERT, with $k$ denoting the segment size (the LSTM component has negligible linear complexity $O(k)$). The positional embeddings are also no longer an issue.
True answer: The transformer layer

Validation ID: 455d4ef8611f62b1361be4f6387b222858bb5e56
Question: How was this data collected?
From the context "We collected the dataset with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities.", so we know that the data was collected with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities.
Answer: 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities.
True answer: CrowdFlower

Validation ID: 455d4ef8611f62b1361be4f6387b222858bb5e56
Question: How was this data collected?
From the context "We collected the dataset with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities.", so we know that the data was collected with 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities.
Answer: 1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities.
True answer: The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English.

Validation ID: bc16ce6e9c61ae13d46970ebe6c4728a47f8f425
Question: What is the average length of dialog?
From the context "We collected the dataset with 1900 dialogs and 8533 turns", so we know that the average length of dialog is 8533 / 1900 = 4.48.
Answer: 4.48
True answer: 4.49 turns

Validation ID: bc16ce6e9c61ae13d46970ebe6c4728a47f8f425
Question: What is the average length of dialog?
From the context "We collected the dataset with 1900 dialogs and 8533 turns", so we know that the average length of dialog is 8533 / 1900 = 4.48.
Answer: 4.48
True answer: 4.5 turns per dialog (8533 turns / 1900 dialogs)

Validation ID: 1ff0fccf0dca95a6630380c84b0422bed854269a
Question: How are models evaluated in this human-machine communication game?
From the context "We quantify the efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", so we know that the efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ is measured as the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.
Answer: the efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ is measured as the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.
True answer: by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews

Validation ID: 1ff0fccf0dca95a6630380c84b0422bed854269a
Question: How are models evaluated in this human-machine communication game?
From the context "We quantify the efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", so we know that the efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ is measured as the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.
Answer: the efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ is measured as the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.
True answer: efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence

Validation ID: 3d7d865e905295d11f1e85af5fa89b210e3e9fdf
Question: How many participants were trying this communication game?
From the context "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus", so we know that there are 100 participants.
Answer: 100
True answer: 100 

Validation ID: 3d7d865e905295d11f1e85af5fa89b210e3e9fdf
Question: How many participants were trying this communication game?
From the context "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus", so we know that there are 100 participants.
Answer: 100
True answer: 100 crowdworkers 

Validation ID: 2ad4d3d222f5237ed97923640bc8e199409cbe52
Question: What user variations have been tested?
From the context "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus", so we know that the user variations have been tested are completion times and accuracies.
Answer: completion times and accuracies
True answer: completion times and accuracies 

Validation ID: 2ad4d3d222f5237ed97923640bc8e199409cbe52
Question: What user variations have been tested?
From the context "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus", so we know that the user variations have been tested are completion times and accuracies.
Answer: completion times and accuracies
True answer: Unanswerable

Validation ID: 3fad42be0fb2052bb404b989cc7d58b440cd23a0
Question: What are the baselines used?
From the context "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword. The Unif encoder randomly keeps tokens to generate keywords with the probability $\delta $. The Stopword encoder keeps all tokens but drops stop words (e.g. `the', `a', `or') all the time ($\delta =0$) or half of the time ($\delta =0.5$). The corresponding decoders for these encoders are optimized using gradient descent to minimize the reconstruction error (i.e. $\mathrm {loss}(x, \alpha, \beta )$).", so we know that the baselines used are Unif and Stopword.
Answer: Unif and Stopword
True answer: Unif and Stopword

Validation ID: 3fad42be0fb2052bb404b989cc7d58b440cd23a0
Question: What are the baselines used?
From the context "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword. The Unif encoder randomly keeps tokens to generate keywords with the probability $\delta $. The Stopword encoder keeps all tokens but drops stop words (e.g. `the', `a', `or') all the time ($\delta =0$) or half of the time ($\delta =0.5$). The corresponding decoders for these encoders are optimized using gradient descent to minimize the reconstruction error (i.e. $\mathrm {loss}(x, \alpha, \beta )$).", so we know that the baselines used are Unif and Stopword.
Answer: Unif and Stopword
True answer: Unif and Stopword

Validation ID: ee417fea65f9b1029455797671da0840c8c1abbe
Question: Do they use off-the-shelf NLP systems to build their assitant?
From the context "We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API", so we know that they use handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API to build their assitant.
Answer: handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API
True answer: No

Validation ID: ee417fea65f9b1029455797671da0840c8c1abbe
Question: Do they use off-the-shelf NLP systems to build their assitant?
From the context "We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API", so we know that they use handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API to build their assitant.
Answer: handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API
True answer: No

Validation ID: ca5a82b54cb707c9b947aa8445aac51ea218b23a
Question: How does the IPA label data after interacting with users?
From the context "Besides that, the system is implemented in a way such that it accumulates labeled dialogues in the background and stores them in a structured form", so we know that the IPA label data after interacting with users.
Answer: accumulates labeled dialogues in the background and stores them in a structured form
True answer: It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).

Validation ID: ca5a82b54cb707c9b947aa8445aac51ea218b23a
Question: How does the IPA label data after interacting with users?
From the context "Besides that, the system is implemented in a way such that it accumulates labeled dialogues in the background and stores them in a structured form", so we know that the IPA label data after interacting with users.
Answer: accumulates labeled dialogues in the background and stores them in a structured form
True answer: Plain dialogues with unique dialogue indexes, Plain Information Dictionary information (e.g., extracted entities) collected for the whole dialogue, Pairs of questions (i.e., user requests) and responses (i.e., bot responses), Triples in the form of (User Request, Next Action, Response)

Validation ID: da55bd769721b878dd17f07f124a37a0a165db02
Question: What kind of repetitive and time-consuming activities does their assistant handle?
From the context "The main objective of the proposed system is to interact with students at the beginning of every conversation and gather information on the topic (and sub-topic), examination mode and level, question number and exact problem formulation. Therefore, the system saves time for tutors and allows them to handle solely complex mathematical questions", so we know that the system saves time for tutors and allows them to handle solely complex mathematical questions.
Answer: complex mathematical questions
True answer: Unanswerable

Validation ID: da55bd769721b878dd17f07f124a37a0a165db02
Question: What kind of repetitive and time-consuming activities does their assistant handle?
From the context "The main objective of the proposed system is to interact with students at the beginning of every conversation and gather information on the topic (and sub-topic), examination mode and level, question number and exact problem formulation. Therefore, the system saves time for tutors and allows them to handle solely complex mathematical questions. Besides that, the system is implemented in a way such that it accumulates labeled dialogues in the background and stores them in a structured form.", so we know that the system saves time for tutors and allows them to handle solely complex mathematical questions.
Answer: complex mathematical questions
True answer:  What kind of topic (or sub-topic) a student has a problem with, At which examination mode (i.e., quiz, chapter level training or exercise, section level training or exercise, or final examination) the student is working right now,  the exact question number and exact problem formulation

Validation ID: feb448860918ef5b905bb25d7b855ba389117c1f
Question: How was the audio data gathered?
From the context "We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages", so we know that the audio data was gathered from news All India Radio news channel.
Answer: news All India Radio news channel
True answer: Through the All India Radio new channel where actors read news.

Validation ID: feb448860918ef5b905bb25d7b855ba389117c1f
Question: How was the audio data gathered?
From the context "We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages", so we know that the audio data was gathered from news All India Radio news channel.
Answer: news All India Radio news channel
True answer:  $\textbf {All India Radio}$ news channel

Validation ID: 4bc2784be43d599000cb71d31928908250d4cef3
Question: What is the GhostVLAD approach?
From the context "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). Which means that we compute the matrix V for both normal cluster K and ghost clusters G, but we will not include the vectors belongs to ghost cluster from V during concatenation of the features. Due to which, during feature aggregation stage the contribution of the noisy and unwanted features to normal VLAD clusters are assigned less weights while Ghost clusters absorb most of the weight. We illustrate this in Figure 1(Right Side), where the ghost clusters are shown in red color. We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into the projection layer, followed by softmax to predict the language label.", so we know that GhostVLAD is an extension of the NetVLAD approach.
Answer: GhostVLAD is an extension of the NetVLAD approach
True answer: extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters

Validation ID: 4bc2784be43d599000cb71d31928908250d4cef3
Question: What is the GhostVLAD approach?
From the context "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). Which means that we compute the matrix V for both normal cluster K and ghost clusters G, but we will not include the vectors belongs to ghost cluster from V during concatenation of the features. Due to which, during feature aggregation stage the contribution of the noisy and unwanted features to normal VLAD clusters are assigned less weights while Ghost clusters absorb most of the weight. We illustrate this in Figure 1(Right Side), where the ghost clusters are shown in red color. We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into the projection layer, followed by softmax to predict the language label.", so we know that GhostVLAD is an extension of the NetVLAD approach.
Answer: GhostVLAD is an extension of the NetVLAD approach
True answer: An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content.

Validation ID: 75df70ce7aa714ec4c6456d0c51f82a16227f2cb
Question: Which 7 Indian languages do they experiment with?
From the context "We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages", so we know that they experiment with 7 Indian languages.
Answer: 7 Indian languages
True answer: Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam

Validation ID: 75df70ce7aa714ec4c6456d0c51f82a16227f2cb
Question: Which 7 Indian languages do they experiment with?
From the context "We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages", so we know that they experiment with 7 Indian languages.
Answer: 7 Indian languages
True answer: Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)

Validation ID: 6424e442b34a576f904d9649d63acf1e4fdefdfc
Question: What datasets do they evaluate on?
From the context "For the task of unsupervised dependency parse induction, we employ the Dependency Model with Valence (DMV) BIBREF2 as the syntax model in our approach. DMV is a generative model that defines a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion. While, traditionally, DMV is trained using gold POS tags as observed syntactic categories, in our approach, we treat each tag as a latent variable, as described in sec:general-neural.
Most existing approaches to this task are not fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difficult even when using unsupervised syntactic categories BIBREF29. However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) BIBREF29, BIBREF30, BIBREF31, acoustic cues BIBREF33, additional training data BIBREF4, or annotated data from related languages BIBREF34. Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies without using gold POS tags or other extra linguistic information.
True answer:  Wall Street Journal (WSJ) portion of the Penn Treebank

Validation ID: 6424e442b34a576f904d9649d63acf1e4fdefdfc
Question: What datasets do they evaluate on?
From the context "For the task of unsupervised dependency parse induction, we employ the Dependency Model with Valence (DMV) BIBREF2 as the syntax model in our approach. DMV is a generative model that defines a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion. While, traditionally, DMV is trained using gold POS tags as observed syntactic categories, in our approach, we treat each tag as a latent variable, as described in sec:general-neural.
Most existing approaches to this task are not fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically parsing from words is difficult even when using unsupervised syntactic categories BIBREF29. However, inducing dependencies from words alone represents a more realistic experimental condition since gold POS tags are often unavailable in practice. Previous work that has trained from words alone often requires additional linguistic constraints (like sentence internal boundaries) BIBREF29, BIBREF30, BIBREF31, acoustic cues BIBREF33, additional training data BIBREF4, or annotated data from related languages BIBREF34. Our approach is naturally designed to train on word embeddings directly, thus we attempt to induce dependencies without using gold POS tags or other extra linguistic information.
True answer: Unanswerable

Validation ID: 5eabfc6cc8aa8a99e6e42514ef9584569cb75dec
Question: Do they evaluate only on English datasets?
From the context "We train basic DMV, extended DMV (E-DMV) BIBREF35 and Gaussian DMV (which treats POS tag as unknown latent variables and generates observed word embeddings directly conditioned on them following Gaussian distribution) as baselines. Basic DMV and E-DMV are trained with Viterbi EM BIBREF40 on unsupervised POS tags induced from our Markov-structured model described in sec:pos. Multinomial parameters of the syntax model in both Gaussian DMV and our model are initialized with the pre-trained DMV baseline. Other parameters are initialized in the same way as in the POS tagging experiment. The directed dependency accuracy (DDA) is used for evaluation and we report accuracy on sentences of length INLINEFORM1 and all lengths. We train the parser until training data likelihood converges, and report the mean and standard deviation over 20 random restarts.", so we know that they evaluate on English datasets.
Answer: English datasets
True answer: Yes

Validation ID: 5eabfc6cc8aa8a99e6e42514ef9584569cb75dec
Question: Do they evaluate only on English datasets?
From the context "We train basic DMV, extended DMV (E-DMV) BIBREF35 and Gaussian DMV (which treats POS tag as unknown latent variables and generates observed word embeddings directly conditioned on them following Gaussian distribution) as baselines. Basic DMV and E-DMV are trained with Viterbi EM BIBREF40 on unsupervised POS tags induced from our Markov-structured model described in sec:pos. Multinomial parameters of the syntax model in both Gaussian DMV and our model are initialized with the pre-trained DMV baseline. Other parameters are initialized in the same way as in the POS tagging experiment. The directed dependency accuracy (DDA) is used for evaluation and we report accuracy on sentences of length INLINEFORM1 and all lengths. We train the parser until training data likelihood converges, and report the mean and standard deviation over 20 random restarts.", so we know that they evaluate on English datasets.
Answer: English datasets
True answer: Unanswerable

Validation ID: 887c6727e9f25ade61b4853a869fe712fe0b703d
Question: What is the invertibility condition?
From the context "For the projection we can use an arbitrary invertible function, and given the representational power of neural networks they seem a natural choice. However, calculating the inverse and Jacobian of an arbitrary neural network can be difficult, as it requires that all component functions be invertible and also requires storage of large Jacobian matrices, which is memory intensive. To address this issue, several recent papers propose specially designed invertible networks that are easily trainable yet still powerful BIBREF16, BIBREF17, BIBREF19. Inspired by these works, we use the invertible transformation proposed by BIBREF16, which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property).", so we know that the invertibility condition is that the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied.
Answer: the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied
True answer: The neural projector must be invertible.

Validation ID: 887c6727e9f25ade61b4853a869fe712fe0b703d
Question: What is the invertibility condition?
From the context "For the projection we can use an arbitrary invertible function, and given the representational power of neural networks they seem a natural choice. However, calculating the inverse and Jacobian of an arbitrary neural network can be difficult, as it requires that all component functions be invertible and also requires storage of large Jacobian matrices, which is memory intensive. To address this issue, several recent papers propose specially designed invertible networks that are easily trainable yet still powerful BIBREF16, BIBREF17, BIBREF19. Inspired by these works, we use the invertible transformation proposed by BIBREF16, which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property).", so we know that the invertibility condition is that the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied.
Answer: the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied
True answer: we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists

Validation ID: 6236762b5631d9e395f81e1ebccc4bf3ab9b24ac
Question: Do they show on which examples how conflict works better than attention?
From the context "We analyzed the gains in Task 1 which we get from the attention-conflict model in order to ensure that they are not due to randomness in weight initialization or simply additional parameters. We particularly focused on the examples which were incorrectly marked in attention model but correctly in attention-conflict model. We saw that 70% of those cases are the ones where the pair was incorrectly marked as duplicate in the previous model but our combined model correctly marked them as non-duplicate.", so we know that they show on which examples how conflict works better than attention.
Answer: They show on which examples how conflict works better than attention
True answer: Yes

Validation ID: 6236762b5631d9e395f81e1ebccc4bf3ab9b24ac
Question: Do they show on which examples how conflict works better than attention?
From the context "We analyzed the gains in Task 1 which we get from the attention-conflict model in order to ensure that they are not due to randomness in weight initialization or simply additional parameters. We particularly focused on the examples which were incorrectly marked in attention model but correctly in attention-conflict model. We saw that 70% of those cases are the ones where the pair was incorrectly marked as duplicate in the previous model but our combined model correctly marked them as non-duplicate.", so we know that they show on which examples how conflict works better than attention.
Answer: they show on which examples how conflict works better than attention
True answer: Yes

Validation ID: 31d695ba855d821d3e5cdb7bea638c7dbb7c87c7
Question: Which neural architecture do they use as a base for their attention conflict mechanisms?
From the context "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11). The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.", so we know that they use two stacked GRU layers as a base for their attention conflict mechanisms.
Answer: two stacked GRU layers
True answer: GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.

Validation ID: 31d695ba855d821d3e5cdb7bea638c7dbb7c87c7
Question: Which neural architecture do they use as a base for their attention conflict mechanisms?
From the context "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11). The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.", so we know that they use two stacked GRU layers as a base for their attention conflict mechanisms.
Answer: two stacked GRU layers
True answer: two stacked GRU layers, attention for one model while for the another one it consists of attention and conflict combined, fully-connected layers

Validation ID: b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab
Question: On which tasks do they test their conflict method?
From the context "We train all our models for roughly 2 epochs with a batch size of 64. We use a hidden dimension of 150 throughout the model. The embedding layer uses ELMO BIBREF9 which has proven to be very useful in various downstream language understanding tasks. Our FC layers consists of four dense layers with INLINEFORM0 activation after each layer. The dropout rate is kept as 0.2 for every recurrent and FC linear layers. We use Adam optimizer in our experiment with epsilon=1e-8, beta=0.9 and learning rate=1e-3.", so we know that they test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.
Answer: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask
True answer: Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions

Validation ID: b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab
Question: On which tasks do they test their conflict method?
From the context "We train all our models for roughly 2 epochs with a batch size of 64. We use a hidden dimension of 150 throughout the model. The embedding layer uses ELMO BIBREF9 which has proven to be very useful in various downstream language understanding tasks. Our FC layers consists of four dense layers with INLINEFORM0 activation after each layer. The dropout rate is kept as 0.2 for every recurrent and FC linear layers. We use Adam optimizer in our experiment with epsilon=1e-8, beta=0.9 and learning rate=1e-3.", so we know that they test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.
Answer: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask
True answer: Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask

Validation ID: a99fdd34422f4231442c220c97eafc26c76508dd
Question: Do they use graphical models?
From the context "We interlace the problem of monolingual clustering with crosslingual clustering. This means that as part of our problem formulation we are also interested in a function INLINEFORM0 that associates each monolingual cluster with a crosslingual cluster, such that each crosslingual cluster only groups one monolingual cluster per different language, at a given time. The crosslingual cluster for a document INLINEFORM1 is INLINEFORM2. As such, a crosslingual cluster groups together monolingual clusters, at most one for each different language.", so we know that they use graphical models.
Answer: Yes
True answer: No

Validation ID: a99fdd34422f4231442c220c97eafc26c76508dd
Question: Do they use graphical models?
From the context "We interlace the problem of monolingual clustering with crosslingual clustering. This means that as part of our problem formulation we are also interested in a function INLINEFORM0 that associates each monolingual cluster with a crosslingual cluster, such that each crosslingual cluster only groups one monolingual cluster per different language, at a given time. The crosslingual cluster for a document INLINEFORM1 is INLINEFORM2. As such, a crosslingual cluster groups together monolingual clusters, at most one for each different language.", so we know that they use graphical models.
Answer: Yes
True answer: No

Validation ID: 2c78993524ca62bf1f525b60f2220a374d0e3535
Question: What are the sources of the datasets?
From the context "We adapted the cluster-linking dataset from rupnik2016news to evaluate our online crosslingual clustering approach", so we know that the sources of the datasets are rupnik2016news.
Answer: rupnik2016news
True answer: rupnik2016news

Validation ID: 2c78993524ca62bf1f525b60f2220a374d0e3535
Question: What are the sources of the datasets?
From the context "We processed it to become a collection of articles annotated with monolingual and crosslingual cluster labels.", so we know that the sources of the datasets are articles annotated with monolingual and crosslingual cluster labels.
Answer: articles annotated with monolingual and crosslingual cluster labels
True answer: rupnik2016news, Deutsche Welle's news website

Validation ID: d604f5fb114169f75f9a38fab18c1e866c5ac28b
Question: What metric is used for evaluation?
From the context "We evaluate our approach using the F INLINEFORM0 score, which is the harmonic mean of precision and recall", so we know that the metric used for evaluation is F INLINEFORM0 score.
Answer: F INLINEFORM0 score
True answer: F1, precision, recall, accuracy

Validation ID: d604f5fb114169f75f9a38fab18c1e866c5ac28b
Question: What metric is used for evaluation?
From the context "We evaluate our approach using the F INLINEFORM0 score, which is the harmonic mean of precision and recall", so we know that the metric used for evaluation is F INLINEFORM0 score.
Answer: F INLINEFORM0 score
True answer: Precision, recall, F1, accuracy

Validation ID: 1d3e914d0890fc09311a70de0b20974bf7f0c9fe
Question: Which eight NER tasks did they evaluate on?
From the context "Table TABREF7 (bottom) shows entity-level precision, recall and F1. For ease of visualization, Figure FIGREF13 shows what portion of the BioBERT – BERT F1 delta is covered. We improve over general-domain BERT on all tasks with varying effect sizes.", so we know that they evaluate on eight NER tasks.
Answer: eight NER tasks
True answer: BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800

Validation ID: 1d3e914d0890fc09311a70de0b20974bf7f0c9fe
Question: Which eight NER tasks did they evaluate on?
From the context "Table TABREF7 (bottom) shows entity-level precision, recall and F1. For ease of visualization, Figure FIGREF13 shows what portion of the BioBERT – BERT F1 delta is covered. We improve over general-domain BERT on all tasks with varying effect sizes.", so we know that they evaluate on eight NER tasks.
Answer: eight NER tasks
True answer: BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800

Validation ID: 16535db1d73a9373ffe9d6eedaa2369cefd91ac4
Question: What in-domain text did they use?
From the context "We downloaded the Deepset-AI Covid-QA dataset from: https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json [24 April 2020]", so we know that the Deepset-AI Covid-QA dataset was downloaded from https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json.
Answer: https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json
True answer: PubMed+PMC

Validation ID: 16535db1d73a9373ffe9d6eedaa2369cefd91ac4
Question: What in-domain text did they use?
From the context "We downloaded the Deepset-AI Covid-QA dataset from: https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json [24 April 2020]", so we know that the Deepset-AI Covid-QA dataset was downloaded from https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json.
Answer: https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json
True answer: PubMed+PMC (the data used for BioBERTv1.0) and/or CORD-19 (Covid-19 Open Research Dataset)

Validation ID: de0b650022ad8693465242ded169313419eed7d9
Question: Does their framework automatically optimize for hyperparameters?
From the context "We then apply our proprosed framework in two demanding scenarios: under-resourced translation and zero-resourced translation. The results show that bringing multilinguality to NMT helps to improve individual translations. With some insightful analyses of the results, we set our goal toward a fully multilingual NMT framework.", so we know that their framework does not automatically optimize for hyperparameters.
Answer: No
True answer: Unanswerable

Validation ID: de0b650022ad8693465242ded169313419eed7d9
Question: Does their framework automatically optimize for hyperparameters?
From the context "We then apply our proprosed framework in two demanding scenarios: under-resourced translation and zero-resourced translation. The results show that bringing multilinguality to NMT helps to improve individual translations. With some insightful analyses of the results, we set our goal toward a fully multilingual NMT framework.", so we know that their framework does not automatically optimize for hyperparameters.
Answer: No
True answer: Unanswerable

Validation ID: 2b3cac7af10d358d4081083962d03ea2798cf622
Question: Does their framework always generate purely attention-based models?
From the context "As we expected Figure FIGREF19 shows how different words in different languages can be close in the shared space after being learned to translate into a common language. We extract the word embeddings from the encoder of the mix-multi-source (En,Fr INLINEFORM0 De,De) after training, remove the language-specific codes (@en@ and @fr@)and project the word vectors to the 2D space using t-SNE BIBREF15", so we know that their framework always generate purely attention-based models.
Answer: Yes
True answer: Yes

Validation ID: 2b3cac7af10d358d4081083962d03ea2798cf622
Question: Does their framework always generate purely attention-based models?
From the context "Our approach features two treatments: 1) Coding the words in different languages as different words in the language-mixed vocabularies and 2) Forcing the NMT to translating a representation of source sentences into the sentences in a desired target language.", so we know that their framework always generate purely attention-based models.
Answer: Yes
True answer: Unanswerable

Validation ID: 897ba53ef44f658c128125edd26abf605060fb13
Question: Do they test their framework performance on commonly used language pairs, such as English-to-German?
From the context "As we expected Figure FIGREF19 shows how different words in different languages can be close in the shared space after being learned to translate into a common language. We extract the word embeddings from the encoder of the mix-multi-source (En,Fr INLINEFORM0 De,De) after training, remove the language-specific codes (@en@ and @fr@)and project the word vectors to the 2", so we know that they test their framework performance on commonly used language pairs, such as English-to-German.
Answer: Yes
True answer: Yes

Validation ID: 897ba53ef44f658c128125edd26abf605060fb13
Question: Do they test their framework performance on commonly used language pairs, such as English-to-German?
From the context "As we expected Figure FIGREF19 shows how different words in different languages can be close in the shared space after being learned to translate into a common language. We extract the word embeddings from the encoder of the mix-multi-source (En,Fr INLINEFORM0 De,De) after training, remove the language-specific codes (@en@ and @fr@)and project the word vectors to the 2", so we know that they test their framework performance on commonly used language pairs, such as English-to-German.
Answer: Yes
True answer: Yes

Validation ID: 41ac23e32bf208b69414f4b687c4f324c6132464
Question: Which languages do they test on for the under-resourced scenario?
From the context "First, we consider the translation for an under-resourced pair of languages. Here a small portion of the large parallel corpus for English-German is used as a simulation for the scenario where we do not have much parallel data: Translating texts in English to German.", so we know that they test on English and German for the under-resourced scenario.
Answer: English and German
True answer: English, German

Validation ID: 41ac23e32bf208b69414f4b687c4f324c6132464
Question: Which languages do they test on for the under-resourced scenario?
From the context "First, we consider the translation for an under-resourced pair of languages. Here a small portion of the large parallel corpus for English-German is used as a simulation for the scenario where we do not have much parallel data: Translating texts in English to German.", so we know that they test on English and German for the under-resourced scenario.
Answer: English and German
True answer: small portion of the large parallel corpus for English-German is used as a simulation

Validation ID: e97186c51d4af490dba6faaf833d269c8256426c
Question: Are the automatically constructed datasets subject to quality control?
From the context "We note that initial attempts to build this dataset through standard random splitting gave rise to certain systematic biases that were exploited by the choice-only baseline models described in the next section, and hence inflated overall model scores. After several efforts at filtering we found that, among other factors, using definitions from entries without example sentences as distractors (e.g., the first two entries in Table TABREF14) had a surprising correlation with such biases. This suggests that possible biases involving differences between dictionary entries with and without examples can taint the resulting automatically generated MCQA dataset (for more discussion on the pitfalls involved with automatic dataset construction, see Section SECREF5).", so we know that the automatically constructed datasets are subject to quality control.
Answer: Yes
True answer: No

Validation ID: e97186c51d4af490dba6faaf833d269c8256426c
Question: Are the automatically constructed datasets subject to quality control?
From the context "We note that initial attempts to build this dataset through standard random splitting gave rise to certain systematic biases that were exploited by the choice-only baseline models described in the next section, and hence inflated overall model scores. After several efforts at filtering we found that, among other factors, using definitions from entries without example sentences as distractors (e.g., the first two entries in Table TABREF14) had a surprising correlation with such biases. This suggests that possible biases involving differences between dictionary entries with and without examples can taint the resulting automatically generated MCQA dataset (for more discussion on the pitfalls involved with automatic dataset construction, see Section SECREF5).", so we know that the automatically constructed datasets are subject to quality control.
Answer: Yes
True answer: No

Validation ID: 5bb3c27606c59d73fd6944ba7382096de4fa58d8
Question: Do they focus on Reading Comprehension or multiple choice question answering?
From the context "In contrast, we focus on understanding the knowledge contained in language models after they have been trained for a QA end-task using benchmark datasets in which such knowledge is expected to be widespread", so we know that they focus on multiple choice question answering.
Answer: multiple choice question answering
True answer: MULTIPLE CHOICE QUESTION ANSWERING

Validation ID: 5bb3c27606c59d73fd6944ba7382096de4fa58d8
Question: Do they focus on Reading Comprehension or multiple choice question answering?
From the context "In contrast, we focus on understanding the knowledge contained in language models after they have been trained for a QA end-task using benchmark datasets in which such knowledge is expected to be widespread", so we know that they focus on multiple choice question answering.
Answer: multiple choice question answering
True answer: multiple-choice

Validation ID: 8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b
Question: After how many hops does accuracy decrease?
From the context "In the latter case, the rules $(\texttt {isa}^{r},c,c^{\prime }) \in \mathcal {T}_i$ in Table TABREF8 cover only direct ISA links from $c$ in direction $r \in \lbrace \uparrow,\downarrow \rbrace $. In practice, for each $c$ and direction $r$, we construct tests that cover the set HOPS$(c,r)$ of all direct as well as derived ISA relations of $c$:", so we know that the accuracy decreases after 2 hops.
Answer: 2
True answer: 1-hop links to 2-hops

Validation ID: 8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b
Question: After how many hops does accuracy decrease?
From the context "For example, the performance of even the best QA models degrades substantially on our hyponym probes (by 8-15%) when going from 1-hop links to 2-hops", so we know that accuracy decreases after 2 hops.
Answer: 2 hops
True answer: one additional hop

Validation ID: 85590bb26fed01a802241bc537d85ba5ef1c6dc2
Question: How do they control for annotation artificats?
From the context "We note that initial attempts to build this dataset through standard random splitting gave rise to certain systematic biases that were exploited by the choice-only baseline models described in the next section, and hence inflated overall model scores. After several efforts at filtering we found that, among other factors, using definitions from entries without example sentences as distractors (e.g., the first two entries in Table TABREF14) had a surprising correlation with such biases. This suggests that possible biases involving differences between dictionary entries with and without examples can taint the resulting automatically generated MCQA dataset (for more discussion on the pitfalls involved with automatic dataset construction, see Section SECREF5).", so we know that they control for annotation artificats by filtering.
Answer: filtering
True answer:  we use several of the MCQA baseline models first introduced in BIBREF0

Validation ID: 85590bb26fed01a802241bc537d85ba5ef1c6dc2
Question: How do they control for annotation artificats?
From the context "We note that initial attempts to build this dataset through standard random splitting gave rise to certain systematic biases that were exploited by the choice-only baseline models described in the next section, and hence inflated overall model scores. After several efforts at filtering we found that, among other factors, using definitions from entries without example sentences as distractors (e.g., the first two entries in Table TABREF14) had a surprising correlation with such biases. This suggests that possible biases involving differences between dictionary entries with and without examples can taint the resulting automatically generated MCQA dataset (for more discussion on the pitfalls involved with automatic dataset construction, see Section SECREF5).", so we know that they control for annotation artificats by using definitions from entries without example sentences as distractors.
Answer: using definitions from entries without example sentences as distractors
True answer: Choice-Only model, which is a variant of the well-known hypothesis-only baseline, Choice-to-choice model, tries to single out a given answer choice relative to other choices, Question-to-choice model, in contrast, uses the contextual representations for each question and individual choice and an attention model Att model to get a score

Validation ID: 75ff6e425ce304a35f18c0230c0d13d3913a31a9
Question: Is WordNet useful for taxonomic reasoning for this task?
From the context "Our main motivation for using WordNet, as opposed to a resource such as ConceptNet BIBREF36, is the availability of glosses ($\mathcal {D}$) and example sentences ($\mathcal {S}$)", so we know that WordNet is useful for taxonomic reasoning for this task.
Answer: Yes
True answer: Unanswerable

Validation ID: 75ff6e425ce304a35f18c0230c0d13d3913a31a9
Question: Is WordNet useful for taxonomic reasoning for this task?
From the context "Our main motivation for using WordNet, as opposed to a resource such as ConceptNet BIBREF36, is the availability of glosses ($\mathcal {D}$) and example sentences ($\mathcal {S}$)", so we know that WordNet is useful for taxonomic reasoning for this task.
Answer: Yes
True answer: Yes

Validation ID: 5cb610d3d5d7d447b4cd5736d6a7d8262140af58
Question: How do they perform multilingual training?
From the context "Since a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across languages.", so we know that they perform multilingual training by randomly alternating between languages for every new minibatch.
Answer: randomly alternating between languages for every new minibatch
True answer: Multilingual training is performed by randomly alternating between languages for every new minibatch

Validation ID: 5cb610d3d5d7d447b4cd5736d6a7d8262140af58
Question: How do they perform multilingual training?
From the context "Since a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across languages.", so we know that they perform multilingual training by randomly alternating between languages for every new minibatch.
Answer: randomly alternating between languages for every new minibatch
True answer: by randomly alternating between languages for every new minibatch

Validation ID: c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a
Question: What languages are evaluated?
From the context "We train models for 50 different random combinations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2", so we know that the languages are evaluated in Track 1 and Track 2.
Answer: Track 1 and Track 2
True answer: German, English, Spanish, Finnish, French, Russian,  Swedish.

Validation ID: c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a
Question: What languages are evaluated?
From the context "We train models for 50 different random combinations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2", so we know that the languages are evaluated in Track 1 and Track 2.
Answer: Track 1 and Track 2
True answer: Unanswerable

Validation ID: b9d168da5321a7d7b812c52bb102a05210fe45bd
Question: Does the model have attention?
From the context "The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the model has attention.
Answer: Yes
True answer: Yes

Validation ID: b9d168da5321a7d7b812c52bb102a05210fe45bd
Question: Does the model have attention?
From the context "The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the model has attention.
Answer: Yes
True answer: Yes

Validation ID: 0c234db3b380c27c4c70579a5d6948e1e3b24ff1
Question: What architecture does the decoder have?
From the context "The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the decoder has an LSTM encoder and an attention mechanism.
Answer: LSTM encoder and an attention mechanism
True answer: LSTM

Validation ID: 0c234db3b380c27c4c70579a5d6948e1e3b24ff1
Question: What architecture does the decoder have?
From the context "The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the decoder has an LSTM encoder and an attention mechanism.
Answer: LSTM encoder and an attention mechanism
True answer: LSTM

Validation ID: fa527becb8e2551f4fd2ae840dbd4a68971349e0
Question: What architecture does the encoder have?
From the context "The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the encoder has LSTM architecture.
Answer: LSTM
True answer: LSTM

Validation ID: fa527becb8e2551f4fd2ae840dbd4a68971349e0
Question: What architecture does the encoder have?
From the context "The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the encoder has LSTM architecture.
Answer: LSTM
True answer: LSTM

Validation ID: 32a3c248b928d4066ce00bbb0053534ee62596e7
Question: What is MSD prediction?
From the context "We introduce an auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form", so we know that MSD prediction is to predict the MSD tag of the target form.
Answer: to predict the MSD tag of the target form
True answer: The task of predicting MSD tags: V, PST, V.PCTP, PASS.

Validation ID: 32a3c248b928d4066ce00bbb0053534ee62596e7
Question: What is MSD prediction?
From the context "We introduce an auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form", so we know that MSD prediction is to predict the MSD tag of the target form.
Answer: to predict the MSD tag of the target form
True answer: morphosyntactic descriptions (MSD)

Validation ID: c9b8d3858c112859eabee54248b874331c48f71b
Question: What type of inflections are considered?
From the context "This paper describes our approach and results for Task 2 of the CoNLL–SIGMORPHON 2018 shared task on universal morphological reinflection BIBREF0. The task is to generate an inflected word form given its lemma and the context in which it occurs.", so we know that the type of inflections are universal morphological reinflection.
Answer: universal morphological reinflection
True answer: Unanswerable

Validation ID: c9b8d3858c112859eabee54248b874331c48f71b
Question: What type of inflections are considered?
From the context "This paper describes our approach and results for Task 2 of the CoNLL–SIGMORPHON 2018 shared task on universal morphological reinflection BIBREF0. The task is to generate an inflected word form given its lemma and the context in which it occurs.", so we know that the type of inflections are universal morphological reinflection.
Answer: universal morphological reinflection
True answer: Unanswerable

Validation ID: 45e9533586199bde19313cd43b3d0ecadcaf7a33
Question: Do they use attention?
From the context "We use the method of BIBREF13 to apply self attention to the passage: INLINEFORM6", so we know that they use attention.
Answer: Yes
True answer: Yes

Validation ID: 45e9533586199bde19313cd43b3d0ecadcaf7a33
Question: Do they use attention?
From the context "We use the method of BIBREF13 to apply self attention to the passage: INLINEFORM6", so we know that they use attention.
Answer: Yes
True answer: Yes

Validation ID: d3dbb5c22ef204d85707d2d24284cc77fa816b6c
Question: What other models do they compare to?
From the context "We utilize spaCy tool to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. The word embeddings are initialized with pre-trained 300-dimensional GloVe BIBREF10. A 2-layer BiLSTM is used encoding the contextual information of both questions and passages.", so we know that they use spaCy tool to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. The word embeddings are initialized with pre-trained 300-dimensional GloVe BIBREF10. A 2-layer BiLSTM is used encoding the contextual information of both questions and passages.
Answer: spaCy tool to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. The word embeddings are initialized with pre-trained 300-dimensional GloVe BIBREF10. A 2-layer BiLSTM is used encoding the contextual information of both questions and passages.
True answer: SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo

Validation ID: d3dbb5c22ef204d85707d2d24284cc77fa816b6c
Question: What other models do they compare to?
From the context "We utilize spaCy tool to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. The word embeddings are initialized with pre-trained 300-dimensional GloVe BIBREF10. A 2-layer BiLSTM is used encoding the contextual information of both questions and passages.", so we know that they use spaCy tool to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. The word embeddings are initialized with pre-trained 300-dimensional GloVe BIBREF10. A 2-layer BiLSTM is used encoding the contextual information of both questions and passages.
Answer: spaCy tool to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. The word embeddings are initialized with pre-trained 300-dimensional GloVe BIBREF10. A 2-layer BiLSTM is used encoding the contextual information of both questions and passages.
True answer: BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo

Validation ID: a5e49cdb91d9fd0ca625cc1ede236d3d4672403c
Question: What is the architecture of the span detector?
From the context "We adopt a multi-turn answer module for the span detector BIBREF1. Formally, at time step INLINEFORM0 in the range of INLINEFORM1, the state is defined by INLINEFORM2. The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5, where INLINEFORM6. Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11. Finally, a bilinear function is used to find the begin and end point of answer spans at each reasoning step INLINEFORM12 : DISPLAYFORM0 DISPLAYFORM1", so we know that the architecture of the span detector is a multi-turn answer module.
Answer: a multi-turn answer module
True answer: adopt a multi-turn answer module for the span detector BIBREF1

Validation ID: a5e49cdb91d9fd0ca625cc1ede236d3d4672403c
Question: What is the architecture of the span detector?
From the context "We adopt a multi-turn answer module for the span detector BIBREF1. Formally, at time step INLINEFORM0 in the range of INLINEFORM1, the state is defined by INLINEFORM2. The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5, where INLINEFORM6. Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11. Finally, a bilinear function is used to find the begin and end point of answer spans at each reasoning step INLINEFORM12 : DISPLAYFORM0 DISPLAYFORM1 The final prediction is the average of each time step: INLINEFORM0. We randomly apply dropout on the step level in each time step during training, as done in BIBREF1.", so we know that the architecture of the span detector is a multi-turn answer module.
Answer: a multi-turn answer module
True answer: Unanswerable

Validation ID: aefa333b2cf0a4000cd40566149816f5b36135e7
Question: What evaluation metric do they use?
From the context "We employ the idea of learning a linear transformation matrix to map or project word embeddings from the semantic space of one language to that of another. This idea was first proposed in BIBREF4, who applied it to lexical translation between English, Spanish, Czech and Vietnamese. We extend it from continuous representations of single words or collocations to `semantic fingerprints' of documents as a whole.
Academic texts as Comparable Corpora
True answer: Accuracy

Validation ID: aefa333b2cf0a4000cd40566149816f5b36135e7
Question: What evaluation metric do they use?
From the context "We employ the idea of learning a linear transformation matrix to map or project word embeddings from the semantic space of one language to that of another. This idea was first proposed in BIBREF4, who applied it to lexical translation between English, Spanish, Czech and Vietnamese. We extend it from continuous representations of single words or collocations to `semantic fingerprints' of documents as a whole.
Academic texts as Comparable Corpora
True answer: ratio of correct `translations'

Validation ID: c5abe97625b9e1c8de8208e15d59c704a597b88c
Question: What are the results from these proposed strategies?
From the context "Figure FIGREF10 shows that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. The knowledge graph cell representation appears to thus be a better indication of what a promising state is as opposed to just the textual observation. Comparing the advanced exploration methods when using the knowledge graph, we see that both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. KG-A2C-chained is significantly more sample efficient and converges faster. We can infer that chaining policies by explicitly detecting bottlenecks lets us pass it more quickly than attempting to find promising cell representations with Go-Explore. This form of chained exploration with backtracking is particularly suited to sequential decision making problems that can be represented as acyclic directed graphs as in Figure FIGREF1.", so we know that the results from these proposed strategies are that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. The knowledge graph cell representation appears to thus be a better indication of what a promising state is as opposed to just the textual observation. Comparing the advanced exploration methods when using the knowledge graph, we see that both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. KG-A2C-chained is significantly more sample efficient and converges faster. We can infer that chaining policies by explicitly detecting bottlenecks lets us pass it more quickly than attempting to find promising cell representations with Go-Explore. This form of chained exploration with backtracking is particularly suited to sequential decision making problems that can be represented as acyclic directed graphs as in Figure FIGREF1.
Answer: agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C
True answer: Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.

Validation ID: c5abe97625b9e1c8de8208e15d59c704a597b88c
!!! Has error during processing !!!
Question: What are the results from these proposed strategies?
Answer: From the context "Figure FIGREF10 shows that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. The knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass the bottleneck. A2C-chained—which explores without a knowledge graph—fails to even outperform the baseline A2C. We hypothesize that this is due to the knowledge graph aiding implicitly in the sample efficiency of bottleneck detection and subsequent exploration. That is, exploring after backtracking from a potentially detected bottleneck is much more efficient in the knowledge graph based agent. Comparing the advanced exploration methods when using the knowledge graph, we see that both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. KG-A2C-chained is significantly more sample efficient and converges faster. We can infer that chaining policies by explicitly detecting bottlenecks lets us pass it more quickly than attempting to find promising cell representations with Go-Explore. This form of chained exploration with backtracking is particularly suited to sequential decision making problems that can be represented as acyclic directed graphs as in Figure FIGREF1.", so we know that the results from these proposed strategies are that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. The knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass the bottleneck. A2C-chained—which explores without a knowledge graph—fails to even outperform the baseline A2C. We hypothesize that this is due to the knowledge graph aiding implicitly in the sample efficiency of bottleneck detection and subsequent exploration. That
True answer: KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40

Validation ID: eb2d5edcdfe18bd708348283f92a32294bb193a5
Question: What are the baselines?
From the context "We compare our two exploration strategies to the following baselines and ablations:", so we know that the baselines are KG-A2C, A2C, A2C-chained and A2C-Explore.
Answer: KG-A2C, A2C, A2C-chained and A2C-Explore
True answer: a score of 40

Validation ID: eb2d5edcdfe18bd708348283f92a32294bb193a5
Question: What are the baselines?
From the context "We compare our two exploration strategies to the following baselines and ablations:", so we know that the baselines are KG-A2C, A2C, A2C-chained and A2C-Explore.
Answer: KG-A2C, A2C, A2C-chained and A2C-Explore
True answer: KG-A2C, A2C, A2C-chained, A2C-Explore

Validation ID: 88ab7811662157680144ed3fdd00939e36552672
Question: What are the two new strategies?
From the context "We first introduce a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen. The second contribution explore how to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9. We additionally present a comparative ablation study analyzing the performance of these methods on the popular text-game Zork1.", so we know that the two new strategies are: 1) a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state; 2) leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9.
Answer: 1) a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state; 2) leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9
True answer: a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state, to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space

Validation ID: 88ab7811662157680144ed3fdd00939e36552672
Question: What are the two new strategies?
From the context "We first introduce a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen. The second contribution explore how to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9. We additionally present a comparative ablation study analyzing the performance of these methods on the popular text-game Zork1.", so we know that the two new strategies are: 1) a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state; 2) leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9.
Answer: 1) a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state; 2) leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9
True answer: KG-A2C-chained, KG-A2C-Explore

Validation ID: cb196725edc9cdb2c54b72364f3bbf7c76471490
Question: Do they report results only on English data?
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: cb196725edc9cdb2c54b72364f3bbf7c76471490
Question: Do they report results only on English data?
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 286078813136943dfafb5155ee15d2429e7601d9
Question: How much better than the baseline is LiLi?
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability", so we know that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.
Answer: highly effective
True answer: In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. 


Validation ID: 286078813136943dfafb5155ee15d2429e7601d9
Question: How much better than the baseline is LiLi?
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability", so we know that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.
Answer: highly effective
True answer: Unanswerable

Validation ID: 8f16dc7d7be0d284069841e456ebb2c69575b32b
Question: What baseline is used in the experiments?
From the context "We solve the OKBC problem by mimicking how humans acquire knowledge and perform reasoning in an interactive conversation. Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. The process typically involves an inference strategy (a sequence of actions), which interleaves a sequence of processing and interactive actions. A processing action can be the selection of related facts, deriving inference chain, etc., that advances the inference process. An interactive action can be deciding what to ask, formulating a suitable question, etc., that enable us to interact. The process helps grow the knowledge over time and the gained knowledge enables us to communicate better in the future. We call this lifelong interactive learning and inference (LiLi). Lifelong learning is reflected by the facts that the newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning. LiLi should have the following capabilities:
This setting is ideal for many NLP applications like dialog and question-answering systems that naturally provide the scope for human interaction and demand real-time inference.", so we know that the baseline is how humans acquire knowledge and perform reasoning in an interactive conversation.
True answer: versions of LiLi

Validation ID: 8f16dc7d7be0d284069841e456ebb2c69575b32b
Question: What baseline is used in the experiments?
From the context "We solve the OKBC problem by mimicking how humans acquire knowledge and perform reasoning in an interactive conversation. Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. The process typically involves an inference strategy (a sequence of actions), which interleaves a sequence of processing and interactive actions. A processing action can be the selection of related facts, deriving inference chain, etc., that advances the inference process. An interactive action can be deciding what to ask, formulating a suitable question, etc., that enable us to interact. The process helps grow the knowledge over time and the gained knowledge enables us to communicate better in the future. We call this lifelong interactive learning and inference (LiLi). Lifelong learning is reflected by the facts that the newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning. LiLi should have the following capabilities:
This setting is ideal for many NLP applications like dialog and question-answering systems that naturally provide the scope for human interaction and demand real-time inference.", so we know that the baseline is lifelong interactive learning and inference (LiLi).
True answer: various versions of LiLi as baselines, Single, Sep, F-th, BG, w/o PTS

Validation ID: a7d020120a45c39bee624f65443e09b895c10533
Question: In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?
From the context "We design LiLi as a combination of two interconnected models: (1) a RL model that learns to formulate a query-specific inference strategy for performing the OKBC task, and (2) a lifelong prediction model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR. The framework improves its performance over time through user interaction and knowledge retention.", so we know that LiLi is a combination of two interconnected models: (1) a RL model that learns to formulate a query-specific inference strategy for performing the OKBC task, and (2) a lifelong prediction model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR. The framework improves its performance over time through user interaction and knowledge retention.
From the context "Inference Strategy Formulation. We view solving the strategy formulation problem as learning to play an inference game, where the goal is to formulate a strategy that "makes the inference task possible". Considering PR methods, inference is possible, iff (1) INLINEFORM0 becomes known to its KB (by acquiring clues when INLINEFORM1 is unknown) and (2) path features are extracted between INLINEFORM2 and INLINEFORM3 (which inturn requires INLINEFORM4 and INLINEFORM5 to be known).
True answer: newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning

Validation ID: a7d020120a45c39bee624f65443e09b895c10533
Question: In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?
From the context "We design LiLi as a combination of two interconnected models: (1) a RL model that learns to formulate a query-specific inference strategy for performing the OKBC task, and (2) a lifelong prediction model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR. The framework improves its performance over time through user interaction and knowledge retention.", so we know that LiLi is a combination of two interconnected models: (1) a RL model that learns to formulate a query-specific inference strategy for performing the OKBC task, and (2) a lifelong prediction model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR. The framework improves its performance over time through user interaction and knowledge retention.
From the context "Inference Strategy Formulation. We view solving the strategy formulation problem as learning to play an inference game, where the goal is to formulate a strategy that "makes the inference task possible". Considering PR methods, inference is possible, iff (1) INLINEFORM0 becomes known to its KB (by acquiring clues when INLINEFORM1 is unknown) and (2) path features are extracted between INLINEFORM2 and INLINEFORM3 (which inturn requires INLINEFORM4 and INLINEFORM5 to be known).
True answer: Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. 

Validation ID: 585626d18a20d304ae7df228c2128da542d248ff
Question: What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation??
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.", so we know that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.
Answer: predictive performance and strategy formulation ability
True answer: Coverage, Avg. MCC and avg. +ve F1 score

Validation ID: 585626d18a20d304ae7df228c2128da542d248ff
Question: What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation??
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.", so we know that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.
Answer: predictive performance and strategy formulation ability
True answer: strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score

Validation ID: bfc2dc913e7b78f3bd45e5449d71383d0aa4a890
Question: What are the components of the general knowledge learning engine?
From the context "As lifelong learning needs to retain knowledge learned from past tasks and use it to help future learning BIBREF31, LiLi uses a Knowledge Store (KS) for knowledge retention. KS has four components: (i) Knowledge Graph ( INLINEFORM0 ): INLINEFORM1 (the KB) is initialized with base KB triples (see $4) and gets updated over time with the acquired knowledge. (ii) Relation-Entity Matrix ( INLINEFORM2 ): INLINEFORM3 is a sparse matrix, with rows as relations and columns as entity-pairs and is used by the prediction model. Given a triple ( INLINEFORM4, INLINEFORM5, INLINEFORM6 ) INLINEFORM7, we set INLINEFORM8 [ INLINEFORM9, ( INLINEFORM10, INLINEFORM11 )] = 1 indicating INLINEFORM12 occurs for pair ( INLINEFORM13, INLINEFORM14 ). (iii) Task Experience Store ( INLINEFORM15 ): INLINEFORM16 stores the predictive performance of LiLi on past learned tasks in terms of Matthews correlation coefficient (MCC) that measures the quality of binary classification. So, for two tasks INLINEFORM17 and INLINEFORM18 (each relation is a task), if INLINEFORM19 [ INLINEFORM20 ] INLINEFORM21 INLINEFORM22 [ INLINEFORM23 ] [where INLINEFORM24 [ INLINEFORM25 ]=MCC( INLINEFORM26 )], we say C-PR has learned INLINEFORM27 well compared to INLINEFORM28. (iv) Incomplete Feature DB ( INLINEFORM29 ): INLINEFORM30 stores the frequency of an incomplete path INLINEFORM31 in the form of a tuple ( INLINEFORM32, INLINEFORM33, INLINEFORM34 ) and is used in formulating MLQs. INLINEFORM35 [( INLINEFORM36, INLINEFORM37, INLINEFORM38 )] = INLINEFORM39 implies LiLi has extracted incomplete path INLINEFORM40 INLINEFORM41 times involving entity-pair INLINEFORM42 [( INLINEFORM43, INLINEFORM44 )] for query relation INLINEFORM45.", so we know that the components of the general knowledge learning engine are Knowledge Graph, Relation-Entity Matrix, Task Experience Store and Incomplete Feature DB.
Answer: Knowledge Graph, Relation-Entity Matrix,
True answer: Answer with content missing: (list)
LiLi should have the following capabilities:
1. to formulate an inference strategy for a given query that embeds processing and interactive actions.
2. to learn interaction behaviors (deciding what to ask and when to ask the user).
3. to leverage the acquired knowledge in the current and future inference process.
4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.

Validation ID: bfc2dc913e7b78f3bd45e5449d71383d0aa4a890
Question: What are the components of the general knowledge learning engine?
From the context "As lifelong learning needs to retain knowledge learned from past tasks and use it to help future learning BIBREF31, LiLi uses a Knowledge Store (KS) for knowledge retention. KS has four components: (i) Knowledge Graph ( INLINEFORM0 ): INLINEFORM1 (the KB) is initialized with base KB triples (see $4) and gets updated over time with the acquired knowledge. (ii) Relation-Entity Matrix ( INLINEFORM2 ): INLINEFORM3 is a sparse matrix, with rows as relations and columns as entity-pairs and is used by the prediction model. Given a triple ( INLINEFORM4, INLINEFORM5, INLINEFORM6 ) INLINEFORM7, we set INLINEFORM8 [ INLINEFORM9, ( INLINEFORM10, INLINEFORM11 )] = 1 indicating INLINEFORM12 occurs for pair ( INLINEFORM13, INLINEFORM14 ). (iii) Task Experience Store ( INLINEFORM15 ): INLINEFORM16 stores the predictive performance of LiLi on past learned tasks in terms of Matthews correlation coefficient (MCC) that measures the quality of binary classification. So, for two tasks INLINEFORM17 and INLINEFORM18 (each relation is a task), if INLINEFORM19 [ INLINEFORM20 ] INLINEFORM21 INLINEFORM22 [ INLINEFORM23 ] [where INLINEFORM24 [ INLINEFORM25 ]=MCC( INLINEFORM26 )], we say C-PR has learned INLINEFORM27 well compared to INLINEFORM28. (iv) Incomplete Feature DB ( INLINEFORM29 ): INLINEFORM30 stores the frequency of an incomplete path INLINEFORM31 in the form of a tuple ( INLINEFORM32, INLINEFORM33, INLINEFORM34 ) and is used in formulating MLQs. INLINEFORM35 [( INLINEFORM36, INLINEFORM37, INLINEFORM38 )] = INLINEFORM39 implies LiLi has extracted incomplete path INLINEFORM40 INLINEFORM41 times involving entity-pair INLINEFORM42 [( INLINEFORM43, INLINEFORM44 )] for query relation INLINEFORM45.", so we know that the components of the general knowledge learning engine are Knowledge Graph, Relation-Entity Matrix, Task Experience Store and Incomplete Feature DB.
Answer: Knowledge Graph, Relation-Entity Matrix,
True answer: Knowledge Store (KS) , Knowledge Graph ( INLINEFORM0 ),  Relation-Entity Matrix ( INLINEFORM2 ), Task Experience Store ( INLINEFORM15 ), Incomplete Feature DB ( INLINEFORM29 )

Validation ID: 6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de
Question: How many labels do the datasets have?
From the context "We set hyper-parameters of the CNN encoder following previous works BIBREF22, BIBREF4 without specific tuning on our datasets. The window size is set to 3 and the size of the hidden layer is set to 300. The nonlinear activation function is Relu. For regularization, we also follow their settings and employ dropout with probability set to 0.5 on INLINEFORM0 before feeding it to the output layer INLINEFORM1, and constrain the INLINEFORM2 -norm of the weight vector INLINEFORM3, setting its max norm to 3.", so we know that the datasets have 3 labels.
Answer: 3
True answer: 719313

Validation ID: 6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de
Question: How many labels do the datasets have?
From the context "We set hyper-parameters of the CNN encoder following previous works BIBREF22, BIBREF4 without specific tuning on our datasets. The window size is set to 3 and the size of the hidden layer is set to 300. The nonlinear activation function is Relu. For regularization, we also follow their settings and employ dropout with probability set to 0.5 on INLINEFORM0 before feeding it to the output layer INLINEFORM1, and constrain the INLINEFORM2 -norm of the weight vector INLINEFORM3, setting its max norm to 3.", so we know that the datasets have 3 labels.
Answer: 3
True answer: Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.

Validation ID: b46c0015a122ee5fb95c2a45691cb97f80de1bb6
Question: What is the architecture of the model?
From the context "We denote INLINEFORM0 parameterized by INLINEFORM1 as a neural-based feature encoder that maps documents from both domains to a shared feature space, and INLINEFORM2 parameterized by INLINEFORM3 as a fully connected layer with softmax activation serving as the sentiment classifier", so we know that the architecture of the model is a neural-based feature encoder that maps documents from both domains to a shared feature space, and a fully connected layer with softmax activation serving as the sentiment classifier.
Answer: a neural-based feature encoder that maps documents from both domains to a shared feature space, and a fully connected layer with softmax activation serving as the sentiment classifier
True answer: one-layer CNN structure from previous works BIBREF22 , BIBREF4

Validation ID: b46c0015a122ee5fb95c2a45691cb97f80de1bb6
Question: What is the architecture of the model?
From the context "We denote INLINEFORM0 parameterized by INLINEFORM1 as a neural-based feature encoder that maps documents from both domains to a shared feature space, and INLINEFORM2 parameterized by INLINEFORM3 as a fully connected layer with softmax activation serving as the sentiment classifier", so we know that the architecture of the model is a neural-based feature encoder that maps documents from both domains to a shared feature space, and a fully connected layer with softmax activation serving as the sentiment classifier.
Answer: a neural-based feature encoder that maps documents from both domains to a shared feature space, and a fully connected layer with softmax activation serving as the sentiment classifier
True answer:  one-layer CNN

Validation ID: 5b7a4994bfdbf8882f391adf1cd2218dbc2255a0
Question: What are the baseline methods?
From the context "We evaluate our method rigorously under multiple experimental settings by taking label distribution and corpus size into consideration. The results show that our model is able to obtain significant improvements over strong baselines", so we know that the baseline methods are strong baselines.
Answer: strong baselines
True answer: (1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD

Validation ID: 5b7a4994bfdbf8882f391adf1cd2218dbc2255a0
Question: What are the baseline methods?
From the context "We evaluate our method rigorously under multiple experimental settings by taking label distribution and corpus size into consideration. The results show that our model is able to obtain significant improvements over strong baselines", so we know that the baseline methods are strong baselines.
Answer: strong baselines
True answer: non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized

Validation ID: 9176d2ba1c638cdec334971c4c7f1bb959495a8e
Question: What are the source and target domains?
From the context "We conduct most of our experiments under an unsupervised domain adaptation setting, where we have no labeled data from the target domain. Consider two sets INLINEFORM0 and INLINEFORM1. INLINEFORM2 is from the source domain with INLINEFORM3 labeled examples, where INLINEFORM4 is a one-hot vector representation of sentiment label and INLINEFORM5 denotes the number of classes. INLINEFORM6 is from the target domain with INLINEFORM7 unlabeled examples. INLINEFORM8 denotes the total number of training documents including both labeled and unlabeled.", so we know that the source domain is INLINEFORM2 and the target domain is INLINEFORM6.
Answer: INLINEFORM2 and INLINEFORM6
True answer: Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen

Validation ID: 9176d2ba1c638cdec334971c4c7f1bb959495a8e
Question: What are the source and target domains?
From the context "We conduct most of our experiments under an unsupervised domain adaptation setting, where we have no labeled data from the target domain. Consider two sets INLINEFORM0 and INLINEFORM1. INLINEFORM2 is from the source domain with INLINEFORM3 labeled examples, where INLINEFORM4 is a one-hot vector representation of sentiment label and INLINEFORM5 denotes the number of classes. INLINEFORM6 is from the target domain with INLINEFORM7 unlabeled examples. INLINEFORM8 denotes the total number of training documents including both labeled and unlabeled.", so we know that the source domain is INLINEFORM2 and the target domain is INLINEFORM6.
Answer: INLINEFORM2 and INLINEFORM6
True answer: we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)

Validation ID: 0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c
Question: Did they use a crowdsourcing platform for annotations?
From the context "The senses used for annotation are coarse-grained – with one sense covering a range of related usages. Each word has between two and four senses.", so we know that the senses used for annotation are coarse-grained.
Answer: coarse-grained
True answer: No

Validation ID: 0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c
Question: Did they use a crowdsourcing platform for annotations?
From the context "The senses used for annotation are coarse-grained – with one sense covering a range of related usages. Each word has between two and four senses.", so we know that the senses used for annotation are coarse-grained.
Answer: coarse-grained
True answer: Unanswerable

Validation ID: 5e324846a99a5573cd2e843d1657e87f4eb22fa6
Question: How do they deal with unknown distribution senses?
From the context "The majority of classification techniques are built on an assumption that the training data approximately reflects the true distribution of the target classes. However, that is not the case when using monosemous relatives. The number of their occurrences seldom agrees with the probabilities of corresponding word senses. Quite often it actually is the opposite: obvious and frequent meanings have very few relatives and vice versa. Some researchers simply copy the a priori probabilities from test data BIBREF9, others employ heuristics, but they are easily beaten by statistics taken from a real annotated corpus, even different than test set BIBREF13.", so we know that they employ heuristics to deal with unknown distribution senses.
Answer: heuristics
True answer: The Näive-Bayes classifier is corrected so it is not biased to most frequent classes

Validation ID: 5e324846a99a5573cd2e843d1657e87f4eb22fa6
Question: How do they deal with unknown distribution senses?
From the context "The majority of classification techniques are built on an assumption that the training data approximately reflects the true distribution of the target classes. However, that is not the case when using monosemous relatives. The number of their occurrences seldom agrees with the probabilities of corresponding word senses. Quite often it actually is the opposite: obvious and frequent meanings have very few relatives and vice versa. Some researchers simply copy the a priori probabilities from test data BIBREF9, others employ heuristics, but they are easily beaten by statistics taken from a real annotated corpus, even different than test set BIBREF13.", so we know that they employ heuristics to deal with unknown distribution senses.
Answer: heuristics
True answer: Bayesian classifier has been modified, removing the bias towards frequent labels in the training data

Validation ID: 2ccc26e11df4eb26fcccdd1f446dc749aff5d572
Question: Do they report results only on English data?
From the context "We converted all the ISIS magazines to texts using pdf readers and all articles that addressed women in both magazines (20 articles) were selected for our analysis. To facilitate comparison with a mainstream, non-violent religious group, we collected articles from catholicwomensforum.org, an online resource catering to Catholic women. We scrapped 132 articles from this domain. While this number is large, the articles themselves are much shorter than those published by ISIS", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 2ccc26e11df4eb26fcccdd1f446dc749aff5d572
Question: Do they report results only on English data?
From the context "We converted all the ISIS magazines to texts using pdf readers and all articles that addressed women in both magazines (20 articles) were selected for our analysis. To facilitate comparison with a mainstream, non-violent religious group, we collected articles from catholicwomensforum.org, an online resource catering to Catholic women. We scrapped 132 articles from this domain. While this number is large, the articles themselves are much shorter than those published by ISIS", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: f318a2851d7061f05a5b32b94251f943480fbd15
!!! Has error during processing !!!
Question: What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?
Answer: From the context "The results suggest that certain topics play a particularly important roles in ISIS propaganda targeting women. These relate to the role of women in early Islam, Islamic ideology, marriage/divorce, motherhood, spousal relationships, and hijrah (moving to a new land). Comparing these topics with those that appeared on a Catholic women forum, it seems that the two groups are not that different in their approach to women. The only difference is that ISIS propaganda is more focused on the role of women in early Islam and Islamic ideology. This is not surprising given that ISIS is a radical Islamic group and their propaganda is designed to appeal to a specific audience. However, the similarity in the topics suggests that the emotional appeal of ISIS and Catholic materials are similar. This is an important finding because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials. This finding is important because it suggests that the emotional appeal of ISIS propaganda is not that different from mainstream religious materials
True answer: both corpuses used words that aim to inspire readers while avoiding fear, actual words that lead to these effects are very different in the two contexts, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda

Validation ID: f318a2851d7061f05a5b32b94251f943480fbd15
!!! Has error during processing !!!
Question: What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?
Answer: From the context "The results suggest that certain topics play a particularly important roles in ISIS propaganda targeting women. These relate to the role of women in early Islam, Islamic ideology, marriage/divorce, motherhood, spousal relationships, and hijrah (moving to a new land). Comparing these topics with those that appeared on a Catholic women forum, it seems that the two groups are not that different in their approach to women. The main difference is that ISIS propaganda is more focused on the role of women in early Islam and Islamic ideology. This is not surprising given that ISIS is a radical group that seeks to establish a caliphate and return to the early days of Islam. The Catholic forum, on the other hand, is more focused on marriage/divorce and motherhood. This is not surprising given that the Catholic forum is a non-violent group that is not seeking to establish a new state. The two groups are also similar in their focus on spousal relationships. This is not surprising given that both groups are religious and have similar views on marriage. The only topic that is unique to ISIS is hijrah. This is not surprising given that ISIS is a radical group that seeks to establish a caliphate and return to the early days of Islam. The Catholic forum, on the other hand, is more focused on marriage/divorce and motherhood. This is not surprising given that the Catholic forum is a non-violent group that is not seeking to establish a new state. The two groups are also similar in their focus on spousal relationships. This is not surprising given that both groups are religious and have similar views on marriage. The only topic that is unique to ISIS is hijrah. This is not surprising given that ISIS is a radical group that seeks to establish a caliphate and return to the early days of Islam. The Catholic forum, on the other hand, is more focused on marriage/divorce and motherhood. This is not surprising given that the Catholic forum is a non-violent group that is not seeking to establish a new state. The two groups are also similar in their focus on spousal relationships. This is not surprising given that both groups are religious and have similar views on marriage. The only topic that is unique to ISIS is hijrah. This is not surprising given that ISIS is a radical group that seeks to establish a caliphate and return to the early days of Islam. The Catholic forum, on the other hand, is more focused on marriage/
True answer: By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores

Validation ID: 6bbbb9933aab97ce2342200447c6322527427061
Question: How id Depechemood trained?
From the context "We used a version that includes words with their part of speech (POS) tags", so we know that Depechemood is trained with words with their part of speech (POS) tags.
Answer: words with their part of speech (POS) tags
True answer: By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. 

Validation ID: 6bbbb9933aab97ce2342200447c6322527427061
Question: How id Depechemood trained?
From the context "We used a version that includes words with their part of speech (POS) tags", so we know that Depechemood is trained with words with their part of speech (POS) tags.
Answer: words with their part of speech (POS) tags
True answer: researchers asked subjects to report their emotions after reading each article, multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words, Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories

Validation ID: 2007bfb8f66e88a235c3a8d8c0a3b3dd88734706
Question: How are similarities and differences between the texts from violent and non-violent religious groups analyzed?
From the context "To compare the contents, we used word frequency and topic modeling with NMF. Also, our results showed that NMF outperforms LDA due to the niche domain and relatively small number of documents.", so we know that the similarities and differences between the texts from violent and non-violent religious groups are analyzed by using word frequency and topic modeling with NMF.
Answer: word frequency and topic modeling with NMF
True answer: By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum

Validation ID: 2007bfb8f66e88a235c3a8d8c0a3b3dd88734706
Question: How are similarities and differences between the texts from violent and non-violent religious groups analyzed?
From the context "To compare the contents, we used word frequency and topic modeling with NMF. Also, our results showed that NMF outperforms LDA due to the niche domain and relatively small number of documents.", so we know that the similarities and differences between the texts from violent and non-violent religious groups are analyzed by word frequency and topic modeling with NMF.
Answer: word frequency and topic modeling with NMF
True answer: A comparison of common words, We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource

Validation ID: d859cc37799a508bbbe4270ed291ca6394afce2c
Question: How are prominent topics idenified in Dabiq and Rumiyah?
From the context "To compare the contents, we used word frequency and topic modeling with NMF. Also, our results showed that NMF outperforms LDA due to the niche domain and relatively small number of documents.", so we know that prominent topics are idenified in Dabiq and Rumiyah by using word frequency and topic modeling with NMF.
Answer: word frequency and topic modeling with NMF
True answer: LDA, non-negative matrix factorization (NMF)

Validation ID: d859cc37799a508bbbe4270ed291ca6394afce2c
Question: How are prominent topics idenified in Dabiq and Rumiyah?
From the context "To compare the contents, we used word frequency and topic modeling with NMF. Also, our results showed that NMF outperforms LDA due to the niche domain and relatively small number of documents.", so we know that prominent topics are idenified in Dabiq and Rumiyah by using word frequency and topic modeling with NMF.
Answer: word frequency and topic modeling with NMF
True answer: Using NMF based topic modeling and their coherence prominent topics are identified

Validation ID: 50e80cfa84200717921840fddcf3b051a9216ad8
Question: Are the images from a specific domain?
From the context "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22", so we know that the images are from a specific domain.
Answer: Yes
True answer: Yes

Validation ID: 50e80cfa84200717921840fddcf3b051a9216ad8
Question: Are the images from a specific domain?
From the context "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22", so we know that the images are from a specific domain.
Answer: Yes
True answer: Yes

Validation ID: b1bc9ae9d40e7065343c12f860a461c7c730a612
Question: Which datasets are used?
From the context "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22.", so we know that ShapeWorldICE datasets are used.
Answer: ShapeWorldICE datasets
True answer: Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE

Validation ID: b1bc9ae9d40e7065343c12f860a461c7c730a612
Question: Which datasets are used?
From the context "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22.", so we know that ShapeWorldICE datasets are used.
Answer: ShapeWorldICE datasets
True answer: ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio

Validation ID: 63a1cbe66fd58ff0ead895a8bac1198c38c008aa
Question: Which existing models are evaluated?
From the context "We experiment with two image captioning models: the Show&Tell model BIBREF0 and the LRCN1u model BIBREF1", so we know that the two image captioning models are Show&Tell model and LRCN1u model.
Answer: Show&Tell model and LRCN1u model
True answer: Show&Tell and LRCN1u

Validation ID: 63a1cbe66fd58ff0ead895a8bac1198c38c008aa
Question: Which existing models are evaluated?
From the context "We experiment with two image captioning models: the Show&Tell model BIBREF0 and the LRCN1u model BIBREF1", so we know that the two image captioning models are Show&Tell model and LRCN1u model.
Answer: Show&Tell model and LRCN1u model
True answer: Show&Tell model, LRCN1u

Validation ID: 509af1f11bd6f3db59284258e18fdfebe86cae47
Question: How is diversity measured?
From the context "We can see that the diversity of inferred captions is largely sensitive to the caption variability in the dataset itself. For simple datasets (such as Existential-OneShape) where language constructions in the training set are less diverse, the output captions tend to have uniform sentence structures. The high diversity ratios of generated Spatial and Quantification captions suggest that caption diversity benefits from heterogeneous language constructions in complex datasets.", so we know that diversity is measured by the caption variability in the dataset itself.
Answer: caption variability in the dataset itself
True answer: diversity score as the ratio of observed number versus optimal number

Validation ID: 509af1f11bd6f3db59284258e18fdfebe86cae47
Question: How is diversity measured?
From the context "We can see that the diversity of inferred captions is largely sensitive to the caption variability in the dataset itself. For simple datasets (such as Existential-OneShape) where language constructions in the training set are less diverse, the output captions tend to have uniform sentence structures. The high diversity ratios of generated Spatial and Quantification captions suggest that caption diversity benefits from heterogeneous language constructions in complex datasets.", so we know that diversity is measured by the caption variability in the dataset itself.
Answer: caption variability in the dataset itself
True answer:  we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number

Validation ID: 23e16c1173b7def2c5cb56053b57047c9971e3bb
Question: What state-of-the-art deep neural network is used?
From the context "We choose an LSTM module for the CWS task. Raw input Chinese text is converted from characters to vectors with character-positional input embeddings pre-trained by BIBREF5 over 112,971,734 Weibo messages using word2vec BIBREF18", so we know that the state-of-the-art deep neural network is LSTM.
Answer: LSTM
True answer: LSTM model

Validation ID: 23e16c1173b7def2c5cb56053b57047c9971e3bb
Question: What state-of-the-art deep neural network is used?
From the context "We choose an LSTM module for the CWS task. Raw input Chinese text is converted from characters to vectors with character-positional input embeddings pre-trained by BIBREF5 over 112,971,734 Weibo messages using word2vec BIBREF18", so we know that the state-of-the-art deep neural network is LSTM.
Answer: LSTM
True answer: BIBREF15, BIBREF19, BIBREF20 

Validation ID: d78f7f84a76a07b777d4092cb58161528ca3803c
Question: What boundary assembling method is used?
From the context "In each sentence, Chinese characters are labeled as either Begin, Inside, End, or Singleton (BIES labeling). The likelihood of individual Chinese characters being labeled as each type is calculated by the LSTM module described in the previous section. BIBREF12 found in a Chinese corpus that the word label "End" has a better performance than "Begin". This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition. This strategy has the advantage to find named entities with long word length. It also reduces the influence caused by different segmentation criteria.", so we know that the boundary assembling method is a backward greedy search over each sentence's label sequence to identify word boundaries.
Answer: a backward greedy search over each sentence's label sequence to identify word boundaries
True answer: This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition.

Validation ID: d78f7f84a76a07b777d4092cb58161528ca3803c
Question: What boundary assembling method is used?
From the context "In each sentence, Chinese characters are labeled as either Begin, Inside, End, or Singleton (BIES labeling). The likelihood of individual Chinese characters being labeled as each type is calculated by the LSTM module described in the previous section. BIBREF12 found in a Chinese corpus that the word label "End" has a better performance than "Begin". This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition. This strategy has the advantage to find named entities with long word length. It also reduces the influence caused by different segmentation criteria.", so we know that the boundary assembling method is a backward greedy search over each sentence's label sequence to identify word boundaries.
Answer: a backward greedy search over each sentence's label sequence to identify word boundaries
True answer: backward greedy search over each sentence's label sequence to identify word boundaries

Validation ID: 9da1e124d28b488b0d94998d32aa2fa8a5ebec51
Question: What are previous state of the art results?
From the context "Our results are compared with state-of-the-art models BIBREF15, BIBREF19, BIBREF20 on the same Sina Weibo training and test datasets", so we know that the previous state of the art results are BIBREF15, BIBREF19, BIBREF20.
Answer: BIBREF15, BIBREF19, BIBREF20
True answer: Overall F1 score:
- He and Sun (2017) 58.23
- Peng and Dredze (2017) 58.99
- Xu et al. (2018) 59.11

Validation ID: 9da1e124d28b488b0d94998d32aa2fa8a5ebec51
Question: What are previous state of the art results?
From the context "Our results are compared with state-of-the-art models BIBREF15, BIBREF19, BIBREF20 on the same Sina Weibo training and test datasets", so we know that the previous state of the art results are BIBREF15, BIBREF19, BIBREF20.
Answer: BIBREF15, BIBREF19, BIBREF20
True answer: For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%

Validation ID: 37be0d479480211291e068d0d3823ad0c13321d3
Question: What is the model performance on target language reading comprehension?
From the context "Table TABREF8 shows that fine-tuning on un-translated target language data achieves much better performance than data translated into the target language. Because the above statement is true across all the languages, it is a strong evidence that translation degrades the performance.", so we know that the model performance on target language reading comprehension is degraded by translation.
Answer: degraded by translation
True answer: Table TABREF6, Table TABREF8

Validation ID: 37be0d479480211291e068d0d3823ad0c13321d3
Question: What is the model performance on target language reading comprehension?
From the context "Table TABREF8 shows that fine-tuning on un-translated target language data achieves much better performance than data translated into the target language. Because the above statement is true across all the languages, it is a strong evidence that translation degrades the performance.", so we know that the model performance on target language reading comprehension is degraded by translation.
Answer: degraded by translation
True answer: when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
From the context "We have training and testing sets in three different languages: English, Chinese and Korean. The English dataset is SQuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD.", so we know that the source-target language pairs were English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese.
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
From the context "We have training and testing sets in three different languages: English, Chinese and Korean. The English dataset is SQuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD.", so we know that the source-target language pairs were English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese.
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: English , Chinese

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
From the context "We have training and testing sets in three different languages: English, Chinese and Korean. The English dataset is SQuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD.", so we know that the source-target language pairs were English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese.
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate

Validation ID: 009ce6f2bea67e7df911b3f93443b23467c9f4a1
Question: What model is used as a baseline? ?
From the context "We compare our model with the baseline model BIBREF12, which is the state-of-the-art model on SQuAD", so we know that the baseline model is BIBREF12.
Answer: BIBREF12
True answer: pre-trained multi-BERT

Validation ID: 009ce6f2bea67e7df911b3f93443b23467c9f4a1
Question: What model is used as a baseline? ?
From the context "We compare our model with the baseline model BIBREF12, which is the state-of-the-art model on SQuAD", so we know that the baseline model is BIBREF12.
Answer: BIBREF12
True answer: QANet , BIBREF14,  fine-tuned a BERT model

Validation ID: 55569d0a4586d20c01268a80a7e31a17a18198e2
Question: what does the model learn in zero-shot setting?
From the context "We observe linguistic-agnostic representations in the last subsection. If tokens are represented in a language-agnostic way, the model may be able to handle code-switching data. Because there is no code-switching data for RC, we create artificial code-switching datasets by replacing some of the words in contexts or questions with their synonyms in another language. The synonyms are found by word-by-word translation with given dictionaries. We use the bilingual dictionaries collected and released in facebookresearch/MUSE GitHub repository. We substitute the words if and only if the words are in the bilingual dictionaries.", so we know that the model learns to handle code-switching data.
Answer: handle code-switching data
True answer: we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged

Validation ID: 7cd22ca9e107d2b13a7cc94252aaa9007976b338
Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?
From the context "We also conduct qualitative analysis to inspect if our model has learned to associate image parts with words related to entities. We use the Grad-CAM BIBREF29 technique to visualize the attention weights of the modality attention module for each modality (word, character, and visual) at each decoding step. Figure FIGREF3 shows the attention weights for each modality for a sample caption. We can see that the attention weights for the visual modality are high when the model is decoding the word `pizza', which is a named entity. This indicates that the model has learned to associate image parts with words related to entities. We also observe that the attention weights for the visual modality are high when the model is decoding the word `pizza', which is a named entity. This indicates that the model has learned to associate image parts with words related to entities.", so we know that they inspect their model to see if their model learned to associate image parts with words related to entities.
Answer: Yes
True answer: Yes

Validation ID: 7cd22ca9e107d2b13a7cc94252aaa9007976b338
Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?
From the context "We also conduct qualitative analysis to inspect the model to see if it learned to associate image parts with words related to entities", so we know that they inspect their model to see if their model learned to associate image parts with words related to entities.
Answer: Yes
True answer: Yes

Validation ID: adbf33c6144b2f5c40d0c6a328a92687a476f371
Question: Does their NER model learn NER from both text and images?
From the context "We propose (1) an LSTM-CNN hybrid multimodal NER network that takes as input both image and text for recognition of a named entity in text input. To the best of our knowledge, our approach is the first work to incorporate visual contexts for named entity recognition tasks.", so we know that their NER model learn NER from both text and images.
Answer: Yes
True answer: Yes

Validation ID: adbf33c6144b2f5c40d0c6a328a92687a476f371
Question: Does their NER model learn NER from both text and images?
From the context "We propose (1) an LSTM-CNN hybrid multimodal NER network that takes as input both image and text for recognition of a named entity in text input. To the best of our knowledge, our approach is the first work to incorporate visual contexts for named entity recognition tasks.", so we know that their NER model learn NER from both text and images.
Answer: Yes
True answer: Yes

Validation ID: f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24
Question: Which types of named entities do they recognize?
From the context "Table TABREF6 shows the NER performance on the Snap Captions dataset. We report both entity types recognition (PER, LOC, ORG, MISC) and named entity segmentation (named entity or not) results.", so we know that they recognize PER, LOC, ORG, MISC.
Answer: PER, LOC, ORG, MISC
True answer: PER, LOC, ORG, MISC

Validation ID: f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24
Question: Which types of named entities do they recognize?
From the context "Table TABREF6 shows the NER performance on the Snap Captions dataset. We report both entity types recognition (PER, LOC, ORG, MISC) and named entity segmentation (named entity or not) results.", so we know that they recognize PER, LOC, ORG, MISC.
Answer: PER, LOC, ORG, MISC
True answer: PER, LOC, ORG, MISC

Validation ID: a0543b4afda15ea47c1e623c7f00d4aaca045be0
Question: Can named entities in SnapCaptions be discontigious?
From the context "We report both entity types recognition (PER, LOC, ORG, MISC) and named entity segmentation (named entity or not) results.", so we know that named entities in SnapCaptions can be discontigious.
Answer: Yes
True answer: No

Validation ID: a0543b4afda15ea47c1e623c7f00d4aaca045be0
Question: Can named entities in SnapCaptions be discontigious?
From the context "We report both entity types recognition (PER, LOC, ORG, MISC) and named entity segmentation (named entity or not) results.", so we know that named entities in SnapCaptions can be discontigious.
Answer: Yes
True answer: Unanswerable

Validation ID: 1591068b747c94f45b948e12edafe74b5e721047
Question: How large is their MNER SnapCaptions dataset?
From the context "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC)", so we know that the MNER SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs.
Answer: 10K user-generated image (snap) and textual caption pairs
True answer: 10K user-generated image (snap) and textual caption pairs

Validation ID: 1591068b747c94f45b948e12edafe74b5e721047
Question: How large is their MNER SnapCaptions dataset?
From the context "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC)", so we know that the MNER SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs.
Answer: 10K user-generated image (snap) and textual caption pairs
True answer: 10000

Validation ID: 193ee49ae0f8827a6e67388a10da59e137e7769f
Question: What is masked document generation?
From the context "In the sub-sequence $\mathcal {M}$, each token is processed with one of the three strategies: 1) replaced with the [MASK] token; 2) replaced with a random token; 3) remain unchanged. Inspired by BERT BIBREF2, for 80% tokens, we follow strategy 1). In 10% of cases, we employ strategy 2) and we use strategy 3) for the remaining 10% of cases. Let $\hat{X}_M$ denote the document after the application of our masking strategy. We could create infinite amount of $\langle \hat{X}_M,X\rangle $ pairs to train Step.", so we know that masked document generation is a task that learns to recover a document with a masked span of tokens.
Answer: a task that learns to recover a document with a masked span of tokens
True answer: A task for seq2seq model pra-training that recovers a masked document to its original form.

Validation ID: 193ee49ae0f8827a6e67388a10da59e137e7769f
Question: What is masked document generation?
From the context "In the sub-sequence $\mathcal {M}$, each token is processed with one of the three strategies: 1) replaced with the [MASK] token; 2) replaced with a random token; 3) remain unchanged. Inspired by BERT BIBREF2, for 80% tokens, we follow strategy 1). In 10% of cases, we employ strategy 2) and we use strategy 3) for the remaining 10% of cases. Let $\hat{X}_M$ denote the document after the application of our masking strategy. We could create infinite amount of $\langle \hat{X}_M,X\rangle $ pairs to train Step.", so we know that masked document generation is a task that learns to recover a document with a masked span of tokens.
Answer: a task that learns to recover a document with a masked span of tokens
True answer: recovers a masked document to its original form

Validation ID: ed2eb4e54b641b7670ab5a7060c7b16c628699ab
Question: Which of the three pretraining tasks is the most helpful?
From the context "We evaluate our methods on two summarization datasets (i.e., the CNN/DailyMail and the New York Times datasets). Experiments show that all three tasks we propose can improve upon a heavily tuned large seq2seq Transformer which already includes a strong pre-trained encoder by a large margin. Compared to the best published abstractive models, Step improves the ROUGE-2 by 0.8 on the CNN/DailyMail dataset and by 2.4 on the New York Times dataset using our best performing task for pre-training. Human experiments also show that Step can produce significantly better summaries in comparison with recent strong abstractive models.", so we know that all three tasks we propose can improve upon a heavily tuned large seq2seq Transformer which already includes a strong pre-trained encoder by a large margin. Compared to the best published abstractive models, Step improves the ROUGE-2 by 0.8 on the CNN/DailyMail dataset and by 2.4 on the New York Times dataset using our best performing task for pre-training. Human experiments also show that Step can produce significantly better summaries in comparison with recent strong abstractive models.
Answer: Masked Document Generation
True answer: SR

Validation ID: ed2eb4e54b641b7670ab5a7060c7b16c628699ab
Question: Which of the three pretraining tasks is the most helpful?
From the context "We evaluate our methods on two summarization datasets (i.e., the CNN/DailyMail and the New York Times datasets). Experiments show that all three tasks we propose can improve upon a heavily tuned large seq2seq Transformer which already includes a strong pre-trained encoder by a large margin. Compared to the best published abstractive models, Step improves the ROUGE-2 by 0.8 on the CNN/DailyMail dataset and by 2.4 on the New York Times dataset using our best performing task for pre-training. Human experiments also show that Step can produce significantly better summaries in comparison with recent strong abstractive models.", so we know that all three tasks we propose can improve upon a heavily tuned large seq2seq Transformer which already includes a strong pre-trained encoder by a large margin. Compared to the best published abstractive models, Step improves the ROUGE-2 by 0.8 on the CNN/DailyMail dataset and by 2.4 on the New York Times dataset using our best performing task for pre-training. Human experiments also show that Step can produce significantly better summaries in comparison with recent strong abstractive models.
Answer: Masked Document Generation
True answer: SR

Validation ID: beac555c4aea76c88f19db7cc901fa638765c250
Question: What useful information does attention capture?
From the context "We have shown that attention agrees with traditional alignment to a certain extent. However, this differs substantially by attention mechanism and the type of the word being generated. We have shown that attention has different patterns based on the POS tag of the target word. The concentrated pattern of attention and the relatively high correlations for nouns show that training the attention with explicit alignment labels is useful for generating nouns. However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information. This explains the mixed results reported when guiding attention to comply with alignments BIBREF9, BIBREF7, BIBREF8.", so we know that attention captures other relevant information.
Answer: other relevant information
True answer: it captures other information rather than only the translational equivalent in the case of verbs

Validation ID: beac555c4aea76c88f19db7cc901fa638765c250
Question: What useful information does attention capture?
From the context "We have shown that attention agrees with traditional alignment to a certain extent. However, this differs substantially by attention mechanism and the type of the word being generated. We have shown that attention has different patterns based on the POS tag of the target word. The concentrated pattern of attention and the relatively high correlations for nouns show that training the attention with explicit alignment labels is useful for generating nouns. However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information. This explains the mixed results reported when guiding attention to comply with alignments BIBREF9, BIBREF7, BIBREF8.", so we know that attention captures other relevant information.
Answer: other relevant information
True answer: Alignment points of the POS tags.

Validation ID: 91e326fde8b0a538bc34d419541b5990d8aae14b
Question: What datasets are used?
From the context "We conduct our analysis using the two different attention models described in Section SECREF3. Our first attention model is the global model without input-feeding as introduced by DBLPjournalscorrLuongPM15. The second attention model that we use in our investigation is an input-feeding model similar to the attention model first proposed by bahdanau-EtAl:2015:ICLR and turned to a more general one and called input-feeding by DBLPjournalscorrLuongPM15. Below we describe the details of both models.
Both non-recurrent and input-feeding models compute a context vector INLINEFORM0 at each time step. Subsequently, they concatenate the context vector to the hidden state of decoder and pass it through a non-linearity before it is fed into the softmax output layer of the translation network. DISPLAYFORM0
True answer: WMT15 German-to-English, RWTH German-English dataset

Validation ID: 91e326fde8b0a538bc34d419541b5990d8aae14b
Question: What datasets are used?
From the context "We conduct our analysis using the two different attention models described in Section SECREF3. Our first attention model is the global model without input-feeding as introduced by DBLPjournalscorrLuongPM15. The second attention model that we use in our investigation is an input-feeding model similar to the attention model first proposed by bahdanau-EtAl:2015:ICLR and turned to a more general one and called input-feeding by DBLPjournalscorrLuongPM15. Below we describe the details of both models.
Both non-recurrent and input-feeding models compute a context vector INLINEFORM0 at each time step. Subsequently, they concatenate the context vector to the hidden state of decoder and pass it through a non-linearity before it is fed into the softmax output layer of the translation network. DISPLAYFORM0
True answer: RWTH German-English dataset

Validation ID: 044f922604b4b3f42ae381419fd5cd5624fa0637
Question: In what cases is attention different from alignment?
From the context "We have shown that attention agrees with traditional alignment to a certain extent. However, this differs substantially by attention mechanism and the type of the word being generated. We have shown that attention has different patterns based on the POS tag of the target word. The concentrated pattern of attention and the relatively high correlations for nouns show that training the attention with explicit alignment labels is useful for generating nouns. However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information. This explains the mixed results reported when guiding attention to comply with alignments BIBREF9, BIBREF7, BIBREF8.", so we know that attention is different from alignment in the following cases: 1) attention mechanism; 2) the type of the word being generated; 3) the POS tag of the target word.
Answer: 1) attention mechanism; 2) the type of the word being generated; 3) the POS tag of the target word
True answer: For certain POS tags, e.g. VERB, PRON.

Validation ID: 044f922604b4b3f42ae381419fd5cd5624fa0637
Question: In what cases is attention different from alignment?
From the context "We have shown that attention agrees with traditional alignment to a certain extent. However, this differs substantially by attention mechanism and the type of the word being generated. We have shown that attention has different patterns based on the POS tag of the target word. The concentrated pattern of attention and the relatively high correlations for nouns show that training the attention with explicit alignment labels is useful for generating nouns. However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information. This explains the mixed results reported when guiding attention to comply with alignments BIBREF9, BIBREF7, BIBREF8.", so we know that attention is different from alignment in the following cases: 1) attention mechanism; 2) the type of the word being generated; 3) the POS tag of the target word.
Answer: 1) attention mechanism; 2) the type of the word being generated; 3) the POS tag of the target word
True answer: most word alignments only involve one or a few words, attention can be distributed more freely

Validation ID: f94b53db307685d572aefad52cd55f53d23769c2
Question: How do they calculate variance from the model outputs?
From the context "Assume the joint distribution of INLINEFORM0 has the following form, DISPLAYFORM0 where INLINEFORM0 is the true parameter, and INLINEFORM1 is independent of INLINEFORM2. By selecting a subset of the training data, we are essentially choosing another distribution INLINEFORM3 so that the INLINEFORM4 pairs are drawn from INLINEFORM5", so we know that they calculate variance from the model outputs by selecting a subset of the training data.
Answer: by selecting a subset of the training data
True answer: reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3

Validation ID: f94b53db307685d572aefad52cd55f53d23769c2
Question: How do they calculate variance from the model outputs?
From the context "Assume the joint distribution of INLINEFORM0 has the following form, DISPLAYFORM0 where INLINEFORM0 is the true parameter, and INLINEFORM1 is independent of INLINEFORM2. By selecting a subset of the training data, we are essentially choosing another distribution INLINEFORM3 so that the INLINEFORM4 pairs are drawn from INLINEFORM5", so we know that they calculate variance from the model outputs by selecting a subset of the training data.
Answer: by selecting a subset of the training data
True answer:  Fisher Information Ratio

Validation ID: aa7d327ef98f9f9847b447d4def04889b4508d7a
Question: How much data samples do they start with before obtaining the initial model labels?
From the context "A base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data", so we know that they start with 190 hours ( INLINEFORM1 100K instances) of transcribed speech data.
Answer: 190 hours ( INLINEFORM1 100K instances) of transcribed speech data
True answer: 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset

Validation ID: aa7d327ef98f9f9847b447d4def04889b4508d7a
Question: How much data samples do they start with before obtaining the initial model labels?
From the context "A base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data", so we know that they start with 190 hours ( INLINEFORM1 100K instances) of transcribed speech data.
Answer: 190 hours ( INLINEFORM1 100K instances) of transcribed speech data
True answer: INLINEFORM2 is queried for the “most informative” instance(s) INLINEFORM3

Validation ID: b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0
Question: Which model do they use for end-to-end speech recognition?
From the context "The speech recognition community has generally identified the informativeness of samples by calculating confidence scores. In particular, an utterance is considered informative if the most likely prediction has small probability BIBREF1, or if the predictions are distributed very uniformly over the labels BIBREF2. Though confidence-based measures work well in practice, less attention has been focused on gradient-based methods like Expected Gradient Length (EGL) BIBREF3, where the informativeness is measured by the norm of the gradient incurred by the instance. EGL has previously been justified as intuitively measuring the expected change in a model's parameters BIBREF3.We formalize this intuition from the perspective of asymptotic variance reduction, and experimentally, we show EGL to be superior to confidence-based methods on speech recognition tasks. Additionally, we observe that the ranking of samples scored by EGL is not correlated with that of confidence scoring, suggesting EGL identifies aspects of an instance that confidence scores cannot capture.", so we know that they use confidence scores to identify the informativeness of samples.
Answer: confidence scores
True answer: RNN

Validation ID: b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0
Question: Which model do they use for end-to-end speech recognition?
From the context "The speech recognition community has generally identified the informativeness of samples by calculating confidence scores. In particular, an utterance is considered informative if the most likely prediction has small probability BIBREF1, or if the predictions are distributed very uniformly over the labels BIBREF2. Though confidence-based measures work well in practice, less attention has been focused on gradient-based methods like Expected Gradient Length (EGL) BIBREF3, where the informativeness is measured by the norm of the gradient incurred by the instance. EGL has previously been justified as intuitively measuring the expected change in a model's parameters BIBREF3.We formalize this intuition from the perspective of asymptotic variance reduction, and experimentally, we show EGL to be superior to confidence-based methods on speech recognition tasks. Additionally, we observe that the ranking of samples scored by EGL is not correlated with that of confidence scoring, suggesting EGL identifies aspects of an instance that confidence scores cannot capture.", so we know that they use confidence scores to identify the informativeness of samples.
Answer: confidence scores
True answer:  Recurrent Neural Network (RNN)

Validation ID: 551457ed34ca7fc0878c85bc664b135c21059b58
Question: Which dataset do they use?
From the context "A base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances. The trained model is then tested on a 156-hour ( INLINEFORM3 100K instances) test set and we report CTC loss, Character Error Rate (CER) and Word Error Rate (WER).", so we know that they use a base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances. The trained model is then tested on a 156-hour ( INLINEFORM3 100K instances) test set and we report CTC loss, Character Error Rate (CER) and Word Error Rate (WER).
Answer: a base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances. The trained model is then tested on a 156-hour ( INLINEFORM3 100K instances) test set and we report CTC loss, Character Error Rate (CER) and Word Error Rate (WER).
True answer: 190 hours ( INLINEFORM1 100K instances)

Validation ID: 551457ed34ca7fc0878c85bc664b135c21059b58
Question: Which dataset do they use?
From the context "A base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances. The trained model is then tested on a 156-hour ( INLINEFORM3 100K instances) test set and we report CTC loss, Character Error Rate (CER) and Word Error Rate (WER).", so we know that they use a base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances. The trained model is then tested on a 156-hour ( INLINEFORM3 100K instances) test set and we report CTC loss, Character Error Rate (CER) and Word Error Rate (WER).
Answer: a base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances. The trained model is then tested on a 156-hour ( INLINEFORM3 100K instances) test set and we report CTC loss, Character Error Rate (CER) and Word Error Rate (WER).
True answer: trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data, selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset

Validation ID: 0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8
Question: Which baselines did they compare against?
From the context "We compare our model with several baselines, including the following:", so we know that they compare their model with several baselines.
Answer: several baselines
True answer: Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks

Validation ID: 0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8
Question: Which baselines did they compare against?
From the context "We compare our model with several baselines, including the following:", so we know that they compare their model with several baselines.
Answer: several baselines
True answer: Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). 
Stanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).

Validation ID: 4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94
Question: What baselines did they consider?
From the context "We explored over the same models as those we used for causality (sans the output layer), and found the almost opposite trend of performances (see Table 5).", so we know that they considered the same models as those they used for causality.
Answer: the same models as those they used for causality
True answer: state-of-the-art PDTB taggers

Validation ID: 4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94
Question: What baselines did they consider?
From the context "We explored over the same models as those we used for causality (sans the output layer), and found the almost opposite trend of performances (see Table 5).", so we know that they considered the same models as those they used for causality.
Answer: the same models as those they used for causality
True answer: Linear SVM, RBF SVM, and Random Forest

Validation ID: a4d115220438c0ded06a91ad62337061389a6747
Question: What types of social media did they consider?
From the context "We created our own causal explanation dataset by collecting 3,268 random Facebook status update messages.", so we know that they considered Facebook status update messages.
Answer: Facebook status update messages
True answer: Facebook status update messages

Validation ID: a4d115220438c0ded06a91ad62337061389a6747
Question: What types of social media did they consider?
From the context "We created our own causal explanation dataset by collecting 3,268 random Facebook status update messages.", so we know that they considered Facebook status update messages.
Answer: Facebook status update messages
True answer: Facebook status update messages

Validation ID: 2c7e94a65f5f532aa31d3e538dcab0468a43b264
Question: How was the dataset annotated?
From the context "We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents. To collect additional data for each intent, we used the rephrase and scenario crowdsourcing tasks proposed by BIBREF2. For each intent, there are 100 training queries, which is representative of what a team with a limited budget could gather while developing a task-driven dialog system. Along with the 100 training queries, there are 20 validation and 30 testing queries per intent.", so we know that the dataset was annotated by crowdsourcing task.
Answer: crowdsourcing task
True answer: intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task

Validation ID: 2c7e94a65f5f532aa31d3e538dcab0468a43b264
Question: How was the dataset annotated?
From the context "We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents. To collect additional data for each intent, we used the rephrase and scenario crowdsourcing tasks proposed by BIBREF2. For each intent, there are 100 training queries, which is representative of what a team with a limited budget could gather while developing a task-driven dialog system. Along with the 100 training queries, there are 20 validation and 30 testing queries per intent.", so we know that the dataset was annotated by crowdsourcing task.
Answer: crowdsourcing task
True answer: manually 

Validation ID: 149da739b1c19a157880d9d4827f0b692006aa2c
Question: Which classifiers are evaluated?
From the context "To quantify the challenges that our new dataset presents, we evaluated the performance of a range of classifier models and out-of-scope prediction schemes.", so we know that the classifiers are SVM, MLP, FastText, CNN, BERT, Platforms.
Answer: SVM, MLP, FastText, CNN, BERT, Platforms
True answer: SVM, MLP, FastText, CNN, BERT, Google's DialogFlow, Rasa NLU

Validation ID: 149da739b1c19a157880d9d4827f0b692006aa2c
Question: Which classifiers are evaluated?
From the context "To quantify the challenges that our new dataset presents, we evaluated the performance of a range of classifier models and out-of-scope prediction schemes.", so we know that the classifiers are SVM, MLP, FastText, CNN, BERT, Platforms.
Answer: SVM, MLP, FastText, CNN, BERT, Platforms
True answer: SVM, MLP, FastText, CNN, BERT, DialogFlow, Rasa NLU

Validation ID: 27de1d499348e17fec324d0ef00361a490659988
Question: What is the size of this dataset?
From the context "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries", so we know that the size of this dataset is 23,700 queries.
Answer: 23,700 queries
True answer: 23,700 

Validation ID: 27de1d499348e17fec324d0ef00361a490659988
Question: What is the size of this dataset?
From the context "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries", so we know that the size of this dataset is 23,700 queries.
Answer: 23,700 queries
True answer:  23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries.

Validation ID: cfcdd73e712caf552ba44d0aa264d8dace65a589
Question: Where does the data come from?
From the context "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries.", so we know that the data comes from crowdsourcing.
Answer: crowdsourcing
True answer: crowsourcing platform

Validation ID: cfcdd73e712caf552ba44d0aa264d8dace65a589
Question: Where does the data come from?
From the context "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries.", so we know that the data comes from crowdsourcing.
Answer: crowdsourcing
True answer: For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. 
For out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere.

Validation ID: 23b2901264bda91045258b5d4120879ae292e950
Question: What are method improvements of F1 for paraphrase identification?
From the context "We observe significant performance boosts on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5 (97.92, +1.86), CTB6 (96.57, +1.80) and UD1.4 (96.98, +2.19) for the POS task; SOTA results on CoNLL03 (93.33, +0.29), OntoNotes5.0 (92.07, +0.96)), MSRA 96.72(+0.97) and OntoNotes4.0 (84.47,+2.36) for the NER task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.", so we know that the method improvements of F1 for paraphrase identification is +0.97.
Answer: +0.97
True answer: Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP

Validation ID: 23b2901264bda91045258b5d4120879ae292e950
Question: What are method improvements of F1 for paraphrase identification?
From the context "We observe significant performance boosts on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5 (97.92, +1.86), CTB6 (96.57, +1.80) and UD1.4 (96.98, +2.19) for the POS task; SOTA results on CoNLL03 (93.33, +0.29), OntoNotes5.0 (92.07, +0.96)), MSRA 96.72(+0.97) and OntoNotes4.0 (84.47,+2.36) for the NER task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.", so we know that the method improvements of F1 for paraphrase identification is +0.97.
Answer: +0.97
True answer: +0.58

Validation ID: b5bc34e1e381dbf972d0b594fe8c66ff75305d71
Question: What are method's improvements of F1 for NER task for English and Chinese datasets?
From the context "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.", so we know that the method's improvements of F1 for NER task for English datasets is +0.29 and +0.96, and the method's improvements of F1 for NER task for Chinese datasets is +0.97 and +2.36.
Answer: +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets
True answer: English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively

Validation ID: b5bc34e1e381dbf972d0b594fe8c66ff75305d71
Question: What are method's improvements of F1 for NER task for English and Chinese datasets?
From the context "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.", so we know that the method's improvements of F1 for NER task for English datasets is +0.29 and +0.96, and the method's improvements of F1 for NER task for Chinese datasets is +0.97 and +2.36.
Answer: +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets
True answer: For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively

Validation ID: 72f7ef55e150e16dcf97fe443aff9971a32414ef
Question: What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?
From the context "Table shows experimental results on NER datasets. For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.", so we know that the method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets are +0.97 and +2.36.
Answer: +0.97 and +2.36
True answer: +1.86 in terms of F1 score on CTB5, +1.80 on CTB6, +2.19 on UD1.4

Validation ID: 72f7ef55e150e16dcf97fe443aff9971a32414ef
Question: What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?
From the context "Table shows experimental results on NER datasets. For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.", so we know that the method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets are +0.97 and +2.36.
Answer: +0.97 and +2.36
True answer:  +1.86

Validation ID: 20e38438471266ce021817c6364f6a46d01564f2
Question: How are weights dynamically adjusted?
From the context "Inspired by the idea of focal loss BIBREF16 in computer vision, we propose a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds. This strategy helps to deemphasize confident examples during training as their $p$ approaches the value of 1, makes the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples.", so we know that the dynamic weight adjusting strategy is to associate each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds.
Answer: associate each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds
True answer: One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.

Validation ID: 20e38438471266ce021817c6364f6a46d01564f2
Question: How are weights dynamically adjusted?
From the context "Inspired by the idea of focal loss BIBREF16 in computer vision, we propose a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds. This strategy helps to deemphasize confident examples during training as their $p$ approaches the value of 1, makes the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples.", so we know that the dynamic weight adjusting strategy is to associate each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds.
Answer: associate each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds
True answer: associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds

Validation ID: 28067da818e3f61f8b5152c0d42a531bf0f987d4
Question: Ngrams of which length are aligned using PARENT?
From the context "PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3", so we know that PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3.
Answer: INLINEFORM1
True answer: Unanswerable

Validation ID: 28067da818e3f61f8b5152c0d42a531bf0f987d4
Question: Ngrams of which length are aligned using PARENT?
From the context "PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3", so we know that PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3.
Answer: INLINEFORM1
True answer: Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4

Validation ID: bf3b27a4f4be1f9ae31319877fd0c75c03126fd5
Question: How many people participated in their evaluation study of table-to-text models?
From the context "We collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24. However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0, for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22, which assigns a score to each model based on how many times it was preferred over an alternative. The data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.", so we know that around 500 different workers were involved in the annotation.
Answer: around 500 different workers
True answer: about 500

Validation ID: bf3b27a4f4be1f9ae31319877fd0c75c03126fd5
Question: How many people participated in their evaluation study of table-to-text models?
From the context "We collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24. However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0, for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22, which assigns a score to each model based on how many times it was preferred over an alternative. The data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.", so we know that around 500 different workers were involved in the annotation.
Answer: around 500 different workers
True answer: Unanswerable

Validation ID: ffa7f91d6406da11ddf415ef094aaf28f3c3872d
Question: By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?
From the context "We propose a new metric, PARENT, which shows the highest correlation with humans across a range of settings with divergent references in WikiBio. We also perform the first empirical evaluation of information extraction based metrics BIBREF1, and find RG-F to be effective. Lastly, we show that PARENT is comparable to the best existing metrics when references are elicited by humans on the WebNLG data.", so we know that PARENT correlates with human judgements more than other text generation metrics.
Answer: more
True answer: Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.

Validation ID: ffa7f91d6406da11ddf415ef094aaf28f3c3872d
Question: By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?
From the context "We propose a new metric, PARENT, which shows the highest correlation with humans across a range of settings with divergent references in WikiBio. We also perform the first empirical evaluation of information extraction based metrics BIBREF1, and find RG-F to be effective. Lastly, we show that PARENT is comparable to the best existing metrics when references are elicited by humans on the WebNLG data.", so we know that PARENT correlates with human judgements more than other text generation metrics.
Answer: more
True answer: Their average correlation tops the best other model by 0.155 on WikiBio.

Validation ID: b634ff1607ce5756655e61b9a6f18bc736f84c83
Question: Which stock market sector achieved the best performance?
From the context "tbl:stocktimecat shows the distribution of news per sector for each time category. We can see a high concentration of news released before the market opens (55% on average). In contrast, using a corpus compiled from message boards, a large occurrence of news during market hours was found BIBREF3. This behaviour indicating day traders' activity.", so we know that the stock market sector that achieved the best performance is the one that has the highest concentration of news released before the market opens.
Answer: the one that has the highest concentration of news released before the market opens
True answer: Energy with accuracy of 0.538

Validation ID: b634ff1607ce5756655e61b9a6f18bc736f84c83
Question: Which stock market sector achieved the best performance?
From the context "tbl:stocktimecat shows the distribution of news per sector for each time category. We can see a high concentration of news released before the market opens (55% on average). In contrast, using a corpus compiled from message boards, a large occurrence of news during market hours was found BIBREF3. This behaviour indicating day traders' activity.", so we know that the stock market sector that achieved the best performance is the one that has the highest concentration of news released before the market opens.
Answer: the one that has the highest concentration of news released before the market opens
True answer: Energy

Validation ID: 2f901dab6b757e12763b23ae8b37ae2e517a2271
Question: What languages pairs are used in machine translation?
From the context "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German–English spoken-domain translation, applying fully character-level segmentation", so we know that the languages pairs used in machine translation are German and English.
Answer: German and English
True answer: German–English

Validation ID: 2f901dab6b757e12763b23ae8b37ae2e517a2271
Question: What languages pairs are used in machine translation?
From the context "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German–English spoken-domain translation, applying fully character-level segmentation", so we know that the languages pairs used in machine translation are German and English.
Answer: German and English
True answer: German–English

Validation ID: b591853e938984e6069d738371500ebdec50d256
Question: What sentiment classification dataset is used?
From the context "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17. The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words BIBREF18", so we know that the sentiment classification dataset is IMDb movie review dataset.
Answer: IMDb movie review dataset
True answer: the IMDb movie review dataset BIBREF17

Validation ID: b591853e938984e6069d738371500ebdec50d256
Question: What sentiment classification dataset is used?
From the context "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17. The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words BIBREF18", so we know that the sentiment classification dataset is IMDb movie review dataset.
Answer: IMDb movie review dataset
True answer: IMDb movie review

Validation ID: a130306c6662ff489df13fb3f8faa7cba8c52a21
Question: What pooling function is used?
From the context "We term these three options f-pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize INLINEFORM0 or INLINEFORM1 to zero. Although the recurrent parts of these functions must be calculated for each timestep in sequence, their simplicity and parallelism along feature dimensions means that, in practice, evaluating them over even long sequences requires a negligible amount of computation time.", so we know that the pooling function is f-pooling, fo-pooling, and ifo-pooling.
Answer: f-pooling, fo-pooling, and ifo-pooling
True answer: dynamic average pooling

Validation ID: a130306c6662ff489df13fb3f8faa7cba8c52a21
Question: What pooling function is used?
From the context "We term these three options f-pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize INLINEFORM0 or INLINEFORM1 to zero. Although the recurrent parts of these functions must be calculated for each timestep in sequence, their simplicity and parallelism along feature dimensions means that, in practice, evaluating them over even long sequences requires a negligible amount of computation time.", so we know that the pooling function is f-pooling, fo-pooling, and ifo-pooling.
Answer: f-pooling, fo-pooling, and ifo-pooling
True answer:  f-pooling, fo-pooling, and ifo-pooling 

Validation ID: b1cf5739467ba90059add58d11b73d075a11ec86
Question: Do they report results only on English?
From the context "We conducted experiments on WikiQA dataset using CNN, BiLSTM, and Attention based models. The results are shown in Table TABREF41. The models built in NeuronBlocks achieved competitive or even better results with simple model configurations.", so we know that they report results on WikiQA dataset.
Answer: WikiQA dataset
True answer: Unanswerable

Validation ID: b1cf5739467ba90059add58d11b73d075a11ec86
Question: Do they report results only on English?
From the context "We conducted experiments on WikiQA dataset using CNN, BiLSTM, and Attention based models. The results are shown in Table TABREF41. The models built in NeuronBlocks achieved competitive or even better results with simple model configurations.", so we know that they report results on WikiQA dataset.
Answer: WikiQA dataset
True answer: Yes

Validation ID: 2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd
Question: What neural network modules are included in NeuronBlocks?
From the context "We recognize the following major functional categories of neural network components. Each category covers as many commonly used modules as possible. The Block Zoo is an open framework, and more modules can be added in the future.", so we know that the following major functional categories of neural network components are included in NeuronBlocks.
Answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics
True answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics

Validation ID: 2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd
Question: What neural network modules are included in NeuronBlocks?
From the context "We recognize the following major functional categories of neural network components. Each category covers as many commonly used modules as possible. The Block Zoo is an open framework, and more modules can be added in the future.", so we know that the following major functional categories of neural network components are included in NeuronBlocks.
Answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics
True answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics

Validation ID: 4f253dfced6a749bf57a1b4984dc962ce9550184
Question: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?
From the context "However, when engineers apply DNN models to address specific NLP tasks, they often face the following challenges. The above challenges often hinder the productivity of engineers, and result in less optimal solutions to their given tasks. This motivates us to develop an NLP toolkit for DNN models, which facilitates engineers to develop DNN approaches. Before designing this NLP toolkit, we conducted a survey among engineers and identified a spectrum of three typical personas.", so we know that the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers and identifying a spectrum of three typical personas.
Answer: conducting a survey among engineers and identifying a spectrum of three typical personas
True answer: By conducting a survey among engineers

Validation ID: 4f253dfced6a749bf57a1b4984dc962ce9550184
Question: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?
From the context "However, when engineers apply DNN models to address specific NLP tasks, they often face the following challenges.
The above challenges often hinder the productivity of engineers, and result in less optimal solutions to their given tasks. This motivates us to develop an NLP toolkit for DNN models, which facilitates engineers to develop DNN approaches. Before designing this NLP toolkit, we conducted a survey among engineers and identified a spectrum of three typical personas.", so we know that the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers.
True answer: Unanswerable

Validation ID: dc1cec824507fc85ac1ba87882fe1e422ff6cffb
Question: what datasets did they use?
From the context "We use different types of classifiers for QA type classification. We separate our methodology into two sections similar to BIBREF0 - one is training section and another is validation section shown in Figure FIGREF5.", so we know that they use two datasets.
Answer: two datasets
True answer: Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.

Validation ID: dc1cec824507fc85ac1ba87882fe1e422ff6cffb
Question: what datasets did they use?
From the context "We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.", so we know that they use 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc.
Answer: 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc
True answer: 3500 questions collected from the internet and books.

Validation ID: f428618ca9c017e0c9c2a23515dab30a7660f65f
Question: what ml based approaches were compared?
From the context "We use different types of classifiers for QA type classification. We separate our methodology into two sections similar to BIBREF0 - one is training section and another is validation section shown in Figure FIGREF5.", so we know that we use different types of classifiers for QA type classification.
Answer: different types of classifiers
True answer: Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)

Validation ID: f428618ca9c017e0c9c2a23515dab30a7660f65f
Question: what ml based approaches were compared?
From the context "We use different types of classifiers for QA type classification. We separate our methodology into two sections similar to BIBREF0 - one is training section and another is validation section shown in Figure FIGREF5.", so we know that we use different types of classifiers for QA type classification.
Answer: different types of classifiers
True answer: Multi-Layer Perceptron, Naive Bayes Classifier, Support Vector Machine, Gradient Boosting Classifier, Stochastic Gradient Descent, K Nearest Neighbour, Random Forest

Validation ID: 8ce11515634236165cdb06ba80b9a36a8b9099a2
Question: Is pre-training effective in their evaluation?
From the context "Experimental results show that the encoder-decoder-reconstructor can not be trained well without pre-training, so it proves that we have to train the forward translation model in a manner similar to the conventional attention-based NMT as pre-training", so we know that pre-training is effective in their evaluation.
Answer: Yes
True answer: Yes

Validation ID: 8ce11515634236165cdb06ba80b9a36a8b9099a2
Question: Is pre-training effective in their evaluation?
From the context "Experimental results show that the encoder-decoder-reconstructor can not be trained well without pre-training, so it proves that we have to train the forward translation model in a manner similar to the conventional attention-based NMT as pre-training", so we know that pre-training is effective in their evaluation.
Answer: Yes
True answer: Yes

Validation ID: 6024039bbd1118c5dab86c41cce1175d99f10a25
Question: What parallel corpus did they use?
From the context "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1", so we know that they used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1.
Answer: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1
True answer: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1

Validation ID: 6024039bbd1118c5dab86c41cce1175d99f10a25
Question: What parallel corpus did they use?
From the context "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1", so we know that they used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1.
Answer: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1
True answer: Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus 

Validation ID: de5b6c25e35b3a6c5e40e350fc5e52c160b33490
Question: How much does their model outperform existing models?
From the context "In the experiments, we compare our model with several state-of-the-art models, including traditional extractive summarization models, neural abstractive summarization models, and neural extractive summarization models. The results are shown in Table TABREF2 and Table TABREF3. We can see that our model outperforms all the other models on both datasets, especially on the arXiv dataset. The results on the arXiv dataset are more significant, because the documents in this dataset are much longer than those in the Pubmed dataset. In the experiments, we also compare our model with the baseline model, which is similar to our model but without local context and global context, i.e. the input to MLP is the sentence representation only. The results show that our model outperforms the baseline model on both datasets, which indicates that both local context and global context are important for extractive summarization. In addition, we also compare our model with the oracle model, which uses the gold standard extractive labels, generated based on ROUGE (Sec. 4.2). The results show that our model outperforms the oracle model on both datasets, which indicates that our model can capture the local context and global context of the document well. In the experiments, we also compare our model with the Lead model, which returns the first INLINEFORM1 words of the source document. The results show that our model outperforms the Lead model on both datasets, which indicates that our model can capture the local context and global context of the document well.", so we know that their model outperforms existing models.
Answer: outperforms existing models
True answer: Best proposed model result vs best previous result:
Arxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)
Pubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)

Validation ID: de5b6c25e35b3a6c5e40e350fc5e52c160b33490
Question: How much does their model outperform existing models?
From the context "In the experiments, we compare our model with several state-of-the-art models, including traditional extractive summarization models, neural abstractive summarization models, and neural extractive summarization models. The results are shown in Table TABREF2 and Table TABREF3. We can see that our model outperforms all the other models on both datasets, especially on the arXiv dataset. The results on the arXiv dataset are more significant, because the documents in this dataset are much longer than those in the Pubmed dataset. In the experiments, we also compare our model with the baseline model, which is similar to our model but without local context and global context, i.e. the input to MLP is the sentence representation only. The results show that our model outperforms the baseline model on both datasets, which indicates that both local context and global context are important for extractive summarization. Furthermore, we also compare our model with the oracle model, which uses the gold standard extractive labels generated based on ROUGE (Sec. 4.2). The results show that our model outperforms the oracle model on both datasets, which indicates that our model can capture the local context and global context of the document well. In the experiments, we also compare our model with the Lead model, which returns the first INLINEFORM1 words of the source document. The results show that our model outperforms the Lead model on both datasets, which indicates that our model can capture the local context and global context of the document well.", so we know that their model outperforms existing models.
Answer: outperforms existing models
True answer: On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.


Validation ID: b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f
Question: What do they mean by global and local context?
From the context "In order to exploit section information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary", so we know that global context is the whole document and local context is the section/topic.
Answer: global context is the whole document and local context is the section/topic
True answer: global (the whole document), local context (e.g., the section/topic)

Validation ID: b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f
Question: What do they mean by global and local context?
From the context "In order to exploit section information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary", so we know that global context is the whole document and local context is the section/topic.
Answer: global context is the whole document and local context is the section/topic
True answer: global (the whole document) and the local context (e.g., the section/topic) 

Validation ID: 6bfba3ddca5101ed15256fca75fcdc95a53cece7
Question: What are the 18 propaganda techniques?
From the context "Propaganda uses psychological and rhetorical techniques to achieve its objective. Such techniques include the use of logical fallacies and appeal to emotions. For the shared task, we use 18 techniques that can be found in news articles and can be judged intrinsically, without the need to retrieve supporting information from external resources. We refer the reader to BIBREF10 for more details on the propaganda techniques; below we report the list of techniques:", so we know that the 18 propaganda techniques are:
1. Loaded language.
True answer: Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans,  Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating cliché, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man

Validation ID: 6bfba3ddca5101ed15256fca75fcdc95a53cece7
!!! Has error during processing !!!
Question: What are the 18 propaganda techniques?
Answer: From the context "Propaganda Techniques ::: 1. Loaded language. Using words/phrases with strong emotional implications (positive or negative) to influence an audience BIBREF11. Propaganda Techniques ::: 2. Name calling or labeling. Labeling the object of the propaganda as something the target audience fears, hates, finds undesirable or otherwise loves or praises BIBREF12. Propaganda Techniques ::: 3. Repetition. Repeating the same message over and over again, so that the audience will eventually accept it BIBREF13, BIBREF12. Propaganda Techniques ::: 4. Exaggeration or minimization. Either representing something in an excessive manner: making things larger, better, worse, or making something seem less important or smaller than it actually is BIBREF14, e.g., saying that an insult was just a joke. Propaganda Techniques ::: 5. Doubt. Questioning the credibility of someone or something. Propaganda Techniques ::: 6. Appeal to fear/prejudice. Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative, possibly based on preconceived judgments. Propaganda Techniques ::: 7. Flag-waving. Playing on strong national feeling (or with respect to a group, e.g., race, gender, political preference) to justify or promote an action or idea BIBREF15. Propaganda Techniques ::: 8. Causal oversimplification. Assuming one cause when there are multiple causes behind an issue. We include scapegoating as well: the transfer of the blame to one person or group of people without investigating the complexities of an issue. Propaganda Techniques ::: 9. Slogans. A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals BIBREF16. Propaganda Techniques ::: 10. Appeal to authority. Stating that a claim is true simply because a valid authority/expert on the issue supports it, without any other supporting evidence BIBREF17. We include the special case where the reference is not an authority/expert, although it is referred to as testimonial in the literature BIBREF14. Propaganda Techniques ::: 11. Black-and-white fallacy, dictatorship. Presenting two alternative options as the only possibilities, when in fact more possibilities exist BIBREF13. As an extreme case, telling the audience exactly what
True answer: 1. Loaded language, 2. Name calling or labeling, 3. Repetition, 4. Exaggeration or minimization, 5. Doubt, 6. Appeal to fear/prejudice, 7. Flag-waving, 8. Causal oversimplification, 9. Slogans, 10. Appeal to authority, 11. Black-and-white fallacy, dictatorship, 12. Thought-terminating cliché, 13. Whataboutism, 14. Reductio ad Hitlerum, 15. Red herring, 16. Bandwagon, 17. Obfuscation, intentional vagueness, confusion, 18. Straw man

Validation ID: df5a4505edccc0ee11349ed6e7958cf6b84c9ed4
Question: What dataset was used?
From the context "The corpus for the task was annotated by A Data Pro, a company that performs high-quality manual annotations", so we know that the dataset was annotated by A Data Pro.
Answer: A Data Pro
True answer:  news articles in free-text format

Validation ID: df5a4505edccc0ee11349ed6e7958cf6b84c9ed4
Question: What dataset was used?
From the context "The corpus for the task was annotated by A Data Pro, a company that performs high-quality manual annotations", so we know that the dataset was annotated by A Data Pro.
Answer: A Data Pro
True answer: collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators

Validation ID: fd753ab5177d7bd27db0e0afc12411876ee607df
Question: What was the baseline for this task?
From the context "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence", so we know that the baseline for this task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.
Answer: a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence
True answer: The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.

Validation ID: fd753ab5177d7bd27db0e0afc12411876ee607df
Question: What was the baseline for this task?
From the context "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence", so we know that the baseline for this task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.
Answer: a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence
True answer: SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly

Validation ID: 88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42
Question: What is a second order co-ocurrence matrix?
From the context "We build an INLINEFORM0 similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above", so we know that a second order co-ocurrence matrix is a similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above.
Answer: a similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above
True answer: frequencies of the other words which occur with both of them (i.e., second order co–occurrences)

Validation ID: 88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42
Question: What is a second order co-ocurrence matrix?
From the context "We build an INLINEFORM0 similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above", so we know that a second order co-ocurrence matrix is a similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above.
Answer: a similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above
True answer: The matrix containing co-occurrences of the words which occur with the both words of every given pair of words.

Validation ID: 4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3
Question: How many humans participated?
From the context "The UMNSRS was annotated for both similarity and relatedness by four medical residents", so we know that four medical residents participated.
Answer: four medical residents
True answer: Unanswerable

Validation ID: 4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3
Question: How many humans participated?
From the context "The UMNSRS was annotated for both similarity and relatedness by four medical residents", so we know that four medical residents participated.
Answer: four medical residents
True answer: 16

Validation ID: 8b3d3953454c88bde88181897a7a2c0c8dd87e23
Question: What embedding techniques are explored in the paper?
From the context "Recently, word embeddings BIBREF9 have become a popular method for", so we know that word embeddings are explored in the paper.
Answer: word embeddings
True answer: Skip–gram, CBOW

Validation ID: 8b3d3953454c88bde88181897a7a2c0c8dd87e23
Question: What embedding techniques are explored in the paper?
From the context "Recently, word embeddings BIBREF9 have become a popular method for", so we know that word embeddings are explored in the paper.
Answer: word embeddings
True answer: integrated vector-res, vector-faith, Skip–gram, CBOW

Validation ID: 784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f
Question: Do the authors also try the model on other datasets?
From the context "We also study the impact of the depth of neural networks in our model. The results are presented in Table 2. In the ablation experiments we compare the performance of a single-layer LSTM to a two-layer LSTM to encode the question, and a single-layer vs. two-layer CNN to encode the KB entries. We find that a two-layer LSTM boosts joint accuracy by over 6%. The majority of accuracy gains are a result of improved predicate predictions, possibly because", so we know that the authors also try the model on other datasets.
Answer: Yes
True answer: No

Validation ID: 784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f
Question: Do the authors also try the model on other datasets?
From the context "We also study the impact of the depth of neural networks in our model. The results are presented in Table 2. In the ablation experiments we compare the performance of a single-layer LSTM to a two-layer LSTM to encode the question, and a single-layer vs. two-layer CNN to encode the KB entries. We find that a two-layer LSTM boosts joint accuracy by over 6%. The majority of accuracy gains are a result of improved predicate predictions, possibly because", so we know that the authors also try the model on other datasets.
Answer: Yes
True answer: No

Validation ID: 7705dd04acedaefee30d8b2c9978537afb2040dc
Question: What word level and character level model baselines are used?
From the context "We first explore using word-level models as an alternative to character-level models to construct embeddings for questions, entities and predicates.", so we know that word level and character level model baselines are used.
Answer: word level and character level model baselines
True answer: None

Validation ID: 7705dd04acedaefee30d8b2c9978537afb2040dc
Question: What word level and character level model baselines are used?
From the context "We first explore using word-level models as an alternative to character-level models to construct embeddings for questions, entities and predicates.", so we know that word level and character level model baselines are used.
Answer: word level and character level model baselines
True answer: Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015)

Validation ID: 44497509fdf5e87cff05cdcbe254fbd288d857ad
Question: By how much do they improve the efficacy of the attention mechanism?
From the context "The use of RNNs in the field of Statistical Machine Translation (SMT) has revolutionised the approaches to automated translation. As opposed to traditional shallow SMT models, which require a lot of memory to run, these neural translation models require only a small fraction of memory used, about 5% BIBREF0. Also, neural translation models are optimized such that every module is trained to jointly improve translation quality. With that being said, one of the main downsides of neural translation models is the heavy corpus requirement in order to ensure learning of deeper contexts. This is where the application of these encoder decoder architectures in translation to and/or from morphologically rich languages takes a severe hit.", so we know that the use of RNNs in the field of Statistical Machine Translation (SMT) has revolutionised the approaches to automated translation. As opposed to traditional shallow SMT models, which require a lot of memory to run, these neural translation models require only a small fraction of memory used, about 5% BIBREF0. Also, neural translation models are optimized such that every module is trained to jointly improve translation quality. With that being said, one of the main downsides of neural translation models is the heavy corpus requirement in order to ensure learning of deeper contexts. This is where the application of these encoder decoder architectures in translation to and/or from morphologically rich languages takes a severe hit.
From the context "The model used for translation is the one implemented by Bahdanau et al. Bahdanau2014. A bidirectional LSTM encoder first takes the source sentence and encodes it into a context vector which acts as input for the decoder. The decoder is attention-based where the hidden states of the decoder get as input the weighted sum of all the hidden layer outputs of the encoder alongwith the output of the previous hidden layer and the previously decoded word. This provides a contextual reference into the source language sentence BIBREF4.", so we know that the model used for translation is the one implemented by Bahdanau et al. Bahdanau2014. A bidirectional LSTM encoder first takes the source sentence and encodes it into a context vector which acts as input for the decoder. The decoder is attention-based where the hidden states of the decoder get as input the weighted sum of all the hidden layer outputs of the encoder alongwith the
True answer: Unanswerable

Validation ID: 44497509fdf5e87cff05cdcbe254fbd288d857ad
Question: By how much do they improve the efficacy of the attention mechanism?
From the context "The use of RNNs in the field of Statistical Machine Translation (SMT) has revolutionised the approaches to automated translation. As opposed to traditional shallow SMT models, which require a lot of memory to run, these neural translation models require only a small fraction of memory used, about 5% BIBREF0. Also, neural translation models are optimized such that every module is trained to jointly improve translation quality. With that being said, one of the main downsides of neural translation models is the heavy corpus requirement in order to ensure learning of deeper contexts. This is where the application of these encoder decoder architectures in translation to and/or from morphologically rich languages takes a severe hit.", so we know that the use of RNNs in the field of Statistical Machine Translation (SMT) has revolutionised the approaches to automated translation. As opposed to traditional shallow SMT models, which require a lot of memory to run, these neural translation models require only a small fraction of memory used, about 5% BIBREF0. Also, neural translation models are optimized such that every module is trained to jointly improve translation quality. With that being said, one of the main downsides of neural translation models is the heavy corpus requirement in order to ensure learning of deeper contexts. This is where the application of these encoder decoder architectures in translation to and/or from morphologically rich languages takes a severe hit.
From the context "The model used for translation is the one implemented by Bahdanau et al. Bahdanau2014. A bidirectional LSTM encoder first takes the source sentence and encodes it into a context vector which acts as input for the decoder. The decoder is attention-based where the hidden states of the decoder get as input the weighted sum of all the hidden layer outputs of the encoder alongwith the output of the previous hidden layer and the previously decoded word. This provides a contextual reference into the source language sentence BIBREF4.", so we know that the model used for translation is the one implemented by Bahdanau et al. Bahdanau2014. A bidirectional LSTM encoder first takes the source sentence and encodes it into a context vector which acts as input for the decoder. The decoder is attention-based where the hidden states of the decoder get as input the weighted sum of all the hidden layer outputs of the encoder alongwith the
True answer: Unanswerable

Validation ID: 0ee73909ac638903da4a0e5565c8571fc794ab96
Question: How were the human judgements assembled?
From the context "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation.", so we know that the human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages.
Answer: a group of 50 native people who were well-versed in both English and Tamil languages
True answer: 50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.

Validation ID: 0ee73909ac638903da4a0e5565c8571fc794ab96
Question: How were the human judgements assembled?
From the context "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation.", so we know that the human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages.
Answer: a group of 50 native people who were well-versed in both English and Tamil languages
True answer: adequacy, precision and ranking values

Validation ID: 1f07e837574519f2b696f3d6fa3230af0b931e5d
Question: Did they only experiment with one language pair?
From the context "We built a phrase-based Chinese-to-English SMT system by using Moses BIBREF18. Our parallel training text is a collection of parallel corpora from LDC, which we divide into older corpora and newer corpora. Due to the dominant older data, we duplicate the newer corpora of various domains by 10 times to achieve better domain balance. To reduce the possibility of alignment errors, parallel sentences in the corpora that are longer than 85 words in either Chinese (after word segmentation) or English are discarded. In the end, the final parallel text consists of around 8.8M sentence pairs, 228M Chinese tokens, and 254M English tokens (a token can be a word or punctuation symbol). We also added two dictionaries by concatenating them to our training parallel text. The total number of words in these two corpora is 1.81M for Chinese and 2.03M for English.", so we know that they only experiment with one language pair.
Answer: one language pair
True answer: Yes

Validation ID: 1f07e837574519f2b696f3d6fa3230af0b931e5d
Question: Did they only experiment with one language pair?
From the context "We built a phrase-based Chinese-to-English SMT system by using Moses BIBREF18. Our parallel training text is a collection of parallel corpora from LDC, which we divide into older corpora and newer corpora. Due to the dominant older data, we duplicate the newer corpora of various domains by 10 times to achieve better domain balance. To reduce the possibility of alignment errors, parallel sentences in the corpora that are longer than 85 words in either Chinese (after word segmentation) or English are discarded. In the end, the final parallel text consists of around 8.8M sentence pairs, 228M Chinese tokens, and 254M English tokens (a token can be a word or punctuation symbol). We also added two dictionaries by concatenating them to our training parallel text. The total number of words in these two corpora is 1.81M for Chinese and 2.03M for English.", so we know that they only experiment with one language pair.
Answer: one language pair
True answer: Yes

Validation ID: 5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76
Question: Which other approaches do they compare their model with?
From the context "We can see comparisons of our model made on Wiki(gold) in Table TABREF20. We note that the model performs similarly to existing systems without being trained or tuned on that particular dataset.", so we know that they compare their model with existing systems.
Answer: existing systems
True answer: Akbik et al. (2018), Link et al. (2012)

Validation ID: 5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76
Question: Which other approaches do they compare their model with?
From the context "We can see comparisons of our model made on Wiki(gold) in Table TABREF20. We note that the model performs similarly to existing systems without being trained or tuned on that particular dataset.", so we know that they compare their model with existing systems.
Answer: existing systems
True answer: They compare to Akbik et al. (2018) and Link et al. (2012).

Validation ID: 729694a9fe1e05d329b7a4078a596fe606bc5a95
Question: What results do they achieve using their proposed approach?
From the context "The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%", so we know that the total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%.
Answer: 88%, 53%
True answer: F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).

Validation ID: 729694a9fe1e05d329b7a4078a596fe606bc5a95
Question: What results do they achieve using their proposed approach?
From the context "The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%", so we know that the total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%.
Answer: 88%, 53%
True answer:  total F-1 score on the OntoNotes dataset is 88%, total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%

Validation ID: 1c997c268c68149ae6fb43d83ffcd53f0e7fe57e
Question: How do they combine a deep learning model with a knowledge base?
From the context "We then pass this to a softmax layer as a tag decoder to predict the entity types. Hyperparameter settings: The hidden-layer size of each LSTM within the model is set 512. We use a dropout with the probability of 0.2 on the output of the LSTM encoders. The embedding dimension from ELMo is 1024. The optimization method we use is Adam BIBREF23. We train with a batch size of 32 for 30 epochs. The model was implemented using the TensorFlow framework.", so we know that they use a softmax layer as a tag decoder to predict the entity types.
Answer: use a softmax layer as a tag decoder to predict the entity types
True answer: Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.

Validation ID: 1c997c268c68149ae6fb43d83ffcd53f0e7fe57e
Question: How do they combine a deep learning model with a knowledge base?
From the context "We then pass this to a softmax layer as a tag decoder to predict the entity types. Hyperparameter settings: The hidden-layer size of each LSTM within the model is set 512. We use a dropout with the probability of 0.2 on the output of the LSTM encoders. The embedding dimension from ELMo is 1024. The optimization method we use is Adam BIBREF23. We train with a batch size of 32 for 30 epochs. The model was implemented using the TensorFlow framework.", so we know that they use a softmax layer as a tag decoder to predict the entity types.
Answer: use a softmax layer as a tag decoder to predict the entity types
True answer: ELMo embeddings are then used with a residual LSTM to learn informative morphological representations from the character sequence of each token

Validation ID: 5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde
Question: What are the models used for the baseline of the three NLP tasks?
From the context "We built neural end-to-end machine translation systems between Mapudungun and Spanish in both directions, using state-of-the-art Transformer architecture BIBREF14 with the toolkit of BIBREF15", so we know that the models used for the baseline of the three NLP tasks are Transformer architecture BIBREF14 with the toolkit of BIBREF15.
Answer: Transformer architecture BIBREF14 with the toolkit of BIBREF15
True answer: state-of-the-art Transformer architecture, Kaldi, speech clustergen statistical speech synthesizer

Validation ID: 5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde
Question: What are the models used for the baseline of the three NLP tasks?
From the context "We built neural end-to-end machine translation systems between Mapudungun and Spanish in both directions, using state-of-the-art Transformer architecture BIBREF14 with the toolkit of BIBREF15", so we know that the models used for the baseline of the three NLP tasks are Transformer architecture BIBREF14 with the toolkit of BIBREF15.
Answer: Transformer architecture BIBREF14 with the toolkit of BIBREF15
True answer: For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15.

Validation ID: f9bf6bef946012dd42835bf0c547c0de9c1d229f
Question: How is non-standard pronunciation identified?
From the context "In addition, the transcription includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses", so we know that non-standard pronunciation is identified by annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses.
Answer: annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses
True answer: Unanswerable

Validation ID: f9bf6bef946012dd42835bf0c547c0de9c1d229f
Question: How is non-standard pronunciation identified?
From the context "In addition, the transcription includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses", so we know that non-standard pronunciation is identified by annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses.
Answer: annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses
True answer: Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.

Validation ID: 6a633811019e9323dc8549ad540550d27aa6d972
Question: Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?
From the context "We have extracted the bio-medical entities present in the Wikipedia medical articles through a n-gram-based technique. A pre-processing phase occurs in a similar way as for the dictionary composition. Given a Wikipedia article written in English, we have pre-processed the textual part through the Tanl pipeline. We have considered the lemmas of the words and the part of speech (POS) of each word. We have also removed punctuation and stopwords. We have then considered all the n-grams (with n between 1 and 10) in the text. For each n-gram, we have computed the frequency of occurrence in the text. We have then computed the TF-IDF score for each n-gram, as follows:", so we know that the medical domain model is based on the TF-IDF score.
Answer: Yes
True answer: Unanswerable

Validation ID: 6a633811019e9323dc8549ad540550d27aa6d972
Question: Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?
From the context "We have extracted the bio-medical entities present in the Wikipedia medical articles through a n-gram-based technique. A pre-processing phase occurs in a similar way as for the dictionary composition. Given a Wikipedia article written in English, we have pre-processed the textual part through the Tanl pipeline. We have considered the lemmas of the words and the part of speech (POS) of each word. We have also removed punctuation and stopwords. We have then considered all the n-grams (with n between 1 and 10) in the text. For each n-gram, we have computed the frequency of occurrence in the text. We have then computed the TF-IDF score for each n-gram, as follows:", so we know that the medical domain model is based on the TF-IDF score.
Answer: Yes
True answer: No

Validation ID: 6b9b9e5d154cb963f6d921093539490daa5ebbae
Question: What novel PMI variants are introduced?
From the context "We also experiment with normalized $\mathit {PMI}$ ($\mathit {NPMI}$) BIBREF7:
such that $NPMI(w,c) = -1$ when $(w,c)$ never cooccur, $NPMI(w,c) = 0$ when they are independent, and $NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\mathit {NNEGPMI}$ which only normalizes $\mathit {\texttt {-}PMI}$:", so we know that $\mathit {NPMI}$ and $\mathit {NNEGPMI}$ are novel PMI variants.
True answer: clipped PMI; NNEGPMI

Validation ID: 6b9b9e5d154cb963f6d921093539490daa5ebbae
Question: What novel PMI variants are introduced?
From the context "We also experiment with normalized $\mathit {PMI}$ ($\mathit {NPMI}$) BIBREF7:
such that $NPMI(w,c) = -1$ when $(w,c)$ never cooccur, $NPMI(w,c) = 0$ when they are independent, and $NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\mathit {NNEGPMI}$ which only normalizes $\mathit {\texttt {-}PMI}$:", so we know that $\mathit {NPMI}$ and $\mathit {NNEGPMI}$ are novel PMI variants.
True answer: clipped $\mathit {PMI}$, $\mathit {NNEGPMI}$

Validation ID: bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2
Question: What semantic and syntactic tasks are used as probes?
From the context "We evaluate weighted factorization of different matrices which use either $\mathit {\texttt {-}PMI}$, $\mathit {\texttt {+}PMI}$, or both on various semantic and syntactic tasks. Results show that $\mathit {\texttt {+}PMI}$ alone performs quite well on most tasks, capturing both semantics and syntax, in contrast to $\mathit {\texttt {-}PMI}$, which performs poorly on nearly all tasks, except those that test for syntax.", so we know that semantic and syntactic tasks are used as probes.
Answer: semantic and syntactic tasks
True answer: Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks

Validation ID: bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2
Question: What semantic and syntactic tasks are used as probes?
From the context "We evaluate weighted factorization of different matrices which use either $\mathit {\texttt {-}PMI}$, $\mathit {\texttt {+}PMI}$, or both on various semantic and syntactic tasks. Results show that $\mathit {\texttt {+}PMI}$ alone performs quite well on most tasks, capturing both semantics and syntax, in contrast to $\mathit {\texttt {-}PMI}$, which performs poorly on nearly all tasks, except those that test for syntax.", so we know that semantic and syntactic tasks are used as probes.
Answer: semantic and syntactic tasks
True answer: SimLex, Rare Word, Google Semantic, Semantic Textual Similarity, Word Content (WC) probing, Google Syntactic analogies, Depth, Top Constituent, part-of-speech (POS) tagging

Validation ID: d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7
Question: What are the disadvantages to clipping negative PMI?
From the context "However, it is not clear what is lost by clipping the negative spectrum of $\mathit {PMI}$, which makes the use of $\mathit {PPMI}$, though it works well in practice, seem unprincipled.", so we know that clipping negative PMI is not clear what is lost.
Answer: not clear what is lost
True answer: It may lead to poor rare word representations and word analogies.

Validation ID: d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7
Question: What are the disadvantages to clipping negative PMI?
From the context "However, it is not clear what is lost by clipping the negative spectrum of $\mathit {PMI}$, which makes the use of $\mathit {PPMI}$, though it works well in practice, seem unprincipled.", so we know that clipping negative PMI is not clear what is lost.
Answer: not clear what is lost
True answer: Unanswerable

Validation ID: 6844683935d0d8f588fa06530f5068bf3e1ed0c0
Question: Why are statistics from finite corpora unreliable?
From the context "Unfortunately, $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora.", so we know that statistics from finite corpora are unreliable because $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus.
Answer: $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus
True answer: $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus

Validation ID: 6844683935d0d8f588fa06530f5068bf3e1ed0c0
Question: Why are statistics from finite corpora unreliable?
From the context "Unfortunately, $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora.", so we know that statistics from finite corpora are unreliable because $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus.
Answer: $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus
True answer: A finite corpora may entirely omit rare word combinations

Validation ID: 8acab64ba72831633e8cc174d5469afecccf3ae9
Question: what is the domain of the corpus?
From the context "Although we did not have access to a low-resource dataset, there is a corpus of noisy multi-speaker speech that simulates many of the conditions we expect to find in our motivating applications: the CALLHOME Spanish–English speech translation dataset (LDC2014T23; Post el al., 2013). We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations. The transcripts contain 168,195 Spanish word tokens (10,674 types), and the translations contain 159,777 English word tokens (6,723 types). Though our system does not require Spanish transcripts, we use them to evaluate UTD and to simulate a perfect UTD system, called the oracle.", so we know that the domain of the corpus is telephone calls.
Answer: telephone calls
True answer: Unanswerable

Validation ID: 8acab64ba72831633e8cc174d5469afecccf3ae9
Question: what is the domain of the corpus?
From the context "Although we did not have access to a low-resource dataset, there is a corpus of noisy multi-speaker speech that simulates many of the conditions we expect to find in our motivating applications: the CALLHOME Spanish–English speech translation dataset (LDC2014T23; Post el al., 2013). We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations. The transcripts contain 168,195 Spanish word tokens (10,674 types), and the translations contain 159,777 English word tokens (6,723 types). Though our system does not require Spanish transcripts, we use them to evaluate UTD and to simulate a perfect UTD system, called the oracle.", so we know that the domain of the corpus is telephone calls.
Answer: telephone calls
True answer: telephone calls

Validation ID: 53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa
Question: what challenges are identified?
From the context "We identified several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering", so we know that low coverage of audio and difficulty in cross-speaker clustering are identified.
Answer: low coverage of audio and difficulty in cross-speaker clustering
True answer: Assigning wrong words to a cluster, Splitting words across different clusters, sparse, giving low coverage

Validation ID: 53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa
Question: what challenges are identified?
From the context "We identified several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering", so we know that low coverage of audio and difficulty in cross-speaker clustering are identified.
Answer: low coverage of audio and difficulty in cross-speaker clustering
True answer: low coverage of audio, difficulty in cross-speaker clustering

Validation ID: 72755c2d79210857cfff60bfbcb55f83c71ada51
Question: what is the size of the speech corpus?
From the context "We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations", so we know that the size of the speech corpus is 11 hours.
Answer: 11 hours
True answer: 104 telephone calls, transcripts contain 168,195 Spanish word tokens,  translations contain 159,777 English word tokens

Validation ID: 72755c2d79210857cfff60bfbcb55f83c71ada51
Question: what is the size of the speech corpus?
From the context "We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations", so we know that the size of the speech corpus is 11 hours.
Answer: 11 hours
True answer: 104 telephone calls, which pair 11 hours of audio

Validation ID: 7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569
Question: Which two pairs of ERPs from the literature benefit from joint training?
From the context "We find that the LAN and P600 benefit from joint training, as do the ELAN and P600", so we know that the LAN and P600 benefit from joint training, as do the ELAN and P600.
Answer: LAN and P600, ELAN and P600
True answer: Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.
Select:
- ELAN, LAN
- PNP ERP

Validation ID: 7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569
Question: Which two pairs of ERPs from the literature benefit from joint training?
From the context "We find that the LAN and P600 benefit from joint training, as do the ELAN and P600", so we know that the LAN and P600 benefit from joint training, as do the ELAN and P600.
Answer: LAN and P600, ELAN and P600
True answer: Unanswerable

Validation ID: bd6dc38a9ac8d329114172194b0820766458dacc
Question: What datasets are used?
From the context "In this work, we consider six ERP components that have been associated in the cognitive neuroscience and psycholinguistics literature with language processing and which we analyze in the data from BIBREF0 (see Figure FIGREF1 for spatial and temporal definitions of these ERP components)", so we know that the datasets are BIBREF0.
Answer: BIBREF0
True answer: Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.
Select:
- ERP data collected and computed by Frank et al. (2015)
- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)

Validation ID: bd6dc38a9ac8d329114172194b0820766458dacc
Question: What datasets are used?
From the context "In this work, we consider six ERP components that have been associated in the cognitive neuroscience and psycholinguistics literature with language processing and which we analyze in the data from BIBREF0 (see Figure FIGREF1 for spatial and temporal definitions of these ERP components)", so we know that the datasets are BIBREF0.
Answer: BIBREF0
True answer: the ERP data: BIBREF0

Validation ID: 3ddff6b707767c3dd54d7104fe88b628765cae58
Question: which datasets did they experiment with?
From the context "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43.", so we know that they experiment with Universal Dependencies v1.2 treebanks.
Answer: Universal Dependencies v1.2 treebanks
True answer: Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,
Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish

Validation ID: 3ddff6b707767c3dd54d7104fe88b628765cae58
Question: which datasets did they experiment with?
From the context "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43.", so we know that they experiment with Universal Dependencies v1.2 treebanks.
Answer: Universal Dependencies v1.2 treebanks
True answer: Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2

Validation ID: 0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7
Question: which languages are explored?
From the context "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43.", so we know that the languages explored are the languages in UD1.2.
Answer: the languages in UD1.2
True answer: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish

Validation ID: 0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7
Question: which languages are explored?
From the context "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43.", so we know that the languages explored are the languages in UD1.2.
Answer: the languages in UD1.2
True answer: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish

Validation ID: 06be47e2f50b902b05ebf1ff1c66051925f5c247
Question: Do they use number of votes as an indicator of preference?
From the context "The most widely used source for deriving preferences in IR is UN roll call data BIBREF10. Voting behavior represent a valuable source of revealed preference information, comparable across states and over time", so we know that the most widely used source for deriving preferences in IR is UN roll call data.
Answer: Yes
True answer: No

Validation ID: 06be47e2f50b902b05ebf1ff1c66051925f5c247
Question: Do they use number of votes as an indicator of preference?
From the context "The most widely used source for deriving preferences in IR is UN roll call data BIBREF10. Voting behavior represent a valuable source of revealed preference information, comparable across states and over time", so we know that the most widely used source for deriving preferences in IR is UN roll call data.
Answer: Yes
True answer: Yes

Validation ID: 003d6f9722ddc2ee13e879fefafc315fb8e87cb9
Question: What does a node in the network approach repesent?
From the context "We draw on a recently released corpus of state speeches delivered during the annual UN General Debate that provides the first dataset of textual output from states that is recorded at regular time-series intervals and includes a sample of all countries that deliver speeches BIBREF11. There are limitations to both votes and speeches in the UN in deriving estimates of states' underlying preferences. However, it is not controversial to suggest that state speeches can valuably complement roll call data, and the use of speeches and votes together can reveal useful preference information beyond that contained in states' voting behavior or GD speeches alone. The question, rather, is how best to represent these texts and how best to theoretically model these data in tandem.", so we know that a node in the network approach repesent a country.
Answer: a country
True answer: Unanswerable

Validation ID: 003d6f9722ddc2ee13e879fefafc315fb8e87cb9
Question: What does a node in the network approach repesent?
From the context "We draw on a recently released corpus of state speeches delivered during the annual UN General Debate that provides the first dataset of textual output from states that is recorded at regular time-series intervals and includes a sample of all countries that deliver speeches BIBREF11. There are limitations to both votes and speeches in the UN in deriving estimates of states' underlying preferences. However, it is not controversial to suggest that state speeches can valuably complement roll call data, and the use of speeches and votes together can reveal useful preference information beyond that contained in states' voting behavior or GD speeches alone. The question, rather, is how best to represent these texts and how best to theoretically model these data in tandem.", so we know that a node in the network approach repesent a country.
Answer: a country
True answer: Unanswerable

Validation ID: c88a846197b72d25e04ec55f00ee3e72f655504c
Question: Which dataset do they use?
From the context "We draw on the newly released UN General Debate Corpus BIBREF11 which contains every country statement in the UN General Debate between 1970 and 2014", so we know that they use the UN General Debate Corpus.
Answer: UN General Debate Corpus
True answer: corpus of state speeches delivered during the annual UN General Debate

Validation ID: c88a846197b72d25e04ec55f00ee3e72f655504c
Question: Which dataset do they use?
From the context "We draw on the newly released UN General Debate Corpus BIBREF11 which contains every country statement in the UN General Debate between 1970 and 2014", so we know that they use the UN General Debate Corpus.
Answer: UN General Debate Corpus
True answer: corpus of state speeches delivered during the annual UN General Debate

Validation ID: 4d28c99750095763c81bcd5544491a0ba51d9070
Question: What kind of celebrities do they obtain tweets from?
From the context "For the binary classification task described in Section SECREF6, we term tweets from $U$ as positive examples, and tweets from other users as negative examples.", so we know that they obtain tweets from 15 celebrities from various domains.
Answer: 15 celebrities from various domains
True answer: Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,
Ellen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey

Validation ID: 4d28c99750095763c81bcd5544491a0ba51d9070
Question: What kind of celebrities do they obtain tweets from?
From the context "For each user in our dataset, we build a set of 20 tweet pairs, with one tweet each from the 50 top-scoring and bottom-scoring tweets written by the user", so we know that they obtain tweets from top-scoring and bottom-scoring tweets written by the user.
Answer: top-scoring and bottom-scoring tweets written by the user
True answer: Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. 

Validation ID: 78292bc57ee68fdb93ed45430d80acca25a9e916
Question: How did they extend LAMA evaluation framework to focus on negation?
From the context "We created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. We did the same for a subset of ConceptNet that is easy to negate. We selected this subset by filtering for sentence length and extracting common queries.", so we know that they created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. They did the same for a subset of ConceptNet that is easy to negate. They selected this subset by filtering for sentence length and extracting common queries.
Answer: manually inserting a negation element in each template or statement
True answer: To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement

Validation ID: 78292bc57ee68fdb93ed45430d80acca25a9e916
Question: How did they extend LAMA evaluation framework to focus on negation?
From the context "We created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. We did the same for a subset of ConceptNet that is easy to negate. We selected this subset by filtering for sentence length and extracting common queries.", so we know that they created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. They did the same for a subset of ConceptNet that is easy to negate. They selected this subset by filtering for sentence length and extracting common queries.
Answer: manually inserting a negation element in each template or statement
True answer: Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions.

Validation ID: 443d2448136364235389039cbead07e80922ec5c
Question: What summarization algorithms did the authors experiment with?
From the context "We also explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries.", so we know that the authors experimented with summarization algorithms provided by the Sumy package.
Answer: summarization algorithms provided by the Sumy package
True answer: LSA, TextRank, LexRank and ILP-based summary.

Validation ID: 443d2448136364235389039cbead07e80922ec5c
Question: What summarization algorithms did the authors experiment with?
From the context "We also explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries.", so we know that the authors experimented with summarization algorithms provided by the Sumy package.
Answer: summarization algorithms provided by the Sumy package
True answer: LSA, TextRank, LexRank

Validation ID: aa6d956c2860f58fc9baea74c353c9d985b05605
Question: What evaluation metrics were used for the summarization task?
From the context "We compared the performance of our ILP-based summarization technique with manual summaries. We used ROUGE BIBREF22 unigram score as the evaluation metric", so we know that ROUGE BIBREF22 unigram score was used for the summarization task.
Answer: ROUGE BIBREF22 unigram score
True answer: ROUGE BIBREF22 unigram score

Validation ID: aa6d956c2860f58fc9baea74c353c9d985b05605
Question: What evaluation metrics were used for the summarization task?
From the context "We compared the performance of our ILP-based summarization technique with manual summaries. We used ROUGE BIBREF22 unigram score as the evaluation metric", so we know that ROUGE BIBREF22 unigram score was used for the summarization task.
Answer: ROUGE BIBREF22 unigram score
True answer: ROUGE

Validation ID: 4c18081ae3b676cc7831403d11bc070c10120f8e
Question: What clustering algorithms were used?
From the context "We also explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3.", so we know that Sumy package was used.
Answer: Sumy package
True answer: CLUTO, Carrot2 Lingo

Validation ID: 4c18081ae3b676cc7831403d11bc070c10120f8e
Question: What clustering algorithms were used?
From the context "We also explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3.", so we know that Sumy package was used.
Answer: Sumy package
True answer: simple clustering algorithm which uses the cosine similarity between word embeddings

Validation ID: fb3d30d59ed49e87f63d3735b876d45c4c6b8939
Question: What evaluation metrics are looked at for classification tasks?
From the context "The overall accuracy for a classifier is defined as INLINEFORM0, where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation.", so we know that the overall accuracy for a classifier is defined as INLINEFORM0, where the denominator is 2000 for dataset D1.
Answer: INLINEFORM0
True answer: Precision, Recall, F-measure, accuracy

Validation ID: fb3d30d59ed49e87f63d3735b876d45c4c6b8939
Question: What evaluation metrics are looked at for classification tasks?
From the context "The overall accuracy for a classifier is defined as INLINEFORM0, where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation.", so we know that the overall accuracy for a classifier is defined as INLINEFORM0, where the denominator is 2000 for dataset D1.
Answer: INLINEFORM0
True answer: Precision, Recall and F-measure

Validation ID: 197b276d0610ebfacd57ab46b0b29f3033c96a40
Question: What methods were used for sentence classification?
From the context "We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation. The overall accuracy for a classifier is defined as INLINEFORM0, where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation.", so we know that the methods used for sentence classification are SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, AdaBoost, Gradient Boosting, Pattern-based approach.
Answer: SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, AdaBoost, Gradient Boosting, Pattern-based approach
True answer: Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based

Validation ID: 197b276d0610ebfacd57ab46b0b29f3033c96a40
Question: What methods were used for sentence classification?
From the context "We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation. The overall accuracy for a classifier is defined as INLINEFORM0, where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation.", so we know that the methods used for sentence classification are SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, AdaBoost, Gradient Boosting, Pattern-based approach.
Answer: SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, AdaBoost, Gradient Boosting, Pattern-based approach
True answer: Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach

Validation ID: e025061e199b121f2ac8f3d9637d9bf987d65cd5
Question: What is the average length of the sentences?
From the context "The summary statistics about the number of words in a sentence is: min:4 max:217 average:15.5 STDEV:9.2 Q1:9 Q2:14 Q3:19", so we know that the average length of the sentences is 15.5.
Answer: 15.5
True answer: 15.5

Validation ID: e025061e199b121f2ac8f3d9637d9bf987d65cd5
Question: What is the average length of the sentences?
From the context "The summary statistics about the number of words in a sentence is: min:4 max:217 average:15.5 STDEV:9.2 Q1:9 Q2:14 Q3:19", so we know that the average length of the sentences is 15.5.
Answer: 15.5
True answer: average:15.5

Validation ID: 61652a3da85196564401d616d251084a25ab4596
Question: What is the size of the real-life dataset?
From the context "In this paper, we used the supervisor assessment and peer feedback text produced during the performance appraisal of 4528 employees in a large multi-national IT company", so we know that the size of the real-life dataset is 4528 employees.
Answer: 4528 employees
True answer: 26972

Validation ID: 61652a3da85196564401d616d251084a25ab4596
Question: What is the size of the real-life dataset?
From the context "In this paper, we used the supervisor assessment and peer feedback text produced during the performance appraisal of 4528 employees in a large multi-national IT company", so we know that the size of the real-life dataset is 4528 employees.
Answer: 4528 employees
True answer: 26972 sentences

Validation ID: 14b74ad5a6f5b0506511c9b454e9c464371ef8c4
Question: What are the language pairs explored in this paper?
From the context "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19, and the WMT 2016 dataset for Ro-En", so we know that the language pairs explored in this paper are De-En, Ja-En and Ro-En.
Answer: De-En, Ja-En and Ro-En
True answer: De-En, Ja-En, Ro-En

Validation ID: 14b74ad5a6f5b0506511c9b454e9c464371ef8c4
Question: What are the language pairs explored in this paper?
From the context "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19, and the WMT 2016 dataset for Ro-En", so we know that the language pairs explored in this paper are De-En, Ja-En and Ro-En.
Answer: De-En, Ja-En and Ro-En
True answer: De-En, Ja-En, Ro-En

Validation ID: 5c88d601e8fca96bffebfa9ef22331ecf31c6d75
Question: Have they made any attempt to correct MRC gold standards according to their findings??
From the context "We demonstrate the usefulness of the proposed framework by applying it to precisely describe and compare six contemporary MRC datasets. Our findings reveal concerns about their factual correctness, the presence of lexical cues that simplify the task of reading comprehension and the lack of semantic altering grammatical modifiers. We release the raw data comprised of 300 paragraphs, questions and answers richly annotated under the proposed framework as a resource for researchers developing natural language understanding models and datasets to utilise further.", so we know that they have made some attempt to correct MRC gold standards according to their findings.
Answer: Yes
True answer: Yes

Validation ID: 5c88d601e8fca96bffebfa9ef22331ecf31c6d75
Question: Have they made any attempt to correct MRC gold standards according to their findings??
From the context "We demonstrate the usefulness of the proposed framework by applying it to precisely describe and compare six contemporary MRC datasets. Our findings reveal concerns about their factual correctness, the presence of lexical cues that simplify the task of reading comprehension and the lack of semantic altering grammatical modifiers. We release the raw data comprised of 300 paragraphs, questions and answers richly annotated under the proposed framework as a resource for researchers developing natural language understanding models and datasets to utilise further.", so we know that they have released the raw data comprised of 300 paragraphs, questions and answers richly annotated under the proposed framework as a resource for researchers developing natural language understanding models and datasets to utilise further.
Answer: Yes
True answer: No

Validation ID: 71bd5db79635d48a0730163a9f2e8ef19a86cd66
Question: What features are absent from MRC gold standards that can result in potential lexical ambiguity?
From the context "Syntactic features contributing to variety and ambiguity that we did not observe in our samples are the exploitation of verb symmetry, the use of dative and genitive cases or ambiguous prepositions and coordination scope (respectively Symmetry, Dative, Genitive, Prepositions, Scope). Therefore we cannot establish whether models are capable of dealing with those features by evaluating them on those gold standards.", so we know that the features that are absent from MRC gold standards that can result in potential lexical ambiguity are the exploitation of verb symmetry, the use of dative and genitive cases or ambiguous prepositions and coordination scope.
Answer: the exploitation of verb symmetry, the use of dative and genitive cases or ambiguous prepositions and coordination scope
True answer: Restrictivity , Factivity , Coreference 

Validation ID: 71bd5db79635d48a0730163a9f2e8ef19a86cd66
Question: What features are absent from MRC gold standards that can result in potential lexical ambiguity?
From the context "Syntactic features contributing to variety and ambiguity that we did not observe in our samples are the exploitation of verb symmetry, the use of dative and genitive cases or ambiguous prepositions and coordination scope (respectively Symmetry, Dative, Genitive, Prepositions, Scope). Therefore we cannot establish whether models are capable of dealing with those features by evaluating them on those gold standards.", so we know that the features that are absent from MRC gold standards that can result in potential lexical ambiguity are the exploitation of verb symmetry, the use of dative and genitive cases or ambiguous prepositions and coordination scope.
Answer: the exploitation of verb symmetry, the use of dative and genitive cases or ambiguous prepositions and coordination scope
True answer: semantics-altering grammatical modifiers

Validation ID: 9ecde59ffab3c57ec54591c3c7826a9188b2b270
Question: What modern MRC gold standards are analyzed?
From the context "We select contemporary MRC benchmarks to represent all four commonly used problem definitions BIBREF15", so we know that the modern MRC gold standards are NewsQA, MsMarco, HotpotQA, ReCoRd, MultiRC and SQuAD.
Answer: NewsQA, MsMarco, HotpotQA, ReCoRd, MultiRC and SQuAD
True answer: fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations

Validation ID: 9ecde59ffab3c57ec54591c3c7826a9188b2b270
Question: What modern MRC gold standards are analyzed?
From the context "We select contemporary MRC benchmarks to represent all four commonly used problem definitions BIBREF15. In selecting relevant datasets, we do not consider those that are considered “solved”, i.e. where the state of the art performance surpasses human performance, as is the case with SQuAD BIBREF28, BIBREF7. Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations, and bucket them according to the answer selection styles as described in Section SECREF4 We randomly draw one from each bucket and add two randomly drawn datasets from the candidate pool. This leaves us with the datasets described in Table TABREF19. For a more detailed description, we refer to Appendix.", so we know that the modern MRC gold standards are analyzed are SQuAD, NewsQA, MultiRC, HotpotQA, ReCoRd and MsMarco.
Answer: SQuAD, NewsQA, MultiRC, HotpotQA, ReCoRd and MsMarco
True answer: MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.

Validation ID: 005cca3c8ab6c3a166e315547a2259020f318ffb
Question: How does proposed qualitative annotation schema looks like?
From the context "The resulting taxonomy of the framework is shown in Figure FIGREF23. The full annotation results can be found in Appendix.", so we know that the proposed qualitative annotation schema is shown in Figure FIGREF23.
Answer: Figure FIGREF23
True answer: The resulting taxonomy of the framework is shown in Figure FIGREF10

Validation ID: 005cca3c8ab6c3a166e315547a2259020f318ffb
Question: How does proposed qualitative annotation schema looks like?
From the context "The resulting taxonomy of the framework is shown in Figure FIGREF23. The full annotation results can be found in Appendix.", so we know that the proposed qualitative annotation schema is shown in Figure FIGREF23.
Answer: Figure FIGREF23
True answer: FIGREF10

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
From the context "We compare our model with the following baselines:", so we know that the baselines are:
Answer:
True answer: Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
From the context "We compare our model with the following baselines:", so we know that the baselines are:
Answer:
True answer: Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
From the context "We compare our model with the following baselines:", so we know that the baselines are:
Answer:
True answer: Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.

Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.

Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\alpha _{st}=0.75$ while $\alpha _{asr}=0.25$ or $\alpha _{mt}=0.25$. For many-to-many setting, we use $\alpha _{st}=0.6, \alpha _{asr}=0.2$ and $\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.

Many-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. 

Validation ID: 022c365a14fdec406c7a945a1a18e7e79df37f08
Question: What is the attention module pretrained on?
From the context "we reuse the pre-trained MT attention module in ST, so we can leverage the alignment information learned in pre-training", so we know that the attention module is pretrained on MT.
Answer: MT
True answer: the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.

Validation ID: 5260cb56b7d127772425583c5c28958c37cb9bea
Question: How long of dialog history is captured?
From the context "The proposed models also illustrate advantageous performance over several competitive contextual language models. Perplexity of the proposed dialog context language models is higher than that of the model using true dialog act tags as context by a small margin. This indicates that the proposed model may implicitly capture the dialog context state for language modeling.", so we know that the proposed model may implicitly capture the dialog context state for language modeling.
Answer: the dialog context state for language modeling
True answer: two previous turns

Validation ID: 5260cb56b7d127772425583c5c28958c37cb9bea
Question: How long of dialog history is captured?
From the context "The proposed models also illustrate advantageous performance over several competitive contextual language models. Perplexity of the proposed dialog context language models is higher than that of the model using true dialog act tags as context by a small margin. This indicates that the proposed model may implicitly capture the dialog context state for language modeling.", so we know that the proposed model may implicitly capture the dialog context state for language modeling.
Answer: the dialog context state for language modeling
True answer: 160

Validation ID: 9b97805a0c093df405391a85e4d3ab447671c86a
Question: What evaluation metrics were used?
From the context "The most intuitive way to evaluate the text answer is to directly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer. We used the standard evaluation script from SQuAD BIBREF1 to evaluate the performance.", so we know that the evaluation metrics are Exact Match (EM) and Macro-averaged F1 scores (F1).
Answer: Exact Match (EM) and Macro-averaged F1 scores (F1)
True answer: Exact Match (EM), Macro-averaged F1 scores (F1)

Validation ID: 9b97805a0c093df405391a85e4d3ab447671c86a
Question: What evaluation metrics were used?
From the context "The most intuitive way to evaluate the text answer is to directly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer. We used the standard evaluation script from SQuAD BIBREF1 to evaluate the performance.", so we know that the evaluation metrics are Exact Match (EM) and Macro-averaged F1 scores (F1).
Answer: Exact Match (EM) and Macro-averaged F1 scores (F1)
True answer: Exact Match (EM) and Macro-averaged F1 scores (F1) 

Validation ID: 38f58f13c7f23442d5952c8caf126073a477bac0
Question: What was the score of the proposed model?
From the context "The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score", so we know that the score of the proposed model is 2% EM score and over 1.5% F1 score.
Answer: 2% EM score and over 1.5% F1 score
True answer: Best results authors obtain is EM 51.10 and F1 63.11

Validation ID: 38f58f13c7f23442d5952c8caf126073a477bac0
Question: What was the score of the proposed model?
From the context "The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score", so we know that the score of the proposed model is 2% EM score and over 1.5% F1 score.
Answer: 2% EM score and over 1.5% F1 score
True answer: EM Score of 51.10

Validation ID: 7ee5c45b127fb284a4a9e72bb9b980a602f7445a
Question: What was the previous best model?
From the context "The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score", so we know that the previous best model is the model that achieves superior performance.
Answer: the model that achieves superior performance
True answer: (c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 

Validation ID: 7ee5c45b127fb284a4a9e72bb9b980a602f7445a
Question: What was the previous best model?
From the context "The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score", so we know that the previous best model is the model that achieves superior performance.
Answer: the model that achieves superior performance
True answer: Unanswerable

Validation ID: ddf5e1f600b9ce2e8f63213982ef4209bab01fd8
Question: Which datasets did they use for evaluation?
From the context "Spoken-SQuAD is chosen as the target domain data for training and testing. Spoken-SQuAD BIBREF5 is an automatically generated corpus in which the document is in spoken form and the question is in text form. The reference transcriptions are from SQuAD BIBREF1. There are 37,111 and 5,351 question answer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%.", so we know that they use Spoken-SQuAD for evaluation.
Answer: Spoken-SQuAD
True answer: Spoken-SQuAD testing set

Validation ID: ddf5e1f600b9ce2e8f63213982ef4209bab01fd8
Question: Which datasets did they use for evaluation?
From the context "Spoken-SQuAD is chosen as the target domain data for training and testing. Spoken-SQuAD BIBREF5 is an automatically generated corpus in which the document is in spoken form and the question is in text form. The reference transcriptions are from SQuAD BIBREF1. There are 37,111 and 5,351 question answer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%.", so we know that they use Spoken-SQuAD for evaluation.
Answer: Spoken-SQuAD
True answer: Spoken-SQuAD

Validation ID: 27275fe9f6a9004639f9ac33c3a5767fea388a98
Question: What hyperparameters are explored?
From the context "Table TABREF2 describes most hyper-parameters explored for each dataset", so we know that the hyperparameters explored are dimension size, training epochs, window size and vocabulary size.
Answer: dimension size, training epochs, window size and vocabulary size
True answer: Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.

Validation ID: 27275fe9f6a9004639f9ac33c3a5767fea388a98
Question: What hyperparameters are explored?
From the context "Table TABREF2 describes most hyper-parameters explored for each dataset", so we know that the hyperparameters explored are dimension size, training epochs, window size and vocabulary size.
Answer: dimension size, training epochs, window size and vocabulary size
True answer: Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.

Validation ID: ef3567ce7301b28e34377e7b62c4ec9b496f00bf
Question: What Named Entity Recognition dataset is used?
From the context "The GMB dataset for NER was trained in an LSTM network, which had an embedding layer for input. The network diagram is shown in fig. FIGREF4. The IMDb dataset for SA was trained in a BiLSTM network, which also used an embedding layer for input. Its network diagram is given in fig. FIGREF4. It includes an additional hidden linear layer. Hyper-parameter details of the two networks for the downstream tasks are given in table TABREF3. The metrics for extrinsic evaluation include F1, precision, recall and accuracy scores. In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by BIBREF0 and ours. In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets. Batch size of 64 was used. For each task, experiments for each embedding was conducted four times and an average value calculated and reported in the next section", so we know that the GMB dataset for NER was trained in an LSTM network.
Answer: GMB
True answer: Groningen Meaning Bank

Validation ID: ef3567ce7301b28e34377e7b62c4ec9b496f00bf
Question: What Named Entity Recognition dataset is used?
From the context "The GMB dataset for NER was trained in an LSTM network, which had an embedding layer for input. The network diagram is shown in fig. FIGREF4. The IMDb dataset for SA was trained in a BiLSTM network, which also used an embedding layer for input. Its network diagram is given in fig. FIGREF4. It includes an additional hidden linear layer. Hyper-parameter details of the two networks for the downstream tasks are given in table TABREF3. The metrics for extrinsic evaluation include F1, precision, recall and accuracy scores. In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by BIBREF0 and ours. In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets. Batch size of 64 was used. For each task, experiments for each embedding was conducted four times and an average value calculated and reported in the next section", so we know that the GMB dataset for NER was trained in an LSTM network.
Answer: GMB
True answer: Groningen Meaning Bank (GMB)

Validation ID: 7595260c5747aede0b32b7414e13899869209506
Question: What sentiment analysis dataset is used?
From the context "The IMDb dataset for SA was trained in a BiLSTM network, which also used an embedding layer for input. Its network diagram is given in fig. FIGREF4. It includes an additional hidden linear layer.", so we know that the sentiment analysis dataset is IMDb dataset.
Answer: IMDb dataset
True answer: IMDb dataset of movie reviews

Validation ID: 7595260c5747aede0b32b7414e13899869209506
Question: What sentiment analysis dataset is used?
From the context "The IMDb dataset for SA was trained in a BiLSTM network, which also used an embedding layer for input. Its network diagram is given in fig. FIGREF4. It includes an additional hidden linear layer.", so we know that the sentiment analysis dataset is IMDb dataset.
Answer: IMDb dataset
True answer: IMDb

Validation ID: c2d1387e08cf25cb6b1f482178cca58030e85b70
Question: Do they test both skipgram and c-bow?
From the context "To form the vocabulary, words occurring less than 5 times in the corpora were dropped, stop words removed using the natural language toolkit (NLTK) (BIBREF22) and data pre-processing carried out. Table TABREF2 describes most hyper-parameters explored for each dataset. In all, 80 runs (of about 160 minutes) were conducted for the 15MB Wiki Abstract dataset with 80 serialized models totaling 15.136GB while 80 runs (for over 320 hours) were conducted for the 711MB SW dataset, with 80 serialized models totaling over 145GB. Experiments for all combinations for 300 dimensions were conducted on the 3.9GB training set of the BW corpus and additional runs for other dimensions for the window 8 + skipgram + heirarchical softmax combination to verify the trend of quality of word vectors as dimensions are increased.", so we know that they test both skipgram and c-bow.
Answer: Yes
True answer: Yes

Validation ID: c2d1387e08cf25cb6b1f482178cca58030e85b70
Question: Do they test both skipgram and c-bow?
From the context "To form the vocabulary, words occurring less than 5 times in the corpora were dropped, stop words removed using the natural language toolkit (NLTK) (BIBREF22) and data pre-processing carried out. Table TABREF2 describes most hyper-parameters explored for each dataset. In all, 80 runs (of about 160 minutes) were conducted for the 15MB Wiki Abstract dataset with 80 serialized models totaling 15.136GB while 80 runs (for over 320 hours) were conducted for the 711MB SW dataset, with 80 serialized models totaling over 145GB. Experiments for all combinations for 300 dimensions were conducted on the 3.9GB training set of the BW corpus and additional runs for other dimensions for the window 8 + skipgram + heirarchical softmax combination to verify the trend of quality of word vectors as dimensions are increased.", so we know that they test both skipgram and c-bow.
Answer: Yes
True answer: Yes

Validation ID: 5a22293b055f5775081d6acdc0450f7bd5f5de04
Question: What is the state-of-the-art model for the task?
From the context "We conducted experiments on ROTOWIRE BIBREF2. For each example, it provides three tables as described in Section SECREF2 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1. In this paper, we followed the data split introduced in Wiseman: 3398 examples in training set, 727 examples in development set and 728 examples in test set. We followed Wiseman's work and use BLEU BIBREF7 and three extractive evaluation metrics RG, CS and CO BIBREF2 for evaluation. The main idea of the extractive evaluation metrics is to use an Information Extraction (IE) model to identify records mentioned in texts. Then compare them with tables or records extracted from reference to evaluate the model. We refer the readers to Wiseman's paper for more details.
Implementation Details
True answer: OpATT BIBREF6, Neural Content Planning with conditional copy (NCP+CC) BIBREF4

Validation ID: 5a22293b055f5775081d6acdc0450f7bd5f5de04
Question: What is the state-of-the-art model for the task?
From the context "We conducted experiments on ROTOWIRE BIBREF2. For each example, it provides three tables as described in Section SECREF2 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1. In this paper, we followed the data split introduced in Wiseman: 3398 examples in training set, 727 examples in development set and 728 examples in test set. We followed Wiseman's work and use BLEU BIBREF7 and three extractive evaluation metrics RG, CS and CO BIBREF2 for evaluation. The main idea of the extractive evaluation metrics is to use an Information Extraction (IE) model to identify records mentioned in texts. Then compare them with tables or records extracted from reference to evaluate the model. RG (Relation Generation) measures content fidelity of texts. CS (Content Selection) measures model's ability on content selection. CO (Content Ordering) measures model's ability on ordering the chosen records in texts. We refer the readers to Wiseman's paper for more details.", so we know that the state-of-the-art model for the task is Wiseman's work.
Answer: Wiseman's work
True answer: Unanswerable

Validation ID: 03c967763e51ef2537793db7902e2c9c17e43e95
Question: What is the strong baseline?
From the context "We use Seq2Seq model with attention BIBREF5 and conditional copy BIBREF3 as the base model", so we know that the strong baseline is Seq2Seq model with attention BIBREF5 and conditional copy BIBREF3.
Answer: Seq2Seq model with attention BIBREF5 and conditional copy BIBREF3
True answer: Conditional Copy (CC) model 

Validation ID: 03c967763e51ef2537793db7902e2c9c17e43e95
Question: What is the strong baseline?
From the context "We use Seq2Seq model with attention BIBREF5 and conditional copy BIBREF3 as the base model", so we know that the strong baseline is Seq2Seq model with attention BIBREF5 and conditional copy BIBREF3.
Answer: Seq2Seq model with attention BIBREF5 and conditional copy BIBREF3
True answer: delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)

Validation ID: 26327ccebc620a73ba37a95aabe968864e3392b2
Question: what aspects of conversation flow do they look at?
From the context "We quantify the use of these strategies by comparing the airtime debaters devote to talking points. For a side INLINEFORM0, let the self-coverage INLINEFORM1 be the fraction of content words uttered by INLINEFORM2 in round INLINEFORM3 that are among their own talking points INLINEFORM4 ; and the opponent-coverage INLINEFORM5 be the fraction of its content words covering opposing talking points INLINEFORM6.", so we know that they look at the self-coverage and the opponent-coverage.
Answer: self-coverage and opponent-coverage
True answer: The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.

Validation ID: 26327ccebc620a73ba37a95aabe968864e3392b2
Question: what aspects of conversation flow do they look at?
From the context "We quantify the use of these strategies by comparing the airtime debaters devote to talking points. For a side INLINEFORM0, let the self-coverage INLINEFORM1 be the fraction of content words uttered by INLINEFORM2 in round INLINEFORM3 that are among their own talking points INLINEFORM4 ; and the opponent-coverage INLINEFORM5 be the fraction of its content words covering opposing talking points INLINEFORM6.", so we know that they look at the self-coverage and the opponent-coverage.
Answer: self-coverage and opponent-coverage
True answer: —promoting one's own points and attacking the opponents' points

Validation ID: ababb79dd3c301f4541beafa181f6a6726839a10
Question: what debates dataset was used?
From the context "In this study we use transcripts and results of Oxford-style debates from the public debate series “Intelligence Squared Debates” (IQ2 for short)", so we know that the debates dataset is Intelligence Squared Debates.
Answer: Intelligence Squared Debates
True answer: Intelligence Squared Debates

Validation ID: ababb79dd3c301f4541beafa181f6a6726839a10
Question: what debates dataset was used?
From the context "In this study we use transcripts and results of Oxford-style debates from the public debate series “Intelligence Squared Debates” (IQ2 for short)", so we know that the debates dataset is Intelligence Squared Debates.
Answer: Intelligence Squared Debates
True answer: “Intelligence Squared Debates” (IQ2 for short)

Validation ID: c2b8ee872b99f698b3d2082d57f9408a91e1b4c1
Question: what is the state of the art?
From the context "We evaluate different configurations of the components that we describe in Section SECREF22. Table TABREF47 shows the results of experiments on both CoNLL2003 and GENIA data sets. We report the highest macro-F1 scores for BIO2 labeling for the configuration of letter-trigram word vectors and bidirectional LSTM.", so we know that the state of the art is the configuration of letter-trigram word vectors and bidirectional LSTM.
Answer: the configuration of letter-trigram word vectors and bidirectional LSTM
True answer: Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
From the context "We evaluate different configurations of the components that we describe in Section SECREF22. Table TABREF47 shows the results of experiments on both CoNLL2003 and GENIA data sets.", so we know that CoNLL2003 and GENIA data sets were used.
Answer: CoNLL2003 and GENIA data sets
True answer: The GENIA Corpus , CoNLL2003

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
From the context "We evaluate different configurations of the components that we describe in Section SECREF22. Table TABREF47 shows the results of experiments on both CoNLL2003 and GENIA data sets.", so we know that CoNLL2003 and GENIA data sets were used.
Answer: CoNLL2003 and GENIA data sets
True answer: GENIA Corpus BIBREF3, CoNLL2003 BIBREF14, KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
From the context "We evaluate different configurations of the components that we describe in Section SECREF22. Table TABREF47 shows the results of experiments on both CoNLL2003 and GENIA data sets.", so we know that CoNLL2003 and GENIA data sets are standard dataset.
Answer: CoNLL2003 and GENIA data sets
True answer: CoNLL2003-testA, GENIA

Validation ID: 133eb4aa4394758be5f41744c60c99901b2bc01c
Question: Do they perform error analysis?
From the context "The nature of the content can vary as its provided by different people", so we know that the nature of the content can vary as its provided by different people.
Answer: The nature of the content can vary as its provided by different people
True answer: No

Validation ID: 133eb4aa4394758be5f41744c60c99901b2bc01c
Question: Do they perform error analysis?
From the context "The nature of the content can vary as its provided by different people", so we know that the nature of the content can vary as its provided by different people.
Answer: The nature of the content can vary as its provided by different people
True answer: No

Validation ID: 3fff37b9f68697d080dbd9d9008a63907137644e
Question: How do their results compare to state-of-the-art?
From the context "The Table TABREF27 shows the classification report on the proposed RKS method taking word vectors generated by Google universal encoder model as features with dimension 512. For this work, such vector is explicitly mapped to dimensions 100, 200, 500 and 1000 using RKS. The maximum accuracy obtained is 90.58% for higher dimension 1000.", so we know that the maximum accuracy obtained is 90.58% for higher dimension 1000.
Answer: 90.58%
True answer: Unanswerable

Validation ID: 3fff37b9f68697d080dbd9d9008a63907137644e
Question: How do their results compare to state-of-the-art?
From the context "The Table TABREF27 shows the classification report on the proposed RKS method taking word vectors generated by Google universal encoder model as features with dimension 512. For this work, such vector is explicitly mapped to dimensions 100, 200, 500 and 1000 using RKS. The maximum accuracy obtained is 90.58% for higher dimension 1000.", so we know that the maximum accuracy obtained is 90.58% for higher dimension 1000.
Answer: 90.58%
True answer: Unanswerable

Validation ID: a778b8204a415b295f73b93623d09599f242f202
Question: What is the Random Kitchen Sink approach?
From the context "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping. Here, $\phi (.)$ denotes the implicit mapping function (used to compute kernel matrix), $Z(.)$ denotes the explicit mapping function using RKS and ${\Omega _k}$ denotes random variable.", so we know that the Random Kitchen Sink approach is an approximate kernel function via explicit mapping.
Answer: approximate kernel function via explicit mapping
True answer: Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.

Validation ID: a778b8204a415b295f73b93623d09599f242f202
Question: What is the Random Kitchen Sink approach?
From the context "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping. Here, $\phi (.)$ denotes the implicit mapping function (used to compute kernel matrix), $Z(.)$ denotes the explicit mapping function using RKS and ${\Omega _k}$ denotes random variable.", so we know that the Random Kitchen Sink approach is a method to map data vectors to a space where linear separation is possible.
Answer: a method to map data vectors to a space where linear separation is possible
True answer: explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping

Validation ID: 642e8cf1d39faa1cd985d16750cdc6696c52db2f
Question: what are the baseline systems?
From the context "Our systems are attentional encoder-decoder networks BIBREF0. We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.", so we know that the baseline systems are attentional encoder-decoder networks.
Answer: attentional encoder-decoder networks
True answer: attentional encoder-decoder networks BIBREF0

Validation ID: 642e8cf1d39faa1cd985d16750cdc6696c52db2f
Question: what are the baseline systems?
From the context "Our systems are attentional encoder-decoder networks BIBREF0. We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.", so we know that the baseline systems are attentional encoder-decoder networks.
Answer: attentional encoder-decoder networks
True answer:  the dl4mt-tutorial

Validation ID: 493e971ee3f57a821ef1f67ef3cd47ade154e7c4
Question: What word embeddings do they test?
From the context "We studied the performance of EqEmb on articles from the arXiv. EqEmb models provide better fits than existing embedding approaches, and infer meaningful semantic relationships between equations and words in the collection. We present a comparison of the proposed models to existing word embeddings approaches. These are: the Bernoulli embeddings (b-emb) BIBREF1, continuous bag-of-words (CBOW) BIBREF5, Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model.", so we know that they test Bernoulli embeddings (b-emb), continuous bag-of-words (CBOW), Distributed Memory version of Paragraph Vector (PV-DM) and the Global Vectors (GloVe).
Answer: Bernoulli embeddings (b-emb), continuous bag-of-words (CBOW), Distributed Memory version of Paragraph Vector (PV-DM) and the Global Vectors (GloVe)
True answer: Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model

Validation ID: 493e971ee3f57a821ef1f67ef3cd47ade154e7c4
Question: What word embeddings do they test?
From the context "We studied the performance of EqEmb on articles from the arXiv. EqEmb models provide better fits than existing embedding approaches, and infer meaningful semantic relationships between equations and words in the collection. We present a comparison of the proposed models to existing word embeddings approaches. These are: the Bernoulli embeddings (b-emb) BIBREF1, continuous bag-of-words (CBOW) BIBREF5, Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model.", so we know that they test Bernoulli embeddings (b-emb), continuous bag-of-words (CBOW), Distributed Memory version of Paragraph Vector (PV-DM) and the Global Vectors (GloVe).
Answer: Bernoulli embeddings (b-emb), continuous bag-of-words (CBOW), Distributed Memory version of Paragraph Vector (PV-DM) and the Global Vectors (GloVe)
True answer: Bernoulli embeddings, continuous bag-of-words, Distributed Memory version of Paragraph Vector, Global Vectors, equation embeddings, equation unit embeddings

Validation ID: 8dd8e5599fc56562f2acbc16dd8544689cddd938
Question: How do they define similar equations?
From the context "We presented unsupervised approaches for semantic representations of mathematical equations using their surrounding words. Across 4 different collections we showed that out methods offer more effective modeling compared to existing embedding models. We also demonstrate that they can capture the semantic similarity between equations and the words in the collection.", so we know that they can capture the semantic similarity between equations and the words in the collection.
Answer: they can capture the semantic similarity between equations and the words in the collection
True answer: By using Euclidean distance computed between the context vector representations of the equations

Validation ID: 8dd8e5599fc56562f2acbc16dd8544689cddd938
Question: How do they define similar equations?
From the context "We presented unsupervised approaches for semantic representations of mathematical equations using their surrounding words. Across 4 different collections we showed that out methods offer more effective modeling compared to existing embedding models. We also demonstrate that they can capture the semantic similarity between equations and the words in the collection.", so we know that they can capture the semantic similarity between equations and the words in the collection.
Answer: they can capture the semantic similarity between equations and the words in the collection
True answer: Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B.

Validation ID: abe2393415e533cb06311e74ed1c5674cff8571f
Question: What evaluation criteria and metrics were used to evaluate the generated text?
From the context "In the minimum edit evaluation, carried out by the annotator who created the news corpus, only factual mistakes and grammatical errors are corrected, resulting in text which may remain awkward or unfluent. The word error rate (WER) of the generated text compared to its corrected variant as a reference is 5.6% (6.2% disregarding punctuation). The WER measure is defined as the number of insertions, substitutions, and deletions divided by the total length of the reference, in terms of tokens. The measure is the edit distance of the generated text and its corrected variant, directly reflecting the amount of effort needed to correct the generated output.", so we know that the evaluation criteria and metrics were used to evaluate the generated text are minimum edit evaluation and word error rate (WER).
Answer: minimum edit evaluation and word error rate (WER)
True answer: BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency

Validation ID: abe2393415e533cb06311e74ed1c5674cff8571f
Question: What evaluation criteria and metrics were used to evaluate the generated text?
From the context "In the minimum edit evaluation, carried out by the annotator who created the news corpus, only factual mistakes and grammatical errors are corrected, resulting in text which may remain awkward or unfluent. The word error rate (WER) of the generated text compared to its corrected variant as a reference is 5.6% (6.2% disregarding punctuation). The WER measure is defined as the number of insertions, substitutions, and deletions divided by the total length of the reference, in terms of tokens. The measure is the edit distance of the generated text and its corrected variant, directly reflecting the amount of effort needed to correct the generated output.", so we know that the evaluation criteria and metrics were used to evaluate the generated text are minimum edit evaluation and word error rate.
Answer: minimum edit evaluation and word error rate
True answer: BLEU, NIST, METEOR, ROUGE-L, CIDEr

Validation ID: 00c57e45ac6afbdfa67350a57e81b4fad0ed2885
Question: Do they evaluate only on English datasets?
From the context "We conducted two feature study experiments: 1) a feature ablation study to assess the contribution of feature groups and 2) a feature elimination study to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy", so we know that they evaluate on Twitter tweets.
Answer: Twitter tweets
True answer: Yes

Validation ID: 00c57e45ac6afbdfa67350a57e81b4fad0ed2885
Question: Do they evaluate only on English datasets?
From the context "We conducted two feature study experiments: 1) a feature ablation study to assess the contribution of feature groups and 2) a feature elimination study to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy", so we know that they evaluate on Twitter tweets.
Answer: Twitter tweets
True answer: Yes

Validation ID: 22714f6cad2d5c54c28823e7285dc85e8d6bc109
Question: What are the three steps to feature elimination?
From the context "We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach: Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset. Selection We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation. Rank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class.", so we know that the three steps to feature elimination are: Reduction, Selection and Rank.
Answer: Reduction, Selection and Rank
True answer: Reduction, Selection, Rank

Validation ID: 22714f6cad2d5c54c28823e7285dc85e8d6bc109
Question: What are the three steps to feature elimination?
From the context "We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach: Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset. Selection We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation. Rank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class.", so we know that the three steps to feature elimination are: Reduction, Selection and Rank.
Answer: Reduction, Selection and Rank
True answer: reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features.

Validation ID: 82642d3111287abf736b781043d49536fe48c350
Question: How is the dataset annotated?
From the context "This paper builds upon related works toward classifying Twitter tweets representing symptoms of major depressive disorder by assessing the contribution of lexical features (e.g., unigrams) and emotion (e.g., strongly negative) to classification performance, and by applying methods to eliminate low-value features.
METHODS
True answer: no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy

Validation ID: 82642d3111287abf736b781043d49536fe48c350
Question: How is the dataset annotated?
From the context "This paper builds upon related works toward classifying Twitter tweets representing symptoms of major depressive disorder by assessing the contribution of lexical features (e.g., unigrams) and emotion (e.g., strongly negative) to classification performance, and by applying methods to eliminate low-value features.
METHODS
True answer: The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression

Validation ID: 5a81732d52f64e81f1f83e8fd3514251227efbc7
Question: What dataset is used for this study?
From the context "This paper builds upon related works toward classifying Twitter tweets representing symptoms of major depressive disorder by assessing the contribution of lexical features (e.g., unigrams) and emotion (e.g., strongly negative) to classification performance, and by applying methods to eliminate low-value features.
METHODS
True answer: BIBREF12 , BIBREF13

Validation ID: 5a81732d52f64e81f1f83e8fd3514251227efbc7
Question: What dataset is used for this study?
From the context "This paper builds upon related works toward classifying Twitter tweets representing symptoms of major depressive disorder by assessing the contribution of lexical features (e.g., unigrams) and emotion (e.g., strongly negative) to classification performance, and by applying methods to eliminate low-value features.
METHODS
True answer: an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13

Validation ID: 9a8b9ea3176d30da2453cac6e9347737c729a538
Question: what were their performance results?
From the context "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23).", so we know that the performance results are $0.995$ on synthesized queries and $0.948$ on clinical notes for the hybrid NER model, and $0.441$ on synthesized queries and $0.927$ on clinical notes for the i2b2 NER model.
Answer: $0.995$ on synthesized queries and $0.948$ on clinical notes for the hybrid NER model, and $0.441$ on synthesized queries and $0.927$ on clinical notes for the i2b2 NER model
True answer:  the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes

Validation ID: 9a8b9ea3176d30da2453cac6e9347737c729a538
Question: what were their performance results?
From the context "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23).", so we know that the performance results are $0.995$ on synthesized queries and $0.948$ on clinical notes for the hybrid NER model, and $0.441$ on synthesized queries and $0.927$ on clinical notes for the i2b2 NER model.
Answer: $0.995$ on synthesized queries and $0.948$ on clinical notes for the hybrid NER model, and $0.441$ on synthesized queries and $0.927$ on clinical notes for the i2b2 NER model
True answer: hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes

Validation ID: 4477bb513d56e57732fba126944073d414d1f75f
Question: where did they obtain the annotated clinical notes from?
From the context "We used the BiLSTM-CRF implementation provided by the flair package BIBREF16. We set the hidden size value to be 256 in the LSTM structure and left everything else at default values for the SequenceTagger model on flair. For word embeddings, we used the ELMo embeddings fine-tuned on PubMed articles and flair embeddings BIBREF13 trained on $5\%$ of PubMed abstracts, respectively. We trained models for 10 epochs and experimented with different learning rate, mini batch size, and dropouts. We ran hyperparameter optimization tests to find the best combination. $S_c$ is set to be 0.6 in our experiment.", so we know that they used the BiLSTM-CRF implementation provided by the flair package BIBREF16. They set the hidden size value to be 256 in the LSTM structure and left everything else at default values for the SequenceTagger model on flair. For word embeddings, they used the ELMo embeddings fine-tuned on PubMed articles and flair embeddings BIBREF13 trained on $5\%$ of PubMed abstracts, respectively. They trained models for 10 epochs and experimented with different learning rate, mini batch size, and dropouts. They ran hyperparameter optimization tests to find the best combination. $S_c$ is set to be 0.6 in their experiment.
Answer: They used the BiLSTM-CRF implementation provided by the flair package BIBREF16. They set the hidden size value to be 256 in the LSTM structure and left everything else at default values for the SequenceTagger model on flair. For word embeddings, they used the ELMo embeddings fine-tuned on PubMed articles and flair embeddings BIBREF13 trained on $5\%$ of PubMed abstracts, respectively. They trained models for 10 epochs and experimented with different learning rate, mini batch size, and dropouts. They ran hyperparameter optimization tests to find the best combination. $S_c$ is set to be 0.6 in their experiment.
True answer: clinical notes from the CE task in 2010 i2b2/VA

Validation ID: 4477bb513d56e57732fba126944073d414d1f75f
Question: where did they obtain the annotated clinical notes from?
From the context "We used the BiLSTM-CRF implementation provided by the flair package BIBREF16. We set the hidden size value to be 256 in the LSTM structure and left everything else at default values for the SequenceTagger model on flair. For word embeddings, we used the ELMo embeddings fine-tuned on PubMed articles and flair embeddings BIBREF13 trained on $5\%$ of PubMed abstracts, respectively. We trained models for 10 epochs and experimented with different learning rate, mini batch size, and dropouts. We ran hyperparameter optimization tests to find the best combination. $S_c$ is set to be 0.6 in our experiment.", so we know that they used the BiLSTM-CRF implementation provided by the flair package BIBREF16. They set the hidden size value to be 256 in the LSTM structure and left everything else at default values for the SequenceTagger model on flair. For word embeddings, they used the ELMo embeddings fine-tuned on PubMed articles and flair embeddings BIBREF13 trained on $5\%$ of PubMed abstracts, respectively. They trained models for 10 epochs and experimented with different learning rate, mini batch size, and dropouts. They ran hyperparameter optimization tests to find the best combination. $S_c$ is set to be 0.6 in their experiment.
Answer: They used the BiLSTM-CRF implementation provided by the flair package BIBREF16. They set the hidden size value to be 256 in the LSTM structure and left everything else at default values for the SequenceTagger model on flair. For word embeddings, they used the ELMo embeddings fine-tuned on PubMed articles and flair embeddings BIBREF13 trained on $5\%$ of PubMed abstracts, respectively. They trained models for 10 epochs and experimented with different learning rate, mini batch size, and dropouts. They ran hyperparameter optimization tests to find the best combination. $S_c$ is set to be 0.6 in their experiment.
True answer: clinical notes from the CE task in 2010 i2b2/VA 

Validation ID: 1b23c4535a6c10eb70bbc95313c465e4a547db5e
Question: Which architecture do they use for the encoder and decoder?
From the context "We construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech. We use a striding mechanism to reduce the dimension from the input frames BIBREF17, while the NIN layer represents more complex structures on the top of the convolutional layers. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2.", so we know that they use convolutional layers, NIN layers, Bi-LSTM and MLP for the encoder and unidirectional LSTM with global attention for the decoder.
Answer: convolutional layers, NIN layers, Bi-LSTM and MLP for the encoder and unidirectional LSTM with global attention for the decoder
True answer: we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part, On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)

Validation ID: 1b23c4535a6c10eb70bbc95313c465e4a547db5e
Question: Which architecture do they use for the encoder and decoder?
From the context "We construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech. We use a striding mechanism to reduce the dimension from the input frames BIBREF17, while the NIN layer represents more complex structures on the top of the convolutional layers. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2.", so we know that they use convolutional layers, NIN layers, Bi-LSTM and MLP for the encoder and unidirectional LSTM with global attention for the decoder.
Answer: convolutional layers, NIN layers, Bi-LSTM and MLP for the encoder and unidirectional LSTM with global attention for the decoder
True answer: In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM 

Validation ID: 0a75a52450ed866df3a304077769e1725a995bb7
Question: How does their decoder generate text?
From the context "The decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4, can be formulated as: DISPLAYFORM0", so we know that the decoder task predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4.
Answer: the decoder task predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4
True answer: decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information

Validation ID: 0a75a52450ed866df3a304077769e1725a995bb7
Question: How does their decoder generate text?
From the context "The decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4, can be formulated as: DISPLAYFORM0", so we know that the decoder task predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4.
Answer: the decoder task predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4
True answer: Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy

Validation ID: fd0a3e9c210163a55d3ed791e95ae3875184b8f8
Question: Which dataset do they use?
From the context "We investigate the performance of our proposed models on standard ASR datasets. In practice, optimizing an encoder-decoder framework is more difficult than a standard neural network architecture BIBREF10. Therefore, we propose a feature transfer learning method to assist the training process for our end-to-end attention-based ASR model.
Speech Data
True answer: WSJ

Validation ID: fd0a3e9c210163a55d3ed791e95ae3875184b8f8
Question: Which dataset do they use?
From the context "In this study, we investigate the performance of our proposed models on WSJ BIBREF5. We used the same definitions of the training, development and test set as the Kaldi s5 recipe BIBREF18. The raw speech waveforms were segmented into multiple frames with a 25ms window size and a 10ms step size. We normalized the raw speech waveform into the range -1 to 1. For spectral based features such as MFCC and log Mel-scale spectrogram, we normalized the features for each dimension into zero mean and unit variance. For WSJ, we separated into two experiments by using WSJ-SI84 only and WSJ-SI284 data. We used dev_93 for our validation set and eval_92 for our test set. We used the character sequence as our decoder target and followed the preprocessing step proposed by BIBREF19. The text from all the utterances was mapped into a 32-character set: 26 (a-z) alphabet, apostrophe, period, dash, space, noise, and “eos".", so we know that they use WSJ dataset.
Answer: WSJ dataset
True answer: WSJ-SI84, WSJ-SI284

Validation ID: c37f65c9f0d543a35c784263b79236ccf1c44fac
Question: What model is used to encode the images?
From the context "The model's first component is a Recurrent Neural Network (RNN), more precisely an LSTM that summarizes the sequence of images. At every timestep $t$ the network takes as input an image $I_i$ where $i\in \lbrace 1,2,3,4,5\rbrace $ from the sequence. At time $t=5$, the LSTM has encoded the 5 images and provides the sequence's context through its last hidden state denoted by $h_e^{(t)}$. The representation of the images was obtained through Inception V3.", so we know that the model used to encode the images is Inception V3.
Answer: Inception V3
True answer: a Convolutional Neural Network (CNN)

Validation ID: c37f65c9f0d543a35c784263b79236ccf1c44fac
Question: What model is used to encode the images?
From the context "The model's first component is a Recurrent Neural Network (RNN), more precisely an LSTM that summarizes the sequence of images. At every timestep $t$ the network takes as input an image $I_i$ where $i\in \lbrace 1,2,3,4,5\rbrace $ from the sequence. At time $t=5$, the LSTM has encoded the 5 images and provides the sequence's context through its last hidden state denoted by $h_e^{(t)}$. The representation of the images was obtained through Inception V3.", so we know that the model is an LSTM that summarizes the sequence of images.
Answer: LSTM
True answer: LSTM

Validation ID: 584af673429c7f8621c6bf83362a37048daa0e5d
Question: How is the sequential nature of the story captured?
From the context "Our model extends the image description model by BIBREF0, which consists of an encoder-decoder architecture. The encoder is a Convolutional Neural Network (CNN) and the decoder is a Long Short-Term Memory (LSTM) network, as presented in Figure 2. The image is passed through the encoder generating the image representation that is used by the decoder to know the content of the image and generate the description word by word. In the following, we describe how we extended this model for the visual storytelling task.", so we know that the sequential nature of the story is captured by the encoder-decoder architecture.
Answer: encoder-decoder architecture
True answer: we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story

Validation ID: 584af673429c7f8621c6bf83362a37048daa0e5d
Question: How is the sequential nature of the story captured?
From the context "Our model extends the image description model by BIBREF0, which consists of an encoder-decoder architecture. The encoder is a Convolutional Neural Network (CNN) and the decoder is a Long Short-Term Memory (LSTM) network, as presented in Figure 2. The image is passed through the encoder generating the image representation that is used by the decoder to know the content of the image and generate the description word by word. In the following, we describe how we extended this model for the visual storytelling task.", so we know that the sequential nature of the story is captured by the encoder-decoder architecture.
Answer: encoder-decoder architecture
True answer: The encoder takes the images in order, one at every timestep $t$ . At time $t=5$ , we obtain the context vector through $h_e^{(t)}$ (represented by $\mathbf {Z}$ ). This vector is used to initialize each decoder's hidden state while the first input to each decoder is its corresponding image embedding $e(I_i)$ . Each decoder generates a sequence of words $\lbrace p_1,...,p_{n}\rbrace $ for each image in the sequence. 

Validation ID: 1be54c5b3ea67d837ffba2290a40c1e720d9587f
Question: Is the position in the sequence part of the input?
From the context "Our model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence. For instance, the first decoder will learn the opening sentences of the story while the last decoder the closing sentences.", so we know that the position in the sequence is part of the input.
Answer: Yes
True answer: No

Validation ID: 1be54c5b3ea67d837ffba2290a40c1e720d9587f
Question: Is the position in the sequence part of the input?
From the context "Our model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence. For instance, the first decoder will learn the opening sentences of the story while the last decoder the closing sentences.", so we know that the position in the sequence is part of the input.
Answer: Yes
True answer: Yes

Validation ID: b08f88d1facefceb87e134ba2c1fa90035018e83
Question: Do the decoder LSTMs all have the same weights?
From the context "Our model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence. For instance, the first decoder will learn the opening sentences of the story while the last decoder the closing sentences.", so we know that the decoder LSTMs all have the same weights.
Answer: Yes
True answer: No

Validation ID: b08f88d1facefceb87e134ba2c1fa90035018e83
Question: Do the decoder LSTMs all have the same weights?
From the context "Our model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence. For instance, the first decoder will learn the opening sentences of the story while the last decoder the closing sentences.", so we know that the decoder LSTMs all have the same weights.
Answer: Yes
True answer: No

Validation ID: b06512c17d99f9339ffdab12cedbc63501ff527e
Question: Is fine-tuning required to incorporate these embeddings into existing models?
From the context "In the remainder of the section, we briefly provide high-level overviews of each task and summarize results for all tasks considered.", so we know that the author will provide high-level overviews of each task and summarize results for all tasks considered.
From the context "We emphasize that our method is very generic and can be used to improve the performance of a large class of knowledge graph prediction models.", so we know that the author emphasize that their method is very generic and can be used to improve the performance of a large class of knowledge graph prediction models.
True answer: No

Validation ID: b06512c17d99f9339ffdab12cedbc63501ff527e
Question: Is fine-tuning required to incorporate these embeddings into existing models?
From the context "In the remainder of the section, we briefly provide high-level overviews of each task and summarize results for all tasks considered.", so we know that the author will provide high-level overviews of each task and summarize results for all tasks considered.
From the context "We emphasize that our method is very generic and can be used to improve the performance of a large class of knowledge graph prediction models.", so we know that the author emphasize that their method is very generic and can be used to improve the performance of a large class of knowledge graph prediction models.
True answer: No

Validation ID: fd8e23947095fe2230ffe1a478945829b09c8c95
Question: How are meaningful chains in the graph selected?
From the context "We generate 20 chains for each node in the knowledge graph, with the length of each chain being 21 (10 relations and 11 entities appear alternately)", so we know that meaningful chains in the graph are selected by generating 20 chains for each node in the knowledge graph, with the length of each chain being 21 (10 relations and 11 entities appear alternately).
Answer: 20 chains for each node in the knowledge graph, with the length of each chain being 21 (10 relations and 11 entities appear alternately)
True answer: No

Validation ID: fd8e23947095fe2230ffe1a478945829b09c8c95
Question: How are meaningful chains in the graph selected?
From the context "We generate 20 chains for each node in the knowledge graph, with the length of each chain being 21 (10 relations and 11 entities appear alternately)", so we know that meaningful chains in the graph are selected by generating 20 chains for each node in the knowledge graph, with the length of each chain being 21 (10 relations and 11 entities appear alternately).
Answer: generating 20 chains for each node in the knowledge graph, with the length of each chain being 21 (10 relations and 11 entities appear alternately)
True answer: utilize the machinery of language modeling using deep neural networks to learn Dolores embeddings.

Validation ID: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0
Question: Do the authors also analyze transformer-based architectures?
From the context "We choose the simple Bi-LSTM max-pooling network from InferSent BIBREF16 :", so we know that the authors do not analyze transformer-based architectures.
Answer: No
True answer: No

Validation ID: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0
Question: Do the authors also analyze transformer-based architectures?
From the context "We choose the simple Bi-LSTM max-pooling network from InferSent BIBREF16 :", so we know that the authors do not analyze transformer-based architectures.
Answer: No
True answer: No

Validation ID: 3611a72f754de1e256fbd25b012197e1c24e8470
Question: Do they remove seasonality from the time series?
From the context "Stock data usually follows a concave function. So, if the stock value on a day is x and the next value present is y with some missing in between. The first missing value is approximated to be (y+x)/2 and the same method is followed to fill all the gaps", so we know that they remove seasonality from the time series.
Answer: Yes
True answer: Unanswerable

Validation ID: 3611a72f754de1e256fbd25b012197e1c24e8470
Question: Do they remove seasonality from the time series?
From the context "Stock data usually follows a concave function. So, if the stock value on a day is x and the next value present is y with some missing in between. The first missing value is approximated to be (y+x)/2 and the same method is followed to fill all the gaps", so we know that they remove seasonality from the time series.
Answer: Yes
True answer: No

Validation ID: 4c07c33dfaf4f3e6db55e377da6fa69825d0ba15
Question: What is the dimension of the embeddings?
From the context "Word2vec representation is far better, advanced and a recent technique which functions by mapping words to a 300 dimensional vector representations", so we know that the dimension of the embeddings is 300.
Answer: 300
True answer: 300

Validation ID: 4c07c33dfaf4f3e6db55e377da6fa69825d0ba15
Question: What is the dimension of the embeddings?
From the context "Word2vec representation is far better, advanced and a recent technique which functions by mapping words to a 300 dimensional vector representations", so we know that the dimension of the embeddings is 300.
Answer: 300
True answer: 300

Validation ID: b1ce129678e37070e69f01332f1a8587e18e06b0
Question: What dataset is used to train the model?
From the context "In this work, we have considered only twitter data for analyzing people's sentiment which may be biased because not all the people who trade in stocks share their opinions on twitter", so we know that the dataset is twitter data.
Answer: twitter data
True answer: 2,50,000 tweets, Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016

Validation ID: b1ce129678e37070e69f01332f1a8587e18e06b0
Question: What dataset is used to train the model?
From the context "In this work, we have considered only twitter data for analyzing people's sentiment which may be biased because not all the people who trade in stocks share their opinions on twitter", so we know that the dataset is twitter data.
Answer: twitter data
True answer: Collected tweets and opening and closing stock prices of Microsoft.

Validation ID: 7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1
Question: What is the previous state of the art?
From the context "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture", so we know that the previous state of the art is based on RoBERTa.
Answer: RoBERTa
True answer: RoBERTa

Validation ID: 7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1
Question: What is the previous state of the art?
From the context "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture", so we know that the previous state of the art is based on RoBERTa.
Answer: RoBERTa
True answer: RoBERTa

Validation ID: 0689904db9b00a814e3109fb1698086370a28fa2
Question: Which text embedding methodologies are used?
From the context "We showcase the methodology through the analysis of a dataset of patient incidents reported to the NRLS. First, we use the 13 million records collected by the NRLS since 2004 to train our text embedding (although a much smaller corpus can be used). We then analyse a subset of 3229 records reported from St Mary's Hospital, London (Imperial College Healthcare NHS Trust) over three months in 2014 to extract clusters of incidents at different levels of resolution in terms of content. Our method reveals multiple levels of intrinsic structure in the topics of the dataset, as shown by the extraction of relevant word descriptors from the grouped records and a high level of topic coherence. Originally, the records had been manually coded by the operator upon reporting with up to 170 features per case, including a two-level manual classification of the incidents. Therefore, we also carried out an a posteriori comparison against the hand-coded categories assigned by the reporter (healthcare personnel) at the time of the report submission. Our results show good overall correspondence with the hand-coded categories across resolutions and, specifically, at the medium level of granularity. Several of our clusters of content correspond strongly to well-defined categories, yet our results also reveal complementary categories of incidents not defined in the external classification. In addition, the tuning of the granularity afforded by the method can be used to provide a distinct level of resolution in certain areas corresponding to specialise or particular sub-themes.", so we know that the text embedding methodologies are Doc2Vec and PV-DBOW.
Answer: Doc2Vec and PV-DBOW
True answer: Document to Vector (Doc2Vec)

Validation ID: 0689904db9b00a814e3109fb1698086370a28fa2
Question: Which text embedding methodologies are used?
From the context "We showcase the methodology through the analysis of a dataset of patient incidents reported to the NRLS. First, we use the 13 million records collected by the NRLS since 2004 to train our text embedding (although a much smaller corpus can be used). We then analyse a subset of 3229 records reported from St Mary's Hospital, London (Imperial College Healthcare NHS Trust) over three months in 2014 to extract clusters of incidents at different levels of resolution in terms of content. Our method reveals multiple levels of intrinsic structure in the topics of the dataset, as shown by the extraction of relevant word descriptors from the grouped records and a high level of topic coherence. Originally, the records had been manually coded by the operator upon reporting with up to 170 features per case, including a two-level manual classification of the incidents. Therefore, we also carried out an a posteriori comparison against the hand-coded categories assigned by the reporter (healthcare personnel) at the time of the report submission. Our results show good overall correspondence with the hand-coded categories across resolutions and, specifically, at the medium level of granularity. Several of our clusters of content correspond strongly to well-defined categories, yet our results also reveal complementary categories of incidents not defined in the external classification. In addition, the tuning of the granularity afforded by the method can be used to provide a distinct level of resolution in certain areas corresponding to specialise or particular sub-themes.", so we know that the text embedding methodologies are Doc2Vec and PV-DBOW.
Answer: Doc2Vec and PV-DBOW
True answer: Doc2Vec, PV-DBOW model

Validation ID: cc354c952b5aaed2d4d1e932175e008ff2d801dd
Question: Which race and gender are given higher sentiment intensity predictions?
From the context "We observe that the systems consistently give higher sentiment intensity predictions to sentences with female noun phrases than to sentences with male noun phrases", so we know that the systems consistently give higher sentiment intensity predictions to sentences with female noun phrases than to sentences with male noun phrases.
Answer: female noun phrases
True answer: Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.
African American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task.

Validation ID: cc354c952b5aaed2d4d1e932175e008ff2d801dd
Question: Which race and gender are given higher sentiment intensity predictions?
From the context "We observe that the systems consistently give higher sentiment intensity predictions to sentences with female noun phrases than to sentences with male noun phrases. The average score difference F INLINEFORM0 –M INLINEFORM1 is 0.03, and the average score difference F INLINEFORM2 –M INLINEFORM3 is 0.04. The maximal score difference ( INLINEFORM4 ) is as high as 0.34. The results are similar for the race bias analysis. The systems consistently give higher sentiment intensity predictions to sentences with white noun phrases than to sentences with black noun phrases. The average score difference W INLINEFORM0 –B INLINEFORM1 is 0.03, and the average score difference W INLINEFORM2 –B INLINEFORM3 is 0.04. The maximal score difference ( INLINEFORM4 ) is as high as 0.34.", so we know that the systems consistently give higher sentiment intensity predictions to sentences with female noun phrases than to sentences with male noun phrases. The average score difference F INLINEFORM0 –M INLINEFORM1 is 0.03, and the average score difference F INLINEFORM2 –M INLINEFORM3 is 0.04. The maximal score difference ( INLINEFORM4 ) is as high as 0.34. The results are similar for the race bias analysis. The systems consistently give higher sentiment intensity predictions to sentences with white noun phrases than to sentences with black noun phrases. The average score difference W INLINEFORM0 –B INLINEFORM1 is 0.03, and the average score difference W INLINEFORM2 –B INLINEFORM3 is 0.04. The maximal score difference ( INLINEFORM4 ) is as high as 0.34.
Answer: female noun phrases and white noun phrases
True answer:  the number of systems consistently giving higher scores to sentences with female noun phrases, higher scores to sentences with African American names on the tasks of anger, fear, and sadness,  joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names

Validation ID: 0f12dc077fe8e5b95ca9163cea1dd17195c96929
Question: What criteria are used to select the 8,640 English sentences?
From the context "We decided to use sentences involving at least one race- or gender-associated word. The sentences were intended to be short and grammatically simple. We also wanted some sentences to include expressions of sentiment and emotion, since the goal is to test sentiment and emotion systems. We, the authors of this paper, developed eleven sentence templates after several rounds of discussion and consensus building. They are shown in Table TABREF3. The templates are divided into two groups. The first type (templates 1–7) includes emotion words. The purpose of this set is to have sentences expressing emotions. The second type (templates 8–11) does not include any emotion words. The purpose of this set is to have non-emotional (neutral) sentences.", so we know that the criteria are sentences involving at least one race- or gender-associated word, short and grammatically simple, and some sentences to include expressions of sentiment and emotion.
Answer: sentences involving at least one race- or gender-associated word, short and grammatically simple, and some sentences to include expressions of sentiment and emotion
True answer: Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.

Validation ID: 0f12dc077fe8e5b95ca9163cea1dd17195c96929
Question: What criteria are used to select the 8,640 English sentences?
From the context "We decided to use sentences involving at least one race- or gender-associated word. The sentences were intended to be short and grammatically simple. We also wanted some sentences to include expressions of sentiment and emotion, since the goal is to test sentiment and emotion systems. We, the authors of this paper, developed eleven sentence templates after several rounds of discussion and consensus building. They are shown in Table TABREF3. The templates are divided into two groups. The first type (templates 1–7) includes emotion words. The purpose of this set is to have sentences expressing emotions. The second type (templates 8–11) does not include any emotion words. The purpose of this set is to have non-emotional (neutral) sentences.", so we know that the criteria are sentences involving at least one race- or gender-associated word, short and grammatically simple, and some sentences to include expressions of sentiment and emotion.
Answer: sentences involving at least one race- or gender-associated word, short and grammatically simple, and some sentences to include expressions of sentiment and emotion
True answer: generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates, differ only in one word corresponding to gender or race

Validation ID: 2ddb51b03163d309434ee403fef42d6b9aecc458
Question: what were the baselines?
From the context "We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31", so we know that the baselines are other models trained using the same data.
Answer: other models trained using the same data
True answer: Unanswerable

Validation ID: 2ddb51b03163d309434ee403fef42d6b9aecc458
Question: what were the baselines?
From the context "We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31", so we know that the baselines are other models trained using the same data.
Answer: other models trained using the same data
True answer: LF-MMI Attention
Seq2Seq 
RNN-T 
Char E2E LF-MMI 
Phone E2E LF-MMI 
CTC + Gram-CTC

Validation ID: e587559f5ab6e42f7d981372ee34aebdc92b646e
Question: what competitive results did they obtain?
From the context "As our architecture studies demonstrated, a combination of standard components leads to SOTA results on LibriSpeech and competitive results on other benchmarks", so we know that they obtained competitive results on other benchmarks.
Answer: competitive results on other benchmarks
True answer: In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.
In case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. 

Validation ID: e587559f5ab6e42f7d981372ee34aebdc92b646e
Question: what competitive results did they obtain?
From the context "As our architecture studies demonstrated, a combination of standard components leads to SOTA results on LibriSpeech and competitive results on other benchmarks", so we know that they obtained competitive results on other benchmarks.
Answer: competitive results on other benchmarks
True answer: On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.
On Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.

Validation ID: bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a
Question: How much is performance improved on NLI?
From the context "Table TABREF21 demonstrates the results on the SQuAD v1.1 dataset. The comparison between BERTBase-PN and BERTBase indicates that the inclusion of the PSP subtask is beneficial (2.4% absolute improvement). When using BERTBase-PNsmth, another 0.3% increase in EM can be obtained. The experimental results on the SQuAD v2.0 BIBREF16 are also shown in Table. TABREF26. The SQuAD v2.0 differs from SQuAD v1.1 by allowing the question-paragraph pairs that have no answer. For SQuAD v2.0, our method also achieved about 4% absolute improvement in both EM and F1 against BERTBase.", so we know that the performance is improved by 2.4% on NLI.
Answer: 2.4%
True answer:  improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase

Validation ID: bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a
Question: How much is performance improved on NLI?
From the context "Table TABREF21 demonstrates the results on the SQuAD v1.1 dataset. The comparison between BERTBase-PN and BERTBase indicates that the inclusion of the PSP subtask is beneficial (2.4% absolute improvement). When using BERTBase-PNsmth, another 0.3% increase in EM can be obtained. The experimental results on the SQuAD v2.0 BIBREF16 are also shown in Table. TABREF26. The SQuAD v2.0 differs from SQuAD v1.1 by allowing the question-paragraph pairs that have no answer. For SQuAD v2.0, our method also achieved about 4% absolute improvement in both EM and F1 against BERTBase.", so we know that performance is improved on NLI by 2.4% and 0.3% on SQuAD v1.1 and SQuAD v2.0 respectively.
Answer: 2.4% and 0.3%
True answer: The average score improved by 1.4 points over the previous best result.

Validation ID: 7b4fb6da74e6bd1baea556788a02969134cf0800
Question: Do they train their model starting from a checkpoint?
From the context "The output vectors are used to compute the score of tokens being start and end of the answer span. The valid span that has the maximum score is selected as the prediction. And similarly, the fine-tuning training was performed for 3 epochs with a learning rate of 3e-5.", so we know that they train their model starting from a checkpoint.
Answer: Yes
True answer: No

Validation ID: 7b4fb6da74e6bd1baea556788a02969134cf0800
Question: Do they train their model starting from a checkpoint?
From the context "The pre-trained model is initialized with the BERTBase model, which is trained on the English Wikipedia and BookCorpus", so we know that they train their model starting from a checkpoint.
Answer: Yes
True answer: No

Validation ID: bc31a3d2f7c608df8c019a64d64cb0ccc5669210
Question: What BERT model do they test?
From the context "We evaluate our method on the MRC tasks. The Stanford Question Answering Dataset (SQuAD v1.1) is a question answering (QA) dataset, which consists of 100K samples BIBREF15. Each data sample has a question and a corresponding Wikipedia passage that contains the answer. The goal is to extract the answer from the passage for the given question. In the fine-tuning procedure, we follow the exact way the BERT performed. The output vectors are used to compute the score of tokens being start and end of the answer span. The valid span that has the maximum score is selected as the prediction. And similarly, the fine-tuning training was performed for 3 epochs with a learning rate of 3e-5.", so we know that they test BERTBase model.
Answer: BERTBase
True answer: BERTbase

Validation ID: bc31a3d2f7c608df8c019a64d64cb0ccc5669210
Question: What BERT model do they test?
From the context "We evaluate our method on the MRC tasks. The Stanford Question Answering Dataset (SQuAD v1.1) is a question answering (QA) dataset, which consists of 100K samples BIBREF15. Each data sample has a question and a corresponding Wikipedia passage that contains the answer. The goal is to extract the answer from the passage for the given question. In the fine-tuning procedure, we follow the exact way the BERT performed. The output vectors are used to compute the score of tokens being start and end of the answer span. The valid span that has the maximum score is selected as the prediction. And similarly, the fine-tuning training was performed for 3 epochs with a learning rate of 3e-5.", so we know that they test BERTBase model.
Answer: BERTBase
True answer: BERTbase

Validation ID: 761de1610e934189850e8fda707dc5239dd58092
Question: what was the baseline?
From the context "Table TABREF46 shows the results of our multistage fine-tuning, where the IDs of each row refer to those described in Section SECREF41. First of all, the final models of our multistage fine-tuning, i.e., V and VII, achieved significantly higher BLEU scores than (b3) in Table TABREF27, a weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data.", so we know that the baseline is (b3) in Table TABREF27, a weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data.
Answer: (b3) in Table TABREF27, a weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data
True answer: pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17

Validation ID: 761de1610e934189850e8fda707dc5239dd58092
Question: what was the baseline?
From the context "Table TABREF46 shows the results of our multistage fine-tuning, where the IDs of each row refer to those described in Section SECREF41. First of all, the final models of our multistage fine-tuning, i.e., V and VII, achieved significantly higher BLEU scores than (b3) in Table TABREF27, a weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data.", so we know that the baseline is (b3) in Table TABREF27, a weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data.
Answer: (b3) in Table TABREF27, a weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data
True answer: M2M Transformer

Validation ID: f8da63df16c4c42093e5778c01a8e7e9b270142e
Question: How is segmentation quality evaluated?
From the context "We have built a second $L_c$ list for the automatically identified segments, following the same criteria of $L_r$. The $L_r$ and $L_c$ lists regroup, pair by pair, the segment border. We then count the common pair intersection of the two lists. Each pair in the $L_c$ list is also present in the $L_r$ reference list and is a correctly assigned to the class pair. A word pair belonging to the $L_c$ list but not belonging to the $L_r$ reference list, will be a pair assigned to the class.", so we know that the segmentation quality is evaluated by counting the common pair intersection of the two lists.
Answer: by counting the common pair intersection of the two lists
True answer: Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.

Validation ID: f8da63df16c4c42093e5778c01a8e7e9b270142e
Question: How is segmentation quality evaluated?
From the context "We have built a second $L_c$ list for the automatically identified segments, following the same criteria of $L_r$. The $L_r$ and $L_c$ lists regroup, pair by pair, the segment border. We then count the common pair intersection of the two lists. Each pair in the $L_c$ list is also present in the $L_r$ reference list and is a correctly assigned to the class pair. A word pair belonging to the $L_c$ list but not belonging to the $L_r$ reference list, will be a pair assigned to the class.", so we know that the segmentation quality is evaluated by counting the common pair intersection of the two lists.
Answer: by counting the common pair intersection of the two lists
True answer: we compare the Annodis segmentation with the automatically produced segmentation

Validation ID: c09a92e25e6a81369fcc4ae6045491f2690ccc10
Question: How do they compare lexicons?
From the context "We compare crowd evaluations to those of expert linguists", so we know that they compare crowd evaluations to those of expert linguists.
Answer: compare crowd evaluations to those of expert linguists
True answer: Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.

Validation ID: c09a92e25e6a81369fcc4ae6045491f2690ccc10
Question: How do they compare lexicons?
From the context "We compare crowd evaluations to those of expert linguists", so we know that they compare crowd evaluations to those of expert linguists.
Answer: compare crowd evaluations to those of expert linguists
True answer: 1000 term groups based on the number of total annotations(200 term groups with 2 total annotations, 200 term groups with 3 total annotations, and so on up to term groups with 6 total annotations)

Validation ID: 63c3550c6fb42f41a0c93133e9fca12ac00df9b3
Question: Is it possible to convert a cloze-style questions to a naturally-looking questions?
From the context "We hypothesize that we can benefit from the un-annotated corpora to better answer various questions – at least ones that are lexically similar to the content in base documents and directly require factual information.", so we know that it is possible to convert a cloze-style questions to a naturally-looking questions.
Answer: Yes
True answer: Unanswerable

Validation ID: 63c3550c6fb42f41a0c93133e9fca12ac00df9b3
Question: Is it possible to convert a cloze-style questions to a naturally-looking questions?
From the context "We hypothesize that we can benefit from the un-annotated corpora to better answer various questions – at least ones that are lexically similar to the content in base documents and directly require factual information.", so we know that it is possible to convert a cloze-style questions to a naturally-looking questions.
Answer: Yes
True answer: Unanswerable

Validation ID: 603fee7314fa65261812157ddfc2c544277fcf90
Question: How larger are the training sets of these versions of ELMo compared to the previous ones?
From the context "We trained each ELMo model using default values used to train the original English ELMo (large) model.", so we know that the training sets of these versions of ELMo are the same as the previous ones.
Answer: the same
True answer: By 14 times.

Validation ID: 603fee7314fa65261812157ddfc2c544277fcf90
Question: How larger are the training sets of these versions of ELMo compared to the previous ones?
From the context "We trained each ELMo model using default values used to train the original English ELMo (large) model.", so we know that the training sets of these versions of ELMo are the same as the previous ones.
Answer: the same
True answer: up to 1.95 times larger

Validation ID: 09a1173e971e0fcdbf2fbecb1b077158ab08f497
Question: What is the improvement in performance for Estonian in the NER task?
From the context "The results are presented in Table TABREF21. We included the evaluation of the original ELMo English model in the same table. NER models have little difficulty distinguishing between types of named entities, but recognizing whether a word is a named entity or not is more difficult. For languages with the smallest NER datasets, Croatian and Lithuanian, ELMo embeddings show the largest improvement over fastText embeddings. However, we can observe significant improvements with ELMo also on English and Finnish, which are among the largest datasets (English being by far the largest). Only on Slovenian dataset did ELMo perform slightly worse than fastText, on all other EMBEDDIA languages, the ELMo embeddings improve the results.", so we know that the improvement in performance for Estonian in the NER task is significant.
Answer: significant
True answer: 5 percent points.

Validation ID: 09a1173e971e0fcdbf2fbecb1b077158ab08f497
Question: What is the improvement in performance for Estonian in the NER task?
From the context "The results are presented in Table TABREF21. We included the evaluation of the original ELMo English model in the same table. NER models have little difficulty distinguishing between types of named entities, but recognizing whether a word is a named entity or not is more difficult. For languages with the smallest NER datasets, Croatian and Lithuanian, ELMo embeddings show the largest improvement over fastText embeddings. However, we can observe significant improvements with ELMo also on English and Finnish, which are among the largest datasets (English being by far the largest). Only on Slovenian dataset did ELMo perform slightly worse than fastText, on all other EMBEDDIA languages, the ELMo embeddings improve the results.", so we know that the improvement in performance for Estonian in the NER task is significant.
Answer: significant
True answer: 0.05 F1

Validation ID: 70e9210fe64f8d71334e5107732d764332a81cb1
Question: what is the state of the art on WSJ?
From the context "Table TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches INLINEFORM0 WER on nov92.", so we know that the state of the art on WSJ is INLINEFORM0 WER.
Answer: INLINEFORM0 WER
True answer: CNN-DNN-BLSTM-HMM

Validation ID: 70e9210fe64f8d71334e5107732d764332a81cb1
Question: what is the state of the art on WSJ?
From the context "Table TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches INLINEFORM0 WER on nov92.", so we know that the state of the art on WSJ is INLINEFORM0 WER.
Answer: INLINEFORM0 WER
True answer: HMM-based system

Validation ID: 051df74dc643498e95d16e58851701628fdfd43e
Question: How did they obtain the OSG dataset?
From the context "Our data has been developed by crawling and pre-processing an OSG web forum", so we know that they obtained the OSG dataset by crawling and pre-processing an OSG web forum.
Answer: crawling and pre-processing an OSG web forum
True answer: crawling and pre-processing an OSG web forum

Validation ID: 051df74dc643498e95d16e58851701628fdfd43e
Question: How did they obtain the OSG dataset?
From the context "Our data has been developed by crawling and pre-processing an OSG web forum", so we know that they obtained the OSG dataset by crawling and pre-processing an OSG web forum.
Answer: crawling and pre-processing an OSG web forum
True answer: data has been developed by crawling and pre-processing an OSG web forum

Validation ID: 33554065284110859a8ea3ca7346474ab2cab100
Question: How large is the Twitter dataset?
From the context "We have downloaded 1,873 Twitter conversation threads, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted", so we know that the Twitter dataset has 1,873 Twitter conversation threads, roughly 14k tweets.
Answer: 1,873 Twitter conversation threads, roughly 14k tweets
True answer: 1,873 Twitter conversation threads, roughly 14k tweets

Validation ID: 33554065284110859a8ea3ca7346474ab2cab100
Question: How large is the Twitter dataset?
From the context "We have downloaded 1,873 Twitter conversation threads, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted", so we know that the Twitter dataset has 1,873 Twitter conversation threads, roughly 14k tweets.
Answer: 1,873 Twitter conversation threads, roughly 14k tweets
True answer: 1,873 Twitter conversation threads, roughly 14k tweets

Validation ID: 57f23dfc264feb62f45d9a9e24c60bd73d7fe563
Question: what is the size of the augmented dataset?
From the context "We augment DB with healthy samples from FP with varying ages (Tab. SECREF11 ), considering 50 samples for each 15 year duration starting from age 30. Adding the same number of samples from bins of age greater than 60 leads to greater increase in performance. This could be because the average age of participants in the datasets (DB, HA etc.) we use are greater than 60", so we know that the size of the augmented dataset is 50 samples for each 15 year duration starting from age 30.
Answer: 50 samples for each 15 year duration starting from age 30
True answer: Unanswerable

Validation ID: 57f23dfc264feb62f45d9a9e24c60bd73d7fe563
Question: what is the size of the augmented dataset?
From the context "We augment DB with healthy samples from FP with varying ages (Tab. SECREF11 ), considering 50 samples for each 15 year duration starting from age 30. Adding the same number of samples from bins of age greater than 60 leads to greater increase in performance. This could be because the average age of participants in the datasets (DB, HA etc.) we use are greater than 60", so we know that the size of the augmented dataset is 50 samples for each 15 year duration starting from age 30.
Answer: 50 samples for each 15 year duration starting from age 30
True answer: 609

Validation ID: 54830abe73fef4e629a36866ceeeca10214bd2c8
Question: How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?
From the context "In this paper, We present a taxonomy of recommendation systems applications based on topic modeling (LDA) of the recent research and evaluate ISWC and WWW publications in computer science between 2013 to 2017 from DBLP dataset", so we know that they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications in computer science between 2013 to 2017 from DBLP dataset.
Answer: LDA and Gibbs sampling
True answer: the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter

Validation ID: 54830abe73fef4e629a36866ceeeca10214bd2c8
Question: How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?
From the context "In this paper, We present a taxonomy of recommendation systems applications based on topic modeling (LDA) of the recent research and evaluate ISWC and WWW publications in computer science between 2013 to 2017 from DBLP dataset", so we know that they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications in computer science between 2013 to 2017 from DBLP dataset.
Answer: LDA and Gibbs sampling
True answer: discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\alpha =0.01$, $\beta = 0.01$ and using Gibbs sampling as a parameter estimation

Validation ID: 2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6
Question: What dataset do they use to evaluate their method?
From the context "We use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik", so we know that they use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik.
Answer: the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik
True answer: antonym and synonym pairs, collected from WordNet BIBREF9 and Wordnik

Validation ID: 2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6
Question: What dataset do they use to evaluate their method?
From the context "We use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik", so we know that they use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik.
Answer: the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik
True answer: English Wikipedia dump from June 2016

Validation ID: ef7212075e80bf35b7889dc8dd52fcbae0d1400a
Question: Why are current ELS's not sufficiently effective?
From the context "Despite its usefulness, linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12. These issues can be summarized into two parts: ambiguity and coarseness.", so we know that current ELS's have issues because of low precision rates and design challenges in training datasets.
Answer: low precision rates and design challenges in training datasets
True answer: Linked entities may be ambiguous or too common

Validation ID: ef7212075e80bf35b7889dc8dd52fcbae0d1400a
Question: Why are current ELS's not sufficiently effective?
From the context "Despite its usefulness, linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12. These issues can be summarized into two parts: ambiguity and coarseness.", so we know that current ELS's have issues because of low precision rates and design challenges in training datasets.
Answer: low precision rates and design challenges in training datasets
True answer: linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . These issues can be summarized into two parts: ambiguity and coarseness., the linked entities may also be too common to be considered an entity.

Validation ID: 567dc9bad8428ea9a2658c88203a0ed0f8da0dc3
Question: What is the best model?
From the context "We also found out that stemming post-positions can help a lot in improving model performance because of inflectional characteristics of Nepali language. So when we separate out its inflections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its inflected versions. We can clearly imply from tables TABREF23, TABREF24, and TABREF35 that we need more data to get better results because OurNepali dataset volume is almost ten times bigger compared to ILPRL dataset in terms of entities.", so we know that the best model is BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS.
Answer: BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS
True answer: BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS 

Validation ID: d51dc36fbf6518226b8e45d4c817e07e8f642003
Question: How many sentences does the dataset contain?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the dataset contains sentences.
Answer: sentences
True answer: 3606

Validation ID: d51dc36fbf6518226b8e45d4c817e07e8f642003
Question: How many sentences does the dataset contain?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the dataset contains sentences.
Answer: sentences
True answer: 6946

Validation ID: d8627ba08b7342e473b8a2b560baa8cdbae3c7fd
Question: Do the authors train a Naive Bayes classifier on their dataset?
From the context "We trained our model with shuffled training set using Adam optimizer with hyper-parameters mentioned in table TABREF30", so we know that the authors train a Naive Bayes classifier on their dataset.
Answer: Yes
True answer: No

Validation ID: d8627ba08b7342e473b8a2b560baa8cdbae3c7fd
Question: Do the authors train a Naive Bayes classifier on their dataset?
From the context "We trained our model with shuffled training set using Adam optimizer with hyper-parameters mentioned in table TABREF30", so we know that the authors train a Naive Bayes classifier on their dataset.
Answer: Yes
True answer: No

Validation ID: cb77d6a74065cb05318faf57e7ceca05e126a80d
Question: What is the baseline?
From the context "We compared grapheme-level representation with character-level representation to evaluate its effect. For example, in character-level embedding, each character of a word npAl results into n + + p + A + l has its own embedding. However, in grapheme level, a word npAl is clustered into graphemes, resulting into n + pA + l. Here, each grapheme has its own embedding. This grapheme-level embedding results good scores on par with character-level embedding in highly inflectional languages like Nepali, because graphemes also capture syntactic information similar to characters. We created grapheme clusters using uniseg package which is helpful in unicode text segmentations.", so we know that the baseline is character-level representation.
Answer: character-level representation
True answer: CNN modelBIBREF0, Stanford CRF modelBIBREF21

Validation ID: cb77d6a74065cb05318faf57e7ceca05e126a80d
Question: What is the baseline?
From the context "We compared grapheme-level representation with character-level representation to evaluate its effect. For example, in character-level embedding, each character of a word npAl results into n + + p + A + l has its own embedding. However, in grapheme level, a word npAl is clustered into graphemes, resulting into n + pA + l. Here, each grapheme has its own embedding. This grapheme-level embedding results good scores on par with character-level embedding in highly inflectional languages like Nepali, because graphemes also capture syntactic information similar to characters. We created grapheme clusters using uniseg package which is helpful in unicode text segmentations.", so we know that the baseline is character-level representation.
Answer: character-level representation
True answer: Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec

Validation ID: 8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d
Question: Which machine learning models do they explore?
From the context "We used Bi-directional LSTM to capture the word representation in forward as well as reverse direction of a sentence. Generally, LSTMs take inputs from left (past) of the sentence and computes the hidden state. However, it is proven beneficialBIBREF23 to use bi-directional LSTM, where, hidden states are computed based from right (future) of sentence and both of these hidden states are concatenated to produce the final output as $h_t$=[$\overrightarrow{h_t}$;$\overleftarrow{h_t}$], where $\overrightarrow{h_t}$, $\overleftarrow{h_t}$ = hidden state computed in forward and backward direction respectively.", so we know that they explore Bi-directional LSTM.
Answer: Bi-directional LSTM
True answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF

Validation ID: 8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d
Question: Which machine learning models do they explore?
From the context "We used Bi-directional LSTM to capture the word representation in forward as well as reverse direction of a sentence. Generally, LSTMs take inputs from left (past) of the sentence and computes the hidden state. However, it is proven beneficialBIBREF23 to use bi-directional LSTM, where, hidden states are computed based from right (future) of sentence and both of these hidden states are concatenated to produce the final output as $h_t$=[$\overrightarrow{h_t}$;$\overleftarrow{h_t}$], where $\overrightarrow{h_t}$, $\overleftarrow{h_t}$ = hidden state computed in forward and backward direction respectively.", so we know that they explore Bi-directional LSTM.
Answer: Bi-directional LSTM
True answer: BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21

Validation ID: a1b3e2107302c5a993baafbe177684ae88d6f505
Question: What is the size of the dataset?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the dataset contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.
Answer: each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.
True answer: Dataset contains 3606 total sentences and 79087 total entities.

Validation ID: a1b3e2107302c5a993baafbe177684ae88d6f505
Question: What is the size of the dataset?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the dataset contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.
Answer: each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.
True answer: ILPRL contains 548 sentences, OurNepali contains 3606 sentences

Validation ID: bb2de20ee5937da7e3e6230e942bec7b6e8f61ee
Question: What is the source of their dataset?
From the context "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016.", so we know that the source of their dataset is daily newspaper of the year 2015-2016.
Answer: daily newspaper of the year 2015-2016
True answer: daily newspaper of the year 2015-2016

Validation ID: bb2de20ee5937da7e3e6230e942bec7b6e8f61ee
Question: What is the source of their dataset?
From the context "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016.", so we know that the source of their dataset is daily newspaper of the year 2015-2016.
Answer: daily newspaper of the year 2015-2016
True answer: daily newspaper of the year 2015-2016

Validation ID: 1170e4ee76fa202cabac9f621e8fbeb4a6c5f094
Question: Do they try to use byte-pair encoding representations?
From the context "We created grapheme clusters using uniseg package which is helpful in unicode text segmentations", so we know that they use uniseg package to create grapheme clusters.
From the context "We have used fasttext embeddings in particular because of its sub-word representation ability, which is very useful in highly inflectional language as shown in Table TABREF25", so we know that they use fasttext embeddings.
True answer: No

Validation ID: 1170e4ee76fa202cabac9f621e8fbeb4a6c5f094
Question: Do they try to use byte-pair encoding representations?
From the context "We created grapheme clusters using uniseg package which is helpful in unicode text segmentations", so we know that they use uniseg package to create grapheme clusters.
From the context "We have used fasttext embeddings in particular because of its sub-word representation ability, which is very useful in highly inflectional language as shown in Table TABREF25", so we know that they use fasttext embeddings.
True answer: No

Validation ID: 1462eb312944926469e7cee067dfc7f1267a2a8c
Question: How many different types of entities exist in the dataset?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the dataset contains each word in newline with space separated POS-tags and Entity-tags.
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the dataset contains each word in newline with space separated POS-tags and Entity-tags.
True answer: OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities

Validation ID: 1462eb312944926469e7cee067dfc7f1267a2a8c
Question: How many different types of entities exist in the dataset?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that there are three types of entities in the dataset.
Answer: three types of entities
True answer: three

Validation ID: f59f1f5b528a2eec5cfb1e49c87699e0c536cc45
Question: How big is the new Nepali NER dataset?
From the context "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed.", so we know that the new Nepali NER dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed.
Answer: the new Nepali NER dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed.
True answer: 3606 sentences

Validation ID: f59f1f5b528a2eec5cfb1e49c87699e0c536cc45
Question: How big is the new Nepali NER dataset?
From the context "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed.", so we know that the new Nepali NER dataset contains the sentences collected from daily newspaper of the year 2015-2016.
Answer: the sentences collected from daily newspaper of the year 2015-2016
True answer: Dataset contains 3606 total sentences and 79087 total entities.

Validation ID: 9bd080bb2a089410fd7ace82e91711136116af6c
Question: What is the performance improvement of the grapheme-level representation model over the character-level model?
From the context "We also found out that stemming post-positions can help a lot in improving model performance because of inflectional characteristics of Nepali language. So when we separate out its inflections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its inflected versions.", so we know that the performance improvement of the grapheme-level representation model over the character-level model is because of the inflectional characteristics of Nepali language.
Answer: because of the inflectional characteristics of Nepali language
True answer: On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement

Validation ID: 9bd080bb2a089410fd7ace82e91711136116af6c
Question: What is the performance improvement of the grapheme-level representation model over the character-level model?
From the context "We also found out that stemming post-positions can help a lot in improving model performance because of inflectional characteristics of Nepali language. So when we separate out its inflections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its inflected versions.", so we know that the performance improvement of the grapheme-level representation model over the character-level model is because of the inflectional characteristics of Nepali language.
Answer: because of the inflectional characteristics of Nepali language
True answer: BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration

Validation ID: 6d1217b3d9cfb04be7fcd2238666fa02855ce9c5
Question: Which models are used to solve NER for Nepali?
From the context "We used Bi-directional LSTM to capture the word representation in forward as well as reverse direction of a sentence. Generally, LSTMs take inputs from left (past) of the sentence and computes the hidden state. However, it is proven beneficialBIBREF23 to use bi-directional LSTM, where, hidden states are computed based from right (future) of sentence and both of these hidden states are concatenated to produce the final output as $h_t$=[$\overrightarrow{h_t}$;$\overleftarrow{h_t}$], where $\overrightarrow{h_t}$, $\overleftarrow{h_t}$ = hidden state computed in forward and backward direction respectively.", so we know that Bi-directional LSTM is used to solve NER for Nepali.
Answer: Bi-directional LSTM
True answer: BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21

Validation ID: 6d1217b3d9cfb04be7fcd2238666fa02855ce9c5
Question: Which models are used to solve NER for Nepali?
From the context "We used Bi-directional LSTM to capture the word representation in forward as well as reverse direction of a sentence. Generally, LSTMs take inputs from left (past) of the sentence and computes the hidden state. However, it is proven beneficialBIBREF23 to use bi-directional LSTM, where, hidden states are computed based from right (future) of sentence and both of these hidden states are concatenated to produce the final output as $h_t$=[$\overrightarrow{h_t}$;$\overleftarrow{h_t}$], where $\overrightarrow{h_t}$, $\overleftarrow{h_t}$ = hidden state computed in forward and backward direction respectively.", so we know that Bi-directional LSTM is used to solve NER for Nepali.
Answer: Bi-directional LSTM
True answer: BiLSTM, BiLSTM+CNN, BiLSTM+CRF, BiLSTM+CNN+CRF, CNN, Stanford CRF

Validation ID: 1e775cf30784e6b1c2b573294a82e145a3f959bb
Question: What language(s) is/are represented in the dataset?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.", so we know that the dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.
Answer: harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.
True answer: english

Validation ID: 1e775cf30784e6b1c2b573294a82e145a3f959bb
Question: What language(s) is/are represented in the dataset?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.", so we know that the dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.
Answer: harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.
True answer: english

Validation ID: 392fb87564c4f45d0d8d491a9bb217c4fce87f03
Question: What baseline model is used?
From the context "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", so we know that the baseline model is the one with the Projected Layer.
Answer: the one with the Projected Layer
True answer:  LastStateRNN, AvgRNN, AttentionRNN

Validation ID: 392fb87564c4f45d0d8d491a9bb217c4fce87f03
Question: What baseline model is used?
From the context "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models. So, we actually include four models in our experiments (having a projected layer or not). Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", so we know that the baseline model is LastStateRNN.
Answer: LastStateRNN
True answer: LastStateRNN, AvgRNN, AttentionRNN 

Validation ID: 203337c15bd1ee05763c748391d295a1f6415b9b
Question: Which variation provides the best results on this dataset?
From the context "According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance", so we know that the multi-attention method having a projected layer provides the best results on this dataset.
Answer: multi-attention method having a projected layer
True answer: the model with multi-attention mechanism and a projected layer

Validation ID: 203337c15bd1ee05763c748391d295a1f6415b9b
Question: Which variation provides the best results on this dataset?
From the context "According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance", so we know that the multi-attention method having a projected layer provides the best results on this dataset.
Answer: multi-attention method having a projected layer
True answer: Projected Layer

Validation ID: d004ca2e999940ac5c1576046e30efa3059832fa
Question: What are the different variations of the attention-based approach which are examined?
From the context "Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models", so we know that the different variations of the attention-based approach are multi-attention and single attention models.
Answer: multi-attention and single attention models
True answer: classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer

Validation ID: d004ca2e999940ac5c1576046e30efa3059832fa
Question: What are the different variations of the attention-based approach which are examined?
From the context "Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models", so we know that the different variations of the attention-based approach are multi-attention and single attention models.
Answer: multi-attention and single attention models
True answer:  four attention mechanisms instead of one, a projection layer for the word embeddings

Validation ID: 21548433abd21346659505296fb0576e78287a74
Question: What dataset is used for this work?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media"", so we know that the dataset is from Twitter.
Answer: Twitter
True answer: Twitter dataset provided by the organizers

Validation ID: 21548433abd21346659505296fb0576e78287a74
Question: What dataset is used for this work?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media"", so we know that the dataset is published for the "First workshop on categorizing different types of online harassment languages in social media".
Answer: "First workshop on categorizing different types of online harassment languages in social media"
True answer: The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.

Validation ID: f0b2289cb887740f9255909018f400f028b1ef26
Question: What types of online harassment are studied?
From the context "In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks. The first one is the classification of the tweets in harassment and non-harassment categories, while the second one is the classification in specific harassment categories like indirect harassment, physical and sexual harassment as well", so we know that the types of online harassment studied are harassment and non-harassment categories, indirect harassment, physical and sexual harassment.
Answer: harassment and non-harassment categories, indirect harassment, physical and sexual harassment
True answer: indirect harassment, sexual and physical harassment

Validation ID: f0b2289cb887740f9255909018f400f028b1ef26
Question: What types of online harassment are studied?
From the context "In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks. The first one is the classification of the tweets in harassment and non-harassment categories, while the second one is the classification in specific harassment categories like indirect harassment, physical and sexual harassment as well", so we know that the types of online harassment studied are harassment and non-harassment categories, indirect harassment, physical and sexual harassment.
Answer: harassment and non-harassment categories, indirect harassment, physical and sexual harassment
True answer: indirect, physical, sexual

Validation ID: 51b1142c1d23420dbf6d49446730b0e82b32137c
Question: What was the baseline?
From the context "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models. So, we actually include four models in our experiments (having a projected layer or not). Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", so we know that the baseline is LastStateRNN.
Answer: LastStateRNN
True answer: LastStateRNN, AvgRNN, AttentionRNN

Validation ID: 58355e2a782bf145b61ee2a3e0e426119985c179
Question: What were the datasets used in this paper?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.", so we know that the datasets used in this paper are train set, validation set and test set.
Answer: train set, validation set and test set
True answer: The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. 

Validation ID: 58355e2a782bf145b61ee2a3e0e426119985c179
Question: What were the datasets used in this paper?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.", so we know that the datasets used in this paper are train set, validation set and test set.
Answer: train set, validation set and test set
True answer: Twitter dataset provided by organizers containing harassment and non-harassment tweets

Validation ID: 25c1c4a91f5dedd4e06d14121af3b5921db125e9
Question: Is car-speak language collection of abstract features that classifier is later trained on?
From the context "We aim to curate a data set of car-speak in order to train a model properly. However, there are a few challenges that present themselves: What is a good source of car-speak? How can we acquire the data? How can we be sure the data set is relevant?", so we know that car-speak is a data set that is used to train a model.
Answer: Yes
True answer: No

Validation ID: 25c1c4a91f5dedd4e06d14121af3b5921db125e9
Question: Is car-speak language collection of abstract features that classifier is later trained on?
From the context "We aim to curate a data set of car-speak in order to train a model properly. However, there are a few challenges that present themselves: What is a good source of car-speak? How can we acquire the data? How can we be sure the data set is relevant?", so we know that car-speak is a data set that is used to train a model.
Answer: Yes
True answer: No

Validation ID: f88036174b4a0dbf4fe70ddad884d16082c5748d
Question: Is order of "words" important in car speak language?
From the context "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$. We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12.", so we know that order of "words" is not important in car speak language.
Answer: No
True answer: Unanswerable

Validation ID: f88036174b4a0dbf4fe70ddad884d16082c5748d
Question: Is order of "words" important in car speak language?
From the context "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$. We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12.", so we know that order of "words" is not important in car speak language.
Answer: No
True answer: No

Validation ID: a267d620af319b48e56c191aa4c433ea3870f6fb
Question: What are labels in car speak language dataset?
From the context "We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12.", so we know that the labels in car speak language dataset are the car it reviews.
Answer: the car it reviews
True answer: car 

Validation ID: a267d620af319b48e56c191aa4c433ea3870f6fb
Question: What are labels in car speak language dataset?
From the context "We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12.", so we know that the labels in car speak language dataset are the car it reviews.
Answer: the car it reviews
True answer: the car

Validation ID: 899ed05c460bf2aa0aa65101cad1986d4f622652
Question: How big is dataset of car-speak language?
From the context "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$", so we know that the dataset of car-speak language is $10,867$.
Answer: $10,867$
True answer: $3,209$ reviews 

Validation ID: 899ed05c460bf2aa0aa65101cad1986d4f622652
Question: How big is dataset of car-speak language?
From the context "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$", so we know that the dataset of car-speak language is $10,867$.
Answer: $10,867$
True answer: $3,209$ reviews about 553 different cars from 49 different car manufacturers

Validation ID: d53299fac8c94bd0179968eb868506124af407d1
Question: What is the performance of classifiers?
From the context "Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.", so we know that the performance of classifiers is F1 micro and F1 macro scores.
Answer: F1 micro and F1 macro scores
True answer: Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject

Validation ID: d53299fac8c94bd0179968eb868506124af407d1
Question: What is the performance of classifiers?
From the context "Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.", so we know that the performance of classifiers is F1 micro and F1 macro scores.
Answer: F1 micro and F1 macro scores
True answer: Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.

Validation ID: 29f2954098f055fb19d9502572f085862d75bf61
Question: What classifiers have been trained?
From the context "Finally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.", so we know that the classifiers have been trained are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP).
Answer: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)
True answer: KNN
RF
SVM
MLP

Validation ID: 29f2954098f055fb19d9502572f085862d75bf61
Question: What classifiers have been trained?
From the context "Finally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.", so we know that the classifiers have been trained are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP).
Answer: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)
True answer:  K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)

Validation ID: 6bf93968110c6e3e3640360440607744007a5228
Question: How does car speak pertains to a car's physical attributes?
From the context "Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term “fast” pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term “fast” refers to", so we know that car-speak pertains to a car's physical attributes.
Answer: car-speak pertains to a car's physical attributes
True answer: we do not know exactly

Validation ID: 37a79be0148e1751ffb2daabe4c8ec6680036106
Question: What topic is covered in the Chinese Facebook data??
From the context "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.", so we know that the topic is anti-nuclear-power.
Answer: anti-nuclear-power
True answer: anti-nuclear-power

Validation ID: 37a79be0148e1751ffb2daabe4c8ec6680036106
Question: What topic is covered in the Chinese Facebook data??
From the context "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.", so we know that the topic is anti-nuclear-power.
Answer: anti-nuclear-power
True answer: anti-nuclear-power

Validation ID: 518dae6f936882152c162058895db4eca815e649
Question: How many layers does the UTCNN model have?
From the context "Figure FIGREF10 illustrates the UTCNN model. As more than one user may interact with a given post, we first add a maximum pooling layer after the user matrix embedding layer and user vector embedding layer to form a moderator matrix embedding INLINEFORM0 and a moderator vector embedding INLINEFORM1 for moderator INLINEFORM2 respectively, where INLINEFORM3 is used for the semantic transformation in the document composition process, as mentioned in the previous section. The term moderator here is to denote the pseudo user who provides the overall semantic/sentiment of all the engaged users for one document. The embedding INLINEFORM4 models the moderator stance preference, that is, the pattern of the revealed user stance: whether a user is willing to show his preference, whether a user likes to show impartiality with neutral statements and reasonable arguments, or just wants to show strong support for one stance. Ideally, the latent user stance is modeled by INLINEFORM5 for each user. Likewise, for topic information, a maximum pooling layer is added after the topic matrix embedding layer and topic vector embedding layer to form a joint topic matrix embedding INLINEFORM6 and a joint topic vector embedding INLINEFORM7 for topic INLINEFORM8 respectively, where INLINEFORM9 models the semantic transformation of topic INLINEFORM10 as in users and INLINEFORM11 models the topic stance tendency. The latent topic stance is also modeled by INLINEFORM12 for each topic.
As for comments, we view them as short documents with authors only but without likers nor their own comments. Therefore we apply document composition on comments although here users are commenters (users who comment). It is noticed that the word embeddings INLINEFORM0 for the same word in the posts and comments are the same, but after being transformed to INLINEFORM1 in the document composition process shown in Figure FIGREF4, they might become different because of their different engaged users. The output comment representation together with the commenter vector embedding INLINEFORM2 and topic vector embedding INLINEFORM3 are concatenated and a maximum pooling layer is added to select the most important feature for comments. Instead of requiring that the comment stance agree with the post, UTCNN simply extracts the most important features of the comment contents; they could be helpful, whether they show obvious agreement or disagreement. Therefore when combining comment information here, the maximum pooling layer
True answer: eight layers

Validation ID: e44a6bf67ce3fde0c6608b150030e44d87eb25e3
Question: What topics are included in the debate data?
From the context "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset.", so we know that the topics in the debate data are multiple.
Answer: multiple
True answer: abortion, gay rights, Obama, marijuana

Validation ID: e44a6bf67ce3fde0c6608b150030e44d87eb25e3
Question: What topics are included in the debate data?
From the context "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset.", so we know that the topics in the debate data are multiple.
Answer: multiple
True answer: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)

Validation ID: 6a31db1aca57a818f36bba9002561724655372a7
Question: What is the size of the Chinese data?
From the context "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.", so we know that the size of the Chinese data is 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.
Answer: 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users
True answer: 32,595 posts

Validation ID: 6a31db1aca57a818f36bba9002561724655372a7
Question: What is the size of the Chinese data?
From the context "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.", so we know that the size of the Chinese data is 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.
Answer: 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users
True answer: 32,595

Validation ID: e330e162ec29722f5ec9f83853d129c9e0693d65
Question: Did they collected the two datasets?
From the context "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset.", so we know that they collected the two datasets.
Answer: Yes
True answer: No

Validation ID: e330e162ec29722f5ec9f83853d129c9e0693d65
Question: Did they collected the two datasets?
From the context "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset.", so we know that they collected the two datasets.
Answer: Yes
True answer: No

Validation ID: d3093062aebff475b4deab90815004051e802aa6
Question: What are the baselines?
From the context "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings INLINEFORM1 and INLINEFORM2 for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset; 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information. All these models were trained on the training set, and parameters as well as the SVM kernel selections (linear or RBF) were fine-tuned on the development set. Also, we adopt oversampling on SVMs, CNN and RCNN because the FBFans dataset is highly imbalanced.", so we know that the baselines are: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN); 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information; 7) UTCNN without the LDA model; 8) UTCNN without comments.
Answer: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN); 5
True answer: SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information

Validation ID: d3093062aebff475b4deab90815004051e802aa6
Question: What are the baselines?
From the context "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings INLINEFORM1 and INLINEFORM2 for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset; 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information. All these models were trained on the training set, and parameters as well as the SVM kernel selections (linear or RBF) were fine-tuned on the development set. Also, we adopt oversampling on SVMs, CNN and RCNN because the FBFans dataset is highly imbalanced.", so we know that the baselines are: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN); 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information; 7) UTCNN without the LDA model; 8) UTCNN without comments.
Answer: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN); 5
True answer: SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
From the context "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:", so we know that the transfer learning tasks are MR, CR, SUBJ, MPQA, SST, TREC, MRPC.
Answer: MR, CR, SUBJ, MPQA, SST, TREC, MRPC
True answer: MR, CR, SUBJ, MPQA, SST, TREC, MRPC

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
From the context "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:", so we know that the transfer learning tasks are MR, CR, SUBJ, MPQA, SST, TREC, MRPC.
Answer: MR, CR, SUBJ, MPQA, SST, TREC, MRPC
True answer: MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.

CR: Sentiment prediction of customer product reviews BIBREF26.

SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.

MPQA: Phrase level opinion polarity classification from newswire BIBREF28.

SST: Stanford Sentiment Treebank with binary labels BIBREF29.

TREC: Fine grained question-type classification from TREC BIBREF30.

MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
From the context "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:", so we know that the transfer learning tasks are MR, CR, SUBJ, MPQA, SST, TREC, MRPC.
Answer: MR, CR, SUBJ, MPQA, SST, TREC, MRPC
True answer: Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification.

Validation ID: a29c071065d26e5ee3c3bcd877e7f215c59d1d33
Question: What metrics are used for the STS tasks?
From the context "We compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels", so we know that the metrics used for the STS tasks are Spearman's rank correlation.
Answer: Spearman's rank correlation
True answer:  Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels

Validation ID: a29c071065d26e5ee3c3bcd877e7f215c59d1d33
Question: What metrics are used for the STS tasks?
From the context "We compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels", so we know that the metrics used for the STS tasks are Spearman's rank correlation.
Answer: Spearman's rank correlation
True answer: Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels

Validation ID: 7f207549c75f5c4388efc15ed28822672b845663
Question: How much time takes its training?
From the context "We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent BIBREF4 and Universal Sentence Encoder BIBREF5. On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval BIBREF6, an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.", so we know that SBERT takes less than 20 minutes to train.
Answer: less than 20 minutes
True answer: 20 minutes

Validation ID: 596aede2b311deb8cb0a82d2e7de314ef6e83e4e
Question: How many GPUs are used for the training of SBERT?
From the context "We use a single NVIDIA V100 GPU with 16 GB memory for training SBERT", so we know that only one NVIDIA V100 GPU with 16 GB memory is used for training SBERT.
Answer: one
True answer: Unanswerable

Validation ID: 2e89ebd2e4008c67bb2413699589ee55f59c4f36
Question: How are the siamese networks trained?
From the context "We train SBERT on the combination of the SNLI BIBREF13 and the Multi-Genre NLI BIBREF14 dataset. The SNLI is a collection of 570,000 sentence pairs annotated with the labels contradiction, eintailment, and neutral. MultiNLI contains 430,000 sentence pairs and covers a range of genres of spoken and written text. We fine-tune SBERT with a 3-way softmax-classifier objective function for one epoch. We used a batch-size of 16, Adam optimizer with learning rate $2\mathrm {e}{-5}$, and a linear learning rate warm-up over 10% of the training data. Our default pooling strategy is MEAN.", so we know that the siamese networks are trained on the combination of the SNLI and the Multi-Genre NLI dataset.
Answer: on the combination of the SNLI and the Multi-Genre NLI dataset
True answer: Unanswerable

Validation ID: 2e89ebd2e4008c67bb2413699589ee55f59c4f36
Question: How are the siamese networks trained?
From the context "We train SBERT on the combination of the SNLI BIBREF13 and the Multi-Genre NLI BIBREF14 dataset. The SNLI is a collection of 570,000 sentence pairs annotated with the labels contradiction, eintailment, and neutral. MultiNLI contains 430,000 sentence pairs and covers a range of genres of spoken and written text. We fine-tune SBERT with a 3-way softmax-classifier objective function for one epoch. We used a batch-size of 16, Adam optimizer with learning rate $2\mathrm {e}{-5}$, and a linear learning rate warm-up over 10% of the training data. Our default pooling strategy is MEAN.", so we know that the siamese networks are trained on the combination of the SNLI and the Multi-Genre NLI dataset.
Answer: on the combination of the SNLI and the Multi-Genre NLI dataset
True answer: update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity., Classification Objective Function, Regression Objective Function, Triplet Objective Function

Validation ID: e2db361ae9ad9dbaa9a85736c5593eb3a471983d
Question: What other sentence embeddings methods are evaluated?
From the context "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:", so we know that the other sentence embeddings methods are:
MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.
True answer: GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent

Validation ID: e2db361ae9ad9dbaa9a85736c5593eb3a471983d
Question: What other sentence embeddings methods are evaluated?
From the context "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:", so we know that the other sentence embeddings methods are InferSent, Universal Sentence Encoder, average BERT embeddings, average GloVe embeddings.
Answer: InferSent, Universal Sentence Encoder, average BERT embeddings, average GloVe embeddings
True answer: Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.

Validation ID: 252a645af9876241fb166e5822992ce17fec6eb6
Question: What is the average length of the title text?
From the context "We cast the problem of popularity prediction as a binary classification task. We assume our data points contain a string of characters representing article title and a popularity metric, such as number of comments or views", so we know that the average length of the title text is the average length of the string of characters representing article title.
Answer: the average length of the string of characters representing article title
True answer: Unanswerable

Validation ID: ed67359889cf61fa11ee291d6c378cccf83d599d
Question: Which pretrained word vectors did they use?
From the context "As a text embedding in our experiments, we use publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC). Since their output dimensionality can be modified, we show the results for varying dimensionality sizes. On top of that, we evaluate two training approaches: using static word vectors and fine-tuning them during training phase.", so we know that they use publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC).
Answer: publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)
True answer:  pre-trained GloVe word vectors 

Validation ID: ed67359889cf61fa11ee291d6c378cccf83d599d
Question: Which pretrained word vectors did they use?
From the context "As a text embedding in our experiments, we use publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC). Since their output dimensionality can be modified, we show the results for varying dimensionality sizes. On top of that, we evaluate two training approaches: using static word vectors and fine-tuning them during training phase.", so we know that they use publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC).
Answer: publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)
True answer: GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)

Validation ID: 425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82
Question: What evaluation metrics are used?
From the context "We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples", so we know that the evaluation metrics are accuracy.
Answer: accuracy
True answer: standard accuracy metric

Validation ID: 425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82
Question: What evaluation metrics are used?
From the context "We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples", so we know that the evaluation metrics are accuracy.
Answer: accuracy
True answer: accuracy

Validation ID: 955de9f7412ba98a0c91998919fa048d339b1d48
Question: Which shallow approaches did they experiment with?
From the context "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21, combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM.", so we know that they use Bag-of-Words and Support Vector Machine with linear kernel as shallow approaches.
Answer: Bag-of-Words and Support Vector Machine with linear kernel
True answer: SVM

Validation ID: 955de9f7412ba98a0c91998919fa048d339b1d48
Question: Which shallow approaches did they experiment with?
From the context "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21, combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM.", so we know that they experiment with Bag-of-Words and Support Vector Machine with linear kernel.
Answer: Bag-of-Words and Support Vector Machine with linear kernel
True answer: SVM with linear kernel using bag-of-words features

Validation ID: 3b371ea554fa6639c76a364060258454e4b931d4
Question: Where do they obtain the news videos from?
From the context "contains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016", so we know that they obtain the news videos from NowThisNews Facebook page.
Answer: NowThisNews Facebook page
True answer: NowThisNews Facebook page

Validation ID: 3b371ea554fa6639c76a364060258454e4b931d4
Question: Where do they obtain the news videos from?
From the context "contains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016", so we know that they obtain the news videos from NowThisNews Facebook page.
Answer: NowThisNews Facebook page
True answer: NowThisNews Facebook page

Validation ID: ddb23a71113cbc092cbc158066d891cae261e2c6
Question: What is the source of the news articles?
From the context "contains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016", so we know that the source of the news articles is NowThisNews Facebook page.
Answer: NowThisNews Facebook page
True answer: main news channels, such as Yahoo News, The Guardian or The Washington Post

Validation ID: ddb23a71113cbc092cbc158066d891cae261e2c6
Question: What is the source of the news articles?
From the context "contains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016", so we know that the source of the news articles is NowThisNews Facebook page.
Answer: NowThisNews Facebook page
True answer: The BreakingNews dataset

Validation ID: e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38
Question: which non-english language had the best performance?
From the context "In this paper we made the sentiment analysis model specific to a single domain. For future work, we would like to investigate the effectiveness of our model on different review domains including hotel reviews and on different problems such as detecting stance.", so we know that the non-english language had the best performance is hotel reviews.
Answer: hotel reviews
True answer: Russian

Validation ID: e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38
Question: which non-english language had the best performance?
From the context "In this paper we made the sentiment analysis model specific to a single domain. For future work, we would like to investigate the effectiveness of our model on different review domains including hotel reviews and on different problems such as detecting stance.", so we know that the non-english language had the best performance is hotel reviews.
Answer: hotel reviews
True answer: Russsian

Validation ID: c7486d039304ca9d50d0571236429f4f6fbcfcf7
Question: which non-english language was the had the worst results?
From the context "We found that 25.84% of Dutch, 21.76% of Turkish, 24.46% Spanish, and 10.71% of Russian reviews that were misclassified had non-English words in them", so we know that the non-english language that had the worst results was Russian.
Answer: Russian
True answer: Turkish

Validation ID: f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9
Question: what datasets were used in evaluation?
From the context "For experimental results, we report majority baseline for each language where the majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset. For example, if the dataset has 60% of all reviews positive and 40% negative, majority baseline would be 60% because a model that always predicts “positive” will be 60% accurate and will make mistakes 40% of the time.", so we know that the datasets used in evaluation are Dutch, Turkish, Spanish, Russian.
Answer: Dutch, Turkish, Spanish, Russian
True answer: SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28

Validation ID: f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9
Question: what datasets were used in evaluation?
From the context "For experimental results, we report majority baseline for each language where the majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset. For example, if the dataset has 60% of all reviews positive and 40% negative, majority baseline would be 60% because a model that always predicts “positive” will be 60% accurate and will make mistakes 40% of the time.", so we know that the datasets used in evaluation are Dutch, Turkish, Spanish, Russian.
Answer: Dutch, Turkish, Spanish, Russian
True answer:  English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)

Validation ID: a103636c8d1dbfa53341133aeb751ffec269415c
Question: what are the baselines?
From the context "In addition to the majority baseline, we also compare our results with a lexicon-based approach. We use SentiWordNet BIBREF29 to obtain a positive and a negative sentiment score for each token in a review. Then sum of positive sentiment scores and negative sentiment scores for each review is obtained by summing up the scores for each token. If the positive sum score for a given review is greater than the negative sum score, we accept that review as a positive review. If negative sum is larger than or equal to the positive sum, the review is labeled as a negative review.", so we know that the baselines are majority baseline and lexicon-based baseline.
Answer: majority baseline and lexicon-based baseline
True answer: majority baseline, lexicon-based approach

Validation ID: a103636c8d1dbfa53341133aeb751ffec269415c
Question: what are the baselines?
From the context "In addition to the majority baseline, we also compare our results with a lexicon-based approach. We use SentiWordNet BIBREF29 to obtain a positive and a negative sentiment score for each token in a review. Then sum of positive sentiment scores and negative sentiment scores for each review is obtained by summing up the scores for each token. If the positive sum score for a given review is greater than the negative sum score, we accept that review as a positive review. If negative sum is larger than or equal to the positive sum, the review is labeled as a negative review.", so we know that the baselines are majority baseline and lexicon-based baseline.
Answer: majority baseline and lexicon-based baseline
True answer: majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset, lexicon-based approach

Validation ID: 55139fcfe04ce90aad407e2e5a0067a45f31e07e
Question: how did the authors translate the reviews to other languages?
From the context "To be able to employ the trained model, test sets are first translated to English via machine translation and then inference takes place.", so we know that the authors translated the reviews to other languages via machine translation.
Answer: machine translation
True answer: Using Google translation API.

Validation ID: 55139fcfe04ce90aad407e2e5a0067a45f31e07e
Question: how did the authors translate the reviews to other languages?
From the context "To be able to employ the trained model, test sets are first translated to English via machine translation and then inference takes place.", so we know that the authors translated the reviews to other languages via machine translation.
Answer: machine translation
True answer: Google translation API

Validation ID: fbaf060004f196a286fef67593d2d76826f0304e
Question: what dataset was used for training?
From the context "The first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian). We focus on polarity detection in reviews, therefore all datasets in this study have two class values (positive, negative).", so we know that the first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian).
Answer: The first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian).
True answer: Amazon reviews, Yelp restaurant reviews, restaurant reviews

Validation ID: fbaf060004f196a286fef67593d2d76826f0304e
Question: what dataset was used for training?
From the context "The first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian). We focus on polarity detection in reviews, therefore all datasets in this study have two class values (positive, negative).", so we know that the first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian).
Answer: The first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian).
True answer: Amazon reviews BIBREF23 , BIBREF24, Yelp restaurant reviews dataset,  restaurant reviews dataset as part of a Kaggle competition BIBREF26

Validation ID: 7ae38f51243cb80b16a1df14872b72a1f8a2048f
Question: How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech?
From the context "As we found the individually-trained parallel networks (CNN and LSTM) to be useful (see Table TABREF12 ), we suspected the combination of these two networks could provide a more powerful discriminative spatial and temporal representation of the data than each independent network. As such, we concatenate the last fully-connected layer from the CNN with its counterpart in the LSTM to compose a single feature vector based on these two penultimate layers. Ultimately, this forms a joint spatio-temporal encoding of the cross-covariance matrix.", so we know that the combination of these two networks could provide a more powerful discriminative spatial and temporal representation of the data than each independent network.
Answer: the combination of these two networks could provide a more powerful discriminative spatial and temporal representation of the data than each independent network
True answer: we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 .

Validation ID: deb89bca0925657e0f91ab5daca78b9e548de2bd
Question: What are the five different binary classification tasks?
From the context "We took BIBREF17, BIBREF18 as the baseline works investigating the same problem and compared our performance with theirs. Our proposed method highly outperforms the existing methods across all the five binary classification tasks by a large average margin of 22.51%.", so we know that the five different binary classification tasks are:
1. presence/absence of consonants
True answer:  presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.

Validation ID: deb89bca0925657e0f91ab5daca78b9e548de2bd
Question: What are the five different binary classification tasks?
From the context "We took BIBREF17, BIBREF18 as the baseline works investigating the same problem and compared our performance with theirs. Our proposed method highly outperforms the existing methods across all the five binary classification tasks by a large average margin of 22.51%.", so we know that the five different binary classification tasks are:
1. presence/absence of consonants
True answer: presence/absence of consonants, presence/absence of phonemic nasal, presence/absence of bilabial, presence/absence of high-front vowels, and presence/absence of high-back vowels

Validation ID: 9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570
Question: How was the spatial aspect of the EEG signal computed?
From the context "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19, in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers", so we know that the spatial aspect of the EEG signal was computed by a CNN.
Answer: CNN
True answer: we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.

Validation ID: 9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570
Question: How was the spatial aspect of the EEG signal computed?
From the context "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19, in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers", so we know that the spatial aspect of the EEG signal was computed by a CNN.
Answer: CNN
True answer: They use four-layered 2D CNN and two fully connected hidden layers on the channel covariance matrix to compute the spatial aspect.

Validation ID: e6583c60b13b87fc37af75ffc975e7e316d4f4e0
Question: What data was presented to the subjects to elicit event-related responses?
From the context "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", so we know that the data presented to the subjects to elicit event-related responses is composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw).
Answer: composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)
True answer: 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)

Validation ID: e6583c60b13b87fc37af75ffc975e7e316d4f4e0
Question: What data was presented to the subjects to elicit event-related responses?
From the context "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", so we know that the data presented to the subjects to elicit event-related responses is composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw).
Answer: composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)
True answer: KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)

Validation ID: c7b6e6cb997de1660fd24d31759fe6bb21c7863f
Question: How many electrodes were used on the subject in EEG sessions?
From the context "Multichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes", so we know that the dimensionality depends on the number of electrodes.
Answer: the number of electrodes
True answer: 1913 signals

Validation ID: c7b6e6cb997de1660fd24d31759fe6bb21c7863f
Question: How many electrodes were used on the subject in EEG sessions?
From the context "Multichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes", so we know that the dimensionality depends on the number of electrodes.
Answer: the number of electrodes
True answer: Unanswerable

Validation ID: f9f59c171531c452bd2767dc332dc74cadee5120
Question: How many subjects does the EEG data come from?
From the context "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual.", so we know that the EEG data comes from 14 subjects.
Answer: 14 subjects
True answer: 14

Validation ID: f9f59c171531c452bd2767dc332dc74cadee5120
Question: How many subjects does the EEG data come from?
From the context "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual.", so we know that the EEG data comes from 14 subjects.
Answer: 14 subjects
True answer: 14 participants

Validation ID: 4ac2c3c259024d7cd8e449600b499f93332dab60
Question: Do they report results only on English data?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 4ac2c3c259024d7cd8e449600b499f93332dab60
Question: Do they report results only on English data?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: bc730e4d964b6a66656078e2da130310142ab641
Question: What type of classifiers are used?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21. Following previous studies BIBREF1, we collect event-related microposts from Twitter using 11 and 8 seed events (see Section SECREF2) for CyberAttack and PoliticianDeath, respectively. Unlabeled microposts are collected by using the keyword `hack' for CyberAttack, while for PoliticianDeath, we use a set of keywords related to `politician' and `death' (such as `bureaucrat', `dead' etc.) For each dataset, we randomly select 500 tweets from the unlabeled subset and manually label them for evaluation. Table TABREF25 shows key statistics from our two datasets.", so we know that the type of classifiers used are Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2.
Answer: Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2
True answer: probabilistic model

Validation ID: bc730e4d964b6a66656078e2da130310142ab641
Question: What type of classifiers are used?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21. Following previous studies BIBREF1, we collect event-related microposts from Twitter using 11 and 8 seed events (see Section SECREF2) for CyberAttack and PoliticianDeath, respectively. Unlabeled microposts are collected by using the keyword `hack' for CyberAttack, while for PoliticianDeath, we use a set of keywords related to `politician' and `death' (such as `bureaucrat', `dead' etc.) For each dataset, we randomly select 500 tweets from the unlabeled subset and manually label them for evaluation. Table TABREF25 shows key statistics from our two datasets.", so we know that the type of classifiers used are Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2.
Answer: Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2
True answer: Logistic Regression, Multilayer Perceptron

Validation ID: 3941401a182a3d6234894a5c8a75d48c6116c45c
Question: Which real-world datasets are used?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21.", so we know that the real-world datasets are CyberAttack and PoliticianDeath.
Answer: CyberAttack and PoliticianDeath
True answer: Tweets related to CyberAttack and tweets related to PoliticianDeath

Validation ID: 3941401a182a3d6234894a5c8a75d48c6116c45c
Question: Which real-world datasets are used?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21.", so we know that the real-world datasets are CyberAttack and PoliticianDeath.
Answer: CyberAttack and PoliticianDeath
True answer: cyber security (CyberAttack), death of politicians (PoliticianDeath)

Validation ID: 67e9e147b2cab5ba43572ce8a17fc863690172f0
Question: How are the interpretability merits of the approach demonstrated?
From the context "Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10.", so we know that our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10.
Answer: Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10.
True answer: By involving humans for post-hoc evaluation of model's interpretability

Validation ID: 67e9e147b2cab5ba43572ce8a17fc863690172f0
Question: How are the interpretability merits of the approach demonstrated?
From the context "Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10.", so we know that our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10.
Answer: Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10.
True answer: directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model

Validation ID: a74190189a6ced2a2d5b781e445e36f4e527e82a
Question: How are the accuracy merits of the approach demonstrated?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21. Following previous studies BIBREF1, we collect event-related microposts from Twitter using 11 and 8 seed events (see Section SECREF2) for CyberAttack and PoliticianDeath, respectively. Unlabeled microposts are collected by using the keyword `hack' for CyberAttack, while for PoliticianDeath, we use a set of keywords related to `politician' and `death' (such as `bureaucrat', `dead' etc.) For each dataset, we randomly select 500 tweets from the unlabeled subset and manually label them for evaluation. Table TABREF25 shows key statistics from our two datasets.", so we know that the accuracy merits of the approach are demonstrated by the experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath).
Answer: the experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath)
True answer: significant improvements clearly demonstrate that our approach is effective at improving model performance

Validation ID: a74190189a6ced2a2d5b781e445e36f4e527e82a
Question: How are the accuracy merits of the approach demonstrated?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21. Following previous studies BIBREF1, we collect event-related microposts from Twitter using 11 and 8 seed events (see Section SECREF2) for CyberAttack and PoliticianDeath, respectively. Unlabeled microposts are collected by using the keyword `hack' for CyberAttack, while for PoliticianDeath, we use a set of keywords related to `politician' and `death' (such as `bureaucrat', `dead' etc.) For each dataset, we randomly select 500 tweets from the unlabeled subset and manually label them for evaluation. Table TABREF25 shows key statistics from our two datasets.", so we know that the accuracy merits of the approach are demonstrated by the experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath).
Answer: the experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath)
True answer: By evaluating the performance of the approach using accuracy and AUC

Validation ID: 43f074bacabd0a355b4e0f91a1afd538c0a6244f
Question: How is the keyword specific expectation elicited from the crowd?
From the context "The keyword expectation is inferred by taking into account both the crowd-contributed labels and the model prediction", so we know that the keyword specific expectation is inferred by taking into account both the crowd-contributed labels and the model prediction.
Answer: taking into account both the crowd-contributed labels and the model prediction
True answer: workers are first asked to find those microposts where the model predictions are deemed correct,  asked to find the keyword that best indicates the class of the microposts

Validation ID: 58ef2442450c392bfc55c4dc35f216542f5f2dbb
Question: Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?
From the context "Researchers can benefit from Macaw for developing algorithms and techniques for conversational information seeking research, for user studies with different interfaces, for data collection from real users, and for preparing a demonstration of a CIS model", so we know that the paper provides case studies to illustrate how one can use Macaw for CIS research.
Answer: Yes
True answer: No

Validation ID: 58ef2442450c392bfc55c4dc35f216542f5f2dbb
Question: Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?
From the context "Researchers can benefit from Macaw for developing algorithms and techniques for conversational information seeking research, for user studies with different interfaces, for data collection from real users, and for preparing a demonstration of a CIS model", so we know that the paper provides case studies to illustrate how one can use Macaw for CIS research.
Answer: Yes
True answer: No

Validation ID: 78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d
Question: What functionality does Macaw provide?
From the context "Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data", so we know that Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.
Answer: conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data
True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d
Question: What functionality does Macaw provide?
From the context "Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data", so we know that Macaw is designed based on a modular architecture to support different information seeking tasks, including conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data.
Answer: conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data
True answer: conversational search, conversational question answering, conversational recommendation, conversational natural language interface to structured and semi-structured data

Validation ID: 375b281e7441547ba284068326dd834216e55c07
Question: What is a wizard of oz setup?
From the context "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", so we know that a wizard of oz setup is a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.
Answer: a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research
True answer: seeker interacts with a real conversational interface, intermediary (or the wizard) receives the seeker's message and performs different information seeking actions

Validation ID: 375b281e7441547ba284068326dd834216e55c07
Question: What is a wizard of oz setup?
From the context "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", so we know that a wizard of oz setup is a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.
Answer: a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research
True answer: a setup where the seeker interacts with a real conversational interface and the wizard, an intermediary, performs actions related to the seeker's message

Validation ID: 05c49b9f84772e6df41f530d86c1f7a1da6aa489
Question: What interface does Macaw currently have?
From the context "We have implemented the following interfaces for Macaw: File IO: This interface is designed for experimental purposes, such as evaluating the performance of a conversational search technique on a dataset with multiple queries. This is not an interactive interface. Standard IO: This interactive command line interface is designed for development purposes to interact with the system, see the logs, and debug or improve the system. Telegram: This interactive interface is designed for interaction with real users (see FIGREF4). Telegram is a popular instant messaging service whose client-side code is open-source. We have implemented a Telegram bot that can be used with different devices (personal computers, tablets, and mobile phones) and different operating systems (Android, iOS, Linux, Mac OS, and Windows). This interface allows multi-modal interactions (text, speech, click, image). It can be also used for speech-only interactions. For speech recognition and generation, Macaw relies on online APIs, e.g., the services provided by Google Cloud and Microsoft Azure. In addition, there exist multiple popular groups and channels in Telegram, which allows further integration of social networks with conversational systems. For example, see the Naseri and Zamani's study on news popularity in Telegram BIBREF12.", so we know that Macaw currently has File IO, Standard IO, and Telegram interfaces.
Answer: File IO, Standard IO, and Telegram interfaces
True answer: File IO, Standard IO, Telegram

Validation ID: 05c49b9f84772e6df41f530d86c1f7a1da6aa489
Question: What interface does Macaw currently have?
From the context "We have implemented the following interfaces for Macaw: File IO: This interface is designed for experimental purposes, such as evaluating the performance of a conversational search technique on a dataset with multiple queries. This is not an interactive interface. Standard IO: This interactive command line interface is designed for development purposes to interact with the system, see the logs, and debug or improve the system. Telegram: This interactive interface is designed for interaction with real users (see FIGREF4). Telegram is a popular instant messaging service whose client-side code is open-source. We have implemented a Telegram bot that can be used with different devices (personal computers, tablets, and mobile phones) and different operating systems (Android, iOS, Linux, Mac OS, and Windows). This interface allows multi-modal interactions (text, speech, click, image). It can be also used for speech-only interactions. For speech recognition and generation, Macaw relies on online APIs, e.g., the services provided by Google Cloud and Microsoft Azure. In addition, there exist multiple popular groups and channels in Telegram, which allows further integration of social networks with conversational systems. For example, see the Naseri and Zamani's study on news popularity in Telegram BIBREF12.", so we know that Macaw currently has File IO, Standard IO, and Telegram interfaces.
Answer: File IO, Standard IO, and Telegram interfaces
True answer: The current implementation of Macaw supports a command line interface as well as mobile, desktop, and web apps.

Validation ID: 6ecb69360449bb9915ac73c0a816c8ac479cbbfc
Question: What modalities are supported by Macaw?
From the context "Macaw supports multi-turn, multi-modal, and mixed-initiative interactions", so we know that Macaw supports multi-modal interactions.
Answer: multi-modal interactions
True answer: text, speech, image, click, etc

Validation ID: 68df324e5fa697baed25c761d0be4c528f7f5cf7
Question: What are the different modules in Macaw?
From the context "Macaw has a modular design, with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup also follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and the interface produces a Message object from the current interaction of user. The interaction can be in multi-modal form, such as text, speech, image, and click. Macaw stores all interactions in an “Interaction Database”. For every interaction, Macaw looks for most recent user-system interactions (including the system's responses) to create a list of Messages, called the conversation list. It is then dispatched to multiple information seeking (and related) actions. The actions run in parallel, and each should respond within a pre-defined time interval. The output selection component selects from (or potentially combines) the outputs generated by different actions and creates a Message object as the system's response. This message is logged into the interaction database and is sent to the interface to be presented to the user. Again, the response message can be multi-modal and include text, speech, link, list of options, etc.
Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.
True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 68df324e5fa697baed25c761d0be4c528f7f5cf7
Question: What are the different modules in Macaw?
From the context "Macaw has a modular design, with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup also follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and the interface produces a Message object from the current interaction of user. The interaction can be in multi-modal form, such as text, speech, image, and click. Macaw stores all interactions in an “Interaction Database”. For every interaction, Macaw looks for most recent user-system interactions (including the system's responses) to create a list of Messages, called the conversation list. It is then dispatched to multiple information seeking (and related) actions. The actions run in parallel, and each should respond within a pre-defined time interval. The output selection component selects from (or potentially combines) the outputs generated by different actions and creates a Message object as the system's response. This message is logged into the interaction database and is sent to the interface to be presented to the user. Again, the response message can be multi-modal and include text, speech, link, list of options, etc.
Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.
True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 77c34f1033702278f7f044806c1eba0c6ecb8b04
Question: Do they report results only on English data?
From the context "We evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 77c34f1033702278f7f044806c1eba0c6ecb8b04
Question: Do they report results only on English data?
From the context "We evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 2ee715c7c6289669f11a79743a6b2b696073805d
Question: What baseline model is used?
From the context "We reimplemented the ones in Dunietz and Gillick BIBREF11. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in", so we know that the baseline model is the ones in Dunietz and Gillick BIBREF11.
Answer: the ones in Dunietz and Gillick BIBREF11
True answer: For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. 

For Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.

Validation ID: 2ee715c7c6289669f11a79743a6b2b696073805d
Question: What baseline model is used?
From the context "We reimplemented the ones in Dunietz and Gillick BIBREF11. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in", so we know that the baseline model is the ones in Dunietz and Gillick BIBREF11.
Answer: the ones in Dunietz and Gillick BIBREF11
True answer: B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .

, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0

Validation ID: 61a9ea36ddc37c60d1a51dabcfff9445a2225725
Question: What news article sources are used?
From the context "We evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages", so we know that the news article sources are crawled from the news external references in Wikipedia from 73,734 entity pages.
Answer: the news external references in Wikipedia from 73,734 entity pages
True answer: Unanswerable

Validation ID: 61a9ea36ddc37c60d1a51dabcfff9445a2225725
Question: What news article sources are used?
From the context "We evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages", so we know that the news article sources are crawled from the news external references in Wikipedia from 73,734 entity pages.
Answer: the news external references in Wikipedia from 73,734 entity pages
True answer:  the news external references in Wikipedia

Validation ID: cc850bc8245a7ae790e1f59014371d4f35cd46d7
Question: How do they determine the exact section to use the input article?
From the context "The article–section placement task, determines the correct section for the triple INLINEFORM4, and is formalized in Equation EQREF15. DISPLAYFORM0", so we know that the article–section placement task determines the correct section for the triple INLINEFORM4.
Answer: determines the correct section for the triple INLINEFORM4
True answer: They use a multi-class classifier to determine the section it should be cited

Validation ID: 984fc3e726848f8f13dfe72b89e3770d00c3a1af
Question: What features are used to represent the novelty of news articles to entity pages?
From the context "An important feature when suggesting an article INLINEFORM0 to an entity INLINEFORM1 is the novelty of INLINEFORM2 w.r.t the already existing entity profile", so we know that the features used to represent the novelty of news articles to entity pages are novelty of news articles w.r.t the already existing entity profile.
Answer: novelty of news articles w.r.t the already existing entity profile
True answer: KL-divergences of language models for the news article and the already added news references

Validation ID: 984fc3e726848f8f13dfe72b89e3770d00c3a1af
Question: What features are used to represent the novelty of news articles to entity pages?
From the context "An important feature when suggesting an article INLINEFORM0 to an entity INLINEFORM1 is the novelty of INLINEFORM2 w.r.t the already existing entity profile", so we know that the features used to represent the novelty of news articles to entity pages are novelty of news articles w.r.t the already existing entity profile.
Answer: novelty of news articles w.r.t the already existing entity profile
True answer: KL divergence between the language model of INLINEFORM5 and articles in INLINEFORM6

Validation ID: fb1227b3681c69f60eb0539e16c5a8cd784177a7
Question: What features are used to represent the salience and relative authority of entities?
From the context "We use the salience features in BIBREF11 as part of our model. However, these features are document-internal — we will show that they are not sufficient to predict news inclusion into an entity page and add features of entity authority, news authority and novelty that measure the relations between several entities, between entity and news article as well as between several competing news articles.", so we know that the features used to represent the salience and relative authority of entities are entity authority, news authority and novelty.
Answer: entity authority, news authority and novelty
True answer: Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.
The relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it.

Validation ID: fb1227b3681c69f60eb0539e16c5a8cd784177a7
Question: What features are used to represent the salience and relative authority of entities?
From the context "We use the salience features in BIBREF11 as part of our model. However, these features are document-internal — we will show that they are not sufficient to predict news inclusion into an entity page and add features of entity authority, news authority and novelty that measure the relations between several entities, between entity and news article as well as between several competing news articles.", so we know that the features used to represent the salience and relative authority of entities are entity authority, news authority and novelty.
Answer: entity authority, news authority and novelty
True answer: positional features, occurrence frequency, internal POS structure of the entity and the sentence it occurs in, relative entity frequency, centrality measures like PageRank 

Validation ID: 8df35c24af9efc3348d3b8d746df116480dfe661
Question: Do they experiment with other tasks?
From the context "The corpus presented in this paper is the very first of its kind and a valuable addition to the scarce number of corpora available in the field of biomedical NLP. Its complexity, given the discontinuous nature of entities and a high number of nested and multi-label entities, poses new challenges for NLP methods applied for NER and can, hence, be a valuable source for insights into what entities “look like in the wild”. Moreover, it can serve as a playground for new modelling techniques such as the resolution of discontinuous entities as well as multi-task learning given the combination of entities and their relations.", so we know that they experiment with other tasks.
Answer: Yes
True answer: No

Validation ID: 277a7e916e65dfefd44d2d05774f95257ac946ae
Question: What baselines do they introduce?
From the context "We evaluate the corpus using Named Entity Recognition (NER), i. e., the task of finding mentions of concepts of interest in unstructured text. We focus on detecting cases, conditions, factors, findings and modifiers in case reports (cf. Section SECREF6). Four systems were developed to offer comparable robust baselines.", so we know that they introduce four baselines.
Answer: four baselines
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT


Validation ID: 277a7e916e65dfefd44d2d05774f95257ac946ae
Question: What baselines do they introduce?
From the context "We evaluate the corpus using Named Entity Recognition (NER), i. e., the task of finding mentions of concepts of interest in unstructured text. We focus on detecting cases, conditions, factors, findings and modifiers in case reports (cf. Section SECREF6). Four systems were developed to offer comparable robust baselines.", so we know that they introduce four baselines.
Answer: four baselines
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: 2916bbdb95ef31ab26527ba67961cf5ec94d6afe
Question: How large is the corpus?
From the context "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total", so we know that the corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total.
Answer: 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total
True answer: 8,275 sentences and 167,739 words in total

Validation ID: 2916bbdb95ef31ab26527ba67961cf5ec94d6afe
Question: How large is the corpus?
From the context "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total", so we know that the corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total.
Answer: 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total
True answer: The corpus comprises 8,275 sentences and 167,739 words in total.

Validation ID: f2e8497aa16327aa297a7f9f7d156e485fe33945
Question: How was annotation performed?
From the context "The annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation. The annotators could choose between a pre-annotated version or a blank version of each text. The pre-annotated versions contained suggested entity spans based on string matches from lists of conditions and findings synonym lists. Their quality varied widely throughout the corpus. The blank version was preferred by the annotators. We distribute the corpus in BioC JSON format. BioC was chosen as it allows us to capture the complexities of the annotations in the biomedical domain. It represented each documents properties ranging from full text, individual passages/sentences along with captured annotations and relationships in an organized manner. BioC is based on character offsets of annotations and allows the stacking of different layers.", so we know that the annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation. The annotators could choose between a pre-annotated version or a blank version of each text. The pre-annotated versions contained suggested entity spans based on string matches from lists of conditions and findings synonym lists. Their quality varied widely throughout the corpus. The blank version was preferred by the annotators. We distribute the corpus in BioC JSON format. BioC was chosen as it allows us to capture the complexities of the annotations in the biomedical domain. It represented each documents properties ranging from full text, individual passages/sentences along with captured annotations and relationships in an organized manner. BioC is based on character offsets of annotations and allows the stacking of different layers.
Answer: WebAnno BIBREF7
True answer: Experienced medical doctors used a linguistic annotation tool to annotate entities.

Validation ID: f2e8497aa16327aa297a7f9f7d156e485fe33945
Question: How was annotation performed?
From the context "The annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation. The annotators could choose between a pre-annotated version or a blank version of each text. The pre-annotated versions contained suggested entity spans based on string matches from lists of conditions and findings synonym lists. Their quality varied widely throughout the corpus. The blank version was preferred by the annotators. We distribute the corpus in BioC JSON format. BioC was chosen as it allows us to capture the complexities of the annotations in the biomedical domain. It represented each documents properties ranging from full text, individual passages/sentences along with captured annotations and relationships in an organized manner. BioC is based on character offsets of annotations and allows the stacking of different layers.", so we know that the annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation. The annotators could choose between a pre-annotated version or a blank version of each text. The pre-annotated versions contained suggested entity spans based on string matches from lists of conditions and findings synonym lists. Their quality varied widely throughout the corpus. The blank version was preferred by the annotators. We distribute the corpus in BioC JSON format. BioC was chosen as it allows us to capture the complexities of the annotations in the biomedical domain. It represented each documents properties ranging from full text, individual passages/sentences along with captured annotations and relationships in an organized manner. BioC is based on character offsets of annotations and allows the stacking of different layers.
Answer: The annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation. The annotators could choose between a pre-annotated version or a blank version of each text. The pre-annotated versions contained suggested entity spans based on string matches from lists of conditions and findings synonym lists. Their quality varied widely throughout the corpus. The blank version was preferred by the annotators. We distribute the corpus in BioC JSON format. BioC was chosen as it allows us to capture the complexities of the annotations in the biomedical domain. It represented each documents properties ranging from full text, individual passages/sentences along with captured annotations and relationships in an organized manner. BioC is based on character offsets of annotations and allows the stacking of different layers.
True answer: WebAnno

Validation ID: 9b76f428b7c8c9fc930aa88ee585a03478bff9b3
Question: How many documents are in the new corpus?
From the context "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total", so we know that the corpus consists of 53 documents.
Answer: 53 documents
True answer: 53 documents

Validation ID: 9b76f428b7c8c9fc930aa88ee585a03478bff9b3
Question: How many documents are in the new corpus?
From the context "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total", so we know that the new corpus consists of 53 documents.
Answer: 53 documents
True answer: 53 documents

Validation ID: dd6b378d89c05058e8f49e48fd48f5c458ea2ebc
Question: What baseline systems are proposed?
From the context "We evaluate the corpus using Named Entity Recognition (NER), i. e., the task of finding mentions of concepts of interest in unstructured text. We focus on detecting cases, conditions, factors, findings and modifiers in case reports (cf. Section SECREF6). Four systems were developed to offer comparable robust baselines.", so we know that four baseline systems are proposed.
Answer: four baseline systems
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: dd6b378d89c05058e8f49e48fd48f5c458ea2ebc
Question: What baseline systems are proposed?
From the context "We evaluate the corpus using Named Entity Recognition (NER), i. e., the task of finding mentions of concepts of interest in unstructured text. We focus on detecting cases, conditions, factors, findings and modifiers in case reports (cf. Section SECREF6). Four systems were developed to offer comparable robust baselines.", so we know that four baseline systems are proposed.
Answer: four baseline systems
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851
Question: How did they obtain the dataset?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that they obtained the dataset by collecting the maximum amount of tweets allowed by Twitter API.
Answer: collecting the maximum amount of tweets allowed by Twitter API
True answer: For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy

Validation ID: e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851
Question: How did they obtain the dataset?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that they obtained the dataset from two lists annotated in previous works.
Answer: two lists annotated in previous works
True answer: public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy

Validation ID: c00ce1e3be14610fb4e1f0614005911bb5ff0302
Question: What activation function do they use in their model?
From the context "We use hyperopt library to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout ($0.0-0.9$), activation function ($relu$, $selu$, $tanh$), optimizer ($sgd$, $adam$, $rmsprop$) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16). The validation split is extracted on the class level using stratified sampling: we took a random 25% of the accounts from each class since the dataset is unbalanced. Discarding the classes' size in the splitting process may affect the minority classes (e.g. hoax). For the baselines' classifier, we tested many classifiers and the LR showed the best overall performance.", so we know that they use hyperopt library to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout ($0.0-0.9$), activation function ($relu$, $selu$, $tanh$), optimizer ($sgd$, $adam$, $rmsprop$) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16). The validation split is extracted on the class level using stratified sampling: they took a random 25% of the accounts from each class since the dataset is unbalanced. Discarding the classes' size in the splitting process may affect the minority classes (e.g. hoax). For the baselines' classifier, they tested many classifiers and the LR showed the best overall performance.
Answer: $relu$, $selu$, $tanh$
True answer: relu, selu, tanh

Validation ID: c00ce1e3be14610fb4e1f0614005911bb5ff0302
Question: What activation function do they use in their model?
From the context "We use hyperopt library to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout ($0.0-0.9$), activation function ($relu$, $selu$, $tanh$), optimizer ($sgd$, $adam$, $rmsprop$) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16)", so we know that they use hyperopt library to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout ($0.0-0.9$), activation function ($relu$, $selu$, $tanh$), optimizer ($sgd$, $adam$, $rmsprop$) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16).
Answer: $relu$, $selu$, $tanh$
True answer: Activation function is hyperparameter. Possible values: relu, selu, tanh.

Validation ID: 71fe5822d9fccb1cb391c11283b223dc8aa1640c
Question: What baselines do they compare to?
From the context "We compare our approach (FacTweet) to the following set of baselines:", so we know that they compare to the following set of baselines:
[leftmargin=4mm]
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 71fe5822d9fccb1cb391c11283b223dc8aa1640c
Question: What baselines do they compare to?
From the context "We compare our approach (FacTweet) to the following set of baselines:", so we know that they compare to the following set of baselines:
[leftmargin=4mm]
True answer: Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words

Validation ID: 97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3
Question: How are chunks defined?
From the context "We sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account", so we know that chunks are defined by the posting date in ascending way.
Answer: by the posting date in ascending way
True answer: Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.

Validation ID: 97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3
Question: How are chunks defined?
From the context "We sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account", so we know that chunks are defined by the posting date in ascending way.
Answer: by the posting date in ascending way
True answer: sequence of $s$ tweets

Validation ID: 1062a0506c3691a93bb914171c2701d2ae9621cb
Question: What features are extracted?
From the context "We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets. We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts. Since we have many chunks for each account, the label for an account is obtained by taking the majority class of the account's chunks.", so we know that the features are extracted from each chunk and fed into a recurrent neural network to model the sequential flow of the chunks' tweets.
Answer: the features are extracted from each chunk and fed into a recurrent neural network to model the sequential flow of the chunks' tweets
True answer: Sentiment, Morality, Style, Words embeddings

Validation ID: 1062a0506c3691a93bb914171c2701d2ae9621cb
Question: What features are extracted?
From the context "We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets. We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts. Since we have many chunks for each account, the label for an account is obtained by taking the majority class of the account's chunks.", so we know that the features are extracted from each chunk and fed into a recurrent neural network to model the sequential flow of the chunks' tweets.
Answer: the features are extracted from each chunk and fed into a recurrent neural network to model the sequential flow of the chunks' tweets.
True answer: 15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings

Validation ID: 8e12b5c459fa963b3e549deadb864c244879fe82
Question: How many layers does their model have?
From the context "We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts", so we know that their model has two layers.
Answer: two layers
True answer: Unanswerable

Validation ID: 483a699563efcb8804e1861b18809279f21c7610
Question: Was the approach used in this work to detect fake news fully supervised?
From the context "We propose an approach to detect non-factual Twitter accounts by treating post streams as a sequence of tweets' chunks. We test several semantic and dictionary-based features together with a neural sequential approach, and apply an ablation test to investigate their contribution.", so we know that the approach used in this work to detect fake news is fully supervised.
Answer: Yes
True answer: Yes

Validation ID: d3ff2986ca8cb85a9a5cec039c266df756947b43
Question: Based on this paper, what is the more predictive set of features to detect fake news?
From the context "The results indicate that most performance gains come from words embeddings, style, and morality features. Other features (emotion and sentiment) show lower importance: nevertheless, they still improve the overall system performance (on average 0.35% Macro-F$_1$ improvement). These performance figures suggest that non-factual accounts use semantic and stylistic hidden signatures mostly while tweeting news, so as to be able to mislead the readers and behave as reputable (i.e., factual) sources.", so we know that the more predictive set of features to detect fake news is words embeddings, style, and morality features.
Answer: words embeddings, style, and morality features
True answer: words embeddings, style, and morality features

Validation ID: d3ff2986ca8cb85a9a5cec039c266df756947b43
Question: Based on this paper, what is the more predictive set of features to detect fake news?
From the context "The results indicate that most performance gains come from words embeddings, style, and morality features. Other features (emotion and sentiment) show lower importance: nevertheless, they still improve the overall system performance (on average 0.35% Macro-F$_1$ improvement). These performance figures suggest that non-factual accounts use semantic and stylistic hidden signatures mostly while tweeting news, so as to be able to mislead the readers and behave as reputable (i.e., factual) sources.", so we know that the more predictive set of features to detect fake news is words embeddings, style, and morality features.
Answer: words embeddings, style, and morality features
True answer: words embeddings, style, and morality features

Validation ID: 3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4
Question: How big is the dataset used in this work?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that the dataset has 180 Twitter accounts from BIBREF1 and 32 Twitter accounts from BIBREF19.
Answer: 180 Twitter accounts from BIBREF1 and 32 Twitter accounts from BIBREF19
True answer: Total dataset size: 171 account (522967 tweets)

Validation ID: 3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4
Question: How big is the dataset used in this work?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that the dataset has 180 Twitter accounts from BIBREF1 and 32 Twitter accounts from BIBREF19.
Answer: 180 Twitter accounts from BIBREF1 and 32 Twitter accounts from BIBREF19
True answer: 212 accounts

Validation ID: 2317ca8d475b01f6632537b95895608dc40c4415
Question: How is a "chunk of posts" defined in this work?
From the context "We sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account", so we know that a "chunk of posts" is a sorted sequence of tweets labeled by the label of its corresponding account.
Answer: a sorted sequence of tweets labeled by the label of its corresponding account
True answer: chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account

Validation ID: 2317ca8d475b01f6632537b95895608dc40c4415
Question: How is a "chunk of posts" defined in this work?
From the context "We sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account", so we know that a "chunk of posts" is a sorted sequence of tweets labeled by the label of its corresponding account.
Answer: a sorted sequence of tweets labeled by the label of its corresponding account
True answer: sequence of $s$ tweets

Validation ID: 3e88fb3d28593309a307eb97e875575644a01463
Question: What baselines were used in this work?
From the context "We compare our approach (FacTweet) to the following set of baselines:", so we know that the baselines used in this work are LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets.
Answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 3e88fb3d28593309a307eb97e875575644a01463
Question: What baselines were used in this work?
From the context "We compare our approach (FacTweet) to the following set of baselines:", so we know that the baselines used in this work are LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets.
Answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 0767ca8ff1424f7a811222ca108a33b6411aaa8a
Question: What is the performance of their method?
From the context "The scores given by the humans do not correlate with ROGUE. Human evaluators gives almost similarly scores to summary generated by the Lead-1 and Lead-1-AMR with Lead-1-AMR actually performing better on readability though it dropped some information as clear from the scores on information contained. On the other hand, ROGUE gives very high score to Lead-1 while models 1,2 and 4 get almost same scores", so we know that the performance of their method is that the scores given by the humans do not correlate with ROGUE. Human evaluators gives almost similarly scores to summary generated by the Lead-1 and Lead-1-AMR with Lead-1-AMR actually performing better on readability though it dropped some information as clear from the scores on information contained. On the other hand, ROGUE gives very high score to Lead-1 while models 1,2 and 4 get almost same scores.
Answer: the scores given by the humans do not correlate with ROGUE. Human evaluators gives almost similarly scores to summary generated by the Lead-1 and Lead-1-AMR with Lead-1-AMR actually performing better on readability though it dropped some information as clear from the scores on information contained. On the other hand, ROGUE gives very high score to Lead-1 while models 1,2 and 4 get almost same scores
True answer: Unanswerable

Validation ID: e8f969ffd637b82d04d3be28c51f0f3ca6b3883e
Question: Which evaluation methods are used?
From the context "We perform human evaluation to check whether the drop in ROGUE scores is mainly due to the inability of the ROGUE to evaluate abstractive summaries. Moreover, the ROGUE gives model 4 higher score compared to model 1 while human evaluators give the opposite scores on information contained in the sentence.", so we know that human evaluation is used.
Answer: human evaluation
True answer: Quantitative evaluation methods using ROUGE, Recall, Precision and F1.

Validation ID: e8f969ffd637b82d04d3be28c51f0f3ca6b3883e
Question: Which evaluation methods are used?
From the context "We perform human evaluation to check whether the drop in ROGUE scores is mainly due to the inability of the ROGUE to evaluate abstractive summaries. Moreover, the ROGUE gives model 4 higher score compared to model 1 while human evaluators give the opposite scores on information contained in the sentence.", so we know that human evaluation is used.
Answer: human evaluation
True answer: standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L

Validation ID: 46227b4265f1d300a5ed71bf40822829de662bc2
Question: What dataset is used in this paper?
From the context "We used two datasets for the task - AMR Bank BIBREF10 and CNN-Dailymail ( BIBREF11 BIBREF12 )", so we know that the dataset used in this paper is AMR Bank and CNN-Dailymail.
Answer: AMR Bank and CNN-Dailymail
True answer: AMR Bank, CNN-Dailymail

Validation ID: 46227b4265f1d300a5ed71bf40822829de662bc2
Question: What dataset is used in this paper?
From the context "We used two datasets for the task - AMR Bank BIBREF10 and CNN-Dailymail ( BIBREF11 BIBREF12 )", so we know that the dataset used in this paper is AMR Bank BIBREF10 and CNN-Dailymail ( BIBREF11 BIBREF12 ).
Answer: AMR Bank BIBREF10 and CNN-Dailymail ( BIBREF11 BIBREF12 )
True answer: AMR Bank BIBREF10, CNN-Dailymail ( BIBREF11 BIBREF12 )

Validation ID: a6a48de63c1928238b37c2a01c924b852fe752f8
Question: Which other methods do they compare with?
From the context "We compare the scores of these four cases as given by ROGUE along with human evaluation", so we know that they compare the scores of these four cases as given by ROGUE along with human evaluation.
Answer: ROGUE
True answer: Lead-3, Lead-1-AMR

Validation ID: a6a48de63c1928238b37c2a01c924b852fe752f8
Question: Which other methods do they compare with?
From the context "We compare the scores of these four cases as given by ROGUE along with human evaluation", so we know that they compare the scores of these four cases as given by ROGUE along with human evaluation.
Answer: ROGUE
True answer: Lead-3 model,  Lead-1-AMR, BIBREF0 

Validation ID: b65a83a24fc66728451bb063cf6ec50134c8bfb0
Question: How are sentences selected from the summary graph?
From the context "We start by finding the position of the most referred entity in the graph, then we find the closest verb to the entity. and finally select the subtree hanging from that verb as the summary AMR.", so we know that sentences are selected from the summary graph by finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR.
Answer: finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR
True answer:  finding the important sentences from the story, extracting the key information from those sentences using their AMR graphs

Validation ID: b65a83a24fc66728451bb063cf6ec50134c8bfb0
Question: How are sentences selected from the summary graph?
From the context "We start by finding the position of the most referred entity in the graph, then we find the closest verb to the entity. and finally select the subtree hanging from that verb as the summary AMR.", so we know that sentences are selected from the summary graph by finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR.
Answer: finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR
True answer:  Two methods: first is to simply pick initial few sentences,  second is to capture the relation between the two most important entities  (select the first sentence which contains both these entities).

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
From the context "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13. We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14, as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.", so we know that the models used in the experiment are linear SVM, BiLSTM, and CNN.
Answer: linear SVM, BiLSTM, and CNN
True answer: linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
From the context "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13. We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14, as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.", so we know that the models used in the experiment are linear SVM, BiLSTM, and CNN.
Answer: linear SVM, BiLSTM, and CNN
True answer: linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
From the context "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13. We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14, as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.", so we know that the models used in the experiment are linear SVM, BiLSTM, and CNN.
Answer: linear SVM, BiLSTM, and CNN
True answer: linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) 

Validation ID: 682e26262abba473412f68cbeb5f69aa3b9968d7
Question: What are the differences between this dataset and pre-existing ones?
From the context "In OLID, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10.", so we know that the differences between this dataset and pre-existing ones are:
1. OLID uses a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language.
True answer: no prior work has explored the target of the offensive language

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
From the context "The data included in OLID has been collected from Twitter. We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'", so we know that the tweets are in English.
Answer: English
True answer: English

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
From the context "The data included in OLID has been collected from Twitter. We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'", so we know that the tweets are in English.
Answer: English
True answer: English 

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
From the context "The data included in OLID has been collected from Twitter. We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'", so we know that the tweets are in English.
Answer: English
True answer: English

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
From the context "The dataset contains 14,100 tweets and is released freely to the research community", so we know that the size of the new dataset is 14,100 tweets.
Answer: 14,100 tweets
True answer: Unanswerable

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
From the context "The dataset contains 14,100 tweets and is released freely to the research community", so we know that the size of the new dataset is 14,100 tweets.
Answer: 14,100 tweets
True answer: 14,100 tweets

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
From the context "The dataset contains 14,100 tweets and is released freely to the research community", so we know that the size of the new dataset is 14,100 tweets.
Answer: 14,100 tweets
True answer: Dataset contains total of 14100 annotations.

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
From the context "Different abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language", so we know that the kinds of offensive content explored are aggression identification, bullying detection, hate speech, toxic comments, and offensive language.
Answer: aggression identification, bullying detection, hate speech, toxic comments, and offensive language
True answer: non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
From the context "Different abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language", so we know that the kinds of offensive content explored are aggression identification, bullying detection, hate speech, toxic comments, and offensive language.
Answer: aggression identification, bullying detection, hate speech, toxic comments, and offensive language
True answer: Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others , Untargeted (UNT): Posts containing non-targeted profanity and swearing.

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
From the context "Different abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language", so we know that the kinds of offensive content explored are aggression identification, bullying detection, hate speech, toxic comments, and offensive language.
Answer: aggression identification, bullying detection, hate speech, toxic comments, and offensive language
True answer: offensive (OFF) and non-offensive (NOT), targeted (TIN) and untargeted (INT) insults, targets of insults and threats as individual (IND), group (GRP), and other (OTH)

Validation ID: 55bd59076a49b19d3283af41c5e3ccb875f3eb0c
Question: What is the best performing model?
From the context "The results show that this is a challenging task. A CNN-based sentence classifier achieved the best results in all three sub-tasks", so we know that the best performing model is CNN-based sentence classifier.
Answer: CNN-based sentence classifier
True answer: CNN 

Validation ID: 521280a87c43fcdf9f577da235e7072a23f0673e
Question: How many annotators participated?
From the context "We first acquired two annotations for each instance. In case of 100% agreement, we considered these as acceptable annotations, and in case of disagreement, we requested more annotations until the agreement was above 66%.", so we know that the number of annotators is more than 2.
Answer: more than 2
True answer: five annotators

Validation ID: 5a8cc8f80509ea77d8213ed28c5ead501c68c725
Question: What is the definition of offensive language?
From the context "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10.
Level A: Offensive language Detection
True answer:  Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .

Validation ID: 290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30
Question: What are the three layers of the annotation scheme?
From the context "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10.", so we know that the three layers of the annotation scheme are A, B and C.
Answer: A, B and C
True answer: Level A: Offensive language Detection
, Level B: Categorization of Offensive Language
, Level C: Offensive Language Target Identification


Validation ID: 1b72aa2ec3ce02131e60626639f0cf2056ec23ca
Question: How long is the dataset for each step of hierarchy?
From the context "The breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15", so we know that the dataset for each step of hierarchy is shown in Table TABREF15.
Answer: shown in Table TABREF15
True answer: Level A: 14100 Tweets
Level B: 4640 Tweets
Level C: 4089 Tweets

Validation ID: c49ee6ac4dc812ff84d255886fd5aff794f53c39
Question: Do the authors report results only on English data?
From the context "We use the TAC 2014 scientific summarization dataset for our experiments. This dataset consists of 20 topics each with a biomedical journal article and 4 gold human written summaries", so we know that the authors report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: c49ee6ac4dc812ff84d255886fd5aff794f53c39
Question: Do the authors report results only on English data?
From the context "We use the TAC 2014 scientific summarization dataset for our experiments. This dataset consists of 20 topics each with a biomedical journal article and 4 gold human written summaries", so we know that the authors report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 3f856097be2246bde8244add838e83a2c793bd17
Question: In the proposed metric, how is content relevance measured?
From the context "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related.", so we know that the proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries.
Answer: information retrieval
True answer: The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. 

Validation ID: 3f856097be2246bde8244add838e83a2c793bd17
Question: In the proposed metric, how is content relevance measured?
From the context "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related.", so we know that the proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries.
Answer: information retrieval
True answer: On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval.

Validation ID: bf52c01bf82612d0c7bbf2e6a5bb2570c322936f
Question: What different correlations result when using different variants of ROUGE scores?
From the context "Table TABREF23 also shows the results of other Sera variants including discounting and query reformulation methods. Some of these variants are the result of applying query reformulation in the process of document retrieval which are described in section SECREF3 As illustrated, the Noun Phrases (NP) query reformulation at cut-off point of 5 (shown as Sera-5-NP) achieves the highest correlation with pyramid scores. This shows that query reformulation can improve the effectiveness of the proposed metric. The results of the other variants of Sera are also shown in Table TABREF23. The results show that the proposed metric is more effective than Rouge in evaluating the quality of scientific summaries.", so we know that the different correlations result when using different variants of ROUGE scores.
Answer: The proposed metric is more effective than Rouge in evaluating the quality of scientific summaries.
True answer: we observe that many variants of Rouge scores do not have high correlations with human pyramid scores

Validation ID: bf52c01bf82612d0c7bbf2e6a5bb2570c322936f
Question: What different correlations result when using different variants of ROUGE scores?
From the context "Table TABREF23 also shows the results of other Sera variants including discounting and query reformulation methods. Some of these variants are the result of applying query reformulation in the process of document retrieval which are described in section SECREF3 As illustrated, the Noun Phrases (NP) query reformulation at cut-off point of 5 (shown as Sera-5-NP) achieves the highest correlation with pyramid scores. This shows that query reformulation can improve the effectiveness of the proposed metric. The results of the other variants of Sera are also shown in Table TABREF23. The results show that the proposed metric is more effective than Rouge in evaluating the quality of scientific summaries.", so we know that the different correlations result when using different variants of ROUGE scores.
Answer: The proposed metric is more effective than Rouge in evaluating the quality of scientific summaries.
True answer: Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.

Validation ID: 74e866137b3452ec50fb6feaf5753c8637459e62
Question: What manual Pyramid scores are used?
From the context "We calculated all variants of Rouge scores, our proposed metric, Sera, and the Pyramid score on the generated summaries from the summarizers described in Section SECREF13. We do not report the Rouge, Sera or pyramid scores of individual systems as it is not the focus of this study. Our aim is to analyze the effectiveness of the evaluation metrics, not the summarization approaches. Therefore, we consider the correlations of the automatic evaluation metrics with the manual Pyramid scores to evaluate their effectiveness; the metrics that show higher correlations with manual judgments are more effective.", so we know that the manual Pyramid scores are used to evaluate the effectiveness of the evaluation metrics.
Answer: the manual Pyramid scores
True answer:  higher tiers of the pyramid

Validation ID: 74e866137b3452ec50fb6feaf5753c8637459e62
Question: What manual Pyramid scores are used?
From the context "We calculated all variants of Rouge scores, our proposed metric, Sera, and the Pyramid score on the generated summaries from the summarizers described in Section SECREF13. We do not report the Rouge, Sera or pyramid scores of individual systems as it is not the focus of this study. Our aim is to analyze the effectiveness of the evaluation metrics, not the summarization approaches. Therefore, we consider the correlations of the automatic evaluation metrics with the manual Pyramid scores to evaluate their effectiveness; the metrics that show higher correlations with manual judgments are more effective.", so we know that the manual Pyramid scores are used to evaluate the effectiveness of the evaluation metrics.
Answer: the manual Pyramid scores
True answer: following the pyramid framework, we design an annotation scheme

Validation ID: 184b0082e10ce191940c1d24785b631828a9f714
Question: What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'?
From the context "We argue that Rouge is not the best metric for all summarization tasks and we propose an alternative metric for evaluation of scientific summarization. The proposed alternative metric shows much higher and more consistent correlations with manual judgments in comparison with the well-established Rouge", so we know that the common belief is that Rouge is the best metric for all summarization tasks.
Answer: Rouge is the best metric for all summarization tasks
True answer: correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization

Validation ID: c59078efa7249acfb9043717237c96ae762c0a8c
Question: which existing strategies are compared?
From the context "We also implement the bias regularization method of BIBREF5 which debiases the word embedding during language model training by minimizing the projection of neutral words on the gender axis. We use hyperparameter tuning to find the best regularization coefficient and report results from the model trained with this coefficient. We later refer to this strategy as REG.", so we know that the bias regularization method of BIBREF5 is compared.
Answer: the bias regularization method of BIBREF5
True answer: CDA, REG

Validation ID: c59078efa7249acfb9043717237c96ae762c0a8c
Question: which existing strategies are compared?
From the context "We also implement the bias regularization method of BIBREF5 which debiases the word embedding during language model training by minimizing the projection of neutral words on the gender axis. We use hyperparameter tuning to find the best regularization coefficient and report results from the model trained with this coefficient. We later refer to this strategy as REG.", so we know that the bias regularization method of BIBREF5 is compared.
Answer: the bias regularization method of BIBREF5
True answer: Unanswerable

Validation ID: 73bddaaf601a4f944a3182ca0f4de85a19cdc1d2
Question: what dataset was used?
From the context "For the training data, we use Daily Mail news articles released by BIBREF9. This dataset is composed of 219,506 articles covering a diverse range of topics including business, sports, travel, etc., and is claimed to be biased and sensational BIBREF5. For manageability, we randomly subsample 5% of the text. The subsample has around 8.25 million tokens in total.", so we know that the dataset is Daily Mail news articles released by BIBREF9.
Answer: Daily Mail news articles released by BIBREF9
True answer: Daily Mail news articles released by BIBREF9 

Validation ID: 73bddaaf601a4f944a3182ca0f4de85a19cdc1d2
Question: what dataset was used?
From the context "For the training data, we use Daily Mail news articles released by BIBREF9. This dataset is composed of 219,506 articles covering a diverse range of topics including business, sports, travel, etc., and is claimed to be biased and sensational BIBREF5. For manageability, we randomly subsample 5% of the text. The subsample has around 8.25 million tokens in total.", so we know that the dataset is Daily Mail news articles released by BIBREF9.
Answer: Daily Mail news articles released by BIBREF9
True answer: Daily Mail news articles

Validation ID: d4e5e3f37679ff68914b55334e822ea18e60a6cf
Question: what kinds of male and female words are looked at?
From the context "We introduce a loss term INLINEFORM0, which aims to equalize the predicted probabilities of gender pairs such as woman and man. INLINEFORM1
INLINEFORM0 and INLINEFORM1 are a set of corresponding gender pairs, INLINEFORM2 is the size of the gender pairs set, and INLINEFORM3 indicates the output softmax probability.", so we know that INLINEFORM0 and INLINEFORM1 are a set of corresponding gender pairs.
True answer: gendered word pairs like he and she

Validation ID: 5f60defb546f35d25a094ff34781cddd4119e400
Question: how is mitigation of gender bias evaluated?
From the context "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1. This is defined as INLINEFORM2
where INLINEFORM0
True answer: Using INLINEFORM0 and INLINEFORM1

Validation ID: 5f60defb546f35d25a094ff34781cddd4119e400
Question: how is mitigation of gender bias evaluated?
From the context "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1. This is defined as INLINEFORM2
where INLINEFORM0
True answer: Unanswerable

Validation ID: 90d946ccc3abf494890e147dd85bd489b8f3f0e8
Question: what bias evaluation metrics are used?
From the context "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1. This is defined as INLINEFORM2
where INLINEFORM0
True answer: Unanswerable

Validation ID: 90d946ccc3abf494890e147dd85bd489b8f3f0e8
Question: what bias evaluation metrics are used?
From the context "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1. This is defined as INLINEFORM2
where INLINEFORM0
True answer: gender bias, normalized version of INLINEFORM0, ratio of occurrence of male and female words in the model generated text, Causal occupation bias conditioned on occupation, causal occupation bias conditioned on gender, INLINEFORM1

Validation ID: b962cc817a4baf6c56150f0d97097f18ad6cd9ed
Question: What kind of questions are present in the dataset?
From the context "This dataset allows evaluating a set of relational reasoning skills through natural language questions. It is composed of a large panel of human-generated questions. Moreover, we propose to augment the dataset with backtranslated reformulations of these questions. Finally, we evaluate 4 models on this dataset, including a projective model of our own design.", so we know that the dataset is composed of a large panel of human-generated questions.
Answer: human-generated questions
True answer: These 8 tasks require different competencies and a different level of understanding of the document to be well answered

Validation ID: fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f
Question: What baselines are presented?
From the context "In this section, we present the performance of four different models on our dataset: a logistic regression and three neural models. The first one is a basic LSTM BIBREF20, the second a MemN2N BIBREF18 and the third one is a model of our own design. This fourth model reuses the encoding layers of the R-net BIBREF12 and we modify the final layers with a projection layer that will be able to select the answer among the set of candidates instead of pointing the answerer directly into the source document.", so we know that the baselines are logistic regression, LSTM, MemN2N and a model of our own design.
Answer: logistic regression, LSTM, MemN2N and a model of our own design
True answer: Logistic regression, LSTM, End-to-end memory networks, Deep projective reader

Validation ID: fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f
Question: What baselines are presented?
From the context "In this section, we present the performance of four different models on our dataset: a logistic regression and three neural models. The first one is a basic LSTM BIBREF20, the second a MemN2N BIBREF18 and the third one is a model of our own design. This fourth model reuses the encoding layers of the R-net BIBREF12 and we modify the final layers with a projection layer that will be able to select the answer among the set of candidates instead of pointing the answerer directly into the source document.", so we know that the baselines are logistic regression, LSTM, MemN2N and a model of our own design.
Answer: logistic regression, LSTM, MemN2N and a model of our own design
True answer: Logistic regression, LSTM, End-to-end memory networks, Deep projective reader

Validation ID: 52f8a3e3cd5d42126b5307adc740b71510a6bdf5
Question: What tasks were evaluated?
From the context "In this paper, we formalize the sentiment analysis task through the framework of machine reading and release ReviewQA, a relational question-answering corpus. This dataset allows evaluating a set of relational reasoning skills through natural language questions. It is composed of a large panel of human-generated questions. Moreover, we propose to augment the dataset with backtranslated reformulations of these questions. Finally, we evaluate 4 models on this dataset, including a projective model of our own design that seems to", so we know that the tasks were evaluated are relational reasoning skills through natural language questions.
Answer: relational reasoning skills through natural language questions
True answer: ReviewQA's test set

Validation ID: 52f8a3e3cd5d42126b5307adc740b71510a6bdf5
Question: What tasks were evaluated?
From the context "In this paper, we formalize the sentiment analysis task through the framework of machine reading and release ReviewQA, a relational question-answering corpus. This dataset allows evaluating a set of relational reasoning skills through natural language questions. It is composed of a large panel of human-generated questions. Moreover, we propose to augment the dataset with backtranslated reformulations of these questions. Finally, we evaluate 4 models on this dataset, including a projective model of our own design that seems to", so we know that the tasks were evaluated are relational reasoning skills through natural language questions.
Answer: relational reasoning skills through natural language questions
True answer: Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review

Validation ID: 2236386729105f5cf42f73cc055ce3acdea2d452
Question: What language are the reviews in?
From the context "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11", so we know that the reviews are in English.
Answer: English
True answer: Unanswerable

Validation ID: 2236386729105f5cf42f73cc055ce3acdea2d452
Question: What language are the reviews in?
From the context "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11", so we know that the reviews are in English.
Answer: English
True answer: English

Validation ID: 18942ab8c365955da3fd8fc901dfb1a3b65c1be1
Question: Where are the hotel reviews from?
From the context "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11", so we know that the hotel reviews are from TripAdvisor.
Answer: TripAdvisor
True answer: TripAdvisor

Validation ID: 18942ab8c365955da3fd8fc901dfb1a3b65c1be1
Question: Where are the hotel reviews from?
From the context "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11", so we know that the hotel reviews are from TripAdvisor.
Answer: TripAdvisor
True answer: TripAdvisor

Validation ID: 7b4992e2d26577246a16ac0d1efc995ab4695d24
Question: What was the baseline used?
From the context "We used the Approximate Randomisation Test BIBREF17, BIBREF18 to calculate statistical significance and found that the improvement for each of the systems using artificial data was significant over using only manual annotation", so we know that the baseline used is using only manual annotation.
Answer: using only manual annotation
True answer: error detection system by Rei2016

Validation ID: 7b4992e2d26577246a16ac0d1efc995ab4695d24
Question: What was the baseline used?
From the context "We used the Approximate Randomisation Test BIBREF17, BIBREF18 to calculate statistical significance and found that the improvement for each of the systems using artificial data was significant over using only manual annotation", so we know that the baseline used is using only manual annotation.
Answer: using only manual annotation
True answer: error detection system by Rei2016

Validation ID: ab9b0bde6113ffef8eb1c39919d21e5913a05081
Question: What are their results on both datasets?
From the context "The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection.", so we know that the results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection.
Answer: error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection.
True answer: Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. 

Validation ID: 9a9d225f9ac35ed35ea02f554f6056af3b42471d
Question: What textual patterns are extracted?
From the context "The original and corrected sentences in the corpus are aligned and used to identify short transformation patterns in the form of (incorrect phrase, correct phrase). The length of each pattern is the affected phrase, plus up to one token of context on both sides. If a word form changes between the incorrect and correct text, it is fully saved in the pattern, otherwise the POS tags are used for matching.", so we know that the textual patterns are (incorrect phrase, correct phrase).
Answer: (incorrect phrase, correct phrase)
True answer: (VVD shop_VV0 II, VVD shopping_VVG II)

Validation ID: 9a9d225f9ac35ed35ea02f554f6056af3b42471d
Question: What textual patterns are extracted?
From the context "The original and corrected sentences in the corpus are aligned and used to identify short transformation patterns in the form of (incorrect phrase, correct phrase). The length of each pattern is the affected phrase, plus up to one token of context on both sides. If a word form changes between the incorrect and correct text, it is fully saved in the pattern, otherwise the POS tags are used for matching.", so we know that the textual patterns are (incorrect phrase, correct phrase).
Answer: (incorrect phrase, correct phrase)
True answer: patterns for generating all types of errors

Validation ID: ea56148a8356a1918bedcf0a99ae667c27792cfe
Question: Which annotated corpus did they use?
From the context "The original and corrected sentences in the corpus are aligned and used to identify short transformation patterns in the form of (incorrect phrase, correct phrase). The length of each pattern is the affected phrase, plus up to one token of context on both sides. If a word form changes between the incorrect and correct text, it is fully saved in the pattern, otherwise the POS tags are used for matching.", so we know that they used the original and corrected sentences in the corpus.
Answer: the original and corrected sentences in the corpus
True answer:  FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) 

Validation ID: ea56148a8356a1918bedcf0a99ae667c27792cfe
Question: Which annotated corpus did they use?
From the context "The original and corrected sentences in the corpus are aligned and used to identify short transformation patterns in the form of (incorrect phrase, correct phrase). The length of each pattern is the affected phrase, plus up to one token of context on both sides. If a word form changes between the incorrect and correct text, it is fully saved in the pattern, otherwise the POS tags are used for matching.", so we know that they used the original and corrected sentences in the corpus.
Answer: the original and corrected sentences in the corpus
True answer: FCE ,  two alternative annotations of the CoNLL 2014 Shared Task dataset

Validation ID: cd32a38e0f33b137ab590e1677e8fb073724df7f
Question: Which languages are explored in this paper?
From the context "We evaluate our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) BIBREF3", so we know that the languages explored in this paper are English.
Answer: English
True answer: English 

Validation ID: cd32a38e0f33b137ab590e1677e8fb073724df7f
Question: Which languages are explored in this paper?
From the context "We evaluate our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) BIBREF3", so we know that the languages explored in this paper are English.
Answer: English
True answer: English 

Validation ID: 2c6b50877133a499502feb79a682f4023ddab63e
Question: what language does this paper focus on?
From the context "Text simplification aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning, which can help children, non-native speakers, and people with cognitive disabilities, to understand text better", so we know that this paper focus on English.
Answer: English
True answer: English

Validation ID: 2c6b50877133a499502feb79a682f4023ddab63e
Question: what language does this paper focus on?
From the context "Text simplification aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning, which can help children, non-native speakers, and people with cognitive disabilities, to understand text better", so we know that this paper focus on English.
Answer: English
True answer: Simple English

Validation ID: f651cd144b7749e82aa1374779700812f64c8799
Question: what evaluation metrics did they use?
From the context "We evaluate the performance of text simplification using neural machine translation on available parallel sentences and additional simplified sentences. We choose three statistical text simplification systems. PBMT-R is a phrase-based method with a reranking post-processing step BIBREF18. Hybrid performs sentence splitting and deletion operations based on discourse representation structures, and then simplifies sentences with PBMT-R BIBREF25. SBMT-SARI BIBREF19 is syntax-based translation model using PPDB paraphrase database BIBREF26 and modifies tuning function (using SARI). We choose two neural text simplification systems. NMT is a basic attention-based encoder-decoder model which uses OpenNMT framework to train with two LSTM layers, hidden states of size 500 and 500 hidden units, SGD optimizer, and a dropout rate of 0.3 BIBREF8. Dress is an encoder-decoder model coupled with a deep reinforcement learning framework, and the parameters are chosen according to the original paper BIBREF20. For the experiments with synthetic parallel data, we back-translate a random sample of 60 000 sentences from the collected simplified sentences into ordinary sentences. Our model is trained on synthetic data and the available parallel data, denoted as NMT+synthetic.
Results. Table 1 shows the results of all models on WikiLarge dataset. We can see that our method (NMT+synthetic) can obtain higher BLEU, lower FKGL and high SARI compared with other models, except Dress on FKGL and SBMT-SARI on SARI. It verified that including synthetic data during training is very effective, and yields an improvement over our baseline NMF by 2.11 BLEU, 1.7 FKGL and 1.07 SARI. We also substantially outperform Dress, who previously reported SOTA result. The results of our human evaluation using Simplicity are also presented in Table 1. NMT on synthetic data is significantly better than PBMT-R, Dress, and SBMT-SARI on Simplicity. It indicates that our method with simplified data is effective at creating simpler output.
True answer: BLEU , FKGL , SARI 

Validation ID: f651cd144b7749e82aa1374779700812f64c8799
Question: what evaluation metrics did they use?
From the context "We evaluate the performance of text simplification using neural machine translation on available parallel sentences and additional simplified sentences. We choose three statistical text simplification systems. PBMT-R is a phrase-based method with a reranking post-processing step BIBREF18. Hybrid performs sentence splitting and deletion operations based on discourse representation structures, and then simplifies sentences with PBMT-R BIBREF25. SBMT-SARI BIBREF19 is syntax-based translation model using PPDB paraphrase database BIBREF26 and modifies tuning function (using SARI). We choose two neural text simplification systems. NMT is a basic attention-based encoder-decoder model which uses OpenNMT framework to train with two LSTM layers, hidden states of size 500 and 500 hidden units, SGD optimizer, and a dropout rate of 0.3 BIBREF8. Dress is an encoder-decoder model coupled with a deep reinforcement learning framework, and the parameters are chosen according to the original paper BIBREF20. For the experiments with synthetic parallel data, we back-translate a random sample of 60 000 sentences from the collected simplified sentences into ordinary sentences. Our model is trained on synthetic data and the available parallel data, denoted as NMT+synthetic.
Results. Table 1 shows the results of all models on WikiLarge dataset. We can see that our method (NMT+synthetic) can obtain higher BLEU, lower FKGL and high SARI compared with other models, except Dress on FKGL and SBMT-SARI on SARI. It verified that including synthetic data during training is very effective, and yields an improvement over our baseline NMF by 2.11 BLEU, 1.7 FKGL and 1.07 SARI. We also substantially outperform Dress, who previously reported SOTA result. The results of our human evaluation using Simplicity are also presented in Table 1. NMT on synthetic data is significantly better than PBMT-R, Dress, and SBMT-SARI on Simplicity. It indicates that our method with simplified data is effective at creating simpler output.
True answer: BLEU, FKGL, SARI, Simplicity

Validation ID: 4625cfba3083346a96e573af5464bc26c34ec943
Question: by how much did their model improve?
From the context "In the experiments on two datasets, we achieve substantial gains in all tasks, and new SOTA results, via back-translation of simplified sentences into the ordinary sentences, and treating this synthetic data as additional training data", so we know that their model improved by substantial gains in all tasks, and new SOTA results.
Answer: substantial gains in all tasks, and new SOTA results
True answer: For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.
For the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.

Validation ID: 4625cfba3083346a96e573af5464bc26c34ec943
Question: by how much did their model improve?
From the context "In the experiments on two datasets, we achieve substantial gains in all tasks, and new SOTA results, via back-translation of simplified sentences into the ordinary sentences, and treating this synthetic data as additional training data", so we know that their model improved by substantial gains in all tasks, and new SOTA results.
Answer: substantial gains in all tasks, and new SOTA results
True answer: 6.37 BLEU

Validation ID: 326588b1de9ba0fd049ab37c907e6e5413e14acd
Question: what state of the art methods did they compare with?
From the context "We evaluate the performance of text simplification using neural machine translation on available parallel sentences and additional simplified sentences. We choose three statistical text simplification systems. PBMT-R is a phrase-based method with a reranking post-processing step BIBREF18. Hybrid performs sentence splitting and deletion operations based on discourse representation structures, and then simplifies sentences with PBMT-R BIBREF25. SBMT-SARI BIBREF19 is syntax-based translation model using PPDB paraphrase database BIBREF26 and modifies tuning function (using SARI). We choose two neural text simplification systems. NMT is a basic attention-based encoder-decoder model which uses OpenNMT framework to train with two LSTM layers, hidden states of size 500 and 500 hidden units, SGD optimizer, and a dropout rate of 0.3 BIBREF8. Dress is an encoder-decoder model coupled with a deep reinforcement learning framework, and the parameters are chosen according to the original paper BIBREF20.", so we know that they compare with PBMT-R, Hybrid, SBMT-SARI, NMT and Dress.
Answer: PBMT-R, Hybrid, SBMT-SARI, NMT and Dress
True answer: OpenNMT, PBMT-R, Hybrid, SBMT-SARI, Dress

Validation ID: ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb
Question: what are the sizes of both datasets?
From the context "We use two simplification datasets (WikiSmall and WikiLarge). WikiSmall consists of ordinary and simplified sentences from the ordinary and simple English Wikipedias, which has been used as benchmark for evaluating text simplification BIBREF17, BIBREF18, BIBREF8. The training set has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge is also from Wikipedia corpus whose training set contains 296,402 sentence pairs BIBREF19, BIBREF20. WikiLarge includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing.", so we know that the size of WikiSmall is 89,042 sentence pairs, and the size of WikiLarge is 296,402 sentence pairs.
Answer: 89,042 sentence pairs and 296,402 sentence pairs
True answer: training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing

Validation ID: ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb
Question: what are the sizes of both datasets?
From the context "We use two simplification datasets (WikiSmall and WikiLarge). WikiSmall consists of ordinary and simplified sentences from the ordinary and simple English Wikipedias, which has been used as benchmark for evaluating text simplification BIBREF17, BIBREF18, BIBREF8. The training set has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge is also from Wikipedia corpus whose training set contains 296,402 sentence pairs BIBREF19, BIBREF20. WikiLarge includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing.", so we know that the size of WikiSmall is 89,042 sentence pairs, and the size of WikiLarge is 296,402 sentence pairs.
Answer: 89,042 sentence pairs and 296,402 sentence pairs
True answer: WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. 

Validation ID: 55507f066073b29c1736b684c09c045064053ba9
!!! Has error during processing !!!
Question: What are the distinctive characteristics of how Arabic speakers use offensive language?
Answer: From the context "We analyzed all tweets labeled as offensive to better understand how Arabic speakers use offensive language. Here is a breakdown of usage: Direct name calling: The most frequent attack is to call a person an animal name, and the most used animals were كلب> (“klb” – “dog”), حمار> (“HmAr” – “donkey”), and صقر> (“Sqr” – “falcon”). The second most common was insulting mental abilities using words such as غبي> (“gby” – “stupid”) and عبيط> (“EbyT” –“idiot”). Some culture-specific differences should be considered. Not all animal names are used as insults. For example, animals such as أسد> (“Asd” – “lion”), صقر> (“Sqr” – “falcon”), and غزال> (“gzAl” – “gazelle”) are typically used for praise. For other insults, people use: some bird names such as دجاجة> (“djAjp” – “chicken”), بومة> (“bwmp” – “owl”), and غراب> (“grAb” – “crow”); insects such as ذبابة> (“*bAbp” – “fly”), صرصور> (“SrSwr” – “cockroach”), and حشرة> (“H$rp” – “insect”); microorganisms such as جرثومة> (“jrvwmp” – “microbe”) and طحالب> (“THAlb” – “algae”); inanimate objects such as جزمة> (“jzmp” – “sh
True answer: Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.

Validation ID: 55507f066073b29c1736b684c09c045064053ba9
!!! Has error during processing !!!
Question: What are the distinctive characteristics of how Arabic speakers use offensive language?
Answer: From the context "We analyzed all tweets labeled as offensive to better understand how Arabic speakers use offensive language. Here is a breakdown of usage: Direct name calling: The most frequent attack is to call a person an animal name, and the most used animals were كلب> (“klb” – “dog”), حمار> (“HmAr” – “donkey”), and صقر> (“Sqr” – “falcon”). The second most common was insulting mental abilities using words such as غبي> (“gby” – “stupid”) and عبيط> (“EbyT” –“idiot”). Some culture-specific differences should be considered. Not all animal names are used as insults. For example, animals such as أسد> (“Asd” – “lion”), صقر> (“Sqr” – “falcon”), and غزال> (“gzAl” – “gazelle”) are typically used for praise. For other insults, people use: some bird names such as دجاجة> (“djAjp” – “chicken”), بومة> (“bwmp” – “owl”), and غراب> (“grAb” – “crow”); insects such as ذبابة> (“*bAbp” – “fly”), صرصور> (“SrSwr” – “cockroach”), and حشرة> (“H$rp” – “insect”); microorganisms such as جرثومة> (“jrvwmp” – “microbe”) and طحالب> (“THAlb” – “algae”); inanimate objects such as جزمة> (“jzmp” – “sh
True answer: Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related

Validation ID: e838275bb0673fba0d67ac00e4307944a2c17be3
Question: How did they analyze which topics, dialects and gender are most associated with tweets?
From the context "Given the annotated tweets, we wanted to ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language.", so we know that they analyzed which topics, dialects and gender are most associated with tweets by analyzing the annotated tweets.
Answer: by analyzing the annotated tweets
True answer: ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language

Validation ID: 8dda1ef371933811e2a25a286529c31623cca0c6
Question: How many annotators tagged each tweet?
From the context "We developed the annotation guidelines jointly with an experienced annotator, who is a native Arabic speaker with a good knowledge of various Arabic dialects. We made sure that our guidelines were compatible with those of OffensEval2019. The annotator carried out all annotation.", so we know that the annotator carried out all annotation.
From the context "To validate the quality of annotation, a random sample of 100 tweets from the data, containing 50 offensive and 50 clean tweets, was given to additional three annotators. We calculated the Inter-Annotator Agreement between the annotators using Fleiss’s Kappa coefficient BIBREF23. The Kappa score was 0.92 indicating high quality annotation and agreement.", so we know that the Kappa score was 0.92 indicating high quality annotation and agreement.
True answer: One

Validation ID: 8dda1ef371933811e2a25a286529c31623cca0c6
Question: How many annotators tagged each tweet?
From the context "We developed the annotation guidelines jointly with an experienced annotator, who is a native Arabic speaker with a good knowledge of various Arabic dialects. We made sure that our guidelines were compatible with those of OffensEval2019. The annotator carried out all annotation.", so we know that the annotator carried out all annotation.
From the context "To validate the quality of annotation, a random sample of 100 tweets from the data, containing 50 offensive and 50 clean tweets, was given to additional three annotators. We calculated the Inter-Annotator Agreement between the annotators using Fleiss’s Kappa coefficient BIBREF23. The Kappa score was 0.92 indicating high quality annotation and agreement.", so we know that the Kappa score was 0.92 indicating high quality annotation and agreement.
True answer: One experienced annotator tagged all tweets

Validation ID: b3de9357c569fb1454be8f2ac5fcecaea295b967
Question: How many tweets are in the dataset?
From the context "We describe in greater detail our annotation guidelines, which we made sure that they are compatible with the OffensEval2019 annotation guidelines BIBREF15. For example, if a tweet has insults or threats targeting a group based on their nationality, ethnicity, gender, political affiliation, religious belief, or other common characteristics, this is considered as hate speech BIBREF15. It is worth mentioning that we also considered insulting groups based on their sport affiliation as a form of hate speech. In most Arab countries, being a fan of a particularly sporting club is considered as part of the personality and ideology which rarely changes over time (similar to political affiliation). Many incidents of violence have occurred among fans of rival clubs.", so we know that the dataset has 1,915 offensive tweets, including 225 vulgar tweet and 506 hate speech tweets, and 8,085 clean tweets.
Answer: 1,915 offensive tweets, including 225 vulgar tweet and 506 hate speech tweets, and 8,085 clean tweets
True answer: 10,000 Arabic tweet dataset 

Validation ID: b3de9357c569fb1454be8f2ac5fcecaea295b967
Question: How many tweets are in the dataset?
From the context "We describe in greater detail our annotation guidelines, which we made sure that they are compatible with the OffensEval2019 annotation guidelines BIBREF15. For example, if a tweet has insults or threats targeting a group based on their nationality, ethnicity, gender, political affiliation, religious belief, or other common characteristics, this is considered as hate speech BIBREF15. It is worth mentioning that we also considered insulting groups based on their sport affiliation as a form of hate speech. In most Arab countries, being a fan of a particularly sporting club is considered as part of the personality and ideology which rarely changes over time (similar to political affiliation). Many incidents of violence have occurred among fans of rival clubs.", so we know that the dataset has 1,915 offensive tweets, including 225 vulgar tweet and 506 hate speech tweets, and 8,085 clean tweets.
Answer: 1,915 offensive tweets, including 225 vulgar tweet and 506 hate speech tweets, and 8,085 clean tweets
True answer: 10,000

Validation ID: 59e58c6fc63cf5b54b632462465bfbd85b1bf3dd
Question: In what way is the offensive dataset not biased by topic, dialect or target?
From the context "Our target was to build a large Arabic offensive language dataset that is representative of their appearance on Twitter and is hopefully not biased to specific dialects, topics, or targets. One of the main challenges is that offensive tweets constitute a very small portion of overall tweets. To quantify their proportion, we took 3 random samples of tweets from different days, with each sample composed of 1,000 tweets, and we found that between 1% and 2% of them were in fact offensive (including pornographic advertisement). This percentage is consistent with previously reported percentages BIBREF19. Thus, annotating random tweets is grossly inefficient. One way to overcome this problem is to use a seed list of offensive words to filter tweets. However, doing so is problematic as it would skew the dataset to particular types of offensive language or to specific dialects. Offensiveness is often dialect and country specific.", so we know that the offensive dataset is not biased by topic, dialect or target.
Answer: not biased by topic, dialect or target
True answer: It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets.

Validation ID: 59e58c6fc63cf5b54b632462465bfbd85b1bf3dd
Question: In what way is the offensive dataset not biased by topic, dialect or target?
From the context "Our target was to build a large Arabic offensive language dataset that is representative of their appearance on Twitter and is hopefully not biased to specific dialects, topics, or targets. One of the main challenges is that offensive tweets constitute a very small portion of overall tweets. To quantify their proportion, we took 3 random samples of tweets from different days, with each sample composed of 1,000 tweets, and we found that between 1% and 2% of them were in fact offensive (including pornographic advertisement). This percentage is consistent with previously reported percentages BIBREF19. Thus, annotating random tweets is grossly inefficient. One way to overcome this problem is to use a seed list of offensive words to filter tweets. However, doing so is problematic as it would skew the dataset to particular types of offensive language or to specific dialects. Offensiveness is often dialect and country specific.", so we know that the offensive dataset is not biased by topic, dialect or target.
Answer: not biased by topic, dialect or target
True answer: our methodology does not use a seed list of offensive words

Validation ID: 5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83
Question: What experiments are conducted?
From the context "Experimental results demonstrate that our model achieves a high irony accuracy with well-preserved sentiment and content", so we know that the experiments are conducted to demonstrate that our model achieves a high irony accuracy with well-preserved sentiment and content.
Answer: demonstrate that our model achieves a high irony accuracy with well-preserved sentiment and content
True answer: Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences

Validation ID: 3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f
Question: What is the combination of rewards for reinforcement learning?
From the context "To encourage our model to focus on both the irony accuracy and the sentiment preservation, we apply the harmonic mean of irony reward and sentiment reward", so we know that the combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward.
Answer: the harmonic mean of irony reward and sentiment reward
True answer: irony accuracy, sentiment preservation

Validation ID: 3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f
Question: What is the combination of rewards for reinforcement learning?
From the context "In order to encourage our model to focus on both the irony accuracy and the sentiment preservation, we apply the harmonic mean of irony reward and sentiment reward", so we know that the combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward.
Answer: the harmonic mean of irony reward and sentiment reward
True answer:  irony accuracy and sentiment preservation

Validation ID: 14b8ae5656e7d4ee02237288372d9e682b24fdb8
Question: What are the difficulties in modelling the ironic pattern?
From the context "As ironic style is hard to model and describe, we mainly focus on studies without editing style attribute words", so we know that the difficulties in modelling the ironic pattern are that ironic style is hard to model and describe.
Answer: ironic style is hard to model and describe
True answer: obscure and hard to understand,  lack of previous work and baselines on irony generation

Validation ID: 14b8ae5656e7d4ee02237288372d9e682b24fdb8
Question: What are the difficulties in modelling the ironic pattern?
From the context "As ironic style is hard to model and describe, we mainly focus on studies without editing style attribute words", so we know that the difficulties in modelling the ironic pattern are that ironic style is hard to model and describe.
Answer: ironic style is hard to model and describe
True answer: ironies are often obscure and hard to understand

Validation ID: e3a2d8886f03e78ed5e138df870f48635875727e
Question: How did the authors find ironic data on twitter?
From the context "In order to address the lack of irony data, we first crawl over 2M tweets from twitter to build a dataset with 262,755 ironic and 112,330 non-ironic tweets", so we know that the authors crawled over 2M tweets from twitter to build a dataset with 262,755 ironic and 112,330 non-ironic tweets.
Answer: crawled over 2M tweets from twitter
True answer: They developed a classifier to find ironic sentences in twitter data

Validation ID: e3a2d8886f03e78ed5e138df870f48635875727e
Question: How did the authors find ironic data on twitter?
From the context "In order to address the lack of irony data, we first crawl over 2M tweets from twitter to build a dataset with 262,755 ironic and 112,330 non-ironic tweets", so we know that the authors crawled over 2M tweets from twitter to build a dataset with 262,755 ironic and 112,330 non-ironic tweets.
Answer: crawled over 2M tweets from twitter
True answer: by crawling

Validation ID: 62f27fe08ddb67f16857fab2a8a721926ecbb6fb
Question: Who judged the irony accuracy, sentiment preservation and content preservation?
From the context "In order to evaluate sentiment preservation, we use the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. We call the value as sentiment delta (senti delta). Besides, we report the sentiment accuracy (Senti ACC) which measures whether the output sentence has the same sentiment polarity as the input sentence", so we know that the sentiment accuracy (Senti ACC) is used to judge the sentiment preservation.
From the context "In order to evaluate irony accuracy, we use the accuracy of the irony classifier to judge whether the output sentence is ironic or not. We call the value as irony accuracy (Irony ACC)", so we know that the irony accuracy (Irony ACC) is used to judge the irony accuracy.
True answer: Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).

Validation ID: 62f27fe08ddb67f16857fab2a8a721926ecbb6fb
Question: Who judged the irony accuracy, sentiment preservation and content preservation?
From the context "In order to evaluate sentiment preservation, we use the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. We call the value as sentiment delta (senti delta). Besides, we report the sentiment accuracy (Senti ACC) which measures whether the output sentence has the same sentiment polarity as the input sentence", so we know that the sentiment accuracy (Senti ACC) is used to judge the sentiment preservation.
From the context "In order to evaluate irony accuracy, we use the accuracy of the irony classifier to judge whether the output sentence is ironic or not. We call the value as irony accuracy (Irony ACC)", so we know that the irony accuracy (Irony ACC) is used to judge the irony accuracy.
True answer: four annotators who are proficient in English

Validation ID: 9ca447c8959a693a3f7bdd0a2c516f4b86f95718
Question: How were the tweets annotated?
From the context "We have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2. Hence, our data set is a balanced one although it is currently limited in size. The corresponding stance annotations are made publicly available at http://ceng.metu.edu.tr/ INLINEFORM0 e120329/ Turkish_Stance_Detection_Tweet_Dataset.csv in Comma Separated Values (CSV) format. The file contains three columns with the corresponding headers. The first column is the tweet id of the corresponding tweet, the second column contains the name of the stance target, and the last column includes the stance of the tweet for the target as Favor or Against.", so we know that the tweets were annotated as Favor or Against.
Answer: Favor or Against
True answer: tweets are annotated with only Favor or Against for two targets - Galatasaray and Fenerbahçe

Validation ID: 9ca447c8959a693a3f7bdd0a2c516f4b86f95718
Question: How were the tweets annotated?
From the context "We have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2. Hence, our data set is a balanced one although it is currently limited in size. The corresponding stance annotations are made publicly available at http://ceng.metu.edu.tr/ INLINEFORM0 e120329/ Turkish_Stance_Detection_Tweet_Dataset.csv in Comma Separated Values (CSV) format. The file contains three columns with the corresponding headers. The first column is the tweet id of the corresponding tweet, the second column contains the name of the stance target, and the last column includes the stance of the tweet for the target as Favor or Against.", so we know that the tweets were annotated as Favor or Against.
Answer: Favor or Against
True answer: Unanswerable

Validation ID: 05887a8466e0a2f0df4d6a5ffc5815acd7d9066a
Question: Which SVM approach resulted in the best performance?
From the context "To sum up, our evaluation results are significant as reference results to be used for comparison purposes and provides evidence for the utility of unigram-based and hashtag-related features in SVM classifiers for the stance detection problem in Turkish tweets", so we know that unigram-based and hashtag-related features in SVM classifiers for the stance detection problem in Turkish tweets resulted in the best performance.
Answer: unigram-based and hashtag-related features in SVM classifiers for the stance detection problem in Turkish tweets
True answer: Target-1

Validation ID: c87fcc98625e82fdb494ff0f5309319620d69040
Question: What are hashtag features?
From the context "To sum up, our evaluation results are significant as reference results to be used for comparison purposes and provides evidence for the utility of unigram-based and hashtag-related features in SVM classifiers for the stance detection problem in Turkish tweets", so we know that hashtag features are related to unigram-based features in SVM classifiers for the stance detection problem in Turkish tweets.
Answer: unigram-based features in SVM classifiers for the stance detection problem in Turkish tweets
True answer: hashtag features contain whether there is any hashtag in the tweet

Validation ID: c87fcc98625e82fdb494ff0f5309319620d69040
Question: What are hashtag features?
From the context "To sum up, our evaluation results are significant as reference results to be used for comparison purposes and provides evidence for the utility of unigram-based and hashtag-related features in SVM classifiers for the stance detection problem in Turkish tweets", so we know that hashtag features are related to unigram-based features in SVM classifiers for the stance detection problem in Turkish tweets.
Answer: unigram-based features in SVM classifiers for the stance detection problem in Turkish tweets
True answer: Unanswerable

Validation ID: 500a8ec1c56502529d6e59ba6424331f797f31f0
Question: How many tweets did they collect?
From the context "In a previous study on the identification of public health-related tweets, two tweet data sets in Turkish (each set containing 1 million random tweets) have been compiled where these sets belong to two different periods of 20 consecutive days BIBREF11", so we know that they collected 2 million tweets.
Answer: 2 million tweets
True answer: 700 

Validation ID: 500a8ec1c56502529d6e59ba6424331f797f31f0
Question: How many tweets did they collect?
From the context "In a previous study on the identification of public health-related tweets, two tweet data sets in Turkish (each set containing 1 million random tweets) have been compiled where these sets belong to two different periods of 20 consecutive days BIBREF11", so we know that they collected 2 million tweets.
Answer: 2 million tweets
True answer: 700

Validation ID: ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc
Question: Which sports clubs are the targets?
From the context "We have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey", so we know that the targets are Galatasaray and Fenerbahçe.
Answer: Galatasaray and Fenerbahçe
True answer: Galatasaray, Fenerbahçe

Validation ID: ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc
Question: Which sports clubs are the targets?
From the context "We have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey.", so we know that the targets are Galatasaray and Fenerbahçe.
Answer: Galatasaray and Fenerbahçe
True answer: Galatasaray , Fenerbahçe 

Validation ID: f2155dc4aeab86bf31a838c8ff388c85440fce6e
Question: Does this method help in sentiment classification task improvement?
From the context "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. Interestingly, the frozen strategy, which takes the least amount of time to converge (24 hours on 1 Tesla V100), also performs almost as well as fine-tuning.", so we know that the frozen strategy performs almost as well as fine-tuning.
Answer: frozen strategy performs almost as well as fine-tuning
True answer: Yes

Validation ID: f2155dc4aeab86bf31a838c8ff388c85440fce6e
Question: Does this method help in sentiment classification task improvement?
From the context "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. Interestingly, the frozen strategy, which takes the least amount of time to converge (24 hours on 1 Tesla V100), also performs almost as well as fine-tuning.", so we know that the frozen strategy performs almost as well as fine-tuning.
Answer: frozen strategy performs almost as well as fine-tuning
True answer: No

Validation ID: ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc
Question: For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding?
From the context "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks.", so we know that mSynC does slightly worse on 7 out of 9 tasks.
Answer: 7
True answer: performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks

Validation ID: ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc
Question: For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding?
From the context "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks.", so we know that mSynC does slightly worse on 7 out of 9 tasks.
Answer: 7
True answer: 3

Validation ID: 4d706ce5bde82caf40241f5b78338ea5ee5eb01e
Question: What are the black-box probes used?
From the context "We further analyze whether awareness of shallow syntax carries over to other linguistic tasks, via probes from BIBREF1. Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.", so we know that the black-box probes used are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.
Answer: linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases
True answer: CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,
Chunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection

Validation ID: 4d706ce5bde82caf40241f5b78338ea5ee5eb01e
Question: What are the black-box probes used?
From the context "We further analyze whether awareness of shallow syntax carries over to other linguistic tasks, via probes from BIBREF1. Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.", so we know that the black-box probes used are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.
Answer: linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases
True answer: Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.

Validation ID: 86bf75245358f17e35fc133e46a92439ac86d472
Question: What are improvements for these two approaches relative to ELMo-only baselines?
From the context "Results are shown in Table TABREF13. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. As we would expect, on the probe for predicting chunk tags, mSynC achieves 96.9 $F_1$ vs. 92.2 $F_1$ for ELMo-transformer, indicating that mSynC is indeed encoding shallow syntax. Overall, the results further confirm that explicit shallow syntax does not offer any benefits over ELMo-transformer.", so we know that mSynC does slightly worse on 7 out of 9 tasks.
Answer: mSynC does slightly worse on 7 out of 9 tasks
True answer: only modest gains on three of the four downstream tasks

Validation ID: 86bf75245358f17e35fc133e46a92439ac86d472
Question: What are improvements for these two approaches relative to ELMo-only baselines?
From the context "Results are shown in Table TABREF13. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. As we would expect, on the probe for predicting chunk tags, mSynC achieves 96.9 $F_1$ vs. 92.2 $F_1$ for ELMo-transformer, indicating that mSynC is indeed encoding shallow syntax. Overall, the results further confirm that explicit shallow syntax does not offer any benefits over ELMo-transformer.", so we know that mSynC does slightly worse on 7 out of 9 tasks.
Answer: mSynC does slightly worse on 7 out of 9 tasks
True answer:  the performance differences across all tasks are small enough 

Validation ID: 9132d56e26844dc13b3355448d0f14b95bd2178a
Question: Which syntactic features are obtained automatically on downstream task data?
From the context "Our second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels.", so we know that chunk boundary information is obtained automatically on downstream task data.
Answer: chunk boundary information
True answer: token-level chunk label embeddings,  chunk boundary information is passed into the task model via BIOUL encoding of the labels

Validation ID: f3c204723da53c7c8ef4dc1018ffbee545e81056
Question: Do they report results only on English data?
From the context "To evaluate the performance of the proposed approach, we use the evaluation metrics such as precision, recall and F-measure. Precision is defined as the proportion of the correctly identified events out of the model generated events. Recall is defined as", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: f3c204723da53c7c8ef4dc1018ffbee545e81056
Question: Do they report results only on English data?
From the context "To evaluate the performance of the proposed approach, we use the evaluation metrics such as precision, recall and F-measure. Precision is defined as the proportion of the correctly identified events out of the model generated events. Recall is defined as", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 0602a974a879e6eae223cdf048410b5a0111665e
Question: What baseline approaches does this approach out-perform?
From the context "We choose the following three models as the baselines: K-means is a well known data clustering algorithm, we implement the algorithm using sklearn toolbox, and represent documents using bag-of-words weighted by TF-IDF. LEM BIBREF13 is a Bayesian modeling approach for open-domain event extraction. It treats an event as a latent variable and models the generation of an event as a joint distribution of its individual event elements. We implement the algorithm with the default configuration. DPEMM BIBREF14 is a non-parametric mixture model for event extraction. It addresses the limitation of LEM that the number of events should be known beforehand. We implement the model with the default configuration.", so we know that the baseline approaches are K-means, LEM and DPEMM.
Answer: K-means, LEM and DPEMM
True answer: K-means, LEM BIBREF13, DPEMM BIBREF14

Validation ID: 0602a974a879e6eae223cdf048410b5a0111665e
Question: What baseline approaches does this approach out-perform?
From the context "We choose the following three models as the baselines: K-means is a well known data clustering algorithm, we implement the algorithm using sklearn toolbox, and represent documents using bag-of-words weighted by TF-IDF. LEM BIBREF13 is a Bayesian modeling approach for open-domain event extraction. It treats an event as a latent variable and models the generation of an event as a joint distribution of its individual event elements. We implement the algorithm with the default configuration. DPEMM BIBREF14 is a non-parametric mixture model for event extraction. It addresses the limitation of LEM that the number of events should be known beforehand. We implement the model with the default configuration.", so we know that the baseline approaches are K-means, LEM and DPEMM.
Answer: K-means, LEM and DPEMM
True answer: K-means, LEM, DPEMM

Validation ID: 56b034c303983b2e276ed6518d6b080f7b8abe6a
Question: What datasets are used?
From the context "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12, Twitter, and Google datasets) are employed. Details are summarized below:", so we know that three datasets are used.
Answer: FSD BIBREF12, Twitter, and Google datasets
True answer: FSD BIBREF12 , Twitter, and Google datasets

Validation ID: 56b034c303983b2e276ed6518d6b080f7b8abe6a
Question: What datasets are used?
From the context "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12, Twitter, and Google datasets) are employed. Details are summarized below:", so we know that three datasets are used.
Answer: FSD BIBREF12, Twitter, and Google datasets
True answer: FSD dataset, Twitter dataset, Google dataset

Validation ID: 15e481e668114e4afe0c78eefb716ffe1646b494
Question: What alternative to Gibbs sampling is used?
From the context "To address the limitation that LEM requires the number of events to be pre-set, Zhou et al. zhou2017event further proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1. However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", so we know that the Gibbs sampler is used in LEM and DPEMM.
Answer: Gibbs sampler
True answer: generator network to capture the event-related patterns

Validation ID: 3d7a982c718ea6bc7e770d8c5da564fbb9d11951
Question: How does this model overcome the assumption that all words in a document are generated from a single event?
From the context "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", so we know that the generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.
Answer: The generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to
True answer: flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions, supervision signal provided by the discriminator will help generator to capture the event-related patterns

Validation ID: 3d7a982c718ea6bc7e770d8c5da564fbb9d11951
Question: How does this model overcome the assumption that all words in a document are generated from a single event?
From the context "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", so we know that the generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).
Answer: The generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).
True answer: by learning a projection function between the document-event distribution and four event related word distributions 

Validation ID: 692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1
Question: How many users do they look at?
From the context "We compile our industry-annotated dataset by identifying blogger profiles located in the U.S. on the profile finder on http://www.blogger.com, and scraping only those users that had the industry profile element completed.", so we know that they look at all the users that had the industry profile element completed.
Answer: all the users that had the industry profile element completed
True answer: 22,880 users

Validation ID: 692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1
Question: How many users do they look at?
From the context "We compile our industry-annotated dataset by identifying blogger profiles located in the U.S. on the profile finder on http://www.blogger.com, and scraping only those users that had the industry profile element completed.", so we know that they look at all the users that had the industry profile element completed.
Answer: all the users that had the industry profile element completed
True answer: 20,000

Validation ID: 935d6a6187e6a0c9c0da8e53a42697f853f5c248
Question: What do they mean by a person's industry?
From the context "This paper explores the potential of predicting a user's industry –the aggregate of enterprises in a particular field– by identifying industry indicative text in social media", so we know that a person's industry is the aggregate of enterprises in a particular field.
Answer: the aggregate of enterprises in a particular field
True answer: the aggregate of enterprises in a particular field

Validation ID: 935d6a6187e6a0c9c0da8e53a42697f853f5c248
Question: What do they mean by a person's industry?
From the context "This paper explores the potential of predicting a user's industry –the aggregate of enterprises in a particular field– by identifying industry indicative text in social media", so we know that a person's industry is the aggregate of enterprises in a particular field.
Answer: the aggregate of enterprises in a particular field
True answer: the aggregate of enterprises in a particular field

Validation ID: 3b77b4defc8a139992bd0b07b5cf718382cb1a5f
Question: What model did they use for their system?
From the context "We build content-based classifiers for the industry prediction task and study the effect of incorporating textual features from the users' profile metadata using various meta-classification techniques, significantly improving both the overall accuracy and the average per industry accuracy.", so we know that they use content-based classifiers for the industry prediction task.
Answer: content-based classifiers
True answer: AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier

Validation ID: 01a41c0a4a7365cd37d28690735114f2ff5229f2
Question: What social media platform did they look at?
From the context "Over the past two decades, the emergence of social media has enabled the proliferation of traceable human behavior. The content posted by users can reflect who their friends are, what topics they are interested in, or which company they are working for. At the same time, users are listing a number of profile fields to define themselves to others. The utilization of such metadata has proven important in facilitating further developments of applications in advertising BIBREF0, personalization BIBREF1, and recommender systems BIBREF2. However, profile information can be limited, depending on the platform, or it is often deliberately omitted BIBREF3. To uncloak this information, a number of studies have utilized social media users' footprints to approximate their profiles.", so we know that they looked at social media platform.
Answer: social media platform
True answer:  http://www.blogger.com

Validation ID: 01a41c0a4a7365cd37d28690735114f2ff5229f2
Question: What social media platform did they look at?
From the context "Over the past two decades, the emergence of social media has enabled the proliferation of traceable human behavior. The content posted by users can reflect who their friends are, what topics they are interested in, or which company they are working for. At the same time, users are listing a number of profile fields to define themselves to others. The utilization of such metadata has proven important in facilitating further developments of applications in advertising BIBREF0, personalization BIBREF1, and recommender systems BIBREF2. However, profile information can be limited, depending on the platform, or it is often deliberately omitted BIBREF3. To uncloak this information, a number of studies have utilized social media users' footprints to approximate their profiles.", so we know that they looked at social media platform.
Answer: social media platform
True answer: http://www.blogger.com

Validation ID: cd2878c5a52542ddf080b20bec005d9a74f2d916
Question: What are the industry classes defined in this paper?
From the context "The final set of categories is shown in Table TABREF1, along with the number of users in each category. The resulting dataset consists of 22,880 users, 41,094 blogs, and 561,003 posts. Table TABREF2 presents additional statistics of our dataset.", so we know that the industry classes defined in this paper are shown in Table TABREF1.
Answer: Table TABREF1
True answer: technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive

Validation ID: cd2878c5a52542ddf080b20bec005d9a74f2d916
Question: What are the industry classes defined in this paper?
From the context "The final set of categories is shown in Table TABREF1, along with the number of users in each category. The resulting dataset consists of 22,880 users, 41,094 blogs, and 561,003 posts. Table TABREF2 presents additional statistics of our dataset.", so we know that the industry classes defined in this paper are shown in Table TABREF1.
Answer: Table TABREF1
True answer: Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.

Validation ID: fd2c6c26fd0ab3c10aae4f2550c5391576a77491
Question: Do they report results only on English data?
From the context "We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation. In future we can focus on improving these representations for example by using attention networks BIBREF37, BIBREF38 to capture a more contextual representation or using language model based methods like BERT BIBREF39 that has been shown very successful in various NLP tasks.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: fd2c6c26fd0ab3c10aae4f2550c5391576a77491
Question: Do they report results only on English data?
From the context "We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation. In future we can focus on improving these representations for example by using attention networks BIBREF37, BIBREF38 to capture a more contextual representation or using language model based methods like BERT BIBREF39 that has been shown very successful in various NLP tasks.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 6b6d498546f856ac20958f666fc3fd55811347e2
Question: What are the hyperparameters of the bi-GRU?
From the context "We used a recurrent network in the process of generating our hidden representation. We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation.", so we know that the hyperparameters of the bi-GRU are max-pooling layer and average pooling layer.
Answer: max-pooling layer and average pooling layer
True answer: They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%.

Validation ID: de3b1145cb4111ea2d4e113f816b537d052d9814
Question: What baseline is used?
From the context "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.", so we know that the baseline is CrowdFlower dataset annotated for emotions.
Answer: CrowdFlower dataset annotated for emotions
True answer:  Wang et al. BIBREF21, paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets

Validation ID: de3b1145cb4111ea2d4e113f816b537d052d9814
Question: What baseline is used?
From the context "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.", so we know that the baseline is CrowdFlower dataset annotated for emotions.
Answer: CrowdFlower dataset annotated for emotions
True answer: Wang et al. , maximum entropy classifier with bag of words model

Validation ID: 132f752169adf6dc5ade3e4ca773c11044985da4
Question: What data is used in experiments?
From the context "There are not many free datasets available for emotion classification. Most datasets are subject-specific (i.e. news headlines, fairy tails, etc.) and not big enough to train deep neural networks. Here we use the tweet dataset created by Wang et al. As mentioned in the previous section, they have collected over 2 million tweets by using hashtags for labeling their data.", so we know that the data used in experiments is the tweet dataset created by Wang et al.
Answer: the tweet dataset created by Wang et al.
True answer: Wang et al., CrowdFlower dataset 

Validation ID: 132f752169adf6dc5ade3e4ca773c11044985da4
Question: What data is used in experiments?
From the context "There are not many free datasets available for emotion classification. Most datasets are subject-specific (i.e. news headlines, fairy tails, etc.) and not big enough to train deep neural networks. Here we use the tweet dataset created by Wang et al. As mentioned in the previous section, they have collected over 2 million tweets by using hashtags for labeling their data.", so we know that the data used in experiments is the tweet dataset created by Wang et al.
Answer: the tweet dataset created by Wang et al
True answer:  tweet dataset created by Wang et al. , CrowdFlower dataset

Validation ID: 1d9aeeaa6efa1367c22be0718f5a5635a73844bd
Question: What meaningful information does the GRU model capture, which traditional ML models do not?
From the context "We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation", so we know that the GRU model captures the most relevant features and the text as a whole.
Answer: the most relevant features and the text as a whole
True answer:  the context and sequential nature of the text

Validation ID: 1d9aeeaa6efa1367c22be0718f5a5635a73844bd
Question: What meaningful information does the GRU model capture, which traditional ML models do not?
From the context "We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation", so we know that the GRU model captures the most relevant features and the text as a whole.
Answer: the most relevant features and the text as a whole
True answer: information about the context and sequential nature of the text

Validation ID: 012b8a89aea27485797373adbcda32f16f9d7b54
Question: What is the approach of previous work?
From the context "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task. In these models text features can include character and word n-grams as well as informative character and word-level features learnt BIBREF25 from the training data. The neural methods seem to work well in tasks where more training data is available.", so we know that the approach of previous work is bidirectional recurrent neural networks or ensembles of recurrent neural networks.
Answer: bidirectional recurrent neural networks or ensembles of recurrent neural networks
True answer: 'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks

Validation ID: 012b8a89aea27485797373adbcda32f16f9d7b54
Question: What is the approach of previous work?
From the context "Researchers have investigated deeper LID models like bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24. The latter is reported to achieve 95.12% in the DSL 2015 shared task. In these models text features can include character and word n-grams as well as informative character and word-level features learnt BIBREF25 from the training data. The neural methods seem to work well in tasks where more training data is available.", so we know that the approach of previous work is bidirectional recurrent neural networks or ensembles of recurrent neural networks.
Answer: bidirectional recurrent neural networks or ensembles of recurrent neural networks
True answer: BIBREF11 that uses a character level n-gram language model, 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15, BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features, The winning approach for DSL 2015 used an ensemble naive Bayes classifier, The fasttext classifier BIBREF17, hierarchical stacked classifiers (including lexicons), bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24

Validation ID: c598028815066089cc1e131b96d6966d2610467a
Question: Is the lexicon the same for all languages?
From the context "The lexicon based classifier is then used to predict the specific language within a language group. For the South African languages this is done for the Nguni and Sotho groups. If the lexicon prediction of the specific language has high confidence then its result is used as the final label else the naive Bayesian classifier's specific language prediction is used as the final result. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", so we know that the lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
Answer: Yes
True answer: Yes

Validation ID: c598028815066089cc1e131b96d6966d2610467a
Question: Is the lexicon the same for all languages?
From the context "The lexicon based classifier is then used to predict the specific language within a language group. For the South African languages this is done for the Nguni and Sotho groups. If the lexicon prediction of the specific language has high confidence then its result is used as the final label else the naive Bayesian classifier's specific language prediction is used as the final result. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", so we know that the lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
Answer: Yes
True answer: Yes

Validation ID: ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed
Question: How do they obtain the lexicon?
From the context "The lexicon based classifier is designed to trade higher precision for lower recall. The proposed implementation is considered confident if the number of words from the winning language is at least one more than the number of words considered to be from the language scored in second place. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", so we know that the lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
Answer: The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
True answer: Unanswerable

Validation ID: ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed
Question: How do they obtain the lexicon?
From the context "The lexicon based classifier is designed to trade higher precision for lower recall. The proposed implementation is considered confident if the number of words from the winning language is at least one more than the number of words considered to be from the language scored in second place. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", so we know that the lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
Answer: The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
True answer: built over all the data and therefore includes the vocabulary from both the training and testing sets

Validation ID: 0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50
Question: What evaluation metric is used?
From the context "The average classification accuracy results are summarised in Table TABREF9", so we know that the evaluation metric is classification accuracy.
Answer: classification accuracy
True answer: average classification accuracy

Validation ID: 0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50
Question: What evaluation metric is used?
From the context "The average classification accuracy results are summarised in Table TABREF9", so we know that the evaluation metric is classification accuracy.
Answer: classification accuracy
True answer: average classification accuracy, execution performance

Validation ID: 92dfacbbfa732ecea006e251be415a6f89fb4ec6
Question: Which languages are similar to each other?
From the context "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.", so we know that the Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.
Answer: The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.
True answer: Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)

Validation ID: 92dfacbbfa732ecea006e251be415a6f89fb4ec6
Question: Which languages are similar to each other?
From the context "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.", so we know that the Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.
Answer: The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.
True answer: The Nguni languages are similar to each other, The same is true of the Sotho languages

Validation ID: c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9
Question: Which datasets are employed for South African languages LID?
From the context "The NCHLT text corpora BIBREF7 is likely a good starting point for a shared LID task dataset for the South African languages BIBREF8. The NCHLT text corpora contains enough data to have 3500 training samples and 600 testing samples of 300+ character sentences per language.", so we know that the NCHLT text corpora is employed for South African languages LID.
Answer: NCHLT text corpora
True answer: DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora

Validation ID: 307e8ab37b67202fe22aedd9a98d9d06aaa169c5
Question: Does the paper report the performance of a baseline model on South African languages LID?
From the context "The stacked classifier is tested against three public LID implementations BIBREF17, BIBREF23, BIBREF8. The LID implementation described in BIBREF17 is available on GitHub and is trained and tested according to a post on the fasttext blog. Character (5-6)-gram features with 16 dimensional vectors worked the best. The implementation discussed in BIBREF23 is available from https://github.com/tomkocmi/LanideNN. Following the instructions for an OSX pip install of an old r0.8 release of TensorFlow, the LanideNN code could be executed in Python 3.7.4. Settings were left at their defaults and a learning rate of 0.001 was used followed by a refinement with learning rate of 0.0001. Only one code modification was applied to return the results from a method that previously just printed to screen. The LID algorithm described in BIBREF8 is also available on GitHub.", so we know that the paper reports the performance of a baseline model on South African languages LID.
Answer: Yes
True answer: Yes

Validation ID: 307e8ab37b67202fe22aedd9a98d9d06aaa169c5
Question: Does the paper report the performance of a baseline model on South African languages LID?
From the context "The stacked classifier is also tested against the results reported for four other algorithms BIBREF16, BIBREF26, BIBREF24, BIBREF15", so we know that the paper reports the performance of a baseline model on South African languages LID.
Answer: Yes
True answer: Yes

Validation ID: 6415f38a06c2f99e8627e8ba6251aa4b364ade2d
Question: What are the languages represented in the DSL datasets??
From the context "The DSL datasets, like other LID datasets, consists of text sentences labelled by language. The 2017 dataset, for example, contains 14 languages over 6 language groups with 18000 training samples and 1000 testing samples per language", so we know that the languages represented in the DSL datasets are 14 languages over 6 language groups.
Answer: 14 languages over 6 language groups
True answer: Unanswerable

Validation ID: e5c8e9e54e77960c8c26e8e238168a603fcdfcc6
Question: Does the algorithm improve on the state-of-the-art methods?
From the context "The accuracy of the proposed algorithm seems to be dependent on the support of the lexicon. Without a good lexicon a non-stacked naive Bayesian classifier might even perform better.", so we know that the algorithm does not improve on the state-of-the-art methods.
Answer: No
True answer: Yes

Validation ID: e5c8e9e54e77960c8c26e8e238168a603fcdfcc6
Question: Does the algorithm improve on the state-of-the-art methods?
From the context "The accuracy of the proposed algorithm seems to be dependent on the support of the lexicon. Without a good lexicon a non-stacked naive Bayesian classifier might even perform better.", so we know that the algorithm does not improve on the state-of-the-art methods.
Answer: No
True answer: From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.

Validation ID: 50be4a737dc0951b35d139f51075011095d77f2a
Question: What background knowledge do they leverage?
From the context "There have been much work that incorporate prior knowledge into learning, and two related lines are surveyed here. One is to use prior knowledge to label unlabeled instances and then apply a standard learning algorithm. The other is to constrain the model directly with prior knowledge.", so we know that they leverage prior knowledge to label unlabeled instances and then apply a standard learning algorithm.
Answer: prior knowledge to label unlabeled instances and then apply a standard learning algorithm
True answer: labeled features

Validation ID: 50be4a737dc0951b35d139f51075011095d77f2a
Question: What background knowledge do they leverage?
From the context "There have been much work that incorporate prior knowledge into learning, and two related lines are surveyed here. One is to use prior knowledge to label unlabeled instances and then apply a standard learning algorithm. The other is to constrain the model directly with prior knowledge.", so we know that they leverage prior knowledge to label unlabeled instances and then apply a standard learning algorithm.
Answer: prior knowledge to label unlabeled instances and then apply a standard learning algorithm
True answer: labelled features, which are words whose presence strongly indicates a specific class or topic

Validation ID: 6becff2967fe7c5256fe0b00231765be5b9db9f1
Question: What are the three regularization terms?
From the context "We explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.", so we know that the three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.
Answer: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution
True answer: a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution

Validation ID: 6becff2967fe7c5256fe0b00231765be5b9db9f1
Question: What are the three regularization terms?
From the context "We explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.", so we know that the three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.
Answer: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution
True answer: a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution

Validation ID: 76121e359dfe3f16c2a352bd35f28005f2a40da3
Question: What NLP tasks do they consider?
From the context "We posses a wealth of prior knowledge about many natural language processing tasks. For example, in text categorization, we know that words such as NBA, player, and basketball are strong indicators of the sports category BIBREF0, and words like terrible, boring, and messing indicate a negative polarity while words like perfect, exciting, and moving suggest a positive polarity in sentiment classification.", so we know that they consider text categorization and sentiment classification.
Answer: text categorization and sentiment classification
True answer: text classification for themes including sentiment, web-page, science, medical and healthcare

Validation ID: 02428a8fec9788f6dc3a86b5d5f3aa679935678d
Question: How do they define robustness of a model?
From the context "We investigate into the problem in the framework of Generalized Expectation Criteria BIBREF7. The study aims to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical.", so we know that the study aims to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical.
Answer: the factors of reducing the sensibility of the prior knowledge
True answer: ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced

Validation ID: 02428a8fec9788f6dc3a86b5d5f3aa679935678d
Question: How do they define robustness of a model?
From the context "We address the robustness problem on top of GE-FL BIBREF0, a GE method which leverages labeled features as prior knowledge. A labeled feature is a strong indicator of a specific class and is manually provided to the classifier. For example, words like amazing, exciting can be labeled features for class positive in sentiment classification.", so we know that robustness of a model is defined as the ability to leverage labeled features as prior knowledge.
Answer: the ability to leverage labeled features as prior knowledge
True answer: Low sensitivity to bias in prior knowledge

Validation ID: 7793805982354947ea9fc742411bec314a6998f6
Question: Are the annotations automatic or manually created?
From the context "We annotated the dataset at two levels: Part of Speech (POS) and syntax. We performed the annotation with freely available tools for the Portuguese language. For POS we added a simple POS, that is, only type of word, and a fine-grained POS, which is the type of word plus its morphological features. We used the LX Parser BIBREF14, for the simple POS and the Portuguese morphological module of Freeling BIBREF15, for detailed POS. Concerning syntactic annotations, we included constituency and dependency annotations. For constituency parsing, we used the LX Parser, and for dependency, the DepPattern toolkit BIBREF16.", so we know that the annotations are automatic.
Answer: automatic
True answer: Automatic

Validation ID: 7793805982354947ea9fc742411bec314a6998f6
Question: Are the annotations automatic or manually created?
From the context "We annotated the dataset at two levels: Part of Speech (POS) and syntax. We performed the annotation with freely available tools for the Portuguese language. For POS we added a simple POS, that is, only type of word, and a fine-grained POS, which is the type of word plus its morphological features. We used the LX Parser BIBREF14, for the simple POS and the Portuguese morphological module of Freeling BIBREF15, for detailed POS. Concerning syntactic annotations, we included constituency and dependency annotations. For constituency parsing, we used the LX Parser, and for dependency, the DepPattern toolkit BIBREF16.", so we know that the annotations are automatic.
Answer: automatic
True answer: We performed the annotation with freely available tools for the Portuguese language.

Validation ID: 007b13f05d234d37966d1aa7d85b5fd78564ff45
Question: Do the errors of the model reflect linguistic similarity between different L1s?
From the context "We evaluated the model using stratified 10-fold cross-validation, achieving 70% accuracy. An important limitation of this experiment is that it does not account for topic bias, an important issue in NLI BIBREF22. This is due to the fact that NLI-PT is not balanced by topic and the model could be learning topic associations instead.", so we know that the errors of the model do not reflect linguistic similarity between different L1s.
Answer: No
True answer: Unanswerable

Validation ID: 2ceced87af4c8fdebf2dc959aa700a5c95bd518f
Question: Is the dataset balanced between speakers of different L1s?
From the context "The three corpora contain written productions from learners of Portuguese with different proficiency levels and native languages (L1s). In the dataset we included all the data in COPLE2 and sections of PEAPL2 and Leiria corpus.", so we know that the dataset is not balanced between speakers of different L1s.
Answer: No
True answer: No

Validation ID: 2ceced87af4c8fdebf2dc959aa700a5c95bd518f
Question: Is the dataset balanced between speakers of different L1s?
From the context "The three corpora contain written productions from learners of Portuguese with different proficiency levels and native languages (L1s). In the dataset we included all the data in COPLE2 and sections of PEAPL2 and Leiria corpus.", so we know that the dataset is not balanced between speakers of different L1s.
Answer: No
True answer: No

Validation ID: 72ed5fed07ace5e3ffe9de6c313625705bc8f0c7
Question: How long are the essays on average?
From the context "The longest text has 787 tokens and the shortest has only 16 tokens. Most texts, however, range roughly from 150 to 250 tokens", so we know that the essays on average range roughly from 150 to 250 tokens.
Answer: 150 to 250 tokens
True answer: 204 tokens

Validation ID: 72ed5fed07ace5e3ffe9de6c313625705bc8f0c7
Question: How long are the essays on average?
From the context "The longest text has 787 tokens and the shortest has only 16 tokens. Most texts, however, range roughly from 150 to 250 tokens", so we know that the essays on average range roughly from 150 to 250 tokens.
Answer: 150 to 250 tokens
True answer: Most texts, however, range roughly from 150 to 250 tokens.

Validation ID: 2e37e681942e28b5b05639baaff4cd5129adb5fb
Question: How large are the textual descriptions of entities?
From the context "Given an entity in most of the existing knowledge bases, there is always an available corresponding text description with valuable semantic information for this entity, which can provide beneficial supplement for entity representation", so we know that the textual descriptions of entities are valuable semantic information.
Answer: valuable semantic information
True answer: Unanswerable

Validation ID: 2e37e681942e28b5b05639baaff4cd5129adb5fb
Question: How large are the textual descriptions of entities?
From the context "Given an entity in most of the existing knowledge bases, there is always an available corresponding text description with valuable semantic information for this entity, which can provide beneficial supplement for entity representation", so we know that the textual descriptions of entities are valuable semantic information.
Answer: valuable semantic information
True answer: Unanswerable

Validation ID: b49598b05358117ab1471b8ebd0b042d2f04b2a4
Question: What neural models are used to encode the text?
From the context "To encode the representation of a entity from its text description, we need to encode the variable-length sentence to a fixed-length vector. There are several kinds of neural models used in sentence modeling. These models generally consist of a projection layer that maps words, sub-word units or n-grams to vector representations (often trained beforehand with unsupervised methods), and then combine them with the different architectures of neural networks, such as neural bag-of-words (NBOW), recurrent neural network (RNN) BIBREF9, BIBREF10, BIBREF11 and convolutional neural network (CNN) BIBREF12, BIBREF13.", so we know that NBOW, RNN and CNN are used to encode the text.
Answer: NBOW, RNN and CNN
True answer: NBOW, LSTM, attentive LSTM

Validation ID: b49598b05358117ab1471b8ebd0b042d2f04b2a4
Question: What neural models are used to encode the text?
From the context "To encode the representation of a entity from its text description, we need to encode the variable-length sentence to a fixed-length vector. There are several kinds of neural models used in sentence modeling. These models generally consist of a projection layer that maps words, sub-word units or n-grams to vector representations (often trained beforehand with unsupervised methods), and then combine them with the different architectures of neural networks, such as neural bag-of-words (NBOW), recurrent neural network (RNN) BIBREF9, BIBREF10, BIBREF11 and convolutional neural network (CNN) BIBREF12, BIBREF13.", so we know that NBOW, RNN and CNN are used to encode the text.
Answer: NBOW, RNN and CNN
True answer: neural bag-of-words (NBOW) model, bidirectional long short-term memory network (LSTM), attention-based encoder

Validation ID: 932b39fd6c47c6a880621a62e6a978491d881d60
Question: What baselines are used for comparison?
From the context "For the comparison of Hits@10 of different kinds of relations, we categorized the relationships according to the cardinalities of their head and tail arguments into four classes: 1-to-1, 1-to-many, many-to-1, many-to-many. Mapping properties of relations follows the same rules in BIBREF2.", so we know that the baselines are TransE, TransH, TransR, TransD, TransA, TransG, TransM, TransE-N, TransE-A, TransE-N-A, TransE-N-A-G, TransE-N-A-G-M.
Answer: TransE, TransH, TransR, TransD, TransA, TransG, TransM, TransE-N, TransE-A, TransE-N-A, TransE-N-A-G, TransE-N-A-G-M
True answer: TransE

Validation ID: 932b39fd6c47c6a880621a62e6a978491d881d60
Question: What baselines are used for comparison?
From the context "For the comparison of Hits@10 of different kinds of relations, we categorized the relationships according to the cardinalities of their head and tail arguments into four classes: 1-to-1, 1-to-many, many-to-1, many-to-many. Mapping properties of relations follows the same rules in BIBREF2.", so we know that the baselines are TransE, TransH, TransR, TransD, TransA, TransG, TransM, TransE-N, TransE-A, TransE-N-A, TransE-N-A-G, TransE-N-A-G-M.
Answer: TransE, TransH, TransR, TransD, TransA, TransG, TransM, TransE-N, TransE-A, TransE-N-A, TransE-N-A-G, TransE-N-A-G-M
True answer: TransE

Validation ID: b36f867fcda5ad62c46d23513369337352aa01d2
Question: What datasets are used to evaluate this paper?
From the context "We use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper", so we know that the datasets used to evaluate this paper are WordNet and Freebase.
Answer: WordNet and Freebase
True answer: WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2

Validation ID: c6a0b9b5dabcefda0233320dd1548518a0ae758e
Question: Which approach out of two proposed in the paper performed better in experiments?
From the context "Graphs in Fig. FIGREF14 illustrate phone classification accuracy and speaker recognition performance for all three models under variation of latent size and window sizes. It is important to note that the target window size remains the same (10 frames) with an increase of $N$. Therefore e.g. $N=70$ describes the target window size is 10 frames, and the other two neighbour windows have 30 frames at either side (30,10,30 left to right). Better speaker recognition results are consistently obtained with the CJFA encoder for any configuration with competitive performance, compared with the VAE baseline and also CJFS settings - and CJFS settings mostly outperform the baseline. However the situation for phone classification is different. It is not surprising to see CJFS perform poorly on phone classification as the target frame in not present in the input, therefore the embedding just does not have the phone segment information.", so we know that the CJFA encoder performed better in experiments.
Answer: CJFA encoder
True answer: CJFA encoder 

Validation ID: c6a0b9b5dabcefda0233320dd1548518a0ae758e
Question: Which approach out of two proposed in the paper performed better in experiments?
From the context "Graphs in Fig. FIGREF14 illustrate phone classification accuracy and speaker recognition performance for all three models under variation of latent size and window sizes. It is important to note that the target window size remains the same (10 frames) with an increase of $N$. Therefore e.g. $N=70$ describes the target window size is 10 frames, and the other two neighbour windows have 30 frames at either side (30,10,30 left to right). Better speaker recognition results are consistently obtained with the CJFA encoder for any configuration with competitive performance, compared with the VAE baseline and also CJFS settings - and CJFS settings mostly outperform the baseline. However the situation for phone classification is different. It is not surprising to see CJFS perform poorly on phone classification as the target frame in not present in the input, therefore the embedding just does not have the phone segment information.", so we know that the CJFA encoder performed better in experiments.
Answer: CJFA encoder
True answer: CJFA encoder

Validation ID: 1e185a3b8cac1da939427b55bf1ba7e768c5dae4
Question: What classification baselines are used for comparison?
From the context "Table TABREF17 shows phone classification and speaker recognition results for the three model configurations: the VAE baseline, the CJFS encoder and the CJFA encoder", so we know that the classification baselines are VAE baseline, the CJFS encoder and the CJFA encoder.
Answer: VAE baseline, the CJFS encoder and the CJFA encoder
True answer: VAE

Validation ID: 1e185a3b8cac1da939427b55bf1ba7e768c5dae4
Question: What classification baselines are used for comparison?
From the context "Table TABREF17 shows phone classification and speaker recognition results for the three model configurations: the VAE baseline, the CJFS encoder and the CJFA encoder", so we know that the classification baselines are VAE baseline, the CJFS encoder and the CJFA encoder.
Answer: VAE baseline, the CJFS encoder and the CJFA encoder
True answer: VAE based phone classification

Validation ID: 26e2d4d0e482e6963a76760323b8e1c26b6eee91
Question: What TIMIT datasets are used for testing?
From the context "For Task a training of embeddings and the classifier is identical, namely consisting of data from blocks (A+B+C+E+F+G). The test data is the remainder, namely blocks (D+H).", so we know that the TIMIT datasets used for testing are blocks (D+H).
Answer: blocks (D+H)
True answer: Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H

Validation ID: 26e2d4d0e482e6963a76760323b8e1c26b6eee91
Question: What TIMIT datasets are used for testing?
From the context "For Task a training of embeddings and the classifier is identical, namely consisting of data from blocks (A+B+C+E+F+G). The test data is the remainder, namely blocks (D+H).", so we know that the TIMIT datasets used for testing are blocks (D+H).
Answer: blocks (D+H)
True answer:  this paper makes use of the official training and test sets, covering in total 630 speakers with 8 utterances each

Validation ID: b80a3fbeb49a8968e149955bdcf199556478eeff
Question: How does this approach compares to the state-of-the-art results on these tasks?
From the context "Results show that the CJFA encoder obtains significantly better phone classification accuracy than the VAE baseline and also than the CJFS encoder. These results are replicated for speaker recognition tasks. The CJFA encoder performs better on all tasks than the VAE baseline by a significant margin. It is noteworthy that performance on Task b is generally significantly lower than for Task a, for reasons of training overlap but also smaller training set sizes. Graphs in Fig. FIGREF14 illustrate phone classification accuracy and speaker recognition performance for all three models under variation of latent size and window sizes. It is important to note that the target window size remains the same (10 frames) with an increase of $N$. Therefore e.g. $N=70$ describes the target window size is 10 frames, and the other two neighbour windows have 30 frames at either side (30,10,30 left to right). Better speaker recognition results are consistently obtained with the CJFA encoder for any configuration with competitive performance, compared with the VAE baseline and also CJFS settings - and CJFS settings mostly outperform the baseline. A variation of the window sizes to larger windows seems to have a positive effect on the performance of the CJFA encoder, while the performance of the VAE baseline and CJFS encoder is not affected. This is likely due to the fact that the CJFA encoder is able to model the context information more effectively. For the phone classification task, the performance of the CJFA encoder is not as good as the VAE baseline and the CJFS encoder. This is likely due to the fact that the target frame is not present in the input, and therefore the embedding does not have the phone segment information. However, the speaker information is retained.
Answer: The CJFA encoder obtains significantly better phone classification accuracy than the VAE baseline and also than the CJFS encoder. These results are replicated for speaker recognition tasks. The CJFA encoder performs better on all tasks than the VAE baseline by a significant margin.
True answer: Unanswerable

Validation ID: badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9
Question: What state-of-the-art results are achieved?
From the context "We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered. We can see that our model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location, compared to baselines that do not adopt joint learning in the first block.", so we know that our model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location.
Answer: new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location
True answer: F1 score of 92.19 on homographic pun detection, 80.19 on homographic pun location, 89.76 on heterographic pun detection.

Validation ID: badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9
Question: What state-of-the-art results are achieved?
From the context "We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered. We can see that our model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location, compared to baselines that do not adopt joint learning in the first block.", so we know that our model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location.
Answer: new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location
True answer: for the homographic dataset F1 score of 92.19 and 80.19 on detection and location and for the heterographic dataset F1 score of 89.76 on detection

Validation ID: 67b66fe67a3cb2ce043070513664203e564bdcbd
Question: What baselines do they compare with?
From the context "We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered.", so we know that they compare with CRF.
Answer: CRF
True answer: They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.

Validation ID: f56d07f73b31a9c72ea737b40103d7004ef6a079
Question: What datasets are used in evaluation?
From the context "We evaluate our model on two benchmark datasets BIBREF9. The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.", so we know that the datasets are homographic dataset and heterographic dataset.
Answer: homographic dataset and heterographic dataset
True answer: The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.

Validation ID: f56d07f73b31a9c72ea737b40103d7004ef6a079
Question: What datasets are used in evaluation?
From the context "We evaluate our model on two benchmark datasets BIBREF9. The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.", so we know that the datasets are homographic dataset and heterographic dataset.
Answer: homographic dataset and heterographic dataset
True answer: A homographic and heterographic benchmark datasets by BIBREF9.

Validation ID: 38e4aaeabf06a63a067b272f8950116733a7895c
Question: What is the tagging scheme employed?
From the context "We first design a simple tagging scheme consisting of two tags { INLINEFORM0 }:", so we know that the tagging scheme employed is { INLINEFORM0 }.
Answer: { INLINEFORM0 }
True answer: A new tagging scheme that tags the words before and after the pun as well as the pun words.

Validation ID: 38e4aaeabf06a63a067b272f8950116733a7895c
Question: What is the tagging scheme employed?
From the context "We first design a simple tagging scheme consisting of two tags { INLINEFORM0 }:", so we know that the tagging scheme employed is { INLINEFORM0 }.
Answer: { INLINEFORM0 }
True answer: a new tagging scheme consisting of three tags, namely { INLINEFORM0 }

Validation ID: 1d197cbcac7b3f4015416f0152a6692e881ada6c
Question: How they extract "structured answer-relevant relation"?
From the context "We utilize an off-the-shelf toolbox of OpenIE to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to keep the extractions as informative as possible and avoid extracting too many similar relations in different granularities from one sentence. We join all arguments in the extracted n-ary relation into a sequence as our to the point context. Figure FIGREF5 shows n-ary relations extracted from OpenIE. As we can see, OpenIE extracts multiple relations for complex sentences. Here we select the most informative relation according to three criteria in the order of descending importance: (1) having the maximal number of overlapped tokens between the answer and the relation; (2) being assigned the highest confidence score by OpenIE; (3) containing maximum non-stop words. As shown in Figure FIGREF5, our criteria can select answer-relevant relations (waved in Figure FIGREF5), which is especially useful for sentences with extraneous information. In rare cases, OpenIE cannot extract any relation, we treat the sentence itself as the to the point context.", so we know that they extract structured answer-relevant relation by using OpenIE.
Answer: OpenIE
True answer: Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation.

Validation ID: 1d197cbcac7b3f4015416f0152a6692e881ada6c
Question: How they extract "structured answer-relevant relation"?
From the context "We utilize an off-the-shelf toolbox of OpenIE to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to keep the extractions as informative as possible and avoid extracting too many similar relations in different granularities from one sentence. We join all arguments in the extracted n-ary relation into a sequence as our to the point context. Figure FIGREF5 shows n-ary relations extracted from OpenIE. As we can see, OpenIE extracts multiple relations for complex sentences. Here we select the most informative relation according to three criteria in the order of descending importance: (1) having the maximal number of overlapped tokens between the answer and the relation; (2) being assigned the highest confidence score by OpenIE; (3) containing maximum non-stop words. As shown in Figure FIGREF5, our criteria can select answer-relevant relations (waved in Figure FIGREF5), which is especially useful for sentences with extraneous information. In rare cases, OpenIE cannot extract any relation, we treat the sentence itself as the to the point context.", so we know that they extract structured answer-relevant relation by using OpenIE.
Answer: OpenIE
True answer: off-the-shelf toolbox of OpenIE

Validation ID: 92294820ac0d9421f086139e816354970f066d8a
Question: How big are significant improvements?
From the context "Table TABREF30 shows automatic evaluation results for our model and baselines (copied from their papers). Our proposed model which combines structured answer-relevant relations and unstructured sentences achieves significant improvements over proximity-based answer-aware models BIBREF9, BIBREF15 on both dataset splits.", so we know that our proposed model achieves significant improvements over proximity-based answer-aware models BIBREF9, BIBREF15 on both dataset splits.
Answer: significant improvements
True answer: Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1

Validation ID: 477d9d3376af4d938bb01280fe48d9ae7c9cf7f7
Question: What metrics do they use?
From the context "We evaluate with all commonly-used metrics in question generation BIBREF13: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19.", so we know that they use BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19.
Answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19
True answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19

Validation ID: 477d9d3376af4d938bb01280fe48d9ae7c9cf7f7
Question: What metrics do they use?
From the context "We evaluate with all commonly-used metrics in question generation BIBREF13: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19.", so we know that they use BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19.
Answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19
True answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET), ROUGE-L (R-L)

Validation ID: f225a9f923e4cdd836dd8fe097848da06ec3e0cc
Question: On what datasets are experiments performed?
From the context "We conduct experiments on the SQuAD dataset BIBREF3. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles.", so we know that experiments are performed on the SQuAD dataset.
Answer: SQuAD dataset
True answer: SQuAD

Validation ID: f225a9f923e4cdd836dd8fe097848da06ec3e0cc
Question: On what datasets are experiments performed?
From the context "We conduct experiments on the SQuAD dataset BIBREF3. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles.", so we know that experiments are performed on the SQuAD dataset.
Answer: SQuAD dataset
True answer: SQuAD

Validation ID: ff338921e34c15baf1eae0074938bf79ee65fdd2
Question: What was the baseline model?
From the context "We have experimented with several systems and their variations, e.g. created by training with specific additional features (see next subsection). Here is their list and short descriptions. Unfortunately we did not pay attention to naming, and the systems evolved between test batches, so the overall picture can only be understood by looking at the details.", so we know that the baseline model is the system that did not pay attention to naming.
Answer: the system that did not pay attention to naming
True answer: by answering always YES (in batch 2 and 3) 

Validation ID: e807d347742b2799bc347c0eff19b4c270449fee
Question: What dataset did they use?
From the context "BioASQ provides the training data. This data is based on previous BioASQ competitions. Train data we have considered is aggregate of all train data sets till the 5th version of BioASQ competition.", so we know that the dataset is aggregate of all train data sets till the 5th version of BioASQ competition.
Answer: aggregate of all train data sets till the 5th version of BioASQ competition
True answer: BioASQ  dataset

Validation ID: e807d347742b2799bc347c0eff19b4c270449fee
Question: What dataset did they use?
From the context "BioASQ provides the training data. This data is based on previous BioASQ competitions. Train data we have considered is aggregate of all train data sets till the 5th version of BioASQ competition. We cleaned the data, that is, question-answering data without answers are removed and left with a total count of ‘530’ question answers.", so we know that the dataset used is the aggregate of all train data sets till the 5th version of BioASQ competition.
Answer: the aggregate of all train data sets till the 5th version of BioASQ competition
True answer: A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers.

Validation ID: 31b92c03d5b9be96abcc1d588d10651703aff716
Question: What was their highest recall score?
From the context "For List-type question answering task our system achieved highest recall score in the fourth test batch set", so we know that their highest recall score is in the fourth test batch set.
From the context "In the third test batch set, our system achieved highest ‘MRR’ score for Factoid Question Answering task", so we know that their highest recall score is in the third test batch set.
True answer: 0.7033

Validation ID: 31b92c03d5b9be96abcc1d588d10651703aff716
Question: What was their highest recall score?
From the context "For List-type question answering task our system achieved highest recall score in the fourth test batch set", so we know that their highest recall score is in the fourth test batch set.
From the context "In the third test batch set, our system achieved highest ‘MRR’ score for Factoid Question Answering task", so we know that their highest recall score is in the third test batch set.
True answer: 0.7033

Validation ID: 9ec1f88ceec84a10dc070ba70e90a792fba8ce71
Question: What was their highest MRR score?
From the context "For factoid question answering task highest ‘MRR’ achieved in the 6th edition of BioASQ competition is ‘0.4325’. Our system is a neural network model based on contextual word embeddings BIBREF1 and achieved a ‘MRR’ score ‘0.6103’ in one of the test batches for Factoid Question Answering task.", so we know that the highest MRR score is 0.6103.
Answer: 0.6103
True answer: 0.5115

Validation ID: 9ec1f88ceec84a10dc070ba70e90a792fba8ce71
Question: What was their highest MRR score?
From the context "For factoid question answering task highest ‘MRR’ achieved in the 6th edition of BioASQ competition is ‘0.4325’. Our system is a neural network model based on contextual word embeddings BIBREF1 and achieved a ‘MRR’ score ‘0.6103’ in one of the test batches for Factoid Question Answering task.", so we know that the highest MRR score is 0.6103.
Answer: 0.6103
True answer: 0.6103

Validation ID: 384bf1f55c34b36cb03f916f50bbefade6c86a75
Question: Does their model suffer exhibit performance drops when incorporating word importance?
From the context "As seen, our Attribution method consistently and significantly outperforms both Erasure and Attention approaches. By exploiting the word importance calculated by Attribution method, we can identify the under-translation errors automatically without the involvement of human interpreters. Although the accuracy is not high, it is worth noting that our under-translation method is very simple and straightforward. This is potentially useful for debugging NMT models, e.g., automatic post-editing with constraint decoding BIBREF26, BIBREF27.", so we know that their model does not suffer exhibit performance drops when incorporating word importance.
Answer: No
True answer: No

Validation ID: aef607d2ac46024be17b1ddd0ed3f13378c563a6
Question: How do they measure which words are under-translated by NMT models?
From the context "We propose to use the estimated word importance to detect the under-translated words by NMT models. Intuitively, under-translated input words should contribute little to the NMT outputs, yielding much smaller word importance.", so we know that they measure which words are under-translated by NMT models by using the estimated word importance to detect the under-translated words.
Answer: using the estimated word importance to detect the under-translated words
True answer: They measured the under-translated words with low word importance score as calculated by Attribution.
method

Validation ID: aef607d2ac46024be17b1ddd0ed3f13378c563a6
Question: How do they measure which words are under-translated by NMT models?
From the context "We propose to use the estimated word importance to detect the under-translated words by NMT models. Intuitively, under-translated input words should contribute little to the NMT outputs, yielding much smaller word importance.", so we know that they measure which words are under-translated by NMT models by using the estimated word importance to detect the under-translated words.
Answer: using the estimated word importance to detect the under-translated words
True answer: we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair

Validation ID: 93beae291b455e5d3ecea6ac73b83632a3ae7ec7
Question: How do their models decide how much improtance to give to the output words?
From the context "In this work, we resort to a gradient-based method, integrated gradients BIBREF9 (IG), which was originally proposed to attribute the model predictions to input features. It exploits the handy model gradient information by integrating first-order derivatives. IG is implementation invariant and does not require neural models to be differentiable or smooth, thereby is suitable for complex neural networks like Transformer. In this work, we use IG to estimate the word importance in an input sentence precisely.", so we know that the models decide how much improtance to give to the output words by using integrated gradients.
Answer: integrated gradients
True answer: Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. 

Validation ID: 93beae291b455e5d3ecea6ac73b83632a3ae7ec7
Question: How do their models decide how much improtance to give to the output words?
From the context "In this work, we resort to a gradient-based method, integrated gradients BIBREF9 (IG), which was originally proposed to attribute the model predictions to input features. It exploits the handy model gradient information by integrating first-order derivatives. IG is implementation invariant and does not require neural models to be differentiable or smooth, thereby is suitable for complex neural networks like Transformer. In this work, we use IG to estimate the word importance in an input sentence precisely.", so we know that the models decide how much improtance to give to the output words by using integrated gradients.
Answer: integrated gradients
True answer: They compute the gradient of the output at each time step with respect to the input words to decide the importance.

Validation ID: 6c91d44d5334a4ac80100eead4e105d34e99a284
Question: Which model architectures do they test their word importance approach on?
From the context "We choose the state-of-the-art Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0 as our test bed", so we know that they test their word importance approach on Transformer and RNN-Search model.
Answer: Transformer and RNN-Search model
True answer:  Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0

Validation ID: 6c91d44d5334a4ac80100eead4e105d34e99a284
Question: Which model architectures do they test their word importance approach on?
From the context "We choose the state-of-the-art Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0 as our test bed", so we know that they test their word importance approach on Transformer and RNN-Search model.
Answer: Transformer and RNN-Search model
True answer: Transformer, RNN-Search model

Validation ID: a69a59b6c0ab27bcee1a780d6867df21e30aec08
Question: Do they compare human-level performance to model performance for their dataset?
From the context "We observe that all of our models far outperform the models presented in original papers for these common stress tests, with our two RoBERTa models performing best. Both perform well on SNLI-Hard and achieve accuracy levels in the high 80s on the `antonym' (AT), `numerical reasoning' (NR), `length' (LN), `spelling error'(SE) sub-datasets, and show marked improvement on both `negation' (NG), and `word overlap' (WO). Training a RoBERTa model also on ANLI appears to be particularly useful for", so we know that they compare human-level performance to model performance for their dataset.
Answer: Yes
True answer: No

Validation ID: a69a59b6c0ab27bcee1a780d6867df21e30aec08
Question: Do they compare human-level performance to model performance for their dataset?
From the context "We observe that all of our models far outperform the models presented in original papers for these common stress tests, with our two RoBERTa models performing best. Both perform well on SNLI-Hard and achieve accuracy levels in the high 80s on the `antonym' (AT), `numerical reasoning' (NR), `length' (LN), `spelling error'(SE) sub-datasets, and show marked improvement on both `negation' (NG), and `word overlap' (WO). Training a RoBERTa model also on ANLI appears to be particularly useful for", so we know that they compare human-level performance to model performance for their dataset.
Answer: Yes
True answer: No

Validation ID: b3d01ac226ee979e188a4141877a6d2a5482de98
Question: What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models?
From the context "We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks. 3) We provide a detailed analysis of the collected data that sheds light on the shortcomings of current models, categorizes the data by inference type to examine weaknesses, and demonstrates good performance on NLI stress tests. The ANLI dataset is available at github.com/facebookresearch/anli/. A demo of the annotation procedure can be viewed at adversarialnli.com.", so we know that the weaknesses found by non-expert annotators of current state-of-the-art NLI models are: 1) the shortcomings of current models; 2) the data categorized by inference type to examine weaknesses; 3) the good performance on NLI stress tests.
Answer: the shortcomings of current models; the data categorized by inference type to examine weaknesses; the good performance on NLI stress tests
True answer: state-of-the-art models learn to exploit spurious statistical patterns in datasets, human annotators—be they seasoned NLP researchers or non-experts—might easily be able to construct examples that expose model brittleness

Validation ID: af5730d82535464cedfa707a03415ac2e7a21295
Question: What data sources do they use for creating their dataset?
From the context "For the first round, we used a BERT-Large model BIBREF10 trained on a concatenation of SNLI BIBREF1 and MNLI BIBREF22, and selected the best-performing model we could train as the starting point for our dataset collection procedure. For Round 1 contexts, we randomly sampled short multi-sentence passages from Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set BIBREF23. Contexts are either ground-truth contexts from that dataset, or they are Wikipedia passages retrieved using TF-IDF BIBREF24 based on a HotpotQA question.", so we know that they use SNLI BIBREF1, MNLI BIBREF22, HotpotQA training set BIBREF23 and Wikipedia for creating their dataset.
Answer: SNLI BIBREF1, MNLI BIBREF22, HotpotQA training set BIBREF23 and Wikipedia
True answer: Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), RTE5

Validation ID: af5730d82535464cedfa707a03415ac2e7a21295
Question: What data sources do they use for creating their dataset?
From the context "For the first round, we used a BERT-Large model BIBREF10 trained on a concatenation of SNLI BIBREF1 and MNLI BIBREF22, and selected the best-performing model we could train as the starting point for our dataset collection procedure. For Round 1 contexts, we randomly sampled short multi-sentence passages from Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set BIBREF23. Contexts are either ground-truth contexts from that dataset, or they are Wikipedia passages retrieved using TF-IDF BIBREF24 based on a HotpotQA question.", so we know that they use SNLI BIBREF1, MNLI BIBREF22, HotpotQA training set BIBREF23 and Wikipedia for creating their dataset.
Answer: SNLI BIBREF1, MNLI BIBREF22, HotpotQA training set BIBREF23 and Wikipedia
True answer: Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, formal spoken text (excerpted from court and presidential debate transcripts in the Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), causal or procedural text, which describes sequences of events or actions, extracted from WikiHow, annotations using the longer contexts present in the GLUE RTE training data, which came from the RTE5 dataset

Validation ID: ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2
Question: Do they use active learning to create their dataset?
From the context "We propose Human-And-Model-in-the-Loop Entailment Training (HAMLET), a training procedure to automatically mitigate problems with current dataset collection procedures (see Figure FIGREF1). In our setup, our starting point is a base model, trained on NLI data. Rather than employing automated adversarial methods, here the model's “adversary” is a human annotator. Given a context (also often called a “premise” in NLI), and a desired target label, we ask the human writer to provide a hypothesis that fools the model into misclassifying the label. One can think of the writer as a “white hat” hacker, trying to identify vulnerabilities in the system. For each human-generated example that is misclassified, we also ask the writer to provide a reason why they believe it was misclassified.", so we know that they use active learning to create their dataset.
Answer: Yes
True answer: Yes

Validation ID: ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2
Question: Do they use active learning to create their dataset?
From the context "We propose Human-And-Model-in-the-Loop Entailment Training (HAMLET), a training procedure to automatically mitigate problems with current dataset collection procedures (see Figure FIGREF1). In our setup, our starting point is a base model, trained on NLI data. Rather than employing automated adversarial methods, here the model's “adversary” is a human annotator. Given a context (also often called a “premise” in NLI), and a desired target label, we ask the human writer to provide a hypothesis that fools the model into misclassifying the label. One can think of the writer as a “white hat” hacker, trying to identify vulnerabilities in the system. For each human-generated example that is misclassified, we also ask the writer to provide a reason why they believe it was misclassified.", so we know that they use active learning to create their dataset.
Answer: Yes
True answer: No

Validation ID: b249b60a8c94d0e40d65f1ffdfcac527dab57516
Question: Do the hashtag and SemEval datasets contain only English data?
From the context "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0, created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.", so we know that the hashtag and SemEval datasets contain only English data.
Answer: Yes
True answer: Yes

Validation ID: b249b60a8c94d0e40d65f1ffdfcac527dab57516
Question: Do the hashtag and SemEval datasets contain only English data?
From the context "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0, created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.", so we know that the hashtag and SemEval datasets contain only English data.
Answer: Yes
True answer: Yes

Validation ID: 0f567251a6566f65170a1329eeeb5105932036b2
Question: What current state of the art method was used for comparison?
From the context "We compare our best model (MSE+multitask), under the name HashtagMaster, in the following experiments.", so we know that the current state of the art method was used for comparison is HashtagMaster.
Answer: HashtagMaster
True answer: current state-of-the-art approach BIBREF14 , BIBREF15

Validation ID: 0f567251a6566f65170a1329eeeb5105932036b2
Question: What current state of the art method was used for comparison?
From the context "We compare the performance of the BiLSTM+Lex BIBREF42 sentiment analysis model under three configurations: (a) tweets with hashtags removed, (b) tweets with hashtags as single tokens excluding the # symbol, and (c) tweets with hashtags as segmented by", so we know that the current state of the art method was used for comparison is BiLSTM+Lex BIBREF42.
Answer: BiLSTM+Lex BIBREF42
True answer:  BIBREF14, BIBREF15 

Validation ID: 4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f
Question: What set of approaches to hashtag segmentation are proposed?
From the context "Current approaches for hashtag segmentation can be broadly divided into three categories: (a) gazeteer and rule based BIBREF11, BIBREF12, BIBREF13, (b) word boundary detection BIBREF14, BIBREF15, and (c) ranking with language model and other features BIBREF16, BIBREF10, BIBREF0, BIBREF17, BIBREF18", so we know that the approaches to hashtag segmentation are (a) gazeteer and rule based, (b) word boundary detection, and (c) ranking with language model and other features.
Answer: (a) gazeteer and rule based, (b) word boundary detection, and (c) ranking with language model and other features
True answer: Adaptive Multi-task Learning
, Margin Ranking (MR) Loss
, Pairwise Neural Ranking Model


Validation ID: 60ce4868af45753c9e124e64e518c32376f12694
Question: How is the dataset of hashtags sourced?
From the context "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0, created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.", so we know that the dataset of hashtags is sourced from Stanford Sentiment Analysis Dataset.
Answer: Stanford Sentiment Analysis Dataset
True answer: 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset

Validation ID: 60ce4868af45753c9e124e64e518c32376f12694
Question: How is the dataset of hashtags sourced?
From the context "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0, created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.", so we know that the dataset of hashtags is sourced from Stanford Sentiment Analysis Dataset.
Answer: Stanford Sentiment Analysis Dataset
True answer: Stanford Sentiment Analysis Dataset BIBREF36

Validation ID: 1b1a30e9e68a9ae76af467e60cefb180d135e285
Question: How big is their created dataset?
From the context "We used recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital. The clinical data was acquired by the Health Management Unit at Changi General Hospital. This research study was approved by the SingHealth Centralised Institutional Review Board (Protocol 1556561515). The patients were recruited during 2014-2016 as part of their routine care delivery, and enrolled into the telemonitoring health management program with consent for use of anonymized versions of their data for research. The dataset comprises a total of 353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers) with consent to the use of anonymized data for research. The speakers are 38 to 88 years old, equally distributed across gender, and comprise a range of ethnic groups (55% Chinese, 17% Malay, 14% Indian, 3% Eurasian, and 11% unspecified). The conversations cover 11 topics (e.g., medication compliance, symptom checking, education, greeting) and 9 symptoms (e.g., chest pain, cough) and amount to 41 hours.", so we know that the dataset has 353 conversations from 40 speakers.
Answer: 353 conversations from 40 speakers
True answer: 353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers), we build templates and expression pools using linguistic analysis

Validation ID: 2c85865a65acd429508f50b5e4db9674813d67f2
Question: Which data do they use as a starting point for the dialogue dataset?
From the context "We used recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital. The clinical data was acquired by the Health Management Unit at Changi General Hospital. This research study was approved by the SingHealth Centralised Institutional Review Board (Protocol 1556561515). The patients were recruited during 2014-2016 as part of their routine care delivery, and enrolled into the telemonitoring health management program with consent for use of anonymized versions of their data for research.", so we know that they use recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital as a starting point for the dialogue dataset.
Answer: recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital
True answer: A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital

Validation ID: 2c85865a65acd429508f50b5e4db9674813d67f2
Question: Which data do they use as a starting point for the dialogue dataset?
From the context "We used recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital. The clinical data was acquired by the Health Management Unit at Changi General Hospital. This research study was approved by the SingHealth Centralised Institutional Review Board (Protocol 1556561515). The patients were recruited during 2014-2016 as part of their routine care delivery, and enrolled into the telemonitoring health management program with consent for use of anonymized versions of their data for research.", so we know that they use recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital as a starting point for the dialogue dataset.
Answer: recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital
True answer: recordings of nurse-initiated telephone conversations for congestive heart failure patients

Validation ID: 73a7acf33b26f5e9475ee975ba00d14fd06f170f
Question: What labels do they create on their dataset?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that they create templates and expression pools.
Answer: templates and expression pools
True answer: (1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer

Validation ID: 73a7acf33b26f5e9475ee975ba00d14fd06f170f
Question: What labels do they create on their dataset?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that they create templates and expression pools.
Answer: templates and expression pools
True answer: the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms

Validation ID: dd53baf26dad3d74872f2d8956c9119a27269bd5
Question: How do they select instances to their hold-out test set?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that they select instances to their hold-out test set by using linguistic analysis followed by manual verification.
Answer: linguistic analysis followed by manual verification
True answer: 1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues

Validation ID: dd53baf26dad3d74872f2d8956c9119a27269bd5
Question: How do they select instances to their hold-out test set?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that they select instances to their hold-out test set by using linguistic analysis followed by manual verification.
Answer: linguistic analysis followed by manual verification
True answer: held out from the simulated data

Validation ID: 218bc82796eb8d91611996979a4a42500131a936
Question: Which models/frameworks do they compare to?
From the context "We validate the performance of the proposed s2sL by providing the preliminary results obtained on two different tasks namely, Speech/Music discrimination and emotion classification. We considered the GTZAN Music-Speech dataset [17], consisting of 120 audio files (60 speech and 60 music), for task of classifying speech and music. Each audio file (of 2 seconds duration) is represented using a 13-dimensional mel-frequency cepstral coefficient (MFCC) vector, where each MFCC vector is the average of all the frame level (frame size of 30 msec and an overlap of 10 msec) MFCC vectors. It is to be noted that our main intention for this task is not better feature selection, but to demonstrate the effectiveness of our approach, in particular for low data scenarios.
The standard Berlin speech emotion database (EMO-DB) [18] consisting of 535 utterances corresponding to 7 different emotions is considered for the task of emotion classification. Each utterance is represented by a 19-dimensional feature vector obtained by using the feature selection algorithm from WEKA toolkit [19] on the 384-dimensional utterance level feature vector obtained using openSMILE toolkit [20]. For two class classification, we considered the two most confusing emotion pairs i.e., (Neutral,Sad) and (Anger, Happy). Data corresponding to Speech/Music classification (60 speech and 60 music samples) and Neutral/Sad classification (79 neutral and 62 sad utterances) is balanced whereas Anger/Happy classification task has data imbalance, with anger forming the majority class (127 samples) and happy forming the minority class (71 samples).
True answer: MLP

Validation ID: 218bc82796eb8d91611996979a4a42500131a936
Question: Which models/frameworks do they compare to?
From the context "We validate the performance of the proposed s2sL by providing the preliminary results obtained on two different tasks namely, Speech/Music discrimination and emotion classification. We considered the GTZAN Music-Speech dataset [17], consisting of 120 audio files (60 speech and 60 music), for task of classifying speech and music. Each audio file (of 2 seconds duration) is represented using a 13-dimensional mel-frequency cepstral coefficient (MFCC) vector, where each MFCC vector is the average of all the frame level (frame size of 30 msec and an overlap of 10 msec) MFCC vectors. It is to be noted that our main intention for this task is not better feature selection, but to demonstrate the effectiveness of our approach, in particular for low data scenarios.
The standard Berlin speech emotion database (EMO-DB) [18] consisting of 535 utterances corresponding to 7 different emotions is considered for the task of emotion classification. Each utterance is represented by a 19-dimensional feature vector obtained by using the feature selection algorithm from WEKA toolkit [19] on the 384-dimensional utterance level feature vector obtained using openSMILE toolkit [20]. For two class classification, we considered the two most confusing emotion pairs i.e., (Neutral,Sad) and (Anger, Happy). Data corresponding to Speech/Music classification (60 speech and 60 music samples) and Neutral/Sad classification (79 neutral and 62 sad utterances) is balanced whereas Anger/Happy classification task has data imbalance, with anger forming the majority class (127 samples) and happy forming the minority class (71 samples).
True answer: Eusboost, MWMOTE

Validation ID: b21bc09193699dc9cfad523f3d5542b0b2ff1b8e
Question: Which classification algorithm do they use for s2sL?
From the context "MLP, the most commonly used feed forward neural network, is considered as the base classifier to validate our proposed s2s framework", so we know that MLP is the base classifier for s2sL.
Answer: MLP
True answer: MLP

Validation ID: b21bc09193699dc9cfad523f3d5542b0b2ff1b8e
Question: Which classification algorithm do they use for s2sL?
From the context "MLP, the most commonly used feed forward neural network, is considered as the base classifier to validate our proposed s2s framework", so we know that MLP is the classification algorithm they use for s2sL.
Answer: MLP
True answer: MLP

Validation ID: 352bc6de5c5068c6c19062bad1b8f644919b1145
Question: Up to how many samples do they experiment with?
From the context "Each sample can be combined with all the samples (i.e., with ( INLINEFORM4 ) samples) in the dataset. Therefore, by representing the data in the s2s format, the number of samples in the train set increases to INLINEFORM5 from INLINEFORM6 samples", so we know that they experiment with INLINEFORM5 samples.
Answer: INLINEFORM5 samples
True answer: 535

Validation ID: 352bc6de5c5068c6c19062bad1b8f644919b1145
Question: Up to how many samples do they experiment with?
From the context "Each sample can be combined with all the samples (i.e., with ( INLINEFORM4 ) samples) in the dataset. Therefore, by representing the data in the s2s format, the number of samples in the train set increases to INLINEFORM5 from INLINEFORM6 samples", so we know that they experiment with INLINEFORM5 samples.
Answer: INLINEFORM5 samples
True answer: we considered 4 different proportions i.e., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 of the training data to train the classifier

Validation ID: d667731ea20605580c398a1224a0094d1155ebbb
Question: Do they use pretrained models?
From the context "We have INLINEFORM0 units (instead of INLINEFORM1 units) in the input layer to accept the two samples i.e., INLINEFORM2 and INLINEFORM3, simultaneously.", so we know that they use pretrained models.
Answer: Yes
True answer: No

Validation ID: 8bb0011ad1d63996d5650770f3be18abdd9f7fc6
Question: Do they report results only on English datasets?
From the context "The MRC dataset used in this paper is SQuAD 1.1, which contains over INLINEFORM0 passage-question pairs and has been randomly partitioned into three parts: a training set ( INLINEFORM1 ), a development set ( INLINEFORM2 ), and a test set ( INLINEFORM3 ).", so we know that they report results only on English datasets.
Answer: Yes
True answer: Unanswerable

Validation ID: 8bb0011ad1d63996d5650770f3be18abdd9f7fc6
Question: Do they report results only on English datasets?
From the context "The MRC dataset used in this paper is SQuAD 1.1, which contains over INLINEFORM0 passage-question pairs and has been randomly partitioned into three parts: a training set ( INLINEFORM1 ), a development set ( INLINEFORM2 ), and a test set ( INLINEFORM3 ).", so we know that they report results only on English datasets.
Answer: Yes
True answer: Yes

Validation ID: b0dbe75047310fec4d4ce787be5c32935fc4e37b
Question: How do the authors examine whether a model is robust to noise or not?
From the context "Besides, we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6, to evaluate the robustness to noise of MRC models. The passages in the adversarial sets contain misleading sentences, which are aimed at distracting MRC models. Specifically, each passage in AddSent contains several sentences that are similar to the question but not", so we know that the authors examine whether a model is robust to noise or not by using two of its adversarial sets, namely AddSent and AddOneSent.
Answer: using two of its adversarial sets, namely AddSent and AddOneSent
True answer: By evaluating their model on adversarial sets containing misleading sentences

Validation ID: b0dbe75047310fec4d4ce787be5c32935fc4e37b
Question: How do the authors examine whether a model is robust to noise or not?
From the context "Besides, we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6, to evaluate the robustness to noise of MRC models. The passages in the adversarial sets contain misleading sentences, which are aimed at distracting MRC models. Specifically, each passage in AddSent contains several sentences that are similar to the question but not", so we know that the authors examine whether a model is robust to noise or not by using two of its adversarial sets, namely AddSent and AddOneSent.
Answer: using two of its adversarial sets, namely AddSent and AddOneSent
True answer: we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise

Validation ID: d64383e39357bd4177b49c02eb48e12ba7ffd4fb
Question: What type of model is KAR?
From the context "In this section, we elaborate our MRC model: Knowledge Aided Reader (KAR). The key components of most existing MRC models are their attention mechanisms BIBREF13, which are aimed at fusing the associated representations of each given passage-question pair. These attention mechanisms generally fall into two categories: the first one, which we name as mutual attention, is aimed at fusing the question representations into the passage representations so as to obtain the question-aware passage representations; the second one, which we name as self attention, is aimed at fusing the question-aware passage representations into themselves so as to obtain the final passage representations.", so we know that KAR is an end-to-end MRC model.
Answer: end-to-end MRC model
True answer: Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer

Validation ID: 52f9cd05d8312ae3c7a43689804bac63f7cac34b
Question: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?
From the context "The reason for these phenomena, we believe, is that MRC models can only utilize the knowledge contained in each given passage-question pair, but in addition to this, human beings can also utilize general knowledge. A typical category of general knowledge is inter-word semantic connections. As shown in Table TABREF1, such general knowledge is essential to the reading comprehension ability of human beings.", so we know that the authors hypothesize that humans' robustness to noise is due to their general knowledge.
Answer: Yes
True answer: Yes

Validation ID: 52f9cd05d8312ae3c7a43689804bac63f7cac34b
Question: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?
From the context "The reason for these phenomena, we believe, is that MRC models can only utilize the knowledge contained in each given passage-question pair, but in addition to this, human beings can also utilize general knowledge. A typical category of general knowledge is inter-word semantic connections. As shown in Table TABREF1, such general knowledge is essential to the reading comprehension ability of human beings.", so we know that the authors hypothesize that humans' robustness to noise is due to their general knowledge.
Answer: Yes
True answer: Yes

Validation ID: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596
Question: What type of system does the baseline classification use?
From the context "The features used in our classification system are n-grams, bag-of-words, common words and hashtags. N-grams when trained with support vector machines with radial basis function kernel performed better than other features and yielded an accuracy of 68.5%. The best accuracy (69.3%) was given by support vector machines with radial basis function kernel.", so we know that the baseline classification system uses support vector machines with radial basis function kernel.
Answer: support vector machines with radial basis function kernel
True answer: support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19

Validation ID: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596
Question: What type of system does the baseline classification use?
From the context "The features used in our classification system are n-grams, bag-of-words, common words and hashtags. N-grams when trained with support vector machines with radial basis function kernel performed better than other features and yielded an accuracy of 68.5%. The best accuracy (69.3%) was given by support vector machines with radial basis function kernel.", so we know that the baseline classification system uses support vector machines with radial basis function kernel.
Answer: support vector machines with radial basis function kernel
True answer: Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers.

Validation ID: 955cbea7e5ead36fb89cd6229a97ccb3febcf8bc
Question: What experiments were carried out on the corpus?
From the context "We experimented with four different classifiers, namely, support vector machine BIBREF18, random forest, extra tree and naive bayes classifier BIBREF19. Chi square feature selection algorithm is applied to reduces the size of our feature vector. For training our system classifier, we used Scikit-learn BIBREF19. 10-fold cross validation on 3543 code-mixed tweets was carried out by dividing the corpus into 10 equal parts with nine parts as training corpus and rest one for testing. Mean accuracy is calculated by taking the average of the accuracy obtained in each iteration of the testing process. Table 2 shows the accuracy for each feature when trained using mentioned classifiers along with the accuracy when all the features are used along with the overall accuracy.", so we know that the experiments were carried out on the corpus.
Answer: 10-fold cross validation
True answer: task of humor identification in social media texts is analyzed as a classification problem

Validation ID: 04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39
Question: How many annotators tagged each text?
From the context "The final code-mixed tweets were forwarded to a group of three annotators who were university students and fluent in both English and Hindi", so we know that the number of annotators is three.
Answer: three
True answer: three 

Validation ID: 04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39
Question: How many annotators tagged each text?
From the context "The final code-mixed tweets were forwarded to a group of three annotators who were university students and fluent in both English and Hindi", so we know that the number of annotators is three.
Answer: three
True answer: three annotators

Validation ID: 15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8
Question: Where did the texts in the corpus come from?
From the context "Data Collection Python package twitterscraper is used to scrap tweets from twitter. 10,478 tweets from the past two years from domains like `sports', `politics', `entertainment' were extracted.", so we know that the texts in the corpus come from twitter.
Answer: twitter
True answer: tweets from the past two years from domains like `sports', `politics', `entertainment'

Validation ID: 15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8
Question: Where did the texts in the corpus come from?
From the context "Data Collection Python package twitterscraper is used to scrap tweets from twitter. 10,478 tweets from the past two years from domains like `sports', `politics', `entertainment' were extracted.", so we know that the texts in the corpus come from twitter.
Answer: twitter
True answer: twitter

Validation ID: ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797
Question: What is the previous state-of-the-art in summarization?
From the context "We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline", so we know that the previous state-of-the-art in summarization is BIBREF26.
Answer: BIBREF26
True answer: BIBREF26 

Validation ID: ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797
Question: What is the previous state-of-the-art in summarization?
From the context "We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline", so we know that the previous state-of-the-art in summarization is BIBREF26.
Answer: BIBREF26
True answer: BIBREF26

Validation ID: 6ca938324dc7e1742a840d0a54dc13cc207394a1
Question: What dataset do they use?
From the context "We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens.", so we know that they use German newscrawl and English newscrawl.
Answer: German newscrawl and English newscrawl
True answer: German newscrawl distributed by WMT'18 , English newscrawl data, WMT'18 English-German (en-de) news translation task , WMT'18 English-Turkish (en-tr) news task

Validation ID: 6ca938324dc7e1742a840d0a54dc13cc207394a1
Question: What dataset do they use?
From the context "We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens.", so we know that they use German newscrawl and English newscrawl.
Answer: German newscrawl and English newscrawl
True answer: German newscrawl, English newscrawl, WMT'18 English-German (en-de) news, WMT'18 English-Turkish (en-tr) news task, WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task

Validation ID: 4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802
Question: What other models do they compare to?
From the context "We compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline", so we know that they compare to BIBREF26.
Answer: BIBREF26
True answer: BIBREF11 , BIBREF26 

Validation ID: 4d47bef19afd70c10bbceafd1846516546641a2f
Question: What language model architectures are used?
From the context "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time BIBREF17. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains INLINEFORM1 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token at position INLINEFORM2. The model has access to the entire input surrounding the current target token. Models use the standard settings for the Big Transformer BIBREF16. The bi-directional model contains 353M parameters and the uni-directional model 190M parameters. Both models were trained for 1M steps using Nesterov's accelerated gradient BIBREF18 with momentum INLINEFORM3 following BIBREF19. The learning rate is linearly warmed up from INLINEFORM4 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 BIBREF20. We train on 32 Nvidia V100 SXM2 GPUs and use the NCCL2 library as well as the torch distributed package for inter-GPU communication. Training relies on 16-bit floating point operations BIBREF21 and it took six days for the bi-directional model and four days for the uni-directional model.", so we know that the language model architectures are bi-directional language model and uni-directional model.
Answer: bi-directional language model and uni-directional model
True answer: uni-directional model to augment the decoder

Validation ID: 4d47bef19afd70c10bbceafd1846516546641a2f
Question: What language model architectures are used?
From the context "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time BIBREF17. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains INLINEFORM1 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token at position INLINEFORM2. The model has access to the entire input surrounding the current target token. Models use the standard settings for the Big Transformer BIBREF16. The bi-directional model contains 353M parameters and the uni-directional model 190M parameters. Both models were trained for 1M steps using Nesterov's accelerated gradient BIBREF18 with momentum INLINEFORM3 following BIBREF19. The learning rate is linearly warmed up from INLINEFORM4 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 BIBREF20. We train on 32 Nvidia V100 SXM2 GPUs and use the NCCL2 library as well as the torch distributed package for inter-GPU communication. Training relies on 16-bit floating point operations BIBREF21 and it took six days for the bi-directional model and four days for the uni-directional model.", so we know that the language model architectures are bi-directional language model and uni-directional model.
Answer: bi-directional language model and uni-directional model
True answer: bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder

Validation ID: 506d21501d54a12d0c9fd3dbbf19067802439a04
Question: What are the user-defined keywords?
From the context "The DMK loss then calculates an additional term, which corresponds to the dot-product attention of each word in the generated output with each keyword specified by the user", so we know that the user-defined keywords are the dot-product attention of each word in the generated output.
Answer: the dot-product attention of each word in the generated output
True answer: Words that a user wants them to appear in the generated output.

Validation ID: 506d21501d54a12d0c9fd3dbbf19067802439a04
Question: What are the user-defined keywords?
From the context "The DMK loss then calculates an additional term, which corresponds to the dot-product attention of each word in the generated output with each keyword specified by the user", so we know that the user-defined keywords are the dot-product attention of each word in the generated output.
Answer: the dot-product attention of each word in the generated output
True answer: terms common to hosts' descriptions of popular Airbnb properties, like 'subway', 'manhattan', or 'parking'

Validation ID: 701571680724c05ca70c11bc267fb1160ea1460a
Question: Does the method achieve sota performance on this dataset?
From the context "We hope that this research paper establishes a first attempt at using generative machine learning models for the purposes of marketing on peer-to-peer platforms", so we know that the method does not achieve sota performance on this dataset.
Answer: No
True answer: No

Validation ID: 701571680724c05ca70c11bc267fb1160ea1460a
Question: Does the method achieve sota performance on this dataset?
From the context "We hope that this research paper establishes a first attempt at using generative machine learning models for the purposes of marketing on peer-to-peer platforms", so we know that the method does not achieve sota performance on this dataset.
Answer: No
True answer: Unanswerable

Validation ID: 600b097475b30480407ce1de81c28c54a0b3b2f8
Question: What are the baselines used in the paper?
From the context "We were similarly influenced by the work of Radford et. al, who argue for the importance of layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization. These findings lead us to question whether it is possible to employ recurrent neural networks with long short-term memory gates, as defined by Mikolov et al., to categorize product descriptions into categories based on the product's popularity [6].", so we know that the baselines used in the paper are layer normalization and data-specific trained word embeddings.
Answer: layer normalization and data-specific trained word embeddings
True answer: GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling

Validation ID: 600b097475b30480407ce1de81c28c54a0b3b2f8
Question: What are the baselines used in the paper?
From the context "We were similarly influenced by the work of Radford et. al, who argue for the importance of layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization. These findings lead us to question whether it is possible to employ recurrent neural networks with long short-term memory gates, as defined by Mikolov et al., to categorize product descriptions into categories based on the product's popularity [6].", so we know that the baselines used in the paper are layer normalization and data-specific trained word embeddings.
Answer: layer normalization and data-specific trained word embeddings
True answer: Unanswerable

Validation ID: ee7e9a948ee6888aa5830b1a3d0d148ff656d864
Question: What is the size of the Airbnb?
From the context "For the sake of simplicity, we focus our analysis on Airbnb listings from Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017", so we know that the Airbnb is from Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017.
Answer: Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017
True answer: roughly 40,000 Manhattan listings

Validation ID: 5fda8539a97828e188ba26aad5cda1b9dd642bc8
Question: How better is performance compared to previous state-of-the-art models?
From the context "Tables TABREF25 and TABREF26 also show the decoding time in different datasets. Our model finishes the segmentation with the least decoding time in all four datasets, thanks to the architecture of model which only takes attention mechanism as basic block.", so we know that our model finishes the segmentation with the least decoding time in all four datasets.
Answer: the least decoding time in all four datasets
True answer: F1 score of 97.5 on MSR and 95.7 on AS

Validation ID: 5fda8539a97828e188ba26aad5cda1b9dd642bc8
Question: How better is performance compared to previous state-of-the-art models?
From the context "Tables TABREF25 and TABREF26 also show the decoding time in different datasets. Our model finishes the segmentation with the least decoding time in all four datasets, thanks to the architecture of model which only takes attention mechanism as basic block.", so we know that our model finishes the segmentation with the least decoding time in all four datasets.
Answer: the least decoding time in all four datasets
True answer: MSR: 97.7 compared to 97.5 of baseline
AS: 95.7 compared to 95.6 of baseline

Validation ID: 709feae853ec0362d4e883db8af41620da0677fe
Question: How does Gaussian-masked directional multi-head attention works?
From the context "To combine the Gaussian weight to the self-attention, we produce the Hadamard product of Gaussian weight matrix $G$ and the score matrix produced by $Q{K^{T}}$", so we know that Gaussian-masked directional multi-head attention works by producing the Hadamard product of Gaussian weight matrix $G$ and the score matrix produced by $Q{K^{T}}$.
Answer: by producing the Hadamard product of Gaussian weight matrix $G$ and the score matrix produced by $Q{K^{T}}$
True answer: pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters

Validation ID: 709feae853ec0362d4e883db8af41620da0677fe
Question: How does Gaussian-masked directional multi-head attention works?
From the context "To combine the Gaussian weight to the self-attention, we produce the Hadamard product of Gaussian weight matrix $G$ and the score matrix produced by $Q{K^{T}}$", so we know that Gaussian-masked directional multi-head attention works by producing the Hadamard product of Gaussian weight matrix $G$ and the score matrix produced by $Q{K^{T}}$.
Answer: by producing the Hadamard product of Gaussian weight matrix $G$ and the score matrix produced by $Q{K^{T}}$
True answer: Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters

Validation ID: 186b7978ee33b563a37139adff1da7d51a60f581
Question: What is meant by closed test setting?
From the context "SIGHAN Bakeoff defines two types of evaluation settings, closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation BIBREF21. In this work, we will focus on the closed test setting by finding a better model design for further CWS performance improvement.", so we know that closed test setting limits all the data for learning should not be beyond the given training set.
Answer: limits all the data for learning should not be beyond the given training set
True answer: closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation

Validation ID: 186b7978ee33b563a37139adff1da7d51a60f581
Question: What is meant by closed test setting?
From the context "SIGHAN Bakeoff defines two types of evaluation settings, closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation BIBREF21. In this work, we will focus on the closed test setting by finding a better model design for further CWS performance improvement.", so we know that closed test setting limits all the data for learning should not be beyond the given training set.
Answer: limits all the data for learning should not be beyond the given training set
True answer: closed test limits all the data for learning should not be beyond the given training set

Validation ID: fabcd71644bb63559d34b38d78f6ef87c256d475
Question: What are strong baselines model is compared to?
From the context "Tables TABREF25 and TABREF26 also show the decoding time in different datasets. Our model finishes the segmentation with the least decoding time in all four datasets, thanks to the architecture of model which only takes attention mechanism as basic block.", so we know that our model finishes the segmentation with the least decoding time in all four datasets.
Answer: our model finishes the segmentation with the least decoding time in all four datasets
True answer: Baseline models are:
- Chen et al., 2015a
- Chen et al., 2015b
- Liu et al., 2016
- Cai and Zhao, 2016
- Cai et al., 2017
- Zhou et al., 2017
- Ma et al., 2018
- Wang et al., 2019

Validation ID: da9c0637623885afaf023a319beee87898948fe9
Question: Does the dataset feature only English language data?
From the context "Hate and Abusive Speech on Twitter BIBREF10 classifies tweets into 4 labels, “normal", “spam", “hateful" and “abusive"", so we know that the dataset feature only English language data.
Answer: Yes
True answer: Yes

Validation ID: da9c0637623885afaf023a319beee87898948fe9
Question: Does the dataset feature only English language data?
From the context "Hate and Abusive Speech on Twitter BIBREF10 classifies tweets into 4 labels, “normal", “spam", “hateful" and “abusive"", so we know that the dataset feature only English language data.
Answer: Yes
True answer: Unanswerable

Validation ID: 8a1c0ef69b6022a0642ca131a8eacb5c97016640
Question: What additional features and context are proposed?
From the context "As humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language. As shown in the examples below, (2) is labeled abusive due to the use of vulgar language. However, the intention of the user can be better understood with its context tweet (1). (1) I hate when I'm sitting in front of the bus and somebody with a wheelchair get on. INLINEFORM0 (2) I hate it when I'm trying to board a bus and there's already an as**ole on it. Similarly, context tweet (3) is important in understanding the abusive tweet (4), especially in identifying the target of the malice. (3) Survivors of #Syria Gas Attack Recount `a Cruel Scene'. INLINEFORM0 (4) Who the HELL is “LIKE" ING this post? Sick people....", so we know that the additional features and context are context tweets.
Answer: context tweets
True answer: using tweets that one has replied or quoted to as contextual information

Validation ID: 8a1c0ef69b6022a0642ca131a8eacb5c97016640
Question: What additional features and context are proposed?
From the context "As humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language. As shown in the examples below, (2) is labeled abusive due to the use of vulgar language. However, the intention of the user can be better understood with its context tweet (1). (1) I hate when I'm sitting in front of the bus and somebody with a wheelchair get on. INLINEFORM0 (2) I hate it when I'm trying to board a bus and there's already an as**ole on it. Similarly, context tweet (3) is important in understanding the abusive tweet (4), especially in identifying the target of the malice. (3) Survivors of #Syria Gas Attack Recount `a Cruel Scene'. INLINEFORM0 (4) Who the HELL is “LIKE" ING this post? Sick people....", so we know that the additional features and context are context tweets.
Answer: context tweets
True answer: text sequences of context tweets

Validation ID: 48088a842f7a433d3290eb45eb0d4c6ab1d8f13c
Question: What learning models are used on the dataset?
From the context "We implement five feature engineering based machine learning classifiers that are most often used for abusive language detection. In data preprocessing, text sequences are converted into Bag Of Words (BOW) representations, and normalized with Term Frequency-Inverse Document Frequency (TF-IDF) values. We experiment with word-level features using n-grams ranging from 1 to 3, and character-level features from 3 to 8-grams. Each classifier is implemented with the following specifications: Naïve Bayes (NB): Multinomial NB with additive smoothing constant 1 Logistic Regression (LR): Linear LR with L2 regularization constant 1 and limited-memory BFGS optimization Support Vector Machine (SVM): Linear SVM with L2 regularization constant 1 and logistic loss function Random Forests (RF): Averaging probabilistic predictions of 10 randomized decision trees Gradient Boosted Trees (GBT): Tree boosting with learning rate 1 and logistic loss function Neural Network based Models Along with traditional machine learning approaches, we investigate neural network based models to evaluate their efficacy within a larger dataset. In particular, we explore Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and their variant models. A pre-trained GloVe BIBREF17 representation is used for word-level features.", so we know that the learning models used on the dataset are Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN).
Answer: Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)
True answer: Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)

Validation ID: 48088a842f7a433d3290eb45eb0d4c6ab1d8f13c
Question: What learning models are used on the dataset?
From the context "We implement five feature engineering based machine learning classifiers that are most often used for abusive language detection. In data preprocessing, text sequences are converted into Bag Of Words (BOW) representations, and normalized with Term Frequency-Inverse Document Frequency (TF-IDF) values. We experiment with word-level features using n-grams ranging from 1 to 3, and character-level features from 3 to 8-grams. Each classifier is implemented with the following specifications: Naïve Bayes (NB): Multinomial NB with additive smoothing constant 1 Logistic Regression (LR): Linear LR with L2 regularization constant 1 and limited-memory BFGS optimization Support Vector Machine (SVM): Linear SVM with L2 regularization constant 1 and logistic loss function Random Forests (RF): Averaging probabilistic predictions of 10 randomized decision trees Gradient Boosted Trees (GBT): Tree boosting with learning rate 1 and logistic loss function Neural Network based Models Along with traditional machine learning approaches, we investigate neural network based models to evaluate their efficacy within a larger dataset. In particular, we explore Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and their variant models. A pre-trained GloVe BIBREF17 representation is used for word-level features.", so we know that the learning models used on the dataset are Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN).
Answer: Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)
True answer: Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN

Validation ID: 4907096cf16d506937e592c50ae63b642da49052
Question: What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give?
From the context "As humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language.", so we know that the authors give the example that humans can better understand a tweet with the reference of its context.
Answer: humans can better understand a tweet with the reference of its context
True answer: detecting abusive language extremely laborious, it is difficult to build a large and reliable dataset

Validation ID: 8748e8f64af57560d124c7b518b853bf2711c13e
Question: Do they report results only on English data?
From the context "We evaluate the effect of different fine-tuning strategies on the performance of our model. Table TABREF17 summarized the obtained results for fine-tuning strategies along with the official baselines. We consider weighted-average F1-score as the most robust metric versus class imbalance, which gives insight into the performance of our proposed models. According to Table TABREF17, F1-scores of all BERT based fine-tuning strategies except BERT + nonlinear classifier on top of BERT are higher than the baselines. Using the pre-trained BERT model as initial embeddings and fine-tuning the model with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to pre-trained BERT model for fine-tuning on downstream task provides the best results as F1- score of 88% and 92% for datasets of Waseem and Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that combining all pre-trained BERT layers with a CNN yields better results in which our model uses all the information included in different layers of pre-trained BERT during the fine-tuning phase.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 8748e8f64af57560d124c7b518b853bf2711c13e
Question: Do they report results only on English data?
From the context "We evaluate the effect of different fine-tuning strategies on the performance of our model. Table TABREF17 summarized the obtained results for fine-tuning strategies along with the official baselines. We consider weighted-average F1-score as the most robust metric versus class imbalance, which gives insight into the performance of our proposed models. According to Table TABREF17, F1-scores of all BERT based fine-tuning strategies except BERT + nonlinear classifier on top of BERT are higher than the baselines. Using the pre-trained BERT model as initial embeddings and fine-tuning the model with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to pre-trained BERT model for fine-tuning on downstream task provides the best results as F1- score of 88% and 92% for datasets of Waseem and Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that combining all pre-trained BERT layers with a CNN yields better results in which our model uses all the information included in different layers of pre-trained BERT during the fine-tuning phase.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 893ec40b678a72760b6802f6abf73b8f487ae639
Question: What evidence do the authors present that the model can capture some biases in data annotation and collection?
From the context "Examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. It can be a valuable clue in using pre-trained BERT model for debiasing hate speech datasets in future studies.", so we know that the model can detect some biases in data annotation and collection.
Answer: the model can detect some biases in data annotation and collection
True answer: The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate

Validation ID: c81f215d457bdb913a5bade2b4283f19c4ee826c
Question: Which publicly available datasets are used?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9", so we know that the publicly available datasets are provided by Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9.
Answer: Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9
True answer: Waseem-dataset, Davidson-dataset,

Validation ID: c81f215d457bdb913a5bade2b4283f19c4ee826c
Question: Which publicly available datasets are used?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9", so we know that the publicly available datasets are provided by Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9.
Answer: Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9
True answer: Waseem and Hovey BIBREF5, Davidson et al. BIBREF9

Validation ID: e101e38efaa4b931f7dd75757caacdc945bb32b4
Question: What baseline is used?
From the context "We consider 80% of each dataset as training data to update the weights in the fine-tuning phase, 10% as validation data to measure the out-of-sample performance of the model during training, and 10% as test data to measure the out-of-sample performance after training. To prevent overfitting, we use stratified sampling to select 0.8, 0.1, and 0.1 portions of tweets from each class (racism/sexism/neither or hate/offensive/neither) for train, validation, and test. Classes' distribution of train, validation, and test datasets are shown in Table TABREF16.", so we know that the baseline is 80% of each dataset as training data to update the weights in the fine-tuning phase, 10% as validation data to measure the out-of-sample performance of the model during training, and 10% as test data to measure the out-of-sample performance after training.
Answer: 80% of each dataset as training data to update the weights in the fine-tuning phase, 10% as validation data to measure the out-of-sample performance of the model during training, and 10% as test data to measure the out-of-sample performance after training.
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10

Validation ID: e101e38efaa4b931f7dd75757caacdc945bb32b4
Question: What baseline is used?
From the context "We consider weighted-average F1-score as the most robust metric versus class imbalance, which gives insight into the performance of our proposed models. According to Table TABREF17, F1-scores of all BERT based fine-tuning strategies except BERT + nonlinear classifier on top of BERT are higher than the baselines. Using the pre-trained BERT model as initial embeddings and fine-tuning the model with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively.", so we know that the baseline is F1-score of 81% and 91% for datasets of Waseem and Davidson respectively.
Answer: F1-score of 81% and 91% for datasets of Waseem and Davidson respectively
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10

Validation ID: afb77b11da41cd0edcaa496d3f634d18e48d7168
Question: What new fine-tuning methods are presented?
From the context "Here, four different fine-tuning approaches are implemented that exploit pre-trained BERTbase transformer encoders for our classification task. More information about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase, the model is initialized with the pre-trained parameters and then are fine-tuned using the labelled datasets. Different fine-tuning approaches on the hate speech detection task are depicted in Figure FIGREF8, in which $X_{i}$ is the vector representation of token $i$ in a tweet sample, and are explained in more detail as follows:", so we know that four different fine-tuning approaches are implemented that exploit pre-trained BERTbase transformer encoders for our classification task.
Answer: four different fine-tuning approaches
True answer: BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer

Validation ID: afb77b11da41cd0edcaa496d3f634d18e48d7168
Question: What new fine-tuning methods are presented?
From the context "Here, four different fine-tuning approaches are implemented that exploit pre-trained BERTbase transformer encoders for our classification task. More information about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase, the model is initialized with the pre-trained parameters and then are fine-tuned using the labelled datasets. Different fine-tuning approaches on the hate speech detection task are depicted in Figure FIGREF8, in which $X_{i}$ is the vector representation of token $i$ in a tweet sample, and are explained in more detail as follows:", so we know that four different fine-tuning approaches are implemented that exploit pre-trained BERTbase transformer encoders for our classification task.
Answer: four different fine-tuning approaches
True answer: BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer

Validation ID: 41b2355766a4260f41b477419d44c3fd37f3547d
Question: What are the existing biases?
From the context "We also include the details of our implementation and error analysis in the respective subsections. We evaluate the effect of different fine-tuning strategies on the performance of our model. Table TABREF17 summarized the obtained results for fine-tuning strategies along with the official baselines. We consider weighted-average F1-score as the most robust metric versus class imbalance, which gives insight into the performance of our proposed models. According to Table TABREF17, F1-scores of all BERT based fine-tuning strategies except BERT + nonlinear classifier on top of BERT are higher than the baselines. Using the pre-trained BERT model as initial embeddings and fine-tuning the model with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to pre-trained BERT model for fine-tuning on downstream task provides the best results as F1- score of 88% and 92% for datasets of Waseem and Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that combining all pre-trained BERT layers with a CNN yields better results in which our model uses all the information included in different layers of pre-trained BERT during the fine-tuning phase. This information contains", so we know that the existing biases are the pre-trained BERT layers with a CNN yields better results in which our model uses all the information included in different layers of pre-trained BERT during the fine-tuning phase. This information contains.
Answer: the pre-trained BERT layers with a CNN yields better results in which our model uses all the information included in different layers of pre-trained BERT during the fine-tuning phase. This information contains
True answer: systematic and substantial racial biases, biases from data collection, rules of annotation

Validation ID: 41b2355766a4260f41b477419d44c3fd37f3547d
!!! Has error during processing !!!
Question: What are the existing biases?
Answer: From the context "We also include the details of our implementation and error analysis in the respective subsections. We consider 80% of each dataset as training data to update the weights in the fine-tuning phase, 10% as validation data to measure the out-of-sample performance of the model during training, and 10% as test data to measure the out-of-sample performance after training. To prevent overfitting, we use stratified sampling to select 0.8, 0.1, and 0.1 portions of tweets from each class (racism/sexism/neither or hate/offensive/neither) for train, validation, and test. Classes' distribution of train, validation, and test datasets are shown in Table TABREF16. As it is understandable from Tables TABREF16(classdistributionwaseem) and TABREF16(classdistributiondavidson), we are dealing with imbalance datasets with various classes’ distribution. Since hate speech and offensive languages are real phenomena, we did not perform oversampling or undersampling techniques to adjust the classes’ distribution and tried to supply the datasets as realistic as possible. We evaluate the effect of different fine-tuning strategies on the performance of our model. Table TABREF17 summarized the obtained results for fine-tuning strategies along with the official baselines. We use Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10 as baselines and compare the results with our different fine-tuning strategies using pre-trained BERTbase model. The evaluation results are reported on the test dataset and on three different metrics: precision, recall, and weighted-average F1-score. We consider weighted-average F1-score as the most robust metric versus class imbalance, which gives insight into the performance of our proposed models. According to Table TABREF17, F1-scores of all BERT based fine-tuning strategies except BERT + nonlinear classifier on top of BERT are higher than the baselines. Using the pre-trained BERT model as initial embeddings and fine-tuning the model with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to
True answer: sampling tweets from specific keywords create systematic and substancial racial biases in datasets

Validation ID: 96a4091f681872e6d98d0efee777d9e820cb8dae
Question: What biases does their model capture?
From the context "Examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. It can be a valuable clue in using pre-trained BERT model for debiasing hate speech datasets in future studies.", so we know that their model can detect some biases in the process of collecting or annotating datasets.
Answer: some biases in the process of collecting or annotating datasets
True answer: Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters

Validation ID: 81a35b9572c9d574a30cc2164f47750716157fc8
Question: What existing approaches do they compare to?
From the context "We evaluate the effect of different fine-tuning strategies on the performance of our model. Table TABREF17 summarized the obtained results for fine-tuning strategies along with the official baselines. We consider weighted-average F1-score as the most robust metric versus class imbalance, which gives insight into the performance of our proposed models. According to Table TABREF17, F1-scores of all BERT based fine-tuning strategies except BERT + nonlinear classifier on top of BERT are higher than the baselines. Using the pre-trained BERT model as initial embeddings and fine-tuning the model with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to pre-trained BERT model for fine-tuning on downstream task provides the best results as F1- score of 88% and 92% for datasets of Waseem and Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that combining all pre-trained BERT layers with a CNN yields better results in which our model uses all the information included in different layers of pre-trained BERT during the fine-tuning phase. This", so we know that they compare to the official baselines.
Answer: the official baselines
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10

Validation ID: 81a35b9572c9d574a30cc2164f47750716157fc8
Question: What existing approaches do they compare to?
From the context "We evaluate the effect of different fine-tuning strategies on the performance of our model. Table TABREF17 summarized the obtained results for fine-tuning strategies along with the official baselines. We consider weighted-average F1-score as the most robust metric versus class imbalance, which gives insight into the performance of our proposed models. According to Table TABREF17, F1-scores of all BERT based fine-tuning strategies except BERT + nonlinear classifier on top of BERT are higher than the baselines. Using the pre-trained BERT model as initial embeddings and fine-tuning the model with a fully connected linear classifier (BERTbase) outperforms previous baselines yielding F1-score of 81% and 91% for datasets of Waseem and Davidson respectively. Inserting a CNN to pre-trained BERT model for fine-tuning on downstream task provides the best results as F1- score of 88% and 92% for datasets of Waseem and Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that combining all pre-trained BERT layers with a CNN yields better results in which our model uses all the information included in different layers of pre-trained BERT during the fine-tuning phase. This", so we know that they compare to the official baselines.
Answer: the official baselines
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10

Validation ID: f4496316ddd35ee2f0ccc6475d73a66abf87b611
Question: What is the benchmark dataset?
From the context "We use the 20 Newsgroups dataset (20NG) BIBREF17 which is commonly used for benchmarking text classification algorithms. The dataset contains 20 categories each has INLINEFORM0 1000 news posts", so we know that the benchmark dataset is 20 Newsgroups dataset.
Answer: 20 Newsgroups dataset
True answer: a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data

Validation ID: f4496316ddd35ee2f0ccc6475d73a66abf87b611
Question: What is the benchmark dataset?
From the context "We use the 20 Newsgroups dataset (20NG) BIBREF17 which is commonly used for benchmarking text classification algorithms. The dataset contains 20 categories each has INLINEFORM0 1000 news posts", so we know that the benchmark dataset is 20 Newsgroups dataset (20NG).
Answer: 20 Newsgroups dataset (20NG)
True answer: dataset created by ceccarelli2013learning from the CoNLL 2003 data

Validation ID: e8a32460fba149003566969f92ab5dd94a8754a4
Question: What are the two neural embedding models?
From the context "We propose two neural embedding models in order to learn continuous concept vectors based on the skip-gram model BIBREF11. Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only).", so we know that the two neural embedding models are Concept Raw Context model (CRC) and Concept-Concept Context model (3C).
Answer: Concept Raw Context model (CRC) and Concept-Concept Context model (3C)
True answer: Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only)

Validation ID: e8a32460fba149003566969f92ab5dd94a8754a4
Question: What are the two neural embedding models?
From the context "In this paper we propose two neural embedding models in order to learn continuous concept vectors based on the skip-gram model BIBREF11. Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only). After learning the concept vectors, we propose an efficient concept vector aggregation method to generate fully dense BOC representations. Our efficient aggregation method allows measuring the similarity between pairs of BOC vectors in linear time. This is more efficient than prior methods which require quadratic time or at least log-linear time if optimized (see equation 2).", so we know that the two neural embedding models are Concept Raw Context model (CRC) and Concept-Concept Context model (3C).
Answer: Concept Raw Context model (CRC) and Concept-Concept Context model (3C)
True answer: Concept Raw Context model, Concept-Concept Context model

Validation ID: 2a6003a74d051d0ebbe62e8883533a5f5e55078b
Question: which neural embedding model works better?
From the context "We evaluate our embedding models on two tasks: 1) Entity semantic relatedness, and 2) Dataless classification. We demonstrate through empirical results the efficiency and effectiveness of our proposed BOC densification scheme in obtaining better classification results compared to the original sparse BOC representation.", so we know that the neural embedding model works better on dataless classification.
Answer: dataless classification
True answer: the CRX model

Validation ID: 2a6003a74d051d0ebbe62e8883533a5f5e55078b
Question: which neural embedding model works better?
From the context "We evaluate our embedding models on two tasks: 1) Entity semantic relatedness, and 2) Dataless classification. We demonstrate through empirical results the efficiency and effectiveness of our proposed BOC densification scheme in obtaining better classification results compared to the original sparse BOC representation.", so we know that the neural embedding model works better on dataless classification.
Answer: dataless classification
True answer: 3C model

Validation ID: 1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b
Question: What is the degree of dimension reduction of the efficient aggregation method?
From the context "As we can notice, this weighted average is done once and for all for a given BOC vector. Other mechanisms that rely on concept alignment BIBREF7, require realignment every time a given BOC vector is compared to another BOC vector. Our approach improves the efficiency especially in the context of dataless classification with large number of classes.
Dataless Classification
True answer: The number of dimensions can be reduced by up to 212 times.

Validation ID: 9c44df7503720709eac933a15569e5761b378046
Question: For which languages do they build word embeddings for?
From the context "We train the standard LexVec (LV), LexVec using n-grams (LV-N), and LexVec using unsupervised morphemes (LV-M) using the same hyper-parameters as BIBREF7 ( $\textnormal {window} = 2$, $\textnormal {initial learning rate} =.025$, $\textnormal {subsampling} = 10^{-5}$, $\textnormal {negative samples} = 5$, $\textnormal {context distribution smoothing} =.75$, $\textnormal {positional contexts} = \textnormal {True} $ ).", so we know that they build word embeddings for English.
Answer: English
True answer: Unanswerable

Validation ID: 9c44df7503720709eac933a15569e5761b378046
Question: For which languages do they build word embeddings for?
From the context "We train the standard LexVec (LV), LexVec using n-grams (LV-N), and LexVec using unsupervised morphemes (LV-M) using the same hyper-parameters as BIBREF7 ( $\textnormal {window} = 2$, $\textnormal {initial learning rate} =.025$, $\textnormal {subsampling} = 10^{-5}$, $\textnormal {negative samples} = 5$, $\textnormal {context distribution smoothing} =.75$, $\textnormal {positional contexts} = \textnormal {True} $ ).", so we know that they build word embeddings for English.
Answer: English
True answer: English

Validation ID: b7381927764536bd97b099b6a172708125364954
Question: How do they evaluate their resulting word embeddings?
From the context "In this paper, we incorporated subword information (simple n-grams and unsupervised morphemes) into the LexVec word embedding model and evaluated its impact on the resulting IV and OOV word vectors", so we know that they evaluate their resulting word embeddings by incorporating subword information (simple n-grams and unsupervised morphemes) into the LexVec word embedding model and evaluating its impact on the resulting IV and OOV word vectors.
Answer: incorporating subword information (simple n-grams and unsupervised morphemes) into the LexVec word embedding model and evaluating its impact on the resulting IV and OOV word vectors
True answer: We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.

Validation ID: df95b3cb6aa0187655fd4856ae2b1f503d533583
Question: What types of subwords do they incorporate in their model?
From the context "We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams", so we know that they incorporate n-gram subwords and unsupervised morphemes in their model.
Answer: n-gram subwords and unsupervised morphemes
True answer: n-gram subwords, unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords 

Validation ID: df95b3cb6aa0187655fd4856ae2b1f503d533583
Question: What types of subwords do they incorporate in their model?
From the context "We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams", so we know that they incorporate n-gram subwords and unsupervised morphemes in their model.
Answer: n-gram subwords and unsupervised morphemes
True answer: simple n-grams (like fastText) and unsupervised morphemes

Validation ID: f7ed3b9ed469ed34f46acde86b8a066c52ecf430
Question: Which matrix factorization methods do they use?
From the context "The LexVec BIBREF7 model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent", so we know that the LexVec model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent.
Answer: stochastic gradient descent
True answer: weighted factorization of a word-context co-occurrence matrix 

Validation ID: f7ed3b9ed469ed34f46acde86b8a066c52ecf430
Question: Which matrix factorization methods do they use?
From the context "The LexVec BIBREF7 model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent", so we know that the LexVec model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent.
Answer: stochastic gradient descent
True answer: The LexVec BIBREF7

Validation ID: c7eb71683f53ab7acffd691a36cad6edc7f5522e
Question: Do they report results only on English data?
From the context "We train the improved projected gradient model proposed in BIBREF20 to obtain word vectors (called OIWE-IPG) using the same corpus we use to train GloVe and our proposed method. Using the methods proposed in BIBREF23, BIBREF26, BIBREF24 on our baseline GloVe embeddings, we obtain SOV, SPINE and Parsimax", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: c7eb71683f53ab7acffd691a36cad6edc7f5522e
Question: Do they report results only on English data?
From the context "We train the improved projected gradient model proposed in BIBREF20 to obtain word vectors (called OIWE-IPG) using the same corpus we use to train GloVe and our proposed method. Using the methods proposed in BIBREF23, BIBREF26, BIBREF24 on our baseline GloVe embeddings, we obtain SOV, SPINE and Parsimax", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 17a1eff7993c47c54eddc7344e7454fbe64191cd
Question: What experiments do they use to quantify the extent of interpretability?
From the context "We quantitatively demonstrate the increase in interpretability by using the measure given in BIBREF27, BIBREF40 as well as demonstrating qualitative results", so we know that they use the measure given in BIBREF27, BIBREF40 to quantify the extent of interpretability.
Answer: the measure given in BIBREF27, BIBREF40
True answer: Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT).

Validation ID: 17a1eff7993c47c54eddc7344e7454fbe64191cd
Question: What experiments do they use to quantify the extent of interpretability?
From the context "We quantitatively demonstrate the increase in interpretability by using the measure given in BIBREF27, BIBREF40 as well as demonstrating qualitative results", so we know that they use the measure given in BIBREF27, BIBREF40 to quantify the extent of interpretability.
Answer: the measure given in BIBREF27, BIBREF40
True answer: semantic category-based approach

Validation ID: a5e5cda1f6195ab1336855f1e39a609d61326d62
Question: Along which dimension do the semantically related words take larger values?
From the context "In the GloVe algorithm BIBREF2, which we base our improvements upon, the cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to. For words that do not belong to any one of the word-groups, the cost term is left untouched.", so we know that the semantically related words take larger values along the embedding vector dimension.
Answer: embedding vector dimension
True answer: dimension corresponding to the concept that the particular word belongs to

Validation ID: 32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9
Question: What is the additive modification to the objective function?
From the context "In ( EQREF6 ), INLINEFORM0 and INLINEFORM1 stand for word and context vector representations, respectively, for words INLINEFORM2 and INLINEFORM3, while INLINEFORM4 represents the (possibly weighted) cooccurrence count for the word pair INLINEFORM5. Intuitively, ( EQREF6 ) represents the requirement that if some word INLINEFORM6 occurs often enough in the context (or vicinity) of another word INLINEFORM7, then the corresponding word representations should have a large enough inner product in keeping with their large INLINEFORM8 value, up to some bias terms INLINEFORM9 ; and vice versa. INLINEFORM10 in ( EQREF6 ) is used as a discounting factor that prohibits rare cooccurrences from disproportionately influencing the resulting embeddings.", so we know that the additive modification to the objective function is INLINEFORM10.
Answer: INLINEFORM10
True answer: The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,

Validation ID: 32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9
Question: What is the additive modification to the objective function?
From the context "In ( SECREF4 ), INLINEFORM0 and INLINEFORM1 stand for word and context vector representations, respectively, for words INLINEFORM2 and INLINEFORM3, while INLINEFORM4 represents the (possibly weighted) cooccurrence count for the word pair INLINEFORM5. Intuitively, ( SECREF4 ) represents the requirement that if some word INLINEFORM6 occurs often enough in the context (or vicinity) of another word INLINEFORM7, then the corresponding word representations should have a large enough inner product in keeping with their large INLINEFORM8 value, up to some bias terms INLINEFORM9 ; and vice versa. INLINEFORM10 in ( SECREF4 ) is used as a discounting factor that prohibits rare cooccurrences from disproportionately influencing the resulting embeddings.", so we know that the additive modification to the objective function is INLINEFORM0 and INLINEFORM1 stand for word and context vector representations, respectively, for words INLINEFORM2 and INLINEFORM3, while INLINEFORM4 represents the (possibly weighted) cooccurrence count for the word pair INLINEFORM5. Intuitively, ( SECREF4 ) represents the requirement that if some word INLINEFORM6 occurs often enough in the context (or vicinity) of another word INLINEFORM7, then the corresponding word representations should have a large enough inner product in keeping with their large INLINEFORM8 value, up to some bias terms INLINEFORM9 ; and vice versa. INLINEFORM10 in ( SECREF4 ) is used as a discounting factor that prohibits rare cooccurrences from disproportionately influencing the resulting embeddings.
Answer: INLINEFORM0 and INLINEFORM1 stand for word and context vector representations, respectively, for words INLINEFORM2 and INLINEFORM3, while INLINEFORM4 represents the (possibly weighted) cooccurrence count for the word pair INLINEFORM5. Intuitively, ( SECREF4 ) represents the requirement that if some word INLINEFORM6 occurs often enough in the context (or vicinity) of another word INLINEFORM7, then the corresponding word representations should have a large enough inner product in keeping with their large INLINEFORM8 value, up to some bias terms INLINEFORM9 ; and vice versa. INLINEFORM10 in ( SECREF4 ) is used as a discounting factor that
True answer: An additive term added to the cost function for any one of the words of concept word-groups

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
From the context "We use the SemEval 2015 Twitter dataset BIBREF28 for our experiments", so we know that they use the SemEval 2015 Twitter dataset.
Answer: SemEval 2015 Twitter dataset
True answer: Unanswerable

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
From the context "We use the SemEval 2015 Twitter dataset BIBREF28 for our experiments", so we know that they use the SemEval 2015 Twitter dataset.
Answer: SemEval 2015 Twitter dataset
True answer:  Paraphrase Database (PPDB) ,  book corpus

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
From the context "We use the SemEval 2015 Twitter dataset BIBREF28 for our experiments", so we know that they use the SemEval 2015 Twitter dataset.
Answer: SemEval 2015 Twitter dataset
True answer: Unanswerable

Validation ID: 2c7494d47b2a69f182e83455fe4c75ae3b2893e9
Question: Do they evaluate their learned representations on downstream tasks?
From the context "These methods enjoy several advantages: (1) they are cheaper to train, as they work with unlabelled data, (2) they reduce the dependence on domain level experts, and (3) they are highly effective across multiple applications, in practice.", so we know that they are highly effective across multiple applications, in practice.
Answer: Yes
True answer: No

Validation ID: 2c7494d47b2a69f182e83455fe4c75ae3b2893e9
Question: Do they evaluate their learned representations on downstream tasks?
From the context "These methods enjoy several advantages: (1) they are cheaper to train, as they work with unlabelled data, (2) they reduce the dependence on domain level experts, and (3) they are highly effective across multiple applications, in practice.", so we know that they are highly effective across multiple applications, in practice.
Answer: Yes
True answer: No

Validation ID: 4d7ff4e5d06902de85b0e9a364dc455196d06a7d
Question: Which representation learning architecture do they adopt?
From the context "Models: Skip-thought vectors BIBREF15 (STV) is a widely popular sentence encoder, which is trained to predict adjacent sentences in the book corpus BIBREF16. Although the testing is cheap as it involves a cheap forward propagation of the test sentence, STV is very slow to train thanks to its complicated model architecture. To combat this computational inefficiency, FastSent BIBREF17 propose a simple additive (log-linear) sentence model, which predicts adjacent sentences (represented as BOW) taking the BOW representation of some sentence in context. This model can exploit the same signal, but at a much lower computational expense. Parallel to this work, Siamase CBOW BIBREF18 develop a model which directly compares the BOW representation of two sentence to bring the embeddings of a sentence closer to its adjacent sentence, away from a randomly occurring sentence in the corpus. For FastSent and Siamese CBOW, the test sentence representation is a simple average of word vectors obtained after training. Both of these models are general purpose sentence representation models trained on book corpus, yet give a competitive performance over previous models on the tweet semantic similarity computation task. BIBREF14's model attempt to exploit these signals directly from Twitter. With the help of attention technique and learned user representation, this log-linear model is able to capture salient semantic information from chronologically adjacent tweets of a target tweet in users' Twitter timeline.", so we know that they adopt Skip-thought vectors, FastSent, Siamase CBOW and log-linear model.
Answer: Skip-thought vectors, FastSent, Siamase CBOW and log-linear model
True answer: Unanswerable

Validation ID: ecc63972b2783ee39b3e522653cfb6dc5917d522
Question: How do they encourage understanding of literature as part of their objective function?
From the context "We believe that the performance of these models is highly dependent on the objective function it optimizes – predicting adjacent word (within-tweet relationships), adjacent tweet (inter-tweet relationships), the tweet itself (autoencoder), modeling from structured resources like paraphrase databases and weak supervision. In this section, we provide the first of its kind survey of the recent tweet-specific unsupervised models in an organized fashion to understand the literature. Specifically, we categorize each model based on the optimized objective function as shown in Figure FIGREF1. Next, we study each category one by one.", so we know that they encourage understanding of literature as part of their objective function by providing the first of its kind survey of the recent tweet-specific unsupervised models in an organized fashion to understand the literature.
Answer: by providing the first of its kind survey of the recent tweet-specific unsupervised models in an organized fashion to understand the literature
True answer: They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.

Validation ID: 8d074aabf4f51c8455618c5bf7689d3f62c4da1d
Question: What are the limitations of existing Vietnamese word segmentation systems?
From the context "The result of this method depends on copora and is based maximal matching strategy. So, they do not solve missing word issue.", so we know that the limitations of existing Vietnamese word segmentation systems are they do not solve missing word issue.
Answer: they do not solve missing word issue
True answer:  ambiguous words, unknown words

Validation ID: 8d074aabf4f51c8455618c5bf7689d3f62c4da1d
Question: What are the limitations of existing Vietnamese word segmentation systems?
From the context "The result of this method depends on copora and is based maximal matching strategy. So, they do not solve missing word issue.", so we know that the limitations of existing Vietnamese word segmentation systems are they do not solve missing word issue.
Answer: they do not solve missing word issue
True answer: lacks of complete review approaches, datasets and toolkits 

Validation ID: fe2666ace293b4bfac3182db6d0c6f03ea799277
Question: Why challenges does word segmentation in Vietnamese pose?
From the context "In computational linguistics, morpheme is the basic unit of languages as Leonard Bloomfield mentioned for English BIBREF11. In our research for Vietnamese, we consider the morpheme as syllable, called “tiếng” in Vietnamese (as Nguyen’s definition BIBREF12 ).", so we know that the morpheme is the basic unit of languages as Leonard Bloomfield mentioned for English.
Answer: the morpheme is the basic unit of languages as Leonard Bloomfield mentioned for English
True answer: Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback.

Validation ID: fe2666ace293b4bfac3182db6d0c6f03ea799277
Question: Why challenges does word segmentation in Vietnamese pose?
From the context "In computational linguistics, morpheme is the basic unit of languages as Leonard Bloomfield mentioned for English BIBREF11. In our research for Vietnamese, we consider the morpheme as syllable, called “tiếng” in Vietnamese (as Nguyen’s definition BIBREF12 ).", so we know that the morpheme is the basic unit of languages as Leonard Bloomfield mentioned for English.
Answer: the morpheme is the basic unit of languages as Leonard Bloomfield mentioned for English
True answer: to acquire very large Vietnamese corpus and to use them in building a classifier,  design and development of big data warehouse and analytic framework for Vietnamese documents, to building a system, which is able to incrementally learn new corpora and interactively process feedback

Validation ID: 70a1b0f9f26f1b82c14783f1b76dfb5400444aa4
Question: How successful are the approaches used to solve word segmentation in Vietnamese?
From the context "All studies mentioned in the table have accuracy around 94-97% based on their provided corpus.", so we know that the approaches used to solve word segmentation in Vietnamese are successful.
Answer: successful
True answer: Their accuracy in word segmentation is about 94%-97%.

Validation ID: d3ca5f1814860a88ff30761fec3d860d35e39167
Question: Which approaches have been applied to solve word segmentation in Vietnamese?
From the context "There are several studies for Vietnamese Word Segmentation during last decade. For instance, Dinh et al. started the word segmentation task for Vietnamese with Neural Network and Weighted Finite State Transducer (WFST) BIBREF9. Nguyen et al. continued with machine learning approaches, Conditional Random Fields and Support Vector Machine BIBREF7. Most of statistical approaches are based on the architecture as shown in Figure 2.", so we know that there are several studies for Vietnamese Word Segmentation during last decade. For instance, Dinh et al. started the word segmentation task for Vietnamese with Neural Network and Weighted Finite State Transducer (WFST) BIBREF9. Nguyen et al. continued with machine learning approaches, Conditional Random Fields and Support Vector Machine BIBREF7. Most of statistical approaches are based on the architecture as shown in Figure 2.
Answer: Neural Network, Weighted Finite State Transducer, Conditional Random Fields, Support Vector Machine
True answer: Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)

Validation ID: d3ca5f1814860a88ff30761fec3d860d35e39167
Question: Which approaches have been applied to solve word segmentation in Vietnamese?
From the context "There are several studies for Vietnamese Word Segmentation during last decade. For instance, Dinh et al. started the word segmentation task for Vietnamese with Neural Network and Weighted Finite State Transducer (WFST) BIBREF9. Nguyen et al. continued with machine learning approaches, Conditional Random Fields and Support Vector Machine BIBREF7. Most of statistical approaches are based on the architecture as shown in Figure 2.", so we know that there are several studies for Vietnamese Word Segmentation during last decade. For instance, Dinh et al. started the word segmentation task for Vietnamese with Neural Network and Weighted Finite State Transducer (WFST) BIBREF9. Nguyen et al. continued with machine learning approaches, Conditional Random Fields and Support Vector Machine BIBREF7. Most of statistical approaches are based on the architecture as shown in Figure 2.
Answer: Neural Network, Weighted Finite State Transducer, Conditional Random Fields, Support Vector Machine
True answer: Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines

Validation ID: dd20d93166c14f1e57644cd7fa7b5e5738025cd0
Question: Which two news domains are country-independent?
From the context "We collected tweets associated to a dozen US mainstream news websites, i.e. most trusted sources described in BIBREF18, with the Streaming API, and we referred to Hoaxy API BIBREF16 for what concerns tweets containing links to 100+ US disinformation outlets. We filtered out articles associated to less than 50 tweets. The resulting dataset contains overall $\sim $1.7 million tweets for mainstream news, collected in a period of three weeks (February 25th, 2019-March 18th, 2019), which are associated to 6,978 news articles, and $\sim $1.6 million tweets for disinformation, collected in a period of three months (January 1st, 2019-March 18th, 2019) for sake of balance of the two classes, which hold 5,775 distinct articles. Diffusion censoring effects BIBREF14 were correctly taken into account in both collection procedures. We provide in Figure FIGREF4 the distribution of articles by source and political bias for both news domains.
As it is reported that conservatives and liberals exhibit different behaviors on online social platforms BIBREF19BIBREF20BIBREF21, we further assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2.
True answer: mainstream news and disinformation

Validation ID: dd20d93166c14f1e57644cd7fa7b5e5738025cd0
Question: Which two news domains are country-independent?
From the context "We collected tweets associated to a dozen US mainstream news websites, i.e. most trusted sources described in BIBREF18, with the Streaming API, and we referred to Hoaxy API BIBREF16 for what concerns tweets containing links to 100+ US disinformation outlets. We filtered out articles associated to less than 50 tweets. The resulting dataset contains overall $\sim $1.7 million tweets for mainstream news, collected in a period of three weeks (February 25th, 2019-March 18th, 2019), which are associated to 6,978 news articles, and $\sim $1.6 million tweets for disinformation, collected in a period of three months (January 1st, 2019-March 18th, 2019) for sake of balance of the two classes, which hold 5,775 distinct articles. Diffusion censoring effects BIBREF14 were correctly taken into account in both collection procedures. We provide in Figure FIGREF4 the distribution of articles by source and political bias for both news domains.
As it is reported that conservatives and liberals exhibit different behaviors on online social platforms BIBREF19BIBREF20BIBREF21, we further assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2.
True answer: mainstream and disinformation news

Validation ID: dc2a2c177cd5df6da5d03e6e74262bf424850ec9
Question: How is the political bias of different sources included in the model?
From the context "As in the US dataset, we also added two indicators for encoding information about pure tweets, namely the number T of pure tweets (containing URLs to a given news article) and the number U of unique users authoring those tweets", so we know that the political bias of different sources is included in the model by adding two indicators for encoding information about pure tweets.
Answer: by adding two indicators for encoding information about pure tweets
True answer: By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains

Validation ID: dc2a2c177cd5df6da5d03e6e74262bf424850ec9
Question: How is the political bias of different sources included in the model?
From the context "As it is reported that conservatives and liberals exhibit different behaviors on online social platforms BIBREF19BIBREF20BIBREF21, we further assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2", so we know that the political bias of different sources is included in the model by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2.
Answer: by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2
True answer: we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries.

Validation ID: ae90c5567746fe25af2fcea0cc5f355751e05c71
Question: What are the two large-scale datasets used?
From the context "We performed classification experiments using a basic off-the-shelf classifier, namely Logistic Regression (LR) with L2 penalty; this also allows us to compare results with our baseline. We used the following evaluation metrics to assess the performances of different classifiers (TP=true positives, FP=false positives, FN=false negatives):", so we know that the two large-scale datasets are TP=true positives, FP=false positives, FN=false negatives.
Answer: TP=true positives, FP=false positives, FN=false negatives
True answer: US dataset, Italian dataset

Validation ID: ae90c5567746fe25af2fcea0cc5f355751e05c71
Question: What are the two large-scale datasets used?
From the context "We performed classification experiments using a basic off-the-shelf classifier, namely Logistic Regression (LR) with L2 penalty; this also allows us to compare results with our baseline. We used the following evaluation metrics to assess the performances of different classifiers (TP=true positives, FP=false positives, FN=false negatives):", so we know that the two large-scale datasets are TP=true positives, FP=false positives, FN=false negatives.
Answer: TP=true positives, FP=false positives, FN=false negatives
True answer: US dataset, Italian dataset

Validation ID: d7644c674887ca9708eb12107acd964ae53b216d
Question: What are the global network features which quantify different aspects of the sharing process?
From the context "We used the following evaluation metrics to assess the performances of different classifiers (TP=true positives, FP=false positives, FN=false negatives): Precision = $\frac{TP}{TP+FP}$, the ability of a classifier not to label as positive a negative sample. Recall = $\frac{TP}{TP+FN}$, the ability of a classifier to retrieve all positive samples. F1-score = $2 \frac{\mbox{Precision} \cdot \mbox{Recall}}{\mbox{Precision} + \mbox{Recall}}$, the harmonic average of Precision and Recall. Area Under the Receiver Operating Characteristic curve (AUROC); the Receiver Operating Characteristic (ROC) curve BIBREF34, which plots the TP rate versus the FP rate, shows the ability of a classifier to discriminate positive samples from negative ones as its threshold is varied; the AUROC value is in the range $[0, 1]$, with the random baseline classifier holding AUROC$=0.5$ and the ideal perfect classifier AUROC$=1$; thus larger AUROC values (and steeper ROCs) correspond to better classifiers.", so we know that the global network features which quantify different aspects of the sharing process are Precision, Recall, F1-score and AUROC.
Answer: Precision, Recall, F1-score and AUROC
True answer: Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)

Validation ID: a3bb9a936f61bafb509fa12ac0a61f91abcc5106
Question: Which datasets are used for evaluation?
From the context "We identified 5 common models in previous work primarily intended for learned classifiers rather than hand-crafted rules. We adapt these models to a multi-label hierarchical classification task by training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy. With the exception of the CNN and BERT models, following previous work BIBREF19, BIBREF3, BIBREF8 we make use of an SVM classifier using the LIBSvM framework BIBREF35 with a linear kernel. Models are trained and evaluated from coarse to fine levels of taxonomic specificity. At each level of taxonomic evaluation, a set of non-overlapping confidence scores for each binary classifier are generated and sorted to produce a list of ranked label predictions. We evaluate these ranks using Mean Average Precision BIBREF36. ARC questions are evaluated using the standard 3,370 questions for training, 869 for development, and 3,548 for testing.", so we know that the datasets used for evaluation are ARC questions, TREC-6, TREC-50, GARD, MLBioMedLAT.
Answer: ARC questions, TREC-6, TREC-50, GARD, MLBioMedLAT
True answer: ARC , TREC, GARD , MLBioMedLAT 

Validation ID: a3bb9a936f61bafb509fa12ac0a61f91abcc5106
Question: Which datasets are used for evaluation?
From the context "We identified 5 common models in previous work primarily intended for learned classifiers rather than hand-crafted rules. We adapt these models to a multi-label hierarchical classification task by training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy. With the exception of the CNN and BERT models, following previous work BIBREF19, BIBREF3, BIBREF8 we make use of an SVM classifier using the LIBSvM framework BIBREF35 with a linear kernel. Models are trained and evaluated from coarse to fine levels of taxonomic specificity. At each level of taxonomic evaluation, a set of non-overlapping confidence scores for each binary classifier are generated and sorted to produce a list of ranked label predictions. We evaluate these ranks using Mean Average Precision BIBREF36. ARC questions are evaluated using the standard 3,370 questions for training, 869 for development, and 3,548 for testing.", so we know that the datasets used for evaluation are ARC questions, TREC-6, TREC-50, GARD, MLBioMedLAT.
Answer: ARC questions, TREC-6, TREC-50, GARD, MLBioMedLAT
True answer: ARC, TREC, GARD, MLBioMedLAT

Validation ID: df6d327e176740da9edcc111a06374c54c8e809c
Question: What previous methods is their model compared to?
From the context "We identified 5 common models in previous work primarily intended for learned classifiers rather than hand-crafted rules. We adapt these models to a multi-label hierarchical classification task by training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy. With the exception of the CNN and BERT models, following previous work BIBREF19, BIBREF3, BIBREF8 we make use of an SVM classifier using the LIBSvM framework BIBREF35 with a linear kernel. Models are trained and evaluated from coarse to fine levels of taxonomic specificity. At each level of taxonomic evaluation, a set of non-overlapping confidence scores for each binary classifier are generated and sorted to produce a list of ranked label predictions. We evaluate these ranks using Mean Average Precision BIBREF36. ARC questions are evaluated using the standard 3,370 questions for training, 869 for development, and 3,548 for testing.", so we know that their model is compared to 5 common models in previous work.
Answer: 5 common models in previous work
True answer: bag-of-words model, CNN

Validation ID: 49764eee7fb523a6a28375cc699f5e0220b81766
Question: Did they use a crowdsourcing platform?
From the context "We independently annotated the entire question set, then questions without complete agreement were discussed until resolution", so we know that they used a crowdsourcing platform.
Answer: Yes
True answer: No

Validation ID: 49764eee7fb523a6a28375cc699f5e0220b81766
Question: Did they use a crowdsourcing platform?
From the context "We independently annotated the entire question set, then questions without complete agreement were discussed until resolution", so we know that they used a crowdsourcing platform.
Answer: Yes
True answer: No

Validation ID: 3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1
Question: How was the dataset collected?
From the context "We identified 5 common models in previous work primarily intended for learned classifiers rather than hand-crafted rules. We adapt these models to a multi-label hierarchical classification task by training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy. With the exception of the CNN and BERT models, following previous work BIBREF19, BIBREF3, BIBREF8 we make use of an SVM classifier using the LIBSvM framework BIBREF35 with a linear kernel. Models are trained and evaluated from coarse to fine levels of taxonomic specificity. At each level of taxonomic evaluation, a set of non-overlapping confidence scores for each binary classifier are generated and sorted to produce a list of ranked label predictions. We evaluate these ranks using Mean Average Precision BIBREF36. ARC questions are evaluated using the standard 3,370 questions for training, 869 for development, and 3,548 for testing.", so we know that the dataset was collected by training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy.
Answer: training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy
True answer: from 3rd to 9th grade science questions collected from 12 US states

Validation ID: 3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1
Question: How was the dataset collected?
From the context "We identified 5 common models in previous work primarily intended for learned classifiers rather than hand-crafted rules. We adapt these models to a multi-label hierarchical classification task by training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy. With the exception of the CNN and BERT models, following previous work BIBREF19, BIBREF3, BIBREF8 we make use of an SVM classifier using the LIBSvM framework BIBREF35 with a linear kernel. Models are trained and evaluated from coarse to fine levels of taxonomic specificity. At each level of taxonomic evaluation, a set of non-overlapping confidence scores for each binary classifier are generated and sorted to produce a list of ranked label predictions. We evaluate these ranks using Mean Average Precision BIBREF36. ARC questions are evaluated using the standard 3,370 questions for training, 869 for development, and 3,548 for testing.", so we know that the dataset was collected by training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy.
Answer: training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy
True answer: Used from  science exam questions of the Aristo Reasoning Challenge (ARC) corpus.

Validation ID: bb3267c3f0a12d8014d51105de5d81686afe5f1b
Question: Which datasets do they use?
From the context "We evaluate the trained NCEL on the above benchmarks", so we know that they use the above benchmarks.
Answer: the above benchmarks
True answer: CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW

Validation ID: bb3267c3f0a12d8014d51105de5d81686afe5f1b
Question: Which datasets do they use?
From the context "We evaluate the trained NCEL on the above benchmarks", so we know that they use the above benchmarks.
Answer: the above benchmarks
True answer: CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW

Validation ID: 114934e1a1e818630ff33ac5c4cd4be6c6f75bb2
Question: How effective is their NCEL approach overall?
From the context "In experiments, we first verify the efficiency of NCEL via theoretically comparing its time complexity with other collective alternatives. Afterwards, we train our neural model using collected Wikipedia hyperlinks instead of dataset-specific annotations, and perform evaluations on five public available benchmarks. The results show that NCEL consistently outperforms various baselines with a favorable generalization ability. Finally, we further present the performance on a challenging dataset WW BIBREF19 as well as qualitative results, investigating the effectiveness of each key module.", so we know that NCEL is effective.
Answer: effective
True answer: NCEL consistently outperforms various baselines with a favorable generalization ability

Validation ID: 2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72
Question: How do they verify generalization ability?
From the context "We then evaluate the trained NCEL on the above benchmarks. We set context window to 20, neighbor mention window to 6, and top INLINEFORM0 candidates for each mention. We use two layers with 2000 and 1 hidden units in MLP encoder, and 3 layers in sub-GCN. We use early stopping to avoid overfitting. We use Adam BIBREF38 with learning rate 0.001 and batch size of 100. We train NCEL for 20 epochs and select the best model based on the development set. We use the same hyper-parameters for all datasets.", so we know that they use the same hyper-parameters for all datasets.
Answer: the same hyper-parameters for all datasets
True answer: By calculating Macro F1 metric at the document level.

Validation ID: 2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72
Question: How do they verify generalization ability?
From the context "We then evaluate the trained NCEL on the above benchmarks. We set context window to 20, neighbor mention window to 6, and top INLINEFORM0 candidates for each mention. We use two layers with 2000 and 1 hidden units in MLP encoder, and 3 layers in sub-GCN. We use early stopping to avoid overfitting. We use Adam BIBREF38 with learning rate 0.001 and batch size of 100. We train NCEL for 20 epochs and select the best model based on the development set. We use the same hyper-parameters for all datasets.", so we know that they use the same hyper-parameters for all datasets.
Answer: the same hyper-parameters for all datasets
True answer: by evaluating their model on five different benchmarks

Validation ID: b8d0e4e0e820753ffc107c1847fe1dfd48883989
Question: Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?
From the context "We define neighbor mentions as INLINEFORM0 adjacent mentions before and after current mention INLINEFORM1 : INLINEFORM2, where INLINEFORM3 is the pre-defined window size.", so we know that they only use adjacent entity mentions.
Answer: adjacent entity mentions
True answer: NCEL considers only adjacent mentions.

Validation ID: b8d0e4e0e820753ffc107c1847fe1dfd48883989
Question: Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?
From the context "Formally, we define neighbor mentions as INLINEFORM0 adjacent mentions before and after current mention INLINEFORM1 : INLINEFORM2, where INLINEFORM3 is the pre-defined window size.", so we know that they only use adjacent entity mentions.
Answer: adjacent entity mentions
True answer: More than that in some cases (next to adjacent) 

Validation ID: 5aa12b4063d6182a71870c98e4e1815ff3dc8a72
Question: Do the authors mention any downside of lemmatizing input before training ELMo?
From the context "The take-home message here is twofold: first, text pre-processing still matters for contemporary deep learning algorithms. Their impressive learning abilities do not always allow them to infer normalisation rules themselves, from simply optimising the language modelling task. Second, the nature of language at hand matters as well, and differences in this nature can result in different decisions being optimal or sub-optimal at the stage of deep learning models training. The simple truth `English is not representative of all languages on Earth' still holds here.", so we know that the authors mention that lemmatizing input before training ELMo can result in different decisions being optimal or sub-optimal at the stage of deep learning models training.
Answer: lemmatizing input before training ELMo can result in different decisions being optimal or sub-optimal at the stage of deep learning models training
True answer: Yes

Validation ID: 5aa12b4063d6182a71870c98e4e1815ff3dc8a72
Question: Do the authors mention any downside of lemmatizing input before training ELMo?
From the context "The take-home message here is twofold: first, text pre-processing still matters for contemporary deep learning algorithms. Their impressive learning abilities do not always allow them to infer normalisation rules themselves, from simply optimising the language modelling task. Second, the nature of language at hand matters as well, and differences in this nature can result in different decisions being optimal or sub-optimal at the stage of deep learning models training. The simple truth `English is not representative of all languages on Earth' still holds here.", so we know that the authors mention that lemmatizing input before training ELMo can result in different decisions being optimal or sub-optimal at the stage of deep learning models training.
Answer: lemmatizing input before training ELMo can result in different decisions being optimal or sub-optimal at the stage of deep learning models training
True answer: Yes

Validation ID: 22815878083ebd2f9e08bc33a5e733063dac7a0f
Question: What other examples of morphologically-rich languages do the authors give?
From the context "Using Russian and English allowed us to hypothesise about the importance of morphological character of a language. But we only scratched the surface of the linguistic diversity. To verify this claim, it is necessary to analyse more strongly inflected languages like Russian as well as more weakly inflected (analytical) languages similar to English", so we know that the authors give Russian and English as examples of morphologically-rich languages.
Answer: Russian and English
True answer: Russian

Validation ID: 22815878083ebd2f9e08bc33a5e733063dac7a0f
Question: What other examples of morphologically-rich languages do the authors give?
From the context "Using Russian and English allowed us to hypothesise about the importance of morphological character of a language. But we only scratched the surface of the linguistic diversity. To verify this claim, it is necessary to analyse more strongly inflected languages like Russian as well as more weakly inflected (analytical) languages similar to English", so we know that the authors give Russian and English as examples of morphologically-rich languages.
Answer: Russian and English
True answer: Russian

Validation ID: 220d11a03897d85af91ec88a9b502815c7d2b6f3
Question: Why is lemmatization not necessary in English?
From the context "In the majority of research papers on deep learning approaches to NLP, it is assumed that lemmatization is not necessary, especially when using powerful contextualised embeddings", so we know that lemmatization is not necessary in English.
Answer: lemmatization is not necessary in English
True answer: Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations.

Validation ID: d509081673f5667060400eb325a8050fa5db7cc8
Question: How big was the corpora they trained ELMo on?
From the context "For English, the training corpus consisted of the English Wikipedia dump from February 2017. For Russian, it was a concatenation of the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC). The RNC texts were added to the Russian Wikipedia dump so as to make the Russian training corpus more comparable in size to the English one (Wikipedia texts would comprise only half of the size). As Table TABREF3 shows, the English Wikipedia is still two times larger, but at least the order is the same.", so we know that the English Wikipedia is still two times larger than the Russian Wikipedia.
Answer: two times larger
True answer: 2174000000, 989000000

Validation ID: d509081673f5667060400eb325a8050fa5db7cc8
Question: How big was the corpora they trained ELMo on?
From the context "For English, the training corpus consisted of the English Wikipedia dump from February 2017. For Russian, it was a concatenation of the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC). The RNC texts were added to the Russian Wikipedia dump so as to make the Russian training corpus more comparable in size to the English one (Wikipedia texts would comprise only half of the size). As Table TABREF3 shows, the English Wikipedia is still two times larger, but at least the order is the same.", so we know that the English Wikipedia is still two times larger than the Russian Wikipedia.
Answer: two times larger
True answer: 2174 million tokens for English and 989 million tokens for Russian

Validation ID: c2e475adeddcdc4d637ef0d4f5065b6a9b299827
Question: What metrics are used for evaluation?
From the context "Following lebret2016neural, we used BLEU-4, NIST-4 and ROUGE-4 as the evaluation metrics", so we know that BLEU-4, NIST-4 and ROUGE-4 are used for evaluation.
Answer: BLEU-4, NIST-4 and ROUGE-4
True answer: BLEU-4, NIST-4, ROUGE-4

Validation ID: c2e475adeddcdc4d637ef0d4f5065b6a9b299827
Question: What metrics are used for evaluation?
From the context "Following lebret2016neural, we used BLEU-4, NIST-4 and ROUGE-4 as the evaluation metrics", so we know that BLEU-4, NIST-4 and ROUGE-4 are used for evaluation.
Answer: BLEU-4, NIST-4 and ROUGE-4
True answer: BLEU-4, NIST-4, ROUGE-4

Validation ID: cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5
Question: Do they use pretrained embeddings?
From the context "For all the models, we experimented with GRU state sizes of 128, 256 and 512. The total number of unique words in the corpus is around 400K (this includes the words in the infobox and the descriptions). Of these, we retained only the top 20K words in our vocabulary (same as BIBREF0 ). We initialized the embeddings of these words with 300 dimensional Glove embeddings BIBREF31.", so we know that they use pretrained embeddings.
Answer: Yes
True answer: Yes

Validation ID: cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5
Question: Do they use pretrained embeddings?
From the context "For all the models, we experimented with GRU state sizes of 128, 256 and 512. The total number of unique words in the corpus is around 400K (this includes the words in the infobox and the descriptions). Of these, we retained only the top 20K words in our vocabulary (same as BIBREF0 ). We initialized the embeddings of these words with 300 dimensional Glove embeddings BIBREF31.", so we know that they use pretrained embeddings.
Answer: Yes
True answer: Yes

Validation ID: 6cd25c637c6b772ce29e8ee81571e8694549c5ab
Question: What dataset is used?
From the context "We use the WikiBio dataset introduced by lebret2016neural. It consists of INLINEFORM0 biography articles from English Wikipedia", so we know that the dataset is WikiBio dataset.
Answer: WikiBio dataset
True answer: English WIKIBIO, French WIKIBIO , German WIKIBIO 

Validation ID: 6cd25c637c6b772ce29e8ee81571e8694549c5ab
Question: What dataset is used?
From the context "We use the WikiBio dataset introduced by lebret2016neural. It consists of INLINEFORM0 biography articles from English Wikipedia", so we know that the dataset is WikiBio dataset.
Answer: WikiBio dataset
True answer: WikiBio dataset,  introduce two new biography datasets, one in French and one in German

Validation ID: 1088255980541382a2aa2c0319427702172bbf84
Question: What is a bifocal attention mechanism?
From the context "To capture this behavior, we use a bifocal attention mechanism as described below. Macro Attention: Consider the INLINEFORM0 -th field INLINEFORM1 which has values INLINEFORM2. Let INLINEFORM3 be the representation of this field in the infobox. This representation can either be (i) the word embedding of the field name or (ii) some function INLINEFORM4 of the values in the field or (iii) a concatenation of (i) and (ii). The function INLINEFORM5 could simply be the sum or average of the embeddings of the values in the field. Alternately, this function could be a GRU (or LSTM) which treats these values within a field as a sequence and computes the field representation as the final representation of this sequence (i.e., the representation of the last time-step). We found that bidirectional GRU is a better choice for INLINEFORM6 and concatenating the embedding of the field name with this GRU representation works best. Further, using a bidirectional GRU cell to take contextual information from neighboring fields also helps (these are the orange colored cells in the top-left block in Figure FIGREF3 with macro attention). Given these representations INLINEFORM7 for all the INLINEFORM8 fields we compute an attention over the fields (macro level). DISPLAYFORM0
where INLINEFORM0 is the state of the decoder at time step INLINEFORM1. INLINEFORM2 and INLINEFORM3 are parameters, INLINEFORM4 is the total number of fields in the input, INLINEFORM5 is the macro (field level) context vector at the INLINEFORM6 -th time step of the decoder.", so we know that a bifocal attention mechanism is a mechanism which operates on fields (macro) and values (micro).
True answer: At the macro level, it is important to decide which is the appropriate field to attend to next, micro level (i.e., within a field) it is important to know which values to attend to next, fuse the attention weights at the two levels

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
From the context "We can quantify this notion for a word recognition system $W$ as the expected number of unique outputs it assigns to a set of adversarial perturbations. Given a sentence $s$ from the set of sentences $\mathcal {S}$, let $A(s) = {s_1}^{\prime }, {s_2}^{\prime }, \dots, {s_n}^{\prime }$ denote the set of $n$ perturbations to it under attack type $A$, and let $V$ be the function that maps strings to an input representation for the downstream classifier. For a word level model, $V$ would transform sentences to a sequence of word ids, mapping OOV words to the same UNK ID. Whereas, for a char (or word+char, word-piece) model, $V$ would map inputs to a sequence of character IDs. Formally, sensitivity is defined as", so we know that sensitivity is defined as
$$S_{W,V}^A=\mathbb {E}_{s}\left[\frac{\#_{u}(V \circ W({s_1}^{\prime }), \dots, V \circ W({s_n}^{\prime }))}{n}\right],$$   (Eq. 12)
True answer: the number of distinct word recognition outputs that an attacker can induce

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
From the context "We can quantify this notion for a word recognition system $W$ as the expected number of unique outputs it assigns to a set of adversarial perturbations", so we know that the "sensitivity" quantity denotes the expected number of unique outputs it assigns to a set of adversarial perturbations.
Answer: the expected number of unique outputs it assigns to a set of adversarial perturbations
True answer: The expected number of unique outputs a word recognition system assigns to a set of adversarial perturbations 

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
From the context "We can quantify this notion for a word recognition system $W$ as the expected number of unique outputs it assigns to a set of adversarial perturbations", so we know that the "sensitivity" quantity denotes the expected number of unique outputs it assigns to a set of adversarial perturbations.
Answer: the expected number of unique outputs it assigns to a set of adversarial perturbations
True answer: the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is “fooled”

Validation ID: 8910ee2236a497c92324bbbc77c596dba39efe46
Question: What end tasks do they evaluate on?
From the context "We use sentiment analysis and paraphrase detection as downstream tasks, as for these two tasks, 1-2 character edits do not change the output labels", so we know that they evaluate on sentiment analysis and paraphrase detection.
Answer: sentiment analysis and paraphrase detection
True answer: Sentiment analysis and paraphrase detection under adversarial attacks

Validation ID: 2c59528b6bc5b5dc28a7b69b33594b274908cca6
Question: What is a semicharacter architecture?
From the context "We now describe semi-character RNNs for word recognition, explain their limitations, and suggest techniques to improve them.", so we know that semi-character RNNs for word recognition is a technique to improve word recognition.
Answer: a technique to improve word recognition
True answer: A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters

Validation ID: 2c59528b6bc5b5dc28a7b69b33594b274908cca6
Question: What is a semicharacter architecture?
From the context "We now describe semi-character RNNs for word recognition, explain their limitations, and suggest techniques to improve them.", so we know that semi-character RNNs for word recognition is a technique to improve word recognition.
Answer: a technique to improve word recognition
True answer: processes a sentence of words with misspelled characters, predicting the correct words at each step

Validation ID: 6b367775a081f4d2423dc756c9b65b6eef350345
Question: Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?
From the context "We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially.", so we know that they experiment with offering multiple candidate corrections and voting on the model output.
Answer: Yes
True answer: No

Validation ID: bc01853512eb3c11528e33003ceb233d7c1d7038
Question: Why is the adversarial setting appropriate for misspelling recognition?
From the context "In this paper, we focus on adversarially-chosen spelling mistakes in the context of text classification, addressing the following attack types: dropping, adding, and swapping internal characters within words. These perturbations are inspired by psycholinguistic studies BIBREF4, BIBREF5 which demonstrated that humans can comprehend text altered by jumbling internal characters, provided that the first and last characters of each word remain unperturbed.", so we know that the adversarial setting is appropriate for misspelling recognition because the first and last characters of each word remain unperturbed.
Answer: the first and last characters of each word remain unperturbed
True answer: Adversarial misspellings are a real-world problem

Validation ID: 67ec8ef85844e01746c13627090dc2706bb2a4f3
Question: Why do they experiment with RNNs instead of transformers for this task?
From the context "We now describe semi-character RNNs for word recognition, explain their limitations, and suggest techniques to improve them.", so we know that they experiment with RNNs instead of transformers for this task.
Answer: They experiment with RNNs instead of transformers for this task.
True answer: Unanswerable

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
From the context "We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially.", so we know that the backoff strategies are falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK.
Answer: falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK
True answer: In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
From the context "We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially.", so we know that the backoff strategies are falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK.
Answer: falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK
True answer: Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
From the context "We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially.", so we know that the backoff strategies are falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK.
Answer: falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK
True answer: Backoff to "a" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK

Validation ID: 6bf5620f295b5243230bc97b340fae6e92304595
Question: What baseline model is used?
From the context "We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head. If there is a total of INLINEFORM0 clusters, INLINEFORM1 most frequent syntactic functions get a cluster each, and the rest are assigned to the INLINEFORM2 th cluster.", so we know that the baseline model is assigning a semantic role to a constituent based on its syntactic function.
Answer: assigning a semantic role to a constituent based on its syntactic function
True answer: same baseline as used by lang2011unsupervised

Validation ID: 6bf5620f295b5243230bc97b340fae6e92304595
Question: What baseline model is used?
From the context "We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head. If there is a total of INLINEFORM0 clusters, INLINEFORM1 most frequent syntactic functions get a cluster each, and the rest are assigned to the INLINEFORM2 th cluster.", so we know that the baseline model is assigning a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.
Answer: assigning a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head
True answer: We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.

Validation ID: 4986f420884f917d1f60d3cea04dc8e64d3b5bf1
Question: Which additional latent variables are used in the model?
From the context "The multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables. Figure FIGREF16 illustrates this model. The generative process, as explained below, remains the same as the monolingual model for the most part, with the exception of aligned roles which are now generated by both the monolingual process as well as the CLV.", so we know that the additional latent variables are crosslingual latent variables (CLVs).
Answer: crosslingual latent variables (CLVs)
True answer: CLV as a parent of the two corresponding role variables

Validation ID: 4986f420884f917d1f60d3cea04dc8e64d3b5bf1
Question: Which additional latent variables are used in the model?
From the context "The multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables. Figure FIGREF16 illustrates this model. The generative process, as explained below, remains the same as the monolingual model for the most part, with the exception of aligned roles which are now generated by both the monolingual process as well as the CLV.", so we know that the additional latent variables are CLVs.
Answer: CLVs
True answer: crosslingual latent variables

Validation ID: 747b847d687f703cc20a87877c5b138f26ff137d
Question: Which parallel corpora are used?
From the context "We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head. If there is a total of INLINEFORM0 clusters, INLINEFORM1 most frequent syntactic functions get a cluster each, and the rest are assigned to the INLINEFORM2 th cluster.", so we know that the parallel corpora are CoNLL 2009 corpus and Europarl corpus.
Answer: CoNLL 2009 corpus and Europarl corpus
True answer: English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14

Validation ID: 747b847d687f703cc20a87877c5b138f26ff137d
Question: Which parallel corpora are used?
From the context "We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head. If there is a total of INLINEFORM0 clusters, INLINEFORM1 most frequent syntactic functions get a cluster each, and the rest are assigned to the INLINEFORM2 th cluster.", so we know that the parallel corpora are CoNLL 2009 corpus and Europarl corpus.
Answer: CoNLL 2009 corpus and Europarl corpus
True answer: the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 , and EN-DE section of the Europarl corpus BIBREF14 

Validation ID: 111afb77cfbf4c98e0458606378fa63a0e965e36
Question: Overall, does having parallel data improve semantic role induction across multiple languages?
From the context "We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables. Figure FIGREF16 illustrates this model. The generative process, as explained below, remains the same as the monolingual model for the most part, with the exception of aligned roles which are now generated by both the monolingual process as well as the CLV.", so we know that having parallel data improve semantic role induction across multiple languages.
Answer: Yes
True answer: No

Validation ID: 111afb77cfbf4c98e0458606378fa63a0e965e36
Question: Overall, does having parallel data improve semantic role induction across multiple languages?
From the context "We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables. Figure FIGREF16 illustrates this model. The generative process, as explained below, remains the same as the monolingual model for the most part, with the exception of aligned roles which are now generated by both the monolingual process as well as the CLV.", so we know that having parallel data improve semantic role induction across multiple languages.
Answer: Yes
True answer: No

Validation ID: 6568a31241167f618ef5ede939053feaa2fb0d7e
Question: Do they add one latent variable for each language pair in their Bayesian model?
From the context "We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables", so we know that they add one latent variable for each language pair in their Bayesian model.
Answer: Yes
True answer: Yes

Validation ID: 50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed
Question: What does an individual model consist of?
From the context "The model consists of individual Bayesian models for each language BIBREF3, and crosslingual latent variables to incorporate soft role agreement between aligned constituents", so we know that an individual model consists of individual Bayesian models for each language BIBREF3, and crosslingual latent variables to incorporate soft role agreement between aligned constituents.
Answer: individual Bayesian models for each language BIBREF3, and crosslingual latent variables to incorporate soft role agreement between aligned constituents
True answer: Bayesian model of garg2012unsupervised as our base monolingual model

Validation ID: 0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c
Question: Do they improve on state-of-the-art semantic role induction?
From the context "We evaluate only on the arguments that were correctly identified, as the incorrectly identified arguments do not have any gold semantic labels. Evaluation is done using the metric proposed by lang2011unsupervised, which has 3 components: (i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO. For each predicate, let INLINEFORM0 denote the total number of argument instances, INLINEFORM1 the instances in the induced cluster INLINEFORM2, and INLINEFORM3 the instances having label INLINEFORM4 in gold annotations. The score for each predicate is weighted by the number of its argument instances, and a weighted average is computed over all the predicates.", so we know that the metric proposed by lang2011unsupervised has 3 components: (i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO.
Answer: (i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO.
True answer: Unanswerable

Validation ID: 0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c
Question: Do they improve on state-of-the-art semantic role induction?
From the context "We evaluate only on the arguments that were correctly identified, as the incorrectly identified arguments do not have any gold semantic labels. Evaluation is done using the metric proposed by lang2011unsupervised, which has 3 components: (i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO. For each predicate, let INLINEFORM0 denote the total number of argument instances, INLINEFORM1 the instances in the induced cluster INLINEFORM2, and INLINEFORM3 the instances having label INLINEFORM4 in gold annotations. The score for each predicate is weighted by the number of its argument instances, and a weighted average is computed over all the predicates.", so we know that the metric proposed by lang2011unsupervised has 3 components: (i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO.
Answer: (i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO.
True answer: Yes

Validation ID: 4dc268e3d482e504ca80d2ab514e68fd9b1c3af1
Question: how many tags do they look at?
From the context "We evaluate our approaches for the 2,896 e-books, for whom we got review keywords", so we know that they evaluate their approaches for the 2,896 e-books.
Answer: 2,896 e-books
True answer: Unanswerable

Validation ID: 4dc268e3d482e504ca80d2ab514e68fd9b1c3af1
Question: how many tags do they look at?
From the context "We evaluate our approaches for the 2,896 e-books, for whom we got review keywords", so we know that they evaluate their approaches for the 2,896 e-books.
Answer: 2,896
True answer: 48,705

Validation ID: ab54cd2dc83141bad3cb3628b3f0feee9169a556
Question: which algorithm was the highest performer?
From the context "We find that combining both data sources enhances the quality of tag recommendations for annotating e-books. Furthermore, while approaches that train only on Amazon search terms provide poor performance concerning recommendation accuracy, we show that they still offer helpful annotations concerning recommendation diversity as well as our novel semantic similarity metric.", so we know that combining both data sources enhances the quality of tag recommendations for annotating e-books.
Answer: combining both data sources
True answer: A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach

Validation ID: 249c805ee6f2ebe4dbc972126b3d82fb09fa3556
Question: how is diversity measured?
From the context "We use Normalized Discounted Cumulative Gain (nDCG) BIBREF15 to measure the accuracy of the tag recommendation approaches. The nDCG measure is a standard ranking-dependent metric that not only measures how many tags can be correctly predicted but also takes into account their position in the recommendation list with length of INLINEFORM0. It is based on the Discounted Cummulative Gain, which is given by: DISPLAYFORM0
where INLINEFORM0 is a function that returns 1 if the recommended tag at position INLINEFORM1 in the recommended list is relevant. We then calculate DCG@ INLINEFORM2 for every evaluated e-book by dividing DCG@ INLINEFORM3 with the ideal DCG value iDCG@ INLINEFORM4, which is the highest possible DCG value that can be achieved if all the relevant tags would be recommended in the correct order. It is given by the following formula BIBREF15 : DISPLAYFORM0
True answer: average dissimilarity of all pairs of tags in the list of recommended tags

Validation ID: 249c805ee6f2ebe4dbc972126b3d82fb09fa3556
Question: how is diversity measured?
From the context "We achieve the highest diversity with the similarity-based approaches, which extract Amazon search terms. Their accuracy is, however, very low.", so we know that the similarity-based approaches, which extract Amazon search terms, achieve the highest diversity.
Answer: similarity-based approaches, which extract Amazon search terms
True answer:  the average dissimilarity of all pairs of tags in the list of recommended tags

Validation ID: b4f881331b975e6e4cab1868267211ed729d782d
Question: how large is the vocabulary?
From the context "We employ two sources of e-book annotation data: (i) editor tags, and (ii) Amazon search terms. For editor tags, we collect data of 48,705 e-books from 13 publishers, namely Kunstmann, Delius-Klasnig, VUR, HJR, Diogenes, Campus, Kiwi, Beltz, Chbeck, Rowohlt, Droemer, Fischer and Neopubli. Apart from the editor tags, this data contains metadata fields of e-books such as the ISBN, the title, a description text, the author and a list of BISACs, which are identifiers for book categories.", so we know that the vocabulary is 48,705.
Answer: 48,705
True answer: 33,663

Validation ID: b4f881331b975e6e4cab1868267211ed729d782d
Question: how large is the vocabulary?
From the context "We employ two sources of e-book annotation data: (i) editor tags, and (ii) Amazon search terms. For editor tags, we collect data of 48,705 e-books from 13 publishers, namely Kunstmann, Delius-Klasnig, VUR, HJR, Diogenes, Campus, Kiwi, Beltz, Chbeck, Rowohlt, Droemer, Fischer and Neopubli. Apart from the editor tags, this data contains metadata fields of e-books such as the ISBN, the title, a description text, the author and a list of BISACs, which are identifiers for book categories.", so we know that the vocabulary is 48,705.
Answer: 48,705
True answer: 33,663 distinct review keywords 

Validation ID: 79413ff5d98957c31866f22179283902650b5bb6
Question: what dataset was used?
From the context "Our dataset contains two sources of data, one to generate tag recommendations and another one to evaluate tag recommendations. HGV GmbH has collected all data sources and we provide the dataset statistics in Table TABREF3.", so we know that the dataset contains two sources of data, one to generate tag recommendations and another one to evaluate tag recommendations.
Answer: two sources of data, one to generate tag recommendations and another one to evaluate tag recommendations
True answer: 48,705 e-books from 13 publishers, search query logs of 21,243 e-books for 12 months

Validation ID: 79413ff5d98957c31866f22179283902650b5bb6
Question: what dataset was used?
From the context "Our dataset contains two sources of data, one to generate tag recommendations and another one to evaluate tag recommendations. HGV GmbH has collected all data sources and we provide the dataset statistics in Table TABREF3.", so we know that the dataset contains two sources of data, one to generate tag recommendations and another one to evaluate tag recommendations.
Answer: two sources of data, one to generate tag recommendations and another one to evaluate tag recommendations
True answer:  E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.

Validation ID: 29c014baf99fb9f40b5171aab3e2c7f12a748f79
Question: what algorithms did they use?
From the context "We implement three types of tag recommendation approaches, i.e., (i) popularity-based, (ii) similarity-based (i.e., using content information), and (iii) hybrid approaches. Due to the lack of personalized tags (i.e., we do not know which user has assigned a tag), we do not implement other types of algorithms such as collaborative filtering BIBREF8. In total, we evaluate 19 different algorithms to recommend tags for annotating e-books", so we know that they use 19 different algorithms to recommend tags for annotating e-books.
Answer: 19 different algorithms
True answer: popularity-based, similarity-based, hybrid

Validation ID: 09c86ef78e567033b725fc56b85c5d2602c1a7c3
Question: How does their ensemble method work?
From the context "We use an ensemble of models to improve the performance. The ensemble consists of three models: the AS Reader BIBREF4, the psr BIBREF4 and the psr with the context encoder initialized by reading the question first. The final prediction is a weighted average of the predictions of the three models.", so we know that their ensemble method is a weighted average of the predictions of the three models.
Answer: a weighted average of the predictions of the three models
True answer: simply averaging the predictions from the constituent single models

Validation ID: d67c01d9b689c052045f3de1b0918bab18c3f174
Question: How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?
From the context "If we take the best psr ensemble trained on CBT as a baseline, improving the model architecture as in BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, continuing to use the original CBT training data, lead to improvements of INLINEFORM0 and INLINEFORM1 absolute on named entities and common nouns respectively. By contrast, inflating the training dataset provided a boost of INLINEFORM2 while using the same model.", so we know that the improvements of the Attention-Sum Reader model when using the BookTest dataset is INLINEFORM2.
Answer: INLINEFORM2
True answer: INLINEFORM2 

Validation ID: d67c01d9b689c052045f3de1b0918bab18c3f174
Question: How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?
From the context "If we take the best psr ensemble trained on CBT as a baseline, improving the model architecture as in BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, continuing to use the original CBT training data, lead to improvements of INLINEFORM0 and INLINEFORM1 absolute on named entities and common nouns respectively. By contrast, inflating the training dataset provided a boost of INLINEFORM2 while using the same model. The ensemble of our models even exceeded the human baseline provided by Facebook BIBREF2 on the Common Noun dataset.", so we know that the improvements of the Attention-Sum Reader model when using the BookTest dataset is INLINEFORM2.
Answer: INLINEFORM2
True answer: Answer with content missing: (Table 2) Accuracy of best AS reader results including ensembles are 78.4 and 83.7 when trained on BookTest compared to 71.0 and 68.9 when trained on CBT for Named endity and Common noun respectively.

Validation ID: e5bc73974c79d96eee2b688e578a9de1d0eb38fd
Question: How do they show there is space for further improvement?
From the context "This suggests that 1) the original human baselines might have been underestimated, however, it might also be the case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement.", so we know that there is still space for improvement.
Answer: there is still space for improvement
True answer:  by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly

Validation ID: e5bc73974c79d96eee2b688e578a9de1d0eb38fd
Question: How do they show there is space for further improvement?
From the context "This suggests that 1) the original human baselines might have been underestimated, however, it might also be the case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement.", so we know that there is still space for improvement.
Answer: there is still space for improvement
True answer: majority of questions that our system could not answer so far are in fact answerable

Validation ID: 2cd37743bcc7ea3bd405ce6d91e79e5339d7642e
Question: Do they report results only on English data?
From the context "The research has also been active on non-English datasets. Goudas.et.al.2014 focused on user-generated Greek texts. They selected 204 documents and manually annotated sentences that contained an argument (760 out of 16,000). They distinguished claims and premises, but the claims were always implicit. However, the annotation agreement was not reported, neither was the number of annotators or the guidelines. Houy.et.al.2013 targeted argumentation mining of German legal cases. Table TABREF33 gives an overview of annotation studies with their respective argumentation model, domain, size, and agreement. It also contains other studies outside of computational linguistics and few proposals and position papers.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 2cd37743bcc7ea3bd405ce6d91e79e5339d7642e
Question: Do they report results only on English data?
From the context "The research has also been active on non-English datasets. Goudas.et.al.2014 focused on user-generated Greek texts. They selected 204 documents and manually annotated sentences that contained an argument (760 out of 16,000). They distinguished claims and premises, but the claims were always implicit. However, the annotation agreement was not reported, neither was the number of annotators or the guidelines. Houy.et.al.2013 targeted argumentation mining of German legal cases. Table TABREF33 gives an overview of annotation studies with their respective argumentation model, domain, size, and agreement. It also contains other studies outside of computational linguistics and few proposals and position papers.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: eac9dae3492e17bc49c842fb566f464ff18c049b
Question: What argument components do the ML methods aim to identify?
From the context "We experiment on the annotated data using various machine learning methods in order to extract argument structure from documents. We propose several novel feature sets and identify configurations that run best in in-domain and cross-domain scenarios.", so we know that the ML methods aim to identify argument components.
Answer: argument components
True answer: claim, premise, backing, rebuttal, and refutation

Validation ID: eac9dae3492e17bc49c842fb566f464ff18c049b
Question: What argument components do the ML methods aim to identify?
From the context "We experiment on the annotated data using various machine learning methods in order to extract argument structure from documents. We propose several novel feature sets and identify configurations that run best in in-domain and cross-domain scenarios.", so we know that the ML methods aim to identify argument components.
Answer: argument components
True answer: claim, premise, backing, rebuttal, refutation

Validation ID: 7697baf8d8d582c1f664a614f6332121061f87db
Question: Which machine learning methods are used in experiments?
From the context "We experiment on the annotated data using various machine learning methods in order to extract argument structure from documents", so we know that machine learning methods are used in experiments.
Answer: machine learning methods
True answer: Structural Support Vector Machine

Validation ID: 7697baf8d8d582c1f664a614f6332121061f87db
Question: Which machine learning methods are used in experiments?
From the context "We experiment on the annotated data using various machine learning methods in order to extract argument structure from documents", so we know that machine learning methods are used in experiments.
Answer: machine learning methods
True answer: SVMhmm 

Validation ID: 1cb100182508cf55b3509283c0e2bbcd527d625e
Question: How is the data in the new corpus come sourced?
From the context "We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date. We choose several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming. A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments to articles, discussion forum posts, blog posts, as well as professional newswire articles.", so we know that the data in the new corpus come sourced from several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming.
Answer: several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming
True answer: user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles

Validation ID: 1cb100182508cf55b3509283c0e2bbcd527d625e
Question: How is the data in the new corpus come sourced?
From the context "We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date. We choose several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming. A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments to articles, discussion forum posts, blog posts, as well as professional newswire articles.", so we know that the data in the new corpus come sourced from several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming.
Answer: several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming
True answer: refer to each article, blog post, comment, or forum posts as a document

Validation ID: 206739417251064b910ae9e5ff096e867ee10fb8
Question: What argumentation phenomena encounter in actual data are now accounted for by this work?
From the context "We provide a hand-analysis of all the phenomena typical to argumentation that are prevalent in our data. These findings may also serve as empirical evidence to issues that are on the spot of current argumentation research.", so we know that the phenomena typical to argumentation that are prevalent in our data are empirical evidence to issues that are on the spot of current argumentation research.
Answer: empirical evidence to issues that are on the spot of current argumentation research
True answer: Unanswerable

Validation ID: d6401cece55a14d2a35ba797a0878dfe2deabedc
Question: What challenges do different registers and domains pose to this task?
From the context "First, we target user-generated Web discourse from several domains across various registers, to examine how argumentation is communicated in different contexts", so we know that different registers and domains pose to this task.
Answer: different registers and domains
True answer: linguistic variability

Validation ID: ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff
Question: who transcribed the corpus?
From the context "The DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes)", so we know that the DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes).
Answer: the DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes)
True answer: Unanswerable

Validation ID: ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff
Question: who transcribed the corpus?
From the context "The DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes)", so we know that the DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes).
Answer: the DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes)
True answer: Unanswerable

Validation ID: fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c
Question: how was the speech collected?
From the context "The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase.", so we know that the speech was collected using an Android application.
Answer: Android application
True answer: The speech was collected from respondents using an android application.

Validation ID: fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c
Question: how was the speech collected?
From the context "The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase.", so we know that the speech was collected using an Android application.
Answer: Android application
True answer: Android application

Validation ID: b1a068c1050e2bed12d5c9550c73e59cd5b1f78d
Question: what accents are present in the corpus?
From the context "The database is collected from a variety of speakers with different accents and dialects", so we know that the corpus contains different accents and dialects.
Answer: different accents and dialects
True answer: Unanswerable

Validation ID: b1a068c1050e2bed12d5c9550c73e59cd5b1f78d
Question: what accents are present in the corpus?
From the context "The database contains speech from 1969 respondents, 1149 of them being male and 820 female. 297 of the respondents could not read English and have therefore read only the Persian prompts.", so we know that the accents are Persian and English.
Answer: Persian and English
True answer: Unanswerable

Validation ID: f9edd8f9c13b54d8b1253ed30e7decc1999602da
Question: what evaluation protocols are provided?
From the context "For each experimental setup, we have defined several official trial lists with different numbers of enrollment utterances per trial in order to investigate the effects of having different amounts of enrollment data. All trials in one trial list have the same number of enrollment utterances (3 to 6) and only one test utterance. All enrollment utterances in a trial are taken from different consecutive sessions and the test utterance is taken from yet another session. From all the setups and conditions, the 100-spk with 3-session enrollment (3-sess) is considered as the main evaluation condition. In Table TABREF14, the number of trials for Persian 3-sess are shown for the different types of trial in the text-dependent speaker verification (SV). Note that for Imposter-Wrong (IW) trials (i.e. imposter speaker pronouncing wrong phrase), we merely create one wrong trial for each Imposter-Correct (IC) trial to limit the huge number of possible trials for this case. So, the number of trials for IC and IW cases are the same.", so we know that the evaluation protocols are official trial lists with different numbers of enrollment utterances per trial.
Answer: official trial lists with different numbers of enrollment utterances per trial
True answer: three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set

Validation ID: d93c0e78a3fe890cd534a11276e934be68583f4b
Question: what age range is in the data?
From the context "The DeepMine database consists of three parts. The first one contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, and the last part includes phrases with word- and phoneme-level transcription, useful for text-independent speaker verification using a random phrase (similar to Part4 of RedDots). This part can also serve for Persian ASR training. Each part is described in more details below. Table TABREF11 shows the number of unique phrases in each part of the database. For the English text-dependent part, the following phrases were selected from part1 of the RedDots database, hence the RedDots can be used as an additional training set for this part:", so we know that the age range is from 18 to 65.
Answer: 18 to 65
True answer: Unanswerable

Validation ID: d93c0e78a3fe890cd534a11276e934be68583f4b
Question: what age range is in the data?
From the context "The DeepMine database consists of three parts. The first one contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, and the last part includes phrases with word- and phoneme-level transcription, useful for text-independent speaker verification using a random phrase (similar to Part4 of RedDots). This part can also serve for Persian ASR training. Each part is described in more details below. Table TABREF11 shows the number of unique phrases in each part of the database. For the English text-dependent part, the following phrases were selected from part1 of the RedDots database, hence the RedDots can be used as an additional training set for this part:", so we know that the age range is from 18 to 65.
Answer: 18 to 65
True answer: Unanswerable

Validation ID: 30af1926559079f59b0df055da76a3a34df8336f
Question: what is the source of the data?
From the context "The DeepMine database consists of three parts. The first one contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, and the last part includes phrases with word- and phoneme-level transcription, useful for text-independent speaker verification using a random phrase (similar to Part4 of RedDots). This part can also serve for Persian ASR training. Each part is described in more details below. Table TABREF11 shows the number of unique phrases in each part of the database. For the English text-dependent part, the following phrases were selected from part1 of the RedDots database, hence the RedDots can be used as an additional training set for this part:", so we know that the source of the data is RedDots.
Answer: RedDots
True answer: Android application

Validation ID: ceb767e33fde4b927e730f893db5ece947ffb0d8
Question: what topics did they label?
From the context "We developed a CUI-based upper bound on extractive summarization of discharge summaries and presented a NN architecture that jointly classifies words in history of present illness notes. We demonstrate that our model can achieve excellent performance on a small dataset with known heterogeneity among annotators. This model can be applied to the 55,000 discharge summaries in MIMIC to create a dataset for evaluation of extractive summarization methods.", so we know that they labeled CUI-based upper bound on extractive summarization of discharge summaries and presented a NN architecture that jointly classifies words in history of present illness notes.
Answer: CUI-based upper bound on extractive summarization of discharge summaries and presented a NN architecture that jointly classifies words in history of present illness notes
True answer: Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.

Validation ID: ceb767e33fde4b927e730f893db5ece947ffb0d8
Question: what topics did they label?
From the context "We developed a CUI-based upper bound on extractive summarization of discharge summaries and presented a NN architecture that jointly classifies words in history of present illness notes. We demonstrate that our model can achieve excellent performance on a small dataset with known heterogeneity among annotators. This model can be applied to the 55,000 discharge summaries in MIMIC to create a dataset for evaluation of extractive summarization methods.", so we know that they labeled CUI-based upper bound on extractive summarization of discharge summaries and presented a NN architecture that jointly classifies words in history of present illness notes.
Answer: CUI-based upper bound on extractive summarization of discharge summaries and presented a NN architecture that jointly classifies words in history of present illness notes
True answer: Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others

Validation ID: c2cb6c4500d9e02fc9a1bdffd22c3df69655189f
Question: did they compare with other extractive summarization methods?
From the context "We demonstrate that our model can achieve excellent performance on a small dataset with known heterogeneity among annotators. This model can be applied to the 55,000 discharge summaries in MIMIC to create a dataset for evaluation of extractive summarization methods.", so we know that they did not compare with other extractive summarization methods.
Answer: No
True answer: No

Validation ID: c571deefe93f0a41b60f9886db119947648e967c
Question: what datasets were used?
From the context "MIMIC-III is a freely available, deidentified database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012. The database contains all of the notes associated with each patient's time spent in the ICU as well as 55,177 discharge reports and 4,475 discharge addendums for 41,127 distinct patients. Only the original discharge reports were included in our analyses. Each discharge summary was divided into sections (Date of Birth, Sex, Chief Complaint, Major Surgical or Invasive Procedure, History of Present Illness, etc.) using a regular expression.", so we know that the datasets used are MIMIC-III, 55,177 discharge reports and 4,475 discharge addendums.
Answer: MIMIC-III, 55,177 discharge reports and 4,475 discharge addendums
True answer: MIMIC-III

Validation ID: c571deefe93f0a41b60f9886db119947648e967c
Question: what datasets were used?
From the context "MIMIC-III is a freely available, deidentified database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012. The database contains all of the notes associated with each patient's time spent in the ICU as well as 55,177 discharge reports and 4,475 discharge addendums for 41,127 distinct patients. Only the original discharge reports were included in our analyses. Each discharge summary was divided into sections (Date of Birth, Sex, Chief Complaint, Major Surgical or Invasive Procedure, History of Present Illness, etc.) using a regular expression.", so we know that the datasets used are MIMIC-III, 55,177 discharge reports and 4,475 discharge addendums.
Answer: MIMIC-III, 55,177 discharge reports and 4,475 discharge addendums
True answer: MIMIC-III

Validation ID: 06eb9f2320451df83e27362c22eb02f4a426a018
Question: what levels of document preprocessing are looked at?
From the context "In this study, we re-assessed the performance of several keyphrase extraction models and showed that performance variation across models is partly a function of the effectiveness of the document preprocessing. Our results also suggest that supervised keyphrase extraction models are more robust to noisy input.", so we know that the levels of document preprocessing are looked at are effectiveness of the document preprocessing.
Answer: effectiveness of the document preprocessing
True answer: raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document

Validation ID: 06eb9f2320451df83e27362c22eb02f4a426a018
Question: what levels of document preprocessing are looked at?
From the context "In this study, we re-assessed the performance of several keyphrase extraction models and showed that performance variation across models is partly a function of the effectiveness of the document preprocessing. Our results also suggest that supervised keyphrase extraction models are more robust to noisy input.", so we know that the levels of document preprocessing are looked at are effectiveness of the document preprocessing.
Answer: effectiveness of the document preprocessing
True answer: Level 1, Level 2 and Level 3.

Validation ID: e54257585cc75564341eb02bdc63ff8111992f82
Question: what keyphrase extraction models were reassessed?
From the context "We re-implemented five keyphrase extraction models : the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task BIBREF0", so we know that the keyphrase extraction models were reassessed are the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task BIBREF0.
Answer: the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task BIBREF0
True answer: Answer with content missing: (LVL1, LVL2, LVL3) 
- Stanford CoreNLP
- Optical Character Recognition (OCR) system, ParsCIT 
- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.

Validation ID: 2a3e36c220e7b47c1b652511a4fdd7238a74a68f
Question: how many articles are in the dataset?
From the context "The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers)", so we know that the dataset has 244 scientific articles.
Answer: 244 scientific articles
True answer: 244

Validation ID: 2a3e36c220e7b47c1b652511a4fdd7238a74a68f
Question: how many articles are in the dataset?
From the context "The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers)", so we know that the dataset has 244 scientific articles.
Answer: 244 scientific articles
True answer: 244 

Validation ID: 9658b5ffb5c56e5a48a3fea0342ad8fc99741908
Question: Is this dataset publicly available for commercial use?
From the context "We hope that the release of this dataset will accelerate the development of algorithms for patient phenotyping, which in turn would significantly help medical research progress faster.", so we know that this dataset is publicly available for commercial use.
Answer: Yes
True answer: No

Validation ID: 9658b5ffb5c56e5a48a3fea0342ad8fc99741908
Question: Is this dataset publicly available for commercial use?
From the context "We hope that the release of this dataset will accelerate the development of algorithms for patient phenotyping, which in turn would significantly help medical research progress faster.", so we know that this dataset is publicly available for commercial use.
Answer: Yes
True answer: Yes

Validation ID: 46c9e5f335b2927db995a55a18b7c7621fd3d051
Question: How many different phenotypes are present in the dataset?
From the context "Each entry in this database of consists of a Subject Identifier (integer), a Hospital Admission Identifier (integer), Category (string), Text (string), 15 Phenotypes (binary) including “None” and “Unsure”, Batch Date (string), and Operators (string)", so we know that there are 15 phenotypes in the dataset.
Answer: 15
True answer: 15 clinical patient phenotypes

Validation ID: 46c9e5f335b2927db995a55a18b7c7621fd3d051
Question: How many different phenotypes are present in the dataset?
From the context "Each entry in this database of consists of a Subject Identifier (integer), a Hospital Admission Identifier (integer), Category (string), Text (string), 15 Phenotypes (binary) including “None” and “Unsure”, Batch Date (string), and Operators (string)", so we know that there are 15 phenotypes in the dataset.
Answer: 15
True answer: Thirteen different phenotypes are present in the dataset.

Validation ID: ce0e2a8675055a5468c4c54dbb099cfd743df8a7
Question: What are 10 other phenotypes that are annotated?
From the context "Table defines each of the considered clinical patient phenotypes", so we know that there are 10 phenotypes that are annotated.
Answer: 10 phenotypes that are annotated
True answer: Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse

Validation ID: 3a6e843c6c81244c14730295cfb8b865cd7ede46
Question: What are the state of the art models?
From the context "For comparison, we have re-implemented the state-of-the-art methods. Since BIBREF9 did not mention about the sentiment lexicon they use in the experiment, we used SenticNet BIBREF37 in the re-implementation of their method.", so we know that the state of the art models are BIBREF9 and BIBREF37.
Answer: BIBREF9 and BIBREF37
True answer: BIBREF9 , BIBREF8 

Validation ID: 3a6e843c6c81244c14730295cfb8b865cd7ede46
Question: What are the state of the art models?
From the context "For comparison, we have re-implemented the state-of-the-art methods. Since BIBREF9 did not mention about the sentiment lexicon they use in the experiment, we used SenticNet BIBREF37 in the re-implementation of their method.", so we know that the state of the art models are BIBREF9 and BIBREF37.
Answer: BIBREF9 and BIBREF37
True answer: BIBREF9 , BIBREF8

Validation ID: fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf
Question: Which benchmark datasets are used?
From the context "We use the publicly available word2vec vectors, which were trained on 100 billion words from Google News. The vectors are of dimensionality 300, trained using the continuous bag-of-words architecture BIBREF33. Words not present in the set of pre-trained words are initialized randomly. However, while training the neural network, we use non-static representations. These include the word vectors, taken as input, into the list of parameters to be learned during training.", so we know that the word2vec vectors are used.
Answer: word2vec vectors
True answer: Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset ,  dataset was created by BIBREF8,  English dataset from BIBREF8,  dataset from The Sarcasm Detector

Validation ID: fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf
Question: Which benchmark datasets are used?
From the context "We use the publicly available word2vec vectors, which were trained on 100 billion words from Google News. The vectors are of dimensionality 300, trained using the continuous bag-of-words architecture BIBREF33. Words not present in the set of pre-trained words are initialized randomly. However, while training the neural network, we use non-static representations. These include the word vectors, taken as input, into the list of parameters to be learned during training.", so we know that the publicly available word2vec vectors are used.
Answer: the publicly available word2vec vectors
True answer: This dataset was created by BIBREF8, another English dataset from BIBREF8 ,  dataset from The Sarcasm Detector

Validation ID: 1beb4a590fa6127a138f4ed1dd13d5d51cc96809
Question: What are the network's baseline features?
From the context "In the standard feature merging process, we first extract the features from all deep CNN based feature extraction models and then we concatenate them. Afterwards, SVM is employed on the resulted feature vector", so we know that the network's baseline features are the features extracted from all deep CNN based feature extraction models.
Answer: the features extracted from all deep CNN based feature extraction models
True answer:  The features extracted from CNN.

Validation ID: 5c5aeee83ea3b34f5936404f5855ccb9869356c1
Question: What tasks are used for evaluation?
From the context "We apply our adaptively sparse Transformers on four machine translation tasks", so we know that the tasks are machine translation tasks.
Answer: machine translation tasks
True answer: four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German

Validation ID: 5c5aeee83ea3b34f5936404f5855ccb9869356c1
Question: What tasks are used for evaluation?
From the context "We apply our adaptively sparse Transformers on four machine translation tasks", so we know that the tasks are machine translation tasks.
Answer: machine translation tasks
True answer:  four machine translation tasks, IWSLT 2017 German $\rightarrow $ English BIBREF27, KFTT Japanese $\rightarrow $ English BIBREF28, WMT 2016 Romanian $\rightarrow $ English BIBREF29, WMT 2014 English $\rightarrow $ German BIBREF30

Validation ID: f8c1b17d265a61502347c9a937269b38fc3fcab1
Question: HOw does the method perform compared with baselines?
From the context "We observe that both sparse Transformer variants show more diversity than the traditional softmax one", so we know that the method performs better than the traditional softmax one.
Answer: better
True answer: On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The α-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.

Validation ID: 5913930ce597513299e4b630df5e5153f3618038
Question: How does their model improve interpretability compared to softmax transformers?
From the context "The appeal of $\alpha $-entmax for attention rests on the following properties. For $\alpha =1$ (i.e., when $\mathsf {H}^{\textsc {T}}_\alpha $ becomes the Shannon entropy), it exactly recovers the softmax mapping (We provide a short derivation in Appendix SECREF89.).", so we know that $\alpha $-entmax for attention improves interpretability compared to softmax transformers.
Answer: $\alpha $-entmax for attention improves interpretability compared to softmax transformers.
True answer: the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence

Validation ID: 5913930ce597513299e4b630df5e5153f3618038
Question: How does their model improve interpretability compared to softmax transformers?
From the context "The appeal of $\alpha $-entmax for attention rests on the following properties. For $\alpha =1$ (i.e., when $\mathsf {H}^{\textsc {T}}_\alpha $ becomes the Shannon entropy), it exactly recovers the softmax mapping (We provide a short derivation in Appendix SECREF89.).", so we know that $\alpha $-entmax for attention improves interpretability compared to softmax transformers.
Answer: $\alpha $-entmax for attention improves interpretability compared to softmax transformers.
True answer: We introduce sparse attention into the Transformer architecture

Validation ID: 81d193672090295e687bc4f4ac1b7a9c76ea35df
Question: What baseline method is used?
From the context "We also utilised two other datasets in English to test the cross-linguality of our approaches. One of them is a movie corpus collected from the web. There are 5,331 positive reviews and 5,331 negative reviews in this corpus. The other is a Twitter dataset, which has nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.", so we know that the baseline method is distant supervised method.
Answer: distant supervised method
True answer: using word2vec to create features that are used as input to the SVM

Validation ID: 81d193672090295e687bc4f4ac1b7a9c76ea35df
Question: What baseline method is used?
From the context "We also utilised two other datasets in English to test the cross-linguality of our approaches. One of them is a movie corpus collected from the web. There are 5,331 positive reviews and 5,331 negative reviews in this corpus. The other is a Twitter dataset, which has nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.", so we know that the baseline method is distant supervised method.
Answer: distant supervised method
True answer: use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach

Validation ID: cf171fad0bea5ab985c53d11e48e7883c23cdc44
Question: What details are given about the Twitter dataset?
From the context "We also utilised two other datasets in English to test the cross-linguality of our approaches. One of them is a movie corpus collected from the web. There are 5,331 positive reviews and 5,331 negative reviews in this corpus. The other is a Twitter dataset, which has nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.", so we know that the Twitter dataset has nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.
Answer: nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.
True answer: Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive.

Validation ID: cf171fad0bea5ab985c53d11e48e7883c23cdc44
Question: What details are given about the Twitter dataset?
From the context "We also utilised two other datasets in English to test the cross-linguality of our approaches. One of them is a movie corpus collected from the web. There are 5,331 positive reviews and 5,331 negative reviews in this corpus. The other is a Twitter dataset, which has nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.", so we know that the Twitter dataset has nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.
Answer: nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.
True answer: one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels

Validation ID: 2a564b092916f2fabbfe893cf13de169945ef2e1
Question: What details are given about the movie domain dataset?
From the context "We utilised two datasets for both Turkish and English to evaluate our methods. For Turkish, as the first dataset, we utilised the movie reviews which are collected from a popular website. The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5. We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, if it is equal to or higher than 4, it is assumed to be positive. We have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.", so we know that the movie domain dataset has 20,244 reviews and the average number of words in reviews is 39.
Answer: 20,244 reviews and the average number of words in reviews is 39
True answer: there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score

Validation ID: 2a564b092916f2fabbfe893cf13de169945ef2e1
Question: What details are given about the movie domain dataset?
From the context "We utilised two datasets for both Turkish and English to evaluate our methods. For Turkish, as the first dataset, we utilised the movie reviews which are collected from a popular website. The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5.", so we know that the movie domain dataset has 20,244 reviews and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5.
Answer: 20,244 reviews and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5.
True answer: The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment.

Validation ID: 0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d
Question: Which hand-crafted features are combined with word2vec?
From the context "As an alternative to multiplying with the supervised polarity scores, we also separately multiplied all the row scores with only +1 if the row word is a positive word, and with -1 if it is a negative word. We have observed it boosts the performance more compared to using raw scores.", so we know that the hand-crafted features are combined with word2vec are +1 if the row word is a positive word, and with -1 if it is a negative word.
Answer: +1 if the row word is a positive word, and with -1 if it is a negative word
True answer: three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores

Validation ID: 0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d
Question: Which hand-crafted features are combined with word2vec?
From the context "As an alternative to multiplying with the supervised polarity scores, we also separately multiplied all the row scores with only +1 if the row word is a positive word, and with -1 if it is a negative word. We have observed it boosts the performance more compared to using raw scores.", so we know that the hand-crafted features are combined with word2vec are +1 if the row word is a positive word, and with -1 if it is a negative word.
Answer: +1 if the row word is a positive word, and with -1 if it is a negative word
True answer: polarity scores, which are minimum, mean, and maximum polarity scores, from each review

Validation ID: 73e83c54251f6a07744413ac8b8bed6480b2294f
Question: What word-based and dictionary-based feature are used?
From the context "We first construct a matrix whose row entries are corpus words and column entries are the words in their dictionary definitions. We followed the boolean approach. For instance, for the word cat, the column words occurring in its dictionary definition are given a score of 1. Those column words not appearing in the definition of cat are assigned a score of 0 for that corresponding row entry.", so we know that the word-based feature is the words in their dictionary definitions.
From the context "We first construct a matrix whose row entries are corpus words and column entries are the words in their dictionary definitions. We followed the boolean approach. For instance, for the word cat, the column words occurring in its dictionary definition are given a score of 1. Those column words not appearing in the definition of cat are assigned a score of 0 for that corresponding row entry.", so we know that the dictionary-based feature is the words in their dictionary definitions.
True answer: generate word embeddings specific to a domain, TDK (Türk Dil Kurumu - “Turkish Language Institution”) dictionary to obtain word polarities

Validation ID: 3355918bbdccac644afe441f085d0ffbbad565d7
Question: How are the supervised scores of the words calculated?
From the context "We extract these scores as follows. For a target word in the corpus, we scan through all of its contexts. In addition to the target word's polarity score (the self score), out of all the polarity scores of words occurring in the same contexts as the target word, minimum, maximum, and average scores are taken into consideration.", so we know that the supervised scores of the words are calculated by taking into account the minimum, maximum, and average scores of the words occurring in the same contexts as the target word.
Answer: by taking into account the minimum, maximum, and average scores of the words occurring in the same contexts as the target word
True answer: (+1 or -1), words of opposite polarities (e.g. “happy" and “unhappy") get far away from each other

Validation ID: e48e750743aef36529fbea4328b8253dbe928b4d
Question: what dataset was used?
From the context "The dev data set BIBREF19 in the competition was small hence, the train and dev sets were merged to perform 10-fold cross validation", so we know that the dataset was dev data set.
Answer: dev data set
True answer: WASSA-2017 Shared Task on Emotion Intensity

Validation ID: c08aab979dcdc8f4fe8ec1337c3c8290ab13414e
Question: how many total combined features were there?
From the context "The final feature vector is the concatenation of all the individual features. For example, we concatenate average word vectors, sum of NRC Affect Intensities, number of positive and negative Bing Liu lexicons, number of negation words and so on to get final feature vector.", so we know that the final feature vector is the concatenation of all the individual features.
Answer: all the individual features
True answer: Fourteen 

Validation ID: c08aab979dcdc8f4fe8ec1337c3c8290ab13414e
Question: how many total combined features were there?
From the context "The final feature vector is the concatenation of all the individual features. For example, we concatenate average word vectors, sum of NRC Affect Intensities, number of positive and negative Bing Liu lexicons, number of negation words and so on to get final feature vector.", so we know that the final feature vector is the concatenation of all the individual features.
Answer: all the individual features
True answer: Unanswerable

Validation ID: 8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f
Question: what pretrained word embeddings were used?
From the context "We focus primarily on the word vector representations (word embeddings) created specifically using the twitter dataset. GloVe BIBREF13 is an unsupervised learning algorithm for obtaining vector representations for words. 200-dimensional GloVe embeddings trained on 2 Billion tweets are integrated. Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15. Since tweets are abundant with emojis, Emoji embeddings BIBREF16 which are learned from the emoji descriptions have been used. Embeddings for each tweet are obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet.", so we know that GloVe embeddings trained on 2 Billion tweets, Edinburgh embeddings and Emoji embeddings are used.
Answer: GloVe embeddings trained on 2 Billion tweets, Edinburgh embeddings and Emoji embeddings
True answer: Pretrained word embeddings  were not used

Validation ID: 8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f
Question: what pretrained word embeddings were used?
From the context "We focus primarily on the word vector representations (word embeddings) created specifically using the twitter dataset. GloVe BIBREF13 is an unsupervised learning algorithm for obtaining vector representations for words. 200-dimensional GloVe embeddings trained on 2 Billion tweets are integrated. Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15. Since tweets are abundant with emojis, Emoji embeddings BIBREF16 which are learned from the emoji descriptions have been used. Embeddings for each tweet are obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet.", so we know that GloVe embeddings trained on 2 Billion tweets, Edinburgh embeddings and Emoji embeddings are used.
Answer: GloVe embeddings trained on 2 Billion tweets, Edinburgh embeddings and Emoji embeddings
True answer: GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16

Validation ID: cc608df2884e1e82679f663ed9d9d67a4b6c03f3
Question: What evaluation metrics did look at?
From the context "We evaluate our model on the VATEX dataset, which is a large-scale video-text dataset. We use BLEU-4, METEOR, CIDEr and ROUGE-L as our evaluation metrics", so we know that the evaluation metrics are BLEU-4, METEOR, CIDEr and ROUGE-L.
Answer: BLEU-4, METEOR, CIDEr and ROUGE-L
True answer: precision, recall, F1 and accuracy

Validation ID: cc608df2884e1e82679f663ed9d9d67a4b6c03f3
Question: What evaluation metrics did look at?
From the context "We evaluate our model on the VATEX dataset, which is a large-scale video-text dataset for video captioning. We use BLEU-4, METEOR, CIDEr and ROUGE-L as our evaluation metrics", so we know that the evaluation metrics are BLEU-4, METEOR, CIDEr and ROUGE-L.
Answer: BLEU-4, METEOR, CIDEr and ROUGE-L
True answer: Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.

Validation ID: 3e432d71512ffbd790a482c716e7079ee78ce732
Question: What datasets are used?
From the context "We utilize the VATEX dataset for video captioning, which contains over 41,250 videos and 825,000 captions in both English and Chinese", so we know that the dataset has over 41,250 videos and 825,000 captions in both English and Chinese.
Answer: over 41,250 videos and 825,000 captions in both English and Chinese
True answer: Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.

Validation ID: 3e432d71512ffbd790a482c716e7079ee78ce732
Question: What datasets are used?
From the context "We utilize the VATEX dataset for video captioning, which contains over 41,250 videos and 825,000 captions in both English and Chinese", so we know that the dataset has over 41,250 videos and 825,000 captions in both English and Chinese.
Answer: over 41,250 videos and 825,000 captions in both English and Chinese
True answer: a self-collected financial intents dataset in Portuguese

Validation ID: dd76130ec5fac477123fe8880472d03fbafddef6
Question: What is the state of the art described in the paper?
From the context "In this section we discuss the state of the art on conversational systems in three perspectives: types of interactions, types of architecture, and types of context reasoning. Then we present a table that consolidates and compares all of them.", so we know that the state of the art described in the paper is types of interactions, types of architecture, and types of context reasoning.
Answer: types of interactions, types of architecture, and types of context reasoning
True answer: ELIZA,  PARRY, A.L.I.C.E., Cleverbot

Validation ID: 43eecc576348411b0634611c81589f618cd4fddf
Question: What GAN models were used as baselines to compare against?
From the context "We compared our model with MLE, RL and GAN baselines. Since COCO and EMNLP2017 WMT don't have input while WeiboDial regards posts as input, we chose the following baselines respectively:", so we know that the GAN models used as baselines to compare against are SeqGAN, LeakGAN, IRL, MaliGAN and ARAML.
Answer: SeqGAN, LeakGAN, IRL, MaliGAN and ARAML
True answer: MLE, SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN

Validation ID: 43eecc576348411b0634611c81589f618cd4fddf
Question: What GAN models were used as baselines to compare against?
From the context "We compared our model with MLE, RL and GAN baselines. Since COCO and EMNLP2017 WMT don't have input while WeiboDial regards posts as input, we chose the following baselines respectively:", so we know that the GAN models used as baselines to compare against are SeqGAN, LeakGAN, IRL, MaliGAN and ARAML.
Answer: SeqGAN, LeakGAN, IRL, MaliGAN and ARAML
True answer: SeqGAN, LeakGAN, MaliGAN, DialogGAN, DPGAN

Validation ID: 79f9468e011670993fd162543d1a4b3dd811ac5d
Question: How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?
From the context "As shown in Table TABREF35, ARAML performs significantly better than other baselines in all the cases. This result indicates that the samples surrounding true responses provide stable rewards for the generator, and stable RAML training paradigm significantly enhances the performance in both metrics.", so we know that ARAML performs significantly better than other baselines in all the cases.
Answer: significantly better
True answer: ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.

Validation ID: 79f9468e011670993fd162543d1a4b3dd811ac5d
Question: How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?
From the context "As shown in Table TABREF35, ARAML performs significantly better than other baselines in all the cases. This result indicates that the samples surrounding true responses provide stable rewards for the generator, and stable RAML training paradigm significantly enhances the performance in both metrics.", so we know that ARAML performs significantly better than other baselines in all the cases.
Answer: significantly better
True answer: Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.

Validation ID: c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf
Question: Is the discriminator's reward made available at each step to the generator?
From the context "The discriminator's reward is made available at each step to the generator", so we know that the discriminator's reward is made available at each step to the generator.
Answer: Yes
True answer: Unanswerable

Validation ID: 902b3123aec0f3a39319ffa9d05ab8e08a2eb567
Question: What is the algorithm used to create word embeddings?
From the context "Participants mostly rely on the models compared in BIBREF0 and apply modifications to improve them. In particular, most teams make use of skip-gram with negative sampling (SGNS) based on BIBREF13 to learn the semantic spaces of the two time periods and orthogonal procrustes (OP) to align these vector spaces, similar to the approach by BIBREF14.", so we know that the algorithm used to create word embeddings is skip-gram with negative sampling (SGNS).
Answer: skip-gram with negative sampling (SGNS)
True answer: Unanswerable

Validation ID: 1038542243efe5ab3e65c89385e53c4831cd9981
Question: What is the corpus used for the task?
From the context "The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. The corpus version used in the task has the following format: "year [tab] lemma1 lemma2 lemma3...".", so we know that the corpus used for the task is DTA18 and DTA19.
Answer: DTA18 and DTA19
True answer: DTA18, DTA19

Validation ID: 1038542243efe5ab3e65c89385e53c4831cd9981
Question: What is the corpus used for the task?
From the context "The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. The corpus version used in the task has the following format: "year [tab] lemma1 lemma2 lemma3...".", so we know that the corpus used for the task is DTA18 and DTA19.
Answer: DTA18 and DTA19
True answer: Diachronic Usage Relatedness (DURel) gold standard data set

Validation ID: e2b0cd30cf56a4b13f96426489367024310c3a05
Question: How is evaluation performed?
From the context "The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used. The higher Spearman's rank-order correlation the better the system's performance.", so we know that the evaluation is performed by comparing the output of a system with the target words in the predicted order to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ is used. The higher Spearman's rank-order correlation the better the system's performance.
Answer: The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used. The higher Spearman's rank-order correlation the better the system's performance.
True answer: As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used

Validation ID: e2b0cd30cf56a4b13f96426489367024310c3a05
Question: How is evaluation performed?
From the context "The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used. The higher Spearman's rank-order correlation the better the system's performance.", so we know that the evaluation is performed by comparing the output of a system with the target words in the predicted order to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ is used. The higher Spearman's rank-order correlation the better the system's performance.
Answer: The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ is used. The higher Spearman's rank-order correlation the better the system's performance.
True answer: Spearman's rank-order correlation

Validation ID: e831041d50f3922265330fcbee5a980d0e2586dd
Question: What is a normal reading paradigm?
From the context "In the first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions.", so we know that a normal reading paradigm is a task that participants were instructed to read the sentences naturally, without any specific task other than comprehension.
Answer: a task that participants were instructed to read the sentences naturally, without any specific task other than comprehension
True answer: read the sentences normally without any special instructions

Validation ID: e831041d50f3922265330fcbee5a980d0e2586dd
Question: What is a normal reading paradigm?
From the context "In the first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions.", so we know that a normal reading paradigm is a task that participants were instructed to read the sentences naturally, without any specific task other than comprehension.
Answer: a task that participants were instructed to read the sentences naturally, without any specific task other than comprehension
True answer: participants were instructed to read the sentences naturally, without any specific task other than comprehension

Validation ID: 7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93
Question: Did they experiment with this new dataset?
From the context "We conducted an extensive validation to ensure the quality of the recorded data", so we know that they conducted an extensive validation to ensure the quality of the recorded data.
Answer: Yes
True answer: No

Validation ID: ac7f6497be4bcca64e75f28934b207c9e8097576
Question: What kind of sentences were read?
From the context "During the recording session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating", so we know that the sentences were selected from the Wikipedia corpus provided by culotta2006integrating.
Answer: the Wikipedia corpus provided by culotta2006integrating
True answer: sentences that were selected from the Wikipedia corpus provided by culotta2006integrating

Validation ID: ac7f6497be4bcca64e75f28934b207c9e8097576
Question: What kind of sentences were read?
From the context "During the recording session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating", so we know that the sentences were selected from the Wikipedia corpus provided by culotta2006integrating.
Answer: the Wikipedia corpus provided by culotta2006integrating
True answer: seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer

Validation ID: 87bb3105e03ed6ac5abfde0a7ca9b8de8985663c
Question: why are their techniques cheaper to implement?
From the context "We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine", so we know that their techniques are cheaper to implement because they do not require the availability of a backward translation engine.
Answer: they do not require the availability of a backward translation engine
True answer: They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper

Validation ID: 87bb3105e03ed6ac5abfde0a7ca9b8de8985663c
Question: why are their techniques cheaper to implement?
From the context "We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine", so we know that their techniques are cheaper to implement because they do not require the availability of a backward translation engine.
Answer: they do not require the availability of a backward translation engine
True answer: They do not require the availability of a backward translation engine.

Validation ID: d9980676a83295dda37c20cfd5d58e574d0a4859
Question: what data simulation techniques were introduced?
From the context "We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine. Setups We use the following cheap ways to generate pseudo-source texts: copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary. copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. In this setup, BIBREF25 ensure that both vocabularies never overlap by marking the target word copies with a special language identifier. Therefore the English word resume cannot be confused with the homographic French word, which is marked @fr@resume. copy-dummies: instead of using actual copies, we replace each word with “dummy” tokens. We use this unrealistic setup to observe the training over noisy and hardly informative source sentences. We then use the procedures described in $ SECREF4, except that the pseudo-source embeddings in the copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining parameters are frozen. This prevents random parameters from hurting the already trained model. GANs can help Results are in Table TABREF32, assuming the same fine-tuning procedure as above. We observe that deep-fusion hardly improves the Europarl results, while we obtain about +0.6 BLEU over the baseline on newstest-2014 for both languages. deep-fusion differs from stupid BT in that the model is not directly optimized on the in-domain data, but uses the LM trained on Europarl to maximize the likelihood of the out-of-domain training data. Therefore, no specific improvement is to be expected in terms of domain adaptation, and the performance increases in the more general domain. Combining deep-fusion and copy-marked + noise + GANs brings slight improvements on the German in-domain test sets, and performance out of domain is also improved.
Answer: copy
True answer: copy, copy-marked, copy-dummies

Validation ID: d9980676a83295dda37c20cfd5d58e574d0a4859
Question: what data simulation techniques were introduced?
From the context "We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine. Setups We use the following cheap ways to generate pseudo-source texts: copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary. copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. In this setup, BIBREF25 ensure that both vocabularies never overlap by marking the target word copies with a special language identifier. Therefore the English word resume cannot be confused with the homographic French word, which is marked @fr@resume. copy-dummies: instead of using actual copies, we replace each word with “dummy” tokens. We use this unrealistic setup to observe the training over noisy and hardly informative source sentences. We then use the procedures described in $ SECREF4, except that the pseudo-source embeddings in the copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining parameters are frozen. This prevents random parameters from hurting the already trained model. GANs can help Results are in Table TABREF32, assuming the same fine-tuning procedure as above. We observe that deep-fusion hardly improves the Europarl results, while we obtain about +0.6 BLEU over the baseline on newstest-2014 for both languages. deep-fusion differs from stupid BT in that the model is not directly optimized on the in-domain data, but uses the LM trained on Europarl to maximize the likelihood of the out-of-domain training data. Therefore, no specific improvement is to be expected in terms of domain adaptation, and the performance increases in the more general domain. Combining deep-fusion and copy-marked + noise + GANs brings slight improvements on the German in-domain test sets, and performance out of domain is also improved.
Answer: copy
True answer: copy, copy-marked, copy-dummies

Validation ID: 9225b651e0fed28d4b6261a9f6b443b52597e401
Question: what is their explanation for the effectiveness of back-translation?
From the context "We observe that backtrans-nmt improves quickly in the earliest updates and then stays horizontal, whereas natural continues improving, even after 400k updates. Therefore BT does not help to avoid overfitting, it actually encourages it, which may be due “easier” training examples (cf. $ SECREF15 ).", so we know that backtrans-nmt improves quickly in the earliest updates and then stays horizontal, whereas natural continues improving, even after 400k updates. Therefore BT does not help to avoid overfitting, it actually encourages it, which may be due “easier” training examples (cf. $ SECREF15 ).
Answer: backtrans-nmt improves quickly in the earliest updates and then stays horizontal, whereas natural continues improving, even after 400k updates. Therefore BT does not help to avoid overfitting, it actually encourages it, which may be due “easier” training examples (cf. $ SECREF15 ).
True answer: when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources

Validation ID: 565189b672efee01d22f4fc6b73cd5287b2ee72c
Question: what dataset is used?
From the context "We have followed BIBREF29 and reimplemented their deep-fusion technique. It requires to first independently learn a RNN-LM on the in-domain target data with a cross-entropy objective; then to train the optimal combination of the translation and the language models by adding the hidden state of the RNN-LM as an additional input to the softmax layer of the decoder. Our RNN-LMs are trained using dl4mt with the target side of the parallel data and the Europarl corpus (about 6M sentences for both French and German), using a one-layer GRU with the same dimension as the MT decoder (1024).", so we know that the dataset is the Europarl corpus (about 6M sentences for both French and German).
Answer: the Europarl corpus (about 6M sentences for both French and German)
True answer: Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)

Validation ID: 565189b672efee01d22f4fc6b73cd5287b2ee72c
Question: what dataset is used?
From the context "We have followed BIBREF29 and reimplemented their deep-fusion technique. It requires to first independently learn a RNN-LM on the in-domain target data with a cross-entropy objective; then to train the optimal combination of the translation and the language models by adding the hidden state of the RNN-LM as an additional input to the softmax layer of the decoder. Our RNN-LMs are trained using dl4mt with the target side of the parallel data and the Europarl corpus (about 6M sentences for both French and German), using a one-layer GRU with the same dimension as the MT decoder (1024).", so we know that the dataset is the Europarl corpus (about 6M sentences for both French and German).
Answer: the Europarl corpus (about 6M sentences for both French and German)
True answer: Europarl tests from 2006, 2007, 2008; WMT newstest 2014.

Validation ID: b6f7fadaa1bb828530c2d6780289f12740229d84
Question: what language pairs are explored?
From the context "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French", so we know that the language pairs explored are English INLINEFORM0 German and English INLINEFORM1 French.
Answer: English INLINEFORM0 German and English INLINEFORM1 French
True answer: English-German, English-French.

Validation ID: b6f7fadaa1bb828530c2d6780289f12740229d84
Question: what language pairs are explored?
From the context "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French", so we know that the language pairs explored are English INLINEFORM0 German and English INLINEFORM1 French.
Answer: English INLINEFORM0 German and English INLINEFORM1 French
True answer: English-German, English-French

Validation ID: 7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613
Question: what language is the data in?
From the context "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French", so we know that the data is in English, German and French.
Answer: English, German and French
True answer: English , German, French

Validation ID: 4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f
Question: Does the experiments focus on a specific domain?
From the context "We obtained our Quora dataset BIBREF7 through web-based crawls between June 2014 to August 2014. This crawling exercise has resulted in the accumulation of a massive Q&A dataset spanning over a period of over four years starting from January 2010 to May 2014. We initiated crawling with 100 questions randomly selected from different topics so that different genre of questions can be covered. The crawling of the questions follow a BFS pattern through the related question links. We obtained 822,040 unique questions across 80,253 different topics with a total of 1,833,125 answers to these questions. For each question, we separately crawl their revision logs that contain different types of edit information for the question and the activity log of the question asker.", so we know that the experiments focus on a specific domain.
Answer: Yes
True answer: No

Validation ID: 4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f
Question: Does the experiments focus on a specific domain?
From the context "We obtained our Quora dataset BIBREF7 through web-based crawls between June 2014 to August 2014. This crawling exercise has resulted in the accumulation of a massive Q&A dataset spanning over a period of over four years starting from January 2010 to May 2014. We initiated crawling with 100 questions randomly selected from different topics so that different genre of questions can be covered. The crawling of the questions follow a BFS pattern through the related question links. We obtained 822,040 unique questions across 80,253 different topics with a total of 1,833,125 answers to these questions. For each question, we separately crawl their revision logs that contain different types of edit information for the question and the activity log of the question asker.", so we know that the experiments focus on a specific domain.
Answer: Yes
True answer: No

Validation ID: 6c96e910bd98c9fd58ba2050f99b9c9bac69840a
Question: how many training samples do you have for training?
From the context "We randomly select 80% of the questions for training and the rest for testing", so we know that we have 80% of the questions for training.
Answer: 80% of the questions
True answer: Unanswerable

Validation ID: 6c96e910bd98c9fd58ba2050f99b9c9bac69840a
Question: how many training samples do you have for training?
From the context "We randomly select 80% of the questions for training and the rest for testing", so we know that we have 80% of the questions for training.
Answer: 80% of the questions
True answer: Unanswerable

Validation ID: 9af3142630b350c93875441e1e1767312df76d17
Question: Do the answered questions measure for the usefulness of the answer?
From the context "The content and way of posing a question is important to attract answers. We have observed in the previous section that these linguistic as well as psycholinguistic aspects of the question asker are discriminatory factors. For the prediction, we use the following features:", so we know that the answered questions measure for the usefulness of the answer.
Answer: the answered questions measure for the usefulness of the answer
True answer: No

Validation ID: e374169ee10f835f660ab8403a5701114586f167
Question: What profile metadata is used for this analysis?
From the context "We utilize a regular expression query to exhaustively search all the variants of the word Chowkidar in our dataset. We found $2,60,607$ instances of users in set $S$ added Chowkidar (or its variants) to their display names. These $2,60,607$ instances comprised a total of 241 unique chowkidar variants of which 225 have been used lesser than 500 times. We also perform the same analysis for the other profile attributes like description and username as well. The number of users who added Chowkidar to their description and username are $14,409$ and $12,651$ respectively. The union and intersection of all these users are $270,945$ and 727 respectively, implying that most users added Chowkidar to only one of their profile attributes.", so we know that the profile metadata is display name, description and username.
Answer: display name, description and username
True answer: username, display name, profile image, location, description

Validation ID: e374169ee10f835f660ab8403a5701114586f167
Question: What profile metadata is used for this analysis?
From the context "We utilize a regular expression query to exhaustively search all the variants of the word Chowkidar in our dataset. We found $2,60,607$ instances of users in set $S$ added Chowkidar (or its variants) to their display names. These $2,60,607$ instances comprised a total of 241 unique chowkidar variants of which 225 have been used lesser than 500 times. We also perform the same analysis for the other profile attributes like description and username as well. The number of users who added Chowkidar to their description and username are $14,409$ and $12,651$ respectively. The union and intersection of all these users are $270,945$ and 727 respectively, implying that most users added Chowkidar to only one of their profile attributes.", so we know that the profile metadata is display name, description and username.
Answer: display name, description and username
True answer: username, display name, profile image, location and description

Validation ID: 82595ca5d11e541ed0c3353b41e8698af40a479b
Question: What are the organic and inorganic ways to show political affiliation through profile changes?
From the context "We believe, the effect of changing the profile attribute in accordance with Prime Minister's campaign is an example of inorganic behavior contagion BIBREF6, BIBREF9. The authors in BIBREF6 argue that opinion diffuses easily in a network if it comes from opinion leaders who are considered to be users with a very high number of followers. We see a similar behavior contagion in our dataset with respect to the Chowkidar movement.", so we know that the organic ways to show political affiliation through profile changes are:
1. mention of political parties names in the profile attributes of users
True answer: Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.
Inorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile.

Validation ID: 82595ca5d11e541ed0c3353b41e8698af40a479b
Question: What are the organic and inorganic ways to show political affiliation through profile changes?
From the context "We analyze the similarity of the display names with respect to the behavior contagion of Chowkidar movement. In the CDF plot of Figure FIGREF18, a significant spike is observed in the region of LCS values between $0.6$-$0.8$. This spike is caused mostly due to the political handles, who added Chowkidar to their display names which accounted for $95.7\%$ of the users in this region. In set $P$, a total of 373 people added the specific keyword Chowkidar to their display names of which 315 lie in the normalized LCS range of $0.6$-$0.8$.", so we know that the organic ways to show political affiliation through profile changes are display name and description, and the inorganic ways are username and profile image.
Answer: display name and description, username and profile image
True answer: Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way.

Validation ID: d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f
Question: How do profile changes vary for influential leads and their followers over the social movement?
From the context "We analyze the similarity of the display names with respect to the behavior contagion of Chowkidar movement. In the CDF plot of Figure FIGREF18, a significant spike is observed in the region of LCS values between $0.6$-$0.8$. This spike is caused mostly due to the political handles, who added Chowkidar to their display names which accounted for $95.7\%$ of the users in this region. In set $P$, a total of 373 people added the specific keyword Chowkidar to their display names of which 315 lie in the normalized LCS range of $0.6$-$0.8$.", so we know that the political handles, who added Chowkidar to their display names which accounted for $95.7\%$ of the users in this region.
Answer: $95.7\%$
True answer: Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.

Validation ID: 53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e
Question: What evaluation metrics do they use?
From the context "As described in the question-answer writing process, the answers in our dataset are different from those in some existing extractive datasets. Thus we consider the task of answer generation for TweetQA and we use several standard metrics for natural language generation to evaluate QA systems on our dataset, namely we consider BLEU-1 BIBREF16, Meteor BIBREF17 and Rouge-L BIBREF18 in this paper.", so we know that they use BLEU-1, Meteor and Rouge-L to evaluate QA systems on their dataset.
Answer: BLEU-1, Meteor and Rouge-L
True answer: BLEU-1, Meteor , Rouge-L 

Validation ID: 53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e
Question: What evaluation metrics do they use?
From the context "As described in the question-answer writing process, the answers in our dataset are different from those in some existing extractive datasets. Thus we consider the task of answer generation for TweetQA and we use several standard metrics for natural language generation to evaluate QA systems on our dataset, namely we consider BLEU-1 BIBREF16, Meteor BIBREF17 and Rouge-L BIBREF18 in this paper.", so we know that they use BLEU-1, Meteor and Rouge-L to evaluate QA systems on their dataset.
Answer: BLEU-1, Meteor and Rouge-L
True answer: BLEU-1, Meteor ,  Rouge-L 

Validation ID: 869feb7f47606105005efdb6bea1c549824baea0
Question: What is the size of this dataset?
From the context "After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed documentation on how we build our dataset. Also note that since we keep the original news article and news titles for each tweet, our dataset can also be used to explore more challenging generation tasks. Table TABREF19 shows the statistics of our current collection, and the frequency of different types of questions is shown in Table TABREF21. All QA pairs were written by 492 individual workers.", so we know that the dataset has 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.
Answer: 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs
True answer: 13,757

Validation ID: 869feb7f47606105005efdb6bea1c549824baea0
Question: What is the size of this dataset?
From the context "After we retrieve the QA pairs from all HITs, we conduct further post-filtering to filter out the pairs from workers that obviously do not follow instructions. We remove QA pairs with yes/no answers. Questions with less than five words are also filtered out. This process filtered INLINEFORM0 of the QA pairs. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs. The collected QA pairs will be directly available to the public, and we will provide a script to download the original tweets and detailed documentation on how we build our dataset. Also note that since we keep the original news article and news titles for each tweet, our dataset can also be used to explore more challenging generation tasks. Table TABREF19 shows the statistics of our current collection, and the frequency of different types of questions is shown in Table TABREF21. All QA pairs were written by 492 individual workers.", so we know that the dataset has 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.
Answer: 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs
True answer: 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs

Validation ID: c497e8701060583d91bb64b9f9202d40047effc4
Question: How do they determine if tweets have been used by journalists?
From the context "To obtain such relevant tweets, we crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets", so we know that they crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets.
Answer: They crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets.
True answer:  we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles

Validation ID: 8060a773f6a136944f7b59758d08cc6f2a59693b
Question: how small of a dataset did they train on?
From the context "For a certain specific scenario, the model trained with the data recorded from it has better adaptation than the model trained with generic scenario. But it spends too much time training a model from the beginning, if there is a well-trained model for generic scenarios. Moreover, labeling a large quantity of training data in new scenario is both costly and time consuming. If a model transfer trained with smaller dataset can obtained the similar recognition accuracy compared with the model directly trained with larger dataset, it is no doubt that transfer learning is more practical.", so we know that they trained on a smaller dataset.
Answer: smaller dataset
True answer: 1000 hours data

Validation ID: 8060a773f6a136944f7b59758d08cc6f2a59693b
Question: how small of a dataset did they train on?
From the context "For a certain specific scenario, the model trained with the data recorded from it has better adaptation than the model trained with generic scenario. But it spends too much time training a model from the beginning, if there is a well-trained model for generic scenarios. Moreover, labeling a large quantity of training data in new scenario is both costly and time consuming.", so we know that they trained on a small dataset.
Answer: a small dataset
True answer: 23085 hours of data

Validation ID: 1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306
Question: what was their character error rate?
From the context "The performance metric of the system in Mandarin is reported with character error rate (CER)", so we know that the character error rate is the performance metric of the system in Mandarin.
Answer: character error rate
True answer: 2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.

Validation ID: 1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306
Question: what was their character error rate?
From the context "The performance metric of the system in Mandarin is reported with character error rate (CER)", so we know that the character error rate is the performance metric of the system in Mandarin.
Answer: character error rate
True answer: Their best model achieved a 2.49% Character Error Rate.

Validation ID: c0af8b7bf52dc15e0b33704822c4a34077e09cd1
Question: which lstm models did they compare with?
From the context "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions BIBREF26, BIBREF23", so we know that they compared with conventional RNNs and deep LSTM RNNs.
Answer: conventional RNNs and deep LSTM RNNs
True answer: Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.

Validation ID: 9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e
Question: Do they use datasets with transcribed text or do they determine text from the audio?
From the context "We assume that speech transcripts can be extracted from audio signals with high accuracy, given the advancement of ASR technologies BIBREF7", so we know that they determine text from the audio.
Answer: determine text from the audio
True answer: They use text transcription.

Validation ID: 9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e
Question: Do they use datasets with transcribed text or do they determine text from the audio?
From the context "We assume that speech transcripts can be extracted from audio signals with high accuracy, given the advancement of ASR technologies BIBREF7", so we know that they determine text from the audio.
Answer: determine text from the audio
True answer: both

Validation ID: e0122fc7b0143d5cbcda2120be87a012fb987627
Question: By how much does their model outperform the state of the art results?
From the context "Extensive experiments show that our proposed model outperforms other state-of-the-art methods in classifying the four emotion categories, and accuracies ranging from 68.8% to 71.8% are obtained when the model is applied to the IEMOCAP dataset", so we know that their model outperforms the state of the art results by 68.8% to 71.8%.
Answer: 68.8% to 71.8%
True answer: the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)

Validation ID: 5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4
Question: How do they combine audio and text sequences in their RNN?
From the context "In this paper, we propose a novel multimodal dual recurrent encoder model that simultaneously utilizes text data, as well as audio signals, to permit the better understanding of speech data. Our model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class.", so we know that they combine audio and text sequences in their RNN using a feed-forward neural model.
Answer: feed-forward neural model
True answer: combines the information from these sources using a feed-forward neural model

Validation ID: 5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4
Question: How do they combine audio and text sequences in their RNN?
From the context "In this paper, we propose a novel multimodal dual recurrent encoder model that simultaneously utilizes text data, as well as audio signals, to permit the better understanding of speech data. Our model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class.", so we know that they combine audio and text sequences in their RNN using a feed-forward neural model.
Answer: feed-forward neural model
True answer: encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model

Validation ID: 37edc25e39515ffc2d92115d2fcd9e6ceb18898b
Question: What was the baseline?
From the context "The upper part of the Table summarizes the performance of the baselines. The entry “Balikas et al.” stands for the winning system of the 2016 edition of the challenge BIBREF2, which to the best of our knowledge holds the state-of-the-art", so we know that the baseline is the winning system of the 2016 edition of the challenge BIBREF2.
Answer: the winning system of the 2016 edition of the challenge BIBREF2
True answer: SVMs, LR, BIBREF2

Validation ID: 37edc25e39515ffc2d92115d2fcd9e6ceb18898b
Question: What was the baseline?
From the context "The upper part of the Table summarizes the performance of the baselines. The entry “Balikas et al.” stands for the winning system of the 2016 edition of the challenge BIBREF2, which to the best of our knowledge holds the state-of-the-art", so we know that the baseline is the winning system of the 2016 edition of the challenge BIBREF2.
Answer: the winning system of the 2016 edition of the challenge BIBREF2
True answer: SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt

Validation ID: e431661f17347607c3d3d9764928385a8f3d9650
Question: By how much did they improve?
From the context "In this paper, we showed that by jointly learning the tasks of ternary and fine-grained classification with a multitask learning model, one can greatly improve the performance on the second", so we know that they improved greatly.
Answer: greatly
True answer: They decrease MAE in 0.34

Validation ID: 876700622bd6811d903e65314ac75971bbe23dcc
Question: What dataset did they use?
From the context "Our goal is to demonstrate how multitask learning can be successfully applied on the task of sentiment classification of tweets. The particularities of tweets are to be short and informal text spans. The common use of abbreviations, creative language etc., makes the sentiment classification problem challenging. To validate our hypothesis, that learning the tasks jointly can benefit the performance, we propose an experimental setting where there are data from two different twitter sentiment classification problems: a fine-grained and a ternary. We consider the fine-grained task to be our primary task as it is more challenging and obtaining bigger datasets, e.g. by distant supervision, is not straightforward and, hence we report the performance achieved for this task.", so we know that they use two different twitter sentiment classification problems: a fine-grained and a ternary.
Answer: two different twitter sentiment classification problems: a fine-grained and a ternary
True answer:  high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task

Validation ID: 876700622bd6811d903e65314ac75971bbe23dcc
Question: What dataset did they use?
From the context "Our goal is to demonstrate how multitask learning can be successfully applied on the task of sentiment classification of tweets. The particularities of tweets are to be short and informal text spans. The common use of abbreviations, creative language etc., makes the sentiment classification problem challenging. To validate our hypothesis, that learning the tasks jointly can benefit the performance, we propose an experimental setting where there are data from two different twitter sentiment classification problems: a fine-grained and a ternary. We consider the fine-grained task to be our primary task as it is more challenging and obtaining bigger datasets, e.g. by distant supervision, is not straightforward and, hence we report the performance achieved for this task.", so we know that they use two different twitter sentiment classification problems: a fine-grained and a ternary.
Answer: two different twitter sentiment classification problems: a fine-grained and a ternary
True answer:  SemEval-2016 “Sentiment Analysis in Twitter”

Validation ID: 312e9cc11b9036a6324bdcb64eca6814053ffa17
!!! Has error during processing !!!
Question: What is the reported agreement for the annotation?
Answer: From the context "The inter-annotator agreement was measured using Cohen's kappa coefficient, which is a statistical measure of inter-rater agreement for categorical items. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than
True answer: Unanswerable

Validation ID: 312e9cc11b9036a6324bdcb64eca6814053ffa17
!!! Has error during processing !!!
Question: What is the reported agreement for the annotation?
Answer: From the context "The inter-annotator agreement was measured using Cohen's kappa coefficient, which is a statistical measure of inter-rater agreement for categorical items. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than
True answer: Unanswerable

Validation ID: 1c0ba6958da09411deded4a14dfea5be55687619
Question: How many annotators participated?
From the context "We assembled a dataset of 1,949 cleaned pathology reports. Each report is associated with one of the 37 different primary diagnoses based on IDC-O codes. The reports are collected from four different body parts or primary sites from multiple patients.", so we know that the reports are collected from multiple patients.
Answer: multiple patients
True answer: Unanswerable

Validation ID: 1c0ba6958da09411deded4a14dfea5be55687619
Question: How many annotators participated?
From the context "We assembled a dataset of 1,949 cleaned pathology reports. Each report is associated with one of the 37 different primary diagnoses based on IDC-O codes. The reports are collected from four different body parts or primary sites from multiple patients", so we know that the reports are collected from four different body parts or primary sites from multiple patients.
Answer: multiple patients
True answer: Unanswerable

Validation ID: 1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b
Question: What features are used?
From the context "We used existing classification model and TF-IDF features to predict the primary diagnosis", so we know that TF-IDF features are used.
Answer: TF-IDF features
True answer: Unanswerable

Validation ID: d915b401bb96c9f104a0353bef9254672e6f5a47
Question: What future possible improvements are listed?
From the context "We report experiments on the RotoWire benchmark BIBREF10 which contains around $5K$ statistical tables of NBA basketball games paired with human-written descriptions. Our model is compared to several state-of-the-art models. Results show that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics.", so we know that the future possible improvements are:
1. The proposed architecture outperforms previous models on BLEU score.
True answer: rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions

Validation ID: d915b401bb96c9f104a0353bef9254672e6f5a47
Question: What future possible improvements are listed?
From the context "We conclude by discussing possible improvements to our model", so we know that the future possible improvements are listed in the last paragraph.
Answer: the last paragraph
True answer: to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions

Validation ID: 79a44a68bb57b375d8a57a0a7f522d33476d9f33
Question: Which qualitative metric are used for evaluation?
From the context "We evaluate our model through two types of metrics. The BLEU score BIBREF34 is commonly used as an evaluation metric in text generation tasks. It estimates the correspondence between a machine output and that of a human by computing the number of co-occurrences for ngrams ($n \in {1, 2, 3, 4}$) between the generated candidate and the ground truth. We use the implementation code released by BIBREF35.
Experimental setup ::: Evaluation metrics ::: Information extraction-oriented metrics.
True answer:  Relation Generation (RG) , Content Selection (CS),  Content Ordering (CO)

Validation ID: 79a44a68bb57b375d8a57a0a7f522d33476d9f33
Question: Which qualitative metric are used for evaluation?
From the context "We evaluate our model through two types of metrics. The BLEU score BIBREF34 is commonly used as an evaluation metric in text generation tasks. It estimates the correspondence between a machine output and that of a human by computing the number of co-occurrences for ngrams ($n \in {1, 2, 3, 4}$) between the generated candidate and the ground truth. We use the implementation code released by BIBREF35.
Experimental setup ::: Evaluation metrics ::: Information extraction-oriented metrics.
True answer: Relation Generation (RG), Content Selection (CS), Content Ordering (CO)

Validation ID: 664db503509b8236bc4d3dc39cebb74498365750
Question: What is quantitative improvement of proposed method (the best variant) w.r.t. baseline (the best variant)?
From the context "We report experiments on the RotoWire benchmark BIBREF10 which contains around $5K$ statistical tables of NBA basketball games paired with human-written descriptions. Our model is compared to several state-of-the-art models. Results show that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics.", so we know that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics.
Answer: outperforms previous models on BLEU score and is generally better on qualitative metrics
True answer: Hierarchical-k

Validation ID: 64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8
Question: How is "propaganda" defined for the purposes of this study?
From the context "The term `propaganda' derives from propagare in post-classical Latin, as in “propagation of the faith" BIBREF1, and thus has from the beginning been associated with an intentional and potentially multicast communication; only later did it become a pejorative term. It was pragmatically defined in the World War II era as “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends" BIBREF2.", so we know that propaganda is defined as "the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends".
Answer: the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends
True answer: an intentional and potentially multicast communication, “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends"

Validation ID: 64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8
Question: How is "propaganda" defined for the purposes of this study?
From the context "The term `propaganda' derives from propagare in post-classical Latin, as in “propagation of the faith" BIBREF1, and thus has from the beginning been associated with an intentional and potentially multicast communication; only later did it become a pejorative term. It was pragmatically defined in the World War II era as “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends" BIBREF2.", so we know that propaganda is defined as "the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends".
Answer: the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends
True answer: First, propaganda is a social phenomenon and takes place as an act of communication BIBREF19, and so it is more than a simple information-theoretic message of zeros and ones—it also incorporates an addresser and addressee(s), each in phatic contact (typically via broadcast media), ideally with a shared denotational code and contextual surround(s) BIBREF20.

Validation ID: b0a18628289146472aa42f992d0db85c200ec64b
Question: What metrics are used in evaluation?
From the context "We note this gives us access to the articles in the development and test sets but not their labels. We participated in this task under the team name ProperGander and were placed 2nd on the sentence level classification task where we make use of our methods of incorporating cost-sensitivity into BERT. We also participated in the fragment level task and were placed 7th. The significant contributions of this work are: We show that common (`easy') methods of data augmentation for dealing with class imbalance do not improve base BERT performance. We provide a statistical method of establishing the similarity of datasets. We incorporate cost-sensitivity into BERT to enable models to adapt to dissimilar datasets. We release all our program code on GitHub and Google Colaboratory, so that other researchers can benefit from this work.", so we know that the metrics used in evaluation are sentence level classification task and fragment level task.
Answer: sentence level classification task and fragment level task
True answer: precision, recall , F1 score

Validation ID: 72ce05546c81ada05885026470f4c8c218805055
Question: Which natural language(s) are studied in this paper?
From the context "The challenges of imbalanced classification—in which the proportion of elements in each class for a classification task significantly differ—and of the ability to generalise on dissimilar data have remained important problems in Natural Language Processing (NLP) and Machine Learning in general", so we know that the natural language(s) studied in this paper is NLP.
Answer: NLP
True answer: Unanswerable

Validation ID: 72ce05546c81ada05885026470f4c8c218805055
Question: Which natural language(s) are studied in this paper?
From the context "The challenges of imbalanced classification—in which the proportion of elements in each class for a classification task significantly differ—and of the ability to generalise on dissimilar data have remained important problems in Natural Language Processing (NLP) and Machine Learning in general", so we know that the natural language(s) studied in this paper is/are NLP.
Answer: NLP
True answer: English

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
From the context "We evaluate only on recipes not in the training set. We observe that personalized models make more diverse recipes than baseline. They thus perform better in BLEU-1 with more key entities (ingredient mentions) present, but worse in BLEU-4, as these recipes are written in a personalized way and deviate from gold on the phrasal level.", so we know that BLEU-1 and BLEU-4 are used for evaluation.
Answer: BLEU-1 and BLEU-4
True answer: Byte-Pair Encoding perplexity  (BPE PPL),
BLEU-1,
BLEU-4,
ROUGE-L,
percentage of distinct unigram (D-1),
percentage of distinct bigrams(D-2),
user matching accuracy(UMA),
Mean Reciprocal Rank(MRR)
Pairwise preference over baseline(PP)

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
From the context "We evaluate only on recipes not in the training set. We observe that personalized models make more diverse recipes than baseline. They thus perform better in BLEU-1 with more key entities (ingredient mentions) present, but worse in BLEU-4, as these recipes are written in a personalized way and deviate from gold on the phrasal level.", so we know that BLEU-1 and BLEU-4 are used for evaluation.
Answer: BLEU-1 and BLEU-4
True answer: BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
From the context "We evaluate only on recipes not in the training set. We observe that personalized models make more diverse recipes than baseline. They thus perform better in BLEU-1 with more key entities (ingredient mentions) present, but worse in BLEU-4, as these recipes are written in a personalized way and deviate from gold on the phrasal level.", so we know that BLEU-1 and BLEU-4 are used for evaluation.
Answer: BLEU-1 and BLEU-4
True answer:  Distinct-1/2, UMA = User Matching Accuracy, MRR
= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that the recipes are written in English.
Answer: English
True answer: English

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that the recipes are written in English.
Answer: English
True answer: English

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that the recipes are written in English.
Answer: English
True answer: Unanswerable

Validation ID: 9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa
Question: What were their results on the new dataset?
From the context "We show that our personalized generative models can generate plausible, personalized, and coherent recipes preferred by human evaluators for consumption", so we know that their results on the new dataset are plausible, personalized, and coherent recipes preferred by human evaluators for consumption.
Answer: plausible, personalized, and coherent recipes preferred by human evaluators for consumption
True answer: average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time

Validation ID: 34dc0838632d643f33c8dbfe7bd4b656586582a2
Question: What are the baseline models?
From the context "We compare our personalized models against two baselines. The first is a name-based Nearest-Neighbor model (NN). We initially adapted the Neural Checklist Model of BIBREF0 as a baseline; however, we ultimately use a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity.", so we know that the baseline models are name-based Nearest-Neighbor model (NN) and Encoder-Decoder baseline with ingredient attention (Enc-Dec).
Answer: name-based Nearest-Neighbor model (NN) and Encoder-Decoder baseline with ingredient attention (Enc-Dec)
True answer: name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)

Validation ID: c77359fb9d3ef96965a9af0396b101f82a0a9de6
Question: How did they obtain the interactions?
From the context "Our raw data consists of 270K recipes and 1.4M user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 2000 to December 2018)", so we know that they scraped the interactions from Food.com.
Answer: scraped from Food.com
True answer: from Food.com

Validation ID: 1bdc990c7e948724ab04e70867675a334fdd3051
Question: Where do they get the recipes from?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that they get the recipes from Food.com.
Answer: Food.com
True answer: from Food.com

Validation ID: 78536da059b884d6ad04680baeb894895458055c
Question: What were the baselines?
From the context "We use the methodology by Khandelwal and Sawant (BIBREF12), and modify it to support experimentation with multiple models", so we know that the baselines are Khandelwal and Sawant (BIBREF12).
Answer: Khandelwal and Sawant (BIBREF12)
True answer: varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)

Validation ID: 96b07373756d7854bccc3c12e8d41454ab8741f5
Question: Does RoBERTa outperform BERT?
From the context "Contrary to popular opinion, we observe that XLNet is better than RoBERTa for the cue detection and scope resolution tasks", so we know that RoBERTa does not outperform BERT.
Answer: No
True answer: Unanswerable

Validation ID: 96b07373756d7854bccc3c12e8d41454ab8741f5
Question: Does RoBERTa outperform BERT?
From the context "Contrary to popular opinion, we observe that XLNet is better than RoBERTa for the cue detection and scope resolution tasks", so we know that RoBERTa does not outperform BERT.
Answer: No
True answer: No

Validation ID: 511517efc96edcd3e91e7783821c9d6d5a6562af
Question: Which multiple datasets did they train on during joint training?
From the context "While training on multiple datasets, we observed a gain of 10.6 F1 points on BF and 1.94 F1 points on BA on the speculation detection task and 2.16 F1 points on BF and 0.25 F1 points on SFU on the scope resolution task over the single dataset training approach", so we know that they trained on BF and BA datasets during joint training.
Answer: BF and BA datasets
True answer: BF, BA, SFU and Sherlock

Validation ID: 511517efc96edcd3e91e7783821c9d6d5a6562af
Question: Which multiple datasets did they train on during joint training?
From the context "While training on multiple datasets, we observed a gain of 10.6 F1 points on BF and 1.94 F1 points on BA on the speculation detection task and 2.16 F1 points on BF and 0.25 F1 points on SFU on the scope resolution task over the single dataset training approach", so we know that they trained on BF and BA datasets during joint training.
Answer: BF and BA datasets
True answer: BioScope Abstracts, SFU, and BioScope Full Papers

Validation ID: 9122de265577e8f6b5160cd7d28be9e22da752b2
Question: What were the previously reported results?
From the context "We use a default train-validation-test split of 70-15-15 for each dataset, and use all 4 datasets (BF, BA, SFU and Sherlock). The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance.", so we know that the previously reported results are taken from BIBREF12.
Answer: BIBREF12
True answer: Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution

Validation ID: e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d
Question: What is the size of SFU Review corpus?
From the context "We also use the SFU Review Corpus (BIBREF2), which is referred to as SFU.", so we know that the size of SFU Review corpus is SFU.
Answer: SFU
True answer: Unanswerable

Validation ID: e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d
Question: What is the size of SFU Review corpus?
From the context "We also use the SFU Review Corpus (BIBREF2), which is referred to as SFU.", so we know that the size of SFU Review corpus is SFU.
Answer: SFU
True answer: Unanswerable

Validation ID: 45be665a4504f0c7f458cf3f75a95d5a75eefd42
Question: What is the size of bioScope corpus?
From the context "We use the sub corpora of the BioScope Corpus (BIBREF1), namely the BioScope Abstracts sub corpora, which is referred to as BA, and the BioScope Full Papers sub corpora, which is referred to as BF. We also use the SFU Review Corpus (BIBREF2), which is referred to as SFU.", so we know that the size of bioScope corpus is BA and BF.
Answer: BA and BF
True answer: Unanswerable

Validation ID: 45be665a4504f0c7f458cf3f75a95d5a75eefd42
Question: What is the size of bioScope corpus?
From the context "We use the sub corpora of the BioScope Corpus (BIBREF1), namely the BioScope Abstracts sub corpora, which is referred to as BA, and the BioScope Full Papers sub corpora, which is referred to as BF. We also use the SFU Review Corpus (BIBREF2), which is referred to as SFU.", so we know that the size of bioScope corpus is BA and BF.
Answer: BA and BF
True answer: Unanswerable

Validation ID: 22b740cc3c8598247ee102279f96575bdb10d53f
Question: Do they study numerical properties of their obtained vectors (such as orthogonality)?
From the context "We formalize composition as a function INLINEFORM0 that maps INLINEFORM1 -dimensional vector representations of phrase constituents INLINEFORM2 to an INLINEFORM3 -dimensional vector representation of the phrase, i.e., the composed representation. A phrase is defined as any contiguous sequence of words of length 2 or greater, and does not have to adhere to constituents in a phrase structure grammar. This definition is in line with our MT application and ignores “gappy” noncontiguous phrases, but this pragmatic choice does exclude many verb-object relations BIBREF13. We assume the existence of word-level vector representations for every word in our vocabulary of size INLINEFORM4. Compositionality is modeled as a bilinear map, and two classes of linear models with different levels of parametrization are proposed. Unlike previous work BIBREF6, BIBREF7, BIBREF14 where the functions are word-specific, our compositional functions operate on part-of-speech (POS) tag pairs, which facilitates learning by drastically reducing the number of parameters, and only requires a shallow syntactic parse of the input.", so we know that they study numerical properties of their obtained vectors (such as orthogonality).
Answer: Yes
True answer: No

Validation ID: 22b740cc3c8598247ee102279f96575bdb10d53f
Question: Do they study numerical properties of their obtained vectors (such as orthogonality)?
From the context "We formalize composition as a function INLINEFORM0 that maps INLINEFORM1 -dimensional vector representations of phrase constituents INLINEFORM2 to an INLINEFORM3 -dimensional vector representation of the phrase, i.e., the composed representation. A phrase is defined as any contiguous sequence of words of length 2 or greater, and does not have to adhere to constituents in a phrase structure grammar. This definition is in line with our MT application and ignores “gappy” noncontiguous phrases, but this pragmatic choice does exclude many verb-object relations BIBREF13. We assume the existence of word-level vector representations for every word in our vocabulary of size INLINEFORM4. Compositionality is modeled as a bilinear map, and two classes of linear models with different levels of parametrization are proposed. Unlike previous work BIBREF6, BIBREF7, BIBREF14 where the functions are word-specific, our compositional functions operate on part-of-speech (POS) tag pairs, which facilitates learning by drastically reducing the number of parameters, and only requires a shallow syntactic parse of the input.", so we know that they study numerical properties of their obtained vectors (such as orthogonality).
Answer: Yes
True answer: No

Validation ID: 74b4779de437c697fe702e51f23e2b0538b0f631
Question: How do they score phrasal compositionality?
From the context "For the context-independent model, we select the hyperparameter INLINEFORM0 in Eq. EQREF20 from the values INLINEFORM1. For the context-dependent model, we vary the context window size INLINEFORM2 by selecting from the values INLINEFORM3. Table TABREF37 presents Spearman's INLINEFORM4 for these setups. In all cases, the context-dependent models outperform the context-independent ones, and using a relatively simple token-based model we can approximately match the performance of the Bayesian model proposed by Hermann2012. The concatenation models are also consistently better than the additive compositional model, indicating the benefit of learning the compositional parameters via PPDB.", so we know that they score phrasal compositionality by computing the likelihoods of surrounding context words given a phrase representation.
Answer: by computing the likelihoods of surrounding context words given a phrase representation
True answer: Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators

Validation ID: 435570723b37ee1f5898c1a34ef86a0b2e8701bb
Question: Which translation systems do they compare against?
From the context "We added the compositionality score as an additional feature, and also added two binary-valued features: the first indicates if the given translation rule has not been decorated with a compositionality score (either because it consists of non-terminals only or the lexical", so we know that they compare against translation systems with compositionality score and without compositionality score.
Answer: translation systems with compositionality score and without compositionality score
True answer: hierarchical phrase-based system BIBREF29, appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn)

Validation ID: 435570723b37ee1f5898c1a34ef86a0b2e8701bb
Question: Which translation systems do they compare against?
From the context "We added the compositionality score as an additional feature, and also added two binary-valued features: the first indicates if the given translation rule has not been decorated with a compositionality score (either because it consists of non-terminals only or the lexical", so we know that they compare against translation systems with non-terminals only or the lexical.
Answer: translation systems with non-terminals only or the lexical
True answer:  English-Spanish MT system 

Validation ID: aa2948209cc33b071dbf294822e72bb136678345
Question: what are their results on the constructed dataset?
From the context "From Table TABREF37, we have the following observations: (1) AutoJudge consistently and significantly outperforms all the baselines, including RC models and other neural text classification models, which shows the effectiveness and robustness of our model.", so we know that AutoJudge consistently and significantly outperforms all the baselines, including RC models and other neural text classification models.
Answer: AutoJudge consistently and significantly outperforms all the baselines, including RC models and other neural text classification models
True answer: AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement

Validation ID: d9412dda3279729e95fcb35cbed09e61577a896e
Question: what evaluation metrics are reported?
From the context "We employ precision, recall, F1 and accuracy for evaluation metrics", so we know that the evaluation metrics are precision, recall, F1 and accuracy.
Answer: precision, recall, F1 and accuracy
True answer: precision, recall, F1 and accuracy

Validation ID: d9412dda3279729e95fcb35cbed09e61577a896e
Question: what evaluation metrics are reported?
From the context "We employ precision, recall, F1 and accuracy for evaluation metrics", so we know that the evaluation metrics are precision, recall, F1 and accuracy.
Answer: precision, recall, F1 and accuracy
True answer: precision, recall, F1 , accuracy 

Validation ID: 41b70699514703820435b00efbc3aac4dd67560a
Question: what civil field is the dataset about?
From the context "We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available. Fact description, pleas, and results can be extracted easily from these case documents with regular expressions, since the original documents have special typographical characteristics indicating the discourse structure. We also take into account law articles and their corresponding juridical interpretations.", so we know that the dataset is about INLINEFORM0 case documents.
Answer: divorce proceedings
True answer: divorce 

Validation ID: 41b70699514703820435b00efbc3aac4dd67560a
Question: what civil field is the dataset about?
From the context "We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available. Fact description, pleas, and results can be extracted easily from these case documents with regular expressions, since the original documents have special typographical characteristics indicating the discourse structure. We also take into account law articles and their corresponding juridical interpretations.", so we know that the dataset is about INLINEFORM0 case documents.
Answer: divorce proceedings
True answer: divorce

Validation ID: e3c9e4bc7bb93461856e1f4354f33010bc7d28d5
Question: what are the state-of-the-art models?
From the context "We implement and train some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6, which are the leading models on SQuAD leaderboard", so we know that r-net BIBREF5 and AoA BIBREF6 are the state-of-the-art models.
Answer: r-net BIBREF5 and AoA BIBREF6
True answer: SVM , CNN , GRU , CNN/GRU+law, r-net , AoA 

Validation ID: e3c9e4bc7bb93461856e1f4354f33010bc7d28d5
Question: what are the state-of-the-art models?
From the context "We implement and train some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6, which are the leading models on SQuAD leaderboard", so we know that r-net BIBREF5 and AoA BIBREF6 are the state-of-the-art models.
Answer: r-net BIBREF5 and AoA BIBREF6
True answer: SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard

Validation ID: 06cc8fcafc0880cf69a2514bb7341642b9833041
Question: what is the size of the real-world civil case dataset?
From the context "We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available", so we know that the size of the real-world civil case dataset is INLINEFORM0.
Answer: INLINEFORM0
True answer: 100 000 documents

Validation ID: 06cc8fcafc0880cf69a2514bb7341642b9833041
Question: what is the size of the real-world civil case dataset?
From the context "We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available", so we know that the size of the real-world civil case dataset is INLINEFORM0.
Answer: INLINEFORM0
True answer:  INLINEFORM1 cases

Validation ID: d650101712e36594bd77b45930a990402a455222
Question: what datasets are used in the experiment?
From the context "We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available", so we know that the dataset is INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available.
Answer: INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available
True answer: build a new one, collect INLINEFORM0 cases from China Judgments Online

Validation ID: cb384dc5366b693f28680374d31ff45356af0461
Question: Do they model semantics?
From the context "We first preprocess the data by lowercasing, removing numbers, and removing punctuation. We then use the word2vec skip-gram model to create word embeddings BIBREF60. The resulting vector spaces have 100 dimensions, and we specify the following additional parameters to the model: context window of size 10, minimum frequency of 5 for a word to be included, 10 negative samples, 10 iterations, and a sampling rate of $10^{-4}$. We then use cosine similarity to determine a group label's nearest neighbors in the vector space. For our diachronic analysis, we first train word2vec on the entire corpus, and then use the resulting vectors to initialize word2vec models for each year of data in order to encourage coherence and stability across years. After training word2vec, we zero-center and normalize all embeddings to alleviate the hubness problem BIBREF61.", so we know that they model semantics.
Answer: Yes
True answer: Yes

Validation ID: cb384dc5366b693f28680374d31ff45356af0461
Question: Do they model semantics?
From the context "We first preprocess the data by lowercasing, removing numbers, and removing punctuation. We then use the word2vec skip-gram model to create word embeddings BIBREF60. The resulting vector spaces have 100 dimensions, and we specify the following additional parameters to the model: context window of size 10, minimum frequency of 5 for a word to be included, 10 negative samples, 10 iterations, and a sampling rate of $10^{-4}$. We then use cosine similarity to determine a group label's nearest neighbors in the vector space. For our diachronic analysis, we first train word2vec on the entire corpus, and then use the resulting vectors to initialize word2vec models for each year of data in order to encourage coherence and stability across years. After training word2vec, we zero-center and normalize all embeddings to alleviate the hubness problem BIBREF61.", so we know that they model semantics.
Answer: Yes
True answer: Yes

Validation ID: d41e20ec716b5904a272938e5a8f5f3f15a7779e
Question: How do they identify discussions of LGBTQ people in the New York Times?
From the context "We extract all subject-verb-object tuples containing at least one target group label using the Spacy dependency parser", so we know that they identify discussions of LGBTQ people in the New York Times by extracting all subject-verb-object tuples containing at least one target group label using the Spacy dependency parser.
Answer: by extracting all subject-verb-object tuples containing at least one target group label using the Spacy dependency parser
True answer: act paragraphs containing any word from a predetermined list of LGTBQ terms 

Validation ID: 0682bf049f96fa603d50f0fdad0b79a5c55f6c97
Question: Do they analyze specific derogatory words?
From the context "We also tried using a group label's vector representation to induce a valence score directly, instead of using nearest neighbors as a proxy, by adapting the regression-based sentiment prediction from BIBREF62 for word embeddings. We found that this approach yielded similar results as analyzing", so we know that they analyze specific derogatory words.
Answer: Yes
True answer: Yes

Validation ID: 0682bf049f96fa603d50f0fdad0b79a5c55f6c97
Question: Do they analyze specific derogatory words?
From the context "We also tried using a group label's vector representation to induce a valence score directly, instead of using nearest neighbors as a proxy, by adapting the regression-based sentiment prediction from BIBREF62 for word embeddings. We found that this approach yielded similar results as analyzing", so we know that they analyze specific derogatory words.
Answer: Yes
True answer: Yes

Validation ID: 97d1ac71eed13d4f51f29aac0e1a554007907df8
Question: What is novel about their document-level encoder?
From the context "In order to represent individual sentences, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it. We also use interval segment embeddings to distinguish multiple sentences within a document. For $sent_i$ we assign segment embedding $E_A$ or $E_B$ depending on whether $i$ is odd or even. For example, for document $[sent_1, sent_2, sent_3, sent_4, sent_5]$, we would assign embeddings $[E_A, E_B, E_A,E_B, E_A]$. This way, document representations are learned hierarchically where lower Transformer layers represent adjacent sentences, while higher layers, in combination with self-attention, represent multi-sentence discourse.", so we know that the novel about their document-level encoder is that they use interval segment embeddings to distinguish multiple sentences within a document.
Answer: They use interval segment embeddings to distinguish multiple sentences within a document.
True answer: Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically

Validation ID: c17b609b0b090d7e8f99de1445be04f8f66367d4
Question: What rouge score do they achieve?
From the context "We evaluated summarization quality automatically using ROUGE BIBREF30. We report ROUGE-1, ROUGE-2, and ROUGE-L F1 scores", so we know that they achieve ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.
Answer: ROUGE-1, ROUGE-2, and ROUGE-L F1 scores
True answer: Best results on unigram:
CNN/Daily Mail: Rogue F1 43.85
NYT: Rogue Recall 49.02
XSum: Rogue F1 38.81

Validation ID: c17b609b0b090d7e8f99de1445be04f8f66367d4
Question: What rouge score do they achieve?
From the context "We evaluated summarization quality automatically using ROUGE BIBREF32. We report ROUGE-1, ROUGE-2, and ROUGE-L F1 scores", so we know that they achieve ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.
Answer: ROUGE-1, ROUGE-2, and ROUGE-L F1 scores
True answer: Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55

Validation ID: 53014cfb506f6fffb22577bf580ae6f4d5317ce5
Question: What are the datasets used for evaluation?
From the context "We evaluated summarization quality automatically using ROUGE BIBREF30. We report ROUGE-1, ROUGE-2, and ROUGE-L F1 scores", so we know that the datasets used for evaluation are ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.
Answer: ROUGE-1, ROUGE-2, and ROUGE-L F1 scores
True answer: CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum

Validation ID: 53014cfb506f6fffb22577bf580ae6f4d5317ce5
Question: What are the datasets used for evaluation?
From the context "We evaluated summarization quality automatically using ROUGE BIBREF30. We report ROUGE-1, ROUGE-2, and ROUGE-L F1 scores", so we know that the datasets used for evaluation are ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.
Answer: ROUGE-1, ROUGE-2, and ROUGE-L F1 scores
True answer: the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22

Validation ID: fa30a938b58fc05131c3854f12efe376cbad887f
Question: What was their performance on emotion detection?
From the context "We have explored the potential of using Facebook reactions in a distant supervised setting to perform emotion classification. The evaluation on standard benchmarks shows that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource.", so we know that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource.
Answer: competitive results
True answer: Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. 

Validation ID: f875337f2ecd686cd7789e111174d0f14972638d
Question: Which existing benchmarks did they compare to?
From the context "In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7. A summary is provided in Table TABREF8, which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation.", so we know that they compared to the development portion of the Affective Text, the Affective test, the Fairy Tales dataset and the ISEAR dataset.
Answer: the development portion of the Affective Text, the Affective test, the Fairy Tales dataset and the ISEAR dataset
True answer: Affective Text, Fairy Tales, ISEAR

Validation ID: f875337f2ecd686cd7789e111174d0f14972638d
Question: Which existing benchmarks did they compare to?
From the context "In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7. A summary is provided in Table TABREF8, which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation.", so we know that they compared to the development portion of the Affective Text, the Affective test, the Fairy Tales dataset and the ISEAR dataset.
Answer: the development portion of the Affective Text, the Affective test, the Fairy Tales dataset and the ISEAR dataset
True answer:  Affective Text dataset, Fairy Tales dataset, ISEAR dataset

Validation ID: de53af4eddbc30c808d90b8a11a29217d377569e
Question: Which Facebook pages did they look at?
From the context "We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ).", so we know that they chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but they did so mainly based on intuition and with an eye to the nature of the datasets available for evaluation.
Answer: different pages (and therefore domains and stances), aiming at a balanced and varied dataset
True answer: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney

Validation ID: de53af4eddbc30c808d90b8a11a29217d377569e
Question: Which Facebook pages did they look at?
From the context "We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ).", so we know that they chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but they did so mainly based on intuition and with an eye to the nature of the datasets available for evaluation.
Answer: different pages (and therefore domains and stances), aiming at a balanced and varied dataset
True answer: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.

Validation ID: dac087e1328e65ca08f66d8b5307d6624bf3943f
Question: LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?
From the context "In this paper, we propose an efficient feature extraction method. In this method, two new topic-based features are extracted and used to discriminate human-like spammers from legitimate users", so we know that this paper is introducing an efficient feature extraction method.
Answer: Yes
True answer: No

Validation ID: dac087e1328e65ca08f66d8b5307d6624bf3943f
Question: LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?
From the context "In this paper, we propose an efficient feature extraction method. In this method, two new topic-based features are extracted and used to discriminate human-like spammers from legitimate users", so we know that this paper is introducing an efficient feature extraction method.
Answer: Yes
True answer: No

Validation ID: a1645d0ba50e4c29f0feb806521093e7b1459081
Question: What is the benchmark dataset and is its quality high?
From the context "Lee et al. lee2011seven created and deployed 60 seed social accounts on Twitter to attract spammers by reporting back what accounts interact with them. They collected 19,276 legitimate users and 22,223 spammers in their datasets along with their tweet content in 7 months. This is our first test dataset.", so we know that the benchmark dataset is Social Honeypot dataset and it is high quality.
Answer: Social Honeypot dataset and it is high quality
True answer: Social Honeypot dataset (public) and Weibo dataset (self-collected); yes

Validation ID: a1645d0ba50e4c29f0feb806521093e7b1459081
Question: What is the benchmark dataset and is its quality high?
From the context "Lee et al. lee2011seven created and deployed 60 seed social accounts on Twitter to attract spammers by reporting back what accounts interact with them. They collected 19,276 legitimate users and 22,223 spammers in their datasets along with their tweet content in 7 months. This is our first test dataset.", so we know that the benchmark dataset is Social Honeypot dataset and it is high quality.
Answer: Social Honeypot dataset and it is high quality
True answer: Social Honeypot, which is not of high quality

Validation ID: 3cd185b7adc835e1c4449eff81222f5fc15c8500
Question: How do they detect spammers?
From the context "In this paper, we propose a novel feature extraction method to effectively detect "smart" spammers who post seemingly legitimate tweets and are thus difficult to identify by existing spammer classification methods", so we know that they detect spammers by a novel feature extraction method.
Answer: a novel feature extraction method
True answer: Extract features from the LDA model and use them in a binary classification task

Validation ID: f03112b868b658c954db62fc64430bebbaa7d9e0
Question: Do they use other evaluation metrics besides ROUGE?
From the context "We report results using automatic metrics in Table TABREF20. The top part of the table compares Refresh against related extractive systems. The bottom part reports the performance of abstractive systems. We present three variants of Lead, one is computed by ourselves and the other two are reported in nallapati17 and see-acl17. Note that they vary slightly due to differences in the preprocessing of the data.", so we know that they use ROUGE as evaluation metrics.
Answer: ROUGE
True answer: Yes

Validation ID: f03112b868b658c954db62fc64430bebbaa7d9e0
Question: Do they use other evaluation metrics besides ROUGE?
From the context "We report results using automatic metrics in Table TABREF20. The top part of the table compares Refresh against related extractive systems. The bottom part reports the performance of abstractive systems. We present three variants of Lead, one is computed by ourselves and the other two are reported in nallapati17 and see-acl17. Note that they vary slightly due to differences in the preprocessing of the data.", so we know that they use ROUGE as evaluation metrics.
Answer: ROUGE
True answer: No

Validation ID: 5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4
Question: What is their ROUGE score?
From the context "Experimental results show that reinforcement learning offers a great means to steer our model towards generating informative, fluent, and concise summaries outperforming state-of-the-art extractive and abstractive systems on the CNN and DailyMail datasets", so we know that their ROUGE score is better than state-of-the-art extractive and abstractive systems.
Answer: better than state-of-the-art extractive and abstractive systems
True answer: Unanswerable

Validation ID: a6d3e57de796172c236e33a6ceb4cca793dc2315
Question: What are the baselines?
From the context "We compare our model against the following baselines:", so we know that the baselines are:

True answer: Unanswerable

Validation ID: a6d3e57de796172c236e33a6ceb4cca793dc2315
Question: What are the baselines?
From the context "We compare our model against the following baselines:", so we know that the baselines are:

True answer: Answer with content missing: (Experimental Setup missing subsections)
To be selected: We compared REFRESH against a baseline which simply selects the first m leading sentences from each document (LEAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss.
Answer: LEAD

Validation ID: 395b61d368e8766014aa960fde0192e4196bcb85
Question: What datasets do they use?
From the context "We construct three datasets based on IMDB reviews and Yelp reviews. The IMDB dataset is binarised and split into a training and test set, each with 25K reviews (2K reviews from the training set are reserved for development). We filter out any review that has more than 400 tokens, producing the final dataset (imdb400). For Yelp, we binarise the ratings, and create 2 datasets, where we keep only reviews with $\le $ 50 tokens (yelp50) and $\le $200 tokens (yelp200). We randomly partition both datasets into train/dev/test sets (90/5/5 for yelp50; 99/0.5/0.5 for yelp200). For all datasets, we use spaCy for tokenisation.", so we know that they use three datasets based on IMDB reviews and Yelp reviews.
Answer: three datasets based on IMDB reviews and Yelp reviews
True answer: three datasets based on IMDB reviews and Yelp reviews

Validation ID: 395b61d368e8766014aa960fde0192e4196bcb85
Question: What datasets do they use?
From the context "We construct three datasets based on IMDB reviews and Yelp reviews. The IMDB dataset is binarised and split into a training and test set, each with 25K reviews (2K reviews from the training set are reserved for development). We filter out any review that has more than 400 tokens, producing the final dataset (imdb400). For Yelp, we binarise the ratings, and create 2 datasets, where we keep only reviews with $\le $ 50 tokens (yelp50) and $\le $200 tokens (yelp200). We randomly partition both datasets into train/dev/test sets (90/5/5 for yelp50; 99/0.5/0.5 for yelp200). For all datasets, we use spaCy for tokenisation.", so we know that they use three datasets based on IMDB reviews and Yelp reviews. The IMDB dataset is binarised and split into a training and test set, each with 25K reviews (2K reviews from the training set are reserved for development). They filter out any review that has more than 400 tokens, producing the final dataset (imdb400). For Yelp, they binarise the ratings, and create 2 datasets, where they keep only reviews with $\le $ 50 tokens (yelp50) and $\le $200 tokens (yelp200). They randomly partition both datasets into train/dev/test sets (90/5/5 for yelp50; 99/0.5/0.5 for yelp200). For all datasets, they use spaCy for tokenisation.
Answer: three datasets based on IMDB reviews and Yelp reviews
True answer: 1 IMDB dataset and 2 Yelp datasets

Validation ID: 92bb41cf7bd1f7886784796a8220ed5aa07bc49b
Question: What other factors affect the performance?
From the context "We think this is because with more words it is more likely for HotFlip to find a vulnerable spot to target. While for TYC and AutoEncoder, we see improvements over BLEU and ACPT as the number of training examples for target classifier decreases from yelp200 (2M) to yelp50 (407K) to imdb400 (18K)", so we know that the number of training examples for target classifier decreases from yelp200 (2M) to yelp50 (407K) to imdb400 (18K) affect the performance.
Answer: the number of training examples for target classifier decreases from yelp200 (2M) to yelp50 (407K) to imdb400 (18K)
True answer: architecture of the classifier, sentence length,  input domain

Validation ID: 4ef11518b40cc55d86c485f14e24732123b0d907
Question: What are the benchmark attacking methods?
From the context "We experiment with five benchmark attacking methods for texts: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4.", so we know that the benchmark attacking methods are FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4.
Answer: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4
True answer: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4

Validation ID: 4ef11518b40cc55d86c485f14e24732123b0d907
Question: What are the benchmark attacking methods?
From the context "We experiment with five benchmark attacking methods for texts: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4.", so we know that the benchmark attacking methods are FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4.
Answer: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4
True answer: FGM, FGVM, DeepFool, HotFlip, TYC

Validation ID: 6a219d7c58451842aa5d6819a7cdf51c55e9fc0f
Question: What domains are covered in the corpus?
From the context "Raw CoVo data contains samples that passed validation as well as those that did not. To build CoVoST, we only use the former one and reuse the official train-development-test partition of the validated data. As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.", so we know that the domains covered in the corpus are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.
Answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese
True answer: No specific domain is covered in the corpus.

Validation ID: cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad
Question: What is the architecture of their model?
From the context "Our ASR and ST models follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing. For MT, we use a Transformer base architecture BIBREF15, but with 3 encoder layers, 3 decoder layers and 0.3 dropout.", so we know that the architecture of their model is a Transformer base architecture with 3 encoder layers, 3 decoder layers and 0.3 dropout.
Answer: a Transformer base architecture with 3 encoder layers, 3 decoder layers and 0.3 dropout
True answer: follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing

Validation ID: f8f4e4a50d2b3fbd193327e79ea32d8d057e1414
Question: How was the dataset collected?
From the context "Raw CoVo data contains samples that passed validation as well as those that did not. To build CoVoST, we only use the former one and reuse the official train-development-test partition of the validated data. As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.", so we know that the dataset was collected by using the official train-development-test partition of the validated data.
Answer: using the official train-development-test partition of the validated data
True answer: Contributors record voice clips by reading from a bank of donated sentences.

Validation ID: f8f4e4a50d2b3fbd193327e79ea32d8d057e1414
Question: How was the dataset collected?
From the context "Raw CoVo data contains samples that passed validation as well as those that did not. To build CoVoST, we only use the former one and reuse the official train-development-test partition of the validated data. As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.", so we know that the dataset was collected by using the official train-development-test partition of the validated data.
Answer: using the official train-development-test partition of the validated data
True answer: crowdsourcing

Validation ID: bc84c5a58c57038910f7720d7a784560054d3e1a
Question: Which languages are part of the corpus?
From the context "We introduce CoVoST, a multilingual ST corpus based on Common Voice BIBREF10 for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that the languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.
Answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese
True answer: French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)

Validation ID: bc84c5a58c57038910f7720d7a784560054d3e1a
Question: Which languages are part of the corpus?
From the context "We introduce CoVoST, a multilingual ST corpus based on Common Voice BIBREF10 for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that the languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.
Answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese
True answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese

Validation ID: 29923a824c98b3ba85ced964a0e6a2af35758abe
Question: How is the quality of the data empirically evaluated??
From the context "We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.", so we know that the quality of the data is empirically evaluated by sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.
Answer: sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint
True answer: Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets

Validation ID: 29923a824c98b3ba85ced964a0e6a2af35758abe
Question: How is the quality of the data empirically evaluated??
From the context "We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.", so we know that the quality of the data is empirically evaluated by sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.
Answer: sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint
True answer: computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations

Validation ID: 559c68802ee2bb8b11e2188127418ca3a6155ba7
Question: Is the data in CoVoST annotated for dialect?
From the context "We also provide baseline results, including, to our knowledge, the first end-to-end many-to-one multilingual model for spoken language translation", so we know that CoVoST is the first end-to-end many-to-one multilingual model for spoken language translation.
Answer: Yes
True answer: No

Validation ID: 8dc707a0daf7bff61a97d9d854283e65c0c85064
Question: Is Arabic one of the 11 languages in CoVost?
From the context "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that CoVost has 11 languages.
From the context "CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese", so we know that Arabic is not one of the 11 languages in CoVost.
True answer: No

Validation ID: 8dc707a0daf7bff61a97d9d854283e65c0c85064
Question: Is Arabic one of the 11 languages in CoVost?
From the context "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that CoVost has 11 languages.
From the context "CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese", so we know that Arabic is not one of the 11 languages in CoVost.
True answer: No

Validation ID: ffde866b1203a01580eb33237a0bb9da71c75ecf
!!! Has error during processing !!!
Question: How big is Augmented LibriSpeech dataset?
Answer: From the context "We augment LibriSpeech with the TED-LIUM corpus BIBREF1, which contains 2,048 hours of speech from TED talks. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech
True answer: Unanswerable

Validation ID: ffde866b1203a01580eb33237a0bb9da71c75ecf
!!! Has error during processing !!!
Question: How big is Augmented LibriSpeech dataset?
Answer: From the context "We augment LibriSpeech with the TED-LIUM corpus BIBREF1, which contains 2,048 hours of speech from TED talks. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech
True answer: Unanswerable

Validation ID: 6cd8bad8a031ce6d802ded90f9754088e0c8d653
Question: By how much does their best model outperform the state-of-the-art?
From the context "With our neural models, we achieved new state-of-the-art results on the SemEval 2010 task 8 benchmark data.", so we know that their best model outperform the state-of-the-art.
Answer: new state-of-the-art results
True answer: 0.8% F1 better than the best state-of-the-art

Validation ID: 6cd8bad8a031ce6d802ded90f9754088e0c8d653
Question: By how much does their best model outperform the state-of-the-art?
From the context "With our neural models, we achieved new state-of-the-art results on the SemEval 2010 task 8 benchmark data.", so we know that their best model outperform the state-of-the-art.
Answer: new state-of-the-art results
True answer: Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.

Validation ID: 30eacb4595014c9c0e5ee9669103d003cfdfe1e5
Question: Which dataset do they train their models on?
From the context "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8. It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set.", so we know that they train their models on the relation classification dataset of the SemEval 2010 task 8.
Answer: the relation classification dataset of the SemEval 2010 task 8
True answer: relation classification dataset of the SemEval 2010 task 8

Validation ID: 30eacb4595014c9c0e5ee9669103d003cfdfe1e5
Question: Which dataset do they train their models on?
From the context "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8. It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set.", so we know that they train their models on the relation classification dataset of the SemEval 2010 task 8.
Answer: the relation classification dataset of the SemEval 2010 task 8
True answer: SemEval 2010 task 8 BIBREF8

Validation ID: 0f7867f888109b9e000ef68965df4dde2511a55f
Question: How does their simple voting scheme work?
From the context "Finally, we combine CNNs and RNNs using a simple voting scheme and achieve new state-of-the-art results on the SemEval 2010 benchmark dataset.", so we know that they combine CNNs and RNNs using a simple voting scheme.
From the context "For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly.", so we know that they apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, they pick one of the most frequent classes randomly.
True answer: we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes, In case of a tie, we pick one of the most frequent classes randomly

Validation ID: 0f7867f888109b9e000ef68965df4dde2511a55f
Question: How does their simple voting scheme work?
From the context "Finally, we combine CNNs and RNNs using a simple voting scheme and achieve new state-of-the-art results on the SemEval 2010 benchmark dataset.", so we know that they combine CNNs and RNNs using a simple voting scheme.
From the context "For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly.", so we know that they apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, they pick one of the most frequent classes randomly.
True answer: Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly.

Validation ID: e2e977d7222654ee8d983fd8ba63b930e9a5a691
Question: Which variant of the recurrent neural network do they use?
From the context "We applied the ranking loss function proposed in deSantos2015 to train our models. It maximizes the distance between the true label INLINEFORM0 and the best competitive label INLINEFORM1 given a data point INLINEFORM2. The objective function is DISPLAYFORM0
with INLINEFORM0 and INLINEFORM1 being the scores for the classes INLINEFORM2 and INLINEFORM3 respectively. The parameter INLINEFORM4 controls the penalization of the prediction errors and INLINEFORM5 and INLINEFORM6 are margins for the correct and incorrect classes. Following deSantos2015, we set INLINEFORM7. We do not learn a pattern for the class Other but increase its difference to the best competitive label by using only the second summand in Equation EQREF10 during training.", so we know that they use ranking loss function proposed in deSantos2015.
True answer: uni-directional RNN

Validation ID: 0cfe0e33fbb100751fc0916001a5a19498ae8cb5
!!! Has error during processing !!!
Question: How do they obtain the new context represetation?
Answer: From the context "One of our contributions is a new input representation especially designed for relation classification. The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, we want to focus on it but not ignore the other regions completely. Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, we force the network to pay special attention to it. The two contexts are processed by two independent convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation. Figure FIGREF3 depicts this procedure. It shows an examplary sentence: “He had chest pain and <e1>headaches</e1> from <e2>mold</e2> in the bedroom.” If we only considered the middle context “from”, the network might be tempted to predict a relation like Entity-Origin(e1,e2). However, by also taking the left and right context into account, the model can detect the relation Cause-Effect(e2,e1). While this could also be achieved by integrating the whole context into the model, using the whole context can have disadvantages for longer sentences: The max pooling step can easily choose a value from a part of the sentence which is far away from the mention of the relation. With splitting the context into two parts, we reduce this danger. Repeating the middle context increases the chance for the max pooling step to pick a value from the middle context.", so we know that they split the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, they want to focus on it but not ignore the other regions completely. Hence, they propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, they force
True answer: They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task. Majority Class Baselines The first pair of baselines uses the most frequent class in the training set for prediction. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question. The latter can only be applied to the supervised test set.", so we know that the paper report the performance of the model for each individual language.
Answer: Yes
True answer: Yes

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task. Majority Class Baselines The first pair of baselines uses the most frequent class in the training set for prediction. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question. The latter can only be applied to the supervised test set.", so we know that the paper reports the performance of the model for each individual language.
Answer: Yes
True answer: Unanswerable

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task. Majority Class Baselines The first pair of baselines uses the most frequent class in the training set for prediction. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question. The latter can only be applied to the supervised test set.", so we know that the paper reports the performance of the model for each individual language.
Answer: Yes
True answer: Yes

Validation ID: 71ba1b09bb03f5977d790d91702481cc406b3767
Question: What is the performance of the baseline?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task. The first pair of baselines uses the most frequent class in the training set for prediction. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question. The latter can only be applied to the supervised test set.", so we know that the performance of the baseline is the accuracy of the global majority class baseline and the target-wise majority class baseline.
Answer: the accuracy of the global majority class baseline and the target-wise majority class baseline
True answer: M-Bert had 76.6 F1 macro score.

Validation ID: 71ba1b09bb03f5977d790d91702481cc406b3767
Question: What is the performance of the baseline?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task. The first pair of baselines uses the most frequent class in the training set for prediction. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question. The latter can only be applied to the supervised test set.", so we know that the performance of the baseline is the accuracy of the global majority class baseline and the target-wise majority class baseline.
Answer: the accuracy of the global majority class baseline and the target-wise majority class baseline
True answer: 75.1% and 75.6% accuracy

Validation ID: 612c3675b6c55b60ae6d24265ed8e20f62cb117e
Question: Did they pefrorm any cross-lingual vs single language evaluation?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task. Majority Class Baselines The first pair of baselines uses the most frequent class in the training set for prediction. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question. The latter can only be applied to the supervised test set.", so we know that they did perform cross-lingual vs single language evaluation.
Answer: Yes
True answer: Yes

Validation ID: bd40f33452da7711b65faaa248aca359b27fddb6
Question: What was the performance of multilingual BERT?
From the context "We fine-tune multilingual Bert (M-Bert) on the task BIBREF5 which has been pretrained jointly in 104 languages and has established itself as a state of the art for various multilingual tasks BIBREF18, BIBREF19.", so we know that multilingual BERT has been pretrained jointly in 104 languages and has established itself as a state of the art for various multilingual tasks.
Answer: state of the art
True answer: BERT had 76.6 F1 macro score on x-stance dataset.

Validation ID: 787c4d4628eac00dbceb1c96020bff0090edca46
Question: What annotations are present in dataset?
From the context "We merged the two labels on each pole into a single label: `yes' and `rather yes' were combined into `favor'; `rather no', or `no' into `against'. This improves the consistency of the data and the comparability to previous stance detection datasets.", so we know that the annotations are `favor' and `against'.
Answer: `favor' and `against'
True answer: answer each question with either `yes', `rather yes', `rather no', or `no'., can supplement each answer with a comment of at most 500 characters

Validation ID: 3c3807f226ba72fc41f59f0338f12a49a0c35605
Question: Are the experts comparable to real-world users?
From the context "The intended audience for privacy policies consists of the general public. This informs the decision to elicit questions from crowdworkers on the contents of privacy policies. We choose not to show the contents of privacy policies to crowdworkers, a procedure motivated by a desire to avoid inadvertent biases BIBREF43, BIBREF44, BIBREF45, BIBREF46, BIBREF47, and encourage crowdworkers to ask a variety of questions beyond only asking questions based on practices described in the document.", so we know that the experts are comparable to real-world users.
Answer: Yes
True answer: No

Validation ID: c70bafc35e27be9d1efae60596bc0dd390c124c0
Question: Are the answers double (and not triple) annotated?
From the context "In order to estimate annotation reliability and provide for better evaluation, every question in the test set is answered by at least two additional experts", so we know that the answers are double annotated.
Answer: double annotated
True answer: Yes

Validation ID: 81d607fc206198162faa54a796717c2805282d9b
Question: Who were the experts used for annotation?
From the context "To identify legally sound answers, we recruit seven experts with legal training to construct answers to Turker questions", so we know that the experts used for annotation are seven experts with legal training.
Answer: seven experts with legal training
True answer: Individuals with legal training

Validation ID: 81d607fc206198162faa54a796717c2805282d9b
Question: Who were the experts used for annotation?
From the context "To identify legally sound answers, we recruit seven experts with legal training to construct answers to Turker questions", so we know that the experts used for annotation are seven experts with legal training.
Answer: seven experts with legal training
True answer: Yes

Validation ID: 51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad
Question: What type of neural model was used?
From the context "We utilize a CNN neural encoder for answerability prediction. We use GloVe word embeddings BIBREF50, and a filter size of 5 with 64 filters to encode questions.", so we know that a CNN neural model was used.
Answer: CNN
True answer: Bert + Unanswerable

Validation ID: 51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad
Question: What type of neural model was used?
From the context "We implement two BERT-based baselines BIBREF51 for evidence identification", so we know that BERT-based baselines were used.
Answer: BERT-based baselines
True answer: CNN, BERT

Validation ID: f0848e7a339da0828278f6803ed7990366c975f0
Question: Were other baselines tested to compare with the neural baseline?
From the context "We describe baselines on this task, including a human performance baseline.", so we know that other baselines were tested to compare with the neural baseline.
Answer: Yes
True answer: SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance

Validation ID: f0848e7a339da0828278f6803ed7990366c975f0
Question: Were other baselines tested to compare with the neural baseline?
From the context "We describe baselines on this task, including a human performance baseline.", so we know that other baselines were tested to compare with the neural baseline.
Answer: Yes
True answer: No-Answer Baseline (NA), Word Count Baseline, Human Performance

Validation ID: b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8
Question: Does the paper clearly establish that the challenges listed here exist in this dataset and task?
From the context "We focus on social media posts from the website Twitter, which are an excellent testing ground for character based models due to the noisy nature of text. Heavy use of slang and abundant misspellings means that there are many orthographically and semantically similar tokens, and special characters such as emojis are also immensely popular and carry useful semantic information. In our moderately sized training dataset of 2 million tweets, there were about 0.92 million unique word types. It would be expensive to capture all these phenomena in a word based model in terms of both the memory requirement (for the increased vocabulary) and the amount of training data required for effective learning. Additional benefits of the character based approach include language independence of the methods, and no requirement of NLP preprocessing such as word-segmentation.", so we know that the challenges listed here exist in this dataset and task.
Answer: Yes
True answer: Yes

Validation ID: 792f6d76d2befba2af07198584aac1b189583ae4
Question: Is this hashtag prediction task an established task, or something new?
From the context "Hashtag prediction for social media has been addressed earlier, for example in BIBREF15, BIBREF16. BIBREF15 also use a neural architecture, but compose text embeddings from a lookup table of words. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations.", so we know that hashtag prediction for social media has been addressed earlier, and it is an established task.
Answer: established task
True answer: established task

Validation ID: 792f6d76d2befba2af07198584aac1b189583ae4
Question: Is this hashtag prediction task an established task, or something new?
From the context "Hashtag prediction for social media has been addressed earlier, for example in BIBREF15, BIBREF16. BIBREF15 also use a neural architecture, but compose text embeddings from a lookup table of words. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations.", so we know that hashtag prediction for social media has been addressed earlier, and it is an established task.
Answer: established task
True answer: Hashtag prediction for social media has been addressed earlier

Validation ID: 127d5ddfabec5c58832e5865cbd8ed0978c25a13
Question: What is the word-level baseline?
From the context "We test the character and word-level variants by predicting hashtags for a held-out test set of posts. Since there may be more than one correct hashtag per post, we generate a ranked list of tags for each post from the output posteriors, and report average precision@1, recall@10 and mean rank of the correct hashtags. These are listed in Table 3.
To see the performance of each model on posts containing rare words (RW) and frequent words (FW) we selected two test sets each containing 2,000 posts. We populated these sets with posts which had the maximum and minimum number of out-of-vocabulary words respectively, where vocabulary is defined by the 20K most frequent words. Overall, tweet2vec outperforms the word model, doing significantly better on RW test set and comparably on FW set.
True answer: a simple word-level encoder, The encoder is essentially the same as tweet2vec, with the input as words instead of characters.

Validation ID: 127d5ddfabec5c58832e5865cbd8ed0978c25a13
Question: What is the word-level baseline?
From the context "We test the character and word-level variants by predicting hashtags for a held-out test set of posts. Since there may be more than one correct hashtag per post, we generate a ranked list of tags for each post from the output posteriors, and report average precision@1, recall@10 and mean rank of the correct hashtags. These are listed in Table 3.
To see the performance of each model on posts containing rare words (RW) and frequent words (FW) we selected two test sets each containing 2,000 posts. We populated these sets with posts which had the maximum and minimum number of out-of-vocabulary words respectively, where vocabulary is defined by the 20K most frequent words. Overall, tweet2vec outperforms the word model, doing significantly better on RW test set and comparably on FW set.
True answer: The encoder is essentially the same as tweet2vec, with the input as words instead of characters.

Validation ID: b91671715ad4fad56c67c28ce6f29e180fe08595
Question: What other tasks do they test their method on?
From the context "We also study the effect of model size on the performance of these models. For the word model we set vocabulary size $V$ to 8K, 15K and 20K respectively. For tweet2vec we set the GRU hidden state size to 300, 400 and 500 respectively. Figure 2 shows precision 1 of the two models as the number of parameters is increased, for each test set described above. There is not much variation in the performance, and moreover tweet2vec always outperforms the word based model for the same number of parameters.", so we know that they test their method on the effect of model size on the performance of these models.
Answer: the effect of model size on the performance of these models
True answer: None

Validation ID: a6d37b5975050da0b1959232ae756fc09e5f87e8
Question: what is the word level baseline they compare to?
From the context "We test the character and word-level variants by predicting hashtags for a held-out test set of posts. Since there may be more than one correct hashtag per post, we generate a ranked list of tags for each post from the output posteriors, and report average precision@1, recall@10 and mean rank of the correct hashtags. These are listed in Table 3.
To see the performance of each model on posts containing rare words (RW) and frequent words (FW) we selected two test sets each containing 2,000 posts. We populated these sets with posts which had the maximum and minimum number of out-of-vocabulary words respectively, where vocabulary is defined by the 20K most frequent words. Overall, tweet2vec outperforms the word model, doing significantly better on RW test set and comparably on FW set.
True answer: a simple word-level encoder, with the input as words instead of characters

Validation ID: a6d37b5975050da0b1959232ae756fc09e5f87e8
Question: what is the word level baseline they compare to?
From the context "We test the character and word-level variants by predicting hashtags for a held-out test set of posts. Since there may be more than one correct hashtag per post, we generate a ranked list of tags for each post from the output posteriors, and report average precision@1, recall@10 and mean rank of the correct hashtags. These are listed in Table 3.
To see the performance of each model on posts containing rare words (RW) and frequent words (FW) we selected two test sets each containing 2,000 posts. We populated these sets with posts which had the maximum and minimum number of out-of-vocabulary words respectively, where vocabulary is defined by the 20K most frequent words. Overall, tweet2vec outperforms the word model, doing significantly better on RW test set and comparably on FW set.
True answer: The encoder is essentially the same as tweet2vec, with the input as words instead of characters

Validation ID: e82fa03f1638a8c59ceb62bb9a6b41b498950e1f
Question: What is the state of the art system mentioned?
From the context "In the last block, we report the performance of our method. BERT(Token-CLS) is our baseline, which does not incorporate gloss information, and it performs slightly worse than previous traditional supervised methods and recent neural-based methods", so we know that the state of the art system mentioned is previous traditional supervised methods and recent neural-based methods.
Answer: previous traditional supervised methods and recent neural-based methods
True answer: Two knowledge-based systems,
two traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.

Validation ID: 7ab9c0b4ceca1c142ff068f85015a249b14282d0
Question: Do they incoprorate WordNet into the model?
From the context "We extract word sense glosses from WordNet 3.0", so we know that they incorporate WordNet into the model.
Answer: Yes
True answer: construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem

Validation ID: 7ab9c0b4ceca1c142ff068f85015a249b14282d0
Question: Do they incoprorate WordNet into the model?
From the context "We extract word sense glosses from WordNet 3.0", so we know that they incorporate WordNet into the model.
Answer: Yes
True answer: construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word

Validation ID: 00050f7365e317dc0487e282a4c33804b58b1fb3
Question: Is SemCor3.0 reflective of English language data in general?
From the context "Following previous work BIBREF13, BIBREF12, BIBREF10, BIBREF17, BIBREF9, BIBREF7, we choose SemCor3.0 as training corpus, which is the largest corpus manually annotated with WordNet sense for WSD.", so we know that SemCor3.0 is the largest corpus manually annotated with WordNet sense for WSD.
Answer: No
True answer: Yes

Validation ID: 00050f7365e317dc0487e282a4c33804b58b1fb3
Question: Is SemCor3.0 reflective of English language data in general?
From the context "Following previous work BIBREF13, BIBREF12, BIBREF10, BIBREF17, BIBREF9, BIBREF7, we choose SemCor3.0 as training corpus, which is the largest corpus manually annotated with WordNet sense for WSD.", so we know that SemCor3.0 is the largest corpus manually annotated with WordNet sense for WSD.
Answer: No
True answer: Unanswerable

Validation ID: c5b0ed5db65051eebd858beaf303809aa815e8e5
Question: Do they use large or small BERT?
From the context "We use the pre-trained uncased BERT$_\mathrm {BASE}$ model for fine-tuning, because we find that BERT$_\mathrm {LARGE}$ model performs slightly worse than BERT$_\mathrm {BASE}$ in this task", so we know that they use BERT$_\mathrm {BASE}$ model.
Answer: BERT$_\mathrm {BASE}$
True answer: small BERT

Validation ID: c5b0ed5db65051eebd858beaf303809aa815e8e5
Question: Do they use large or small BERT?
From the context "We use the pre-trained uncased BERT$_\mathrm {BASE}$ model for fine-tuning, because we find that BERT$_\mathrm {LARGE}$ model performs slightly worse than BERT$_\mathrm {BASE}$ in this task", so we know that they use BERT$_\mathrm {BASE}$ model.
Answer: BERT$_\mathrm {BASE}$
True answer: small BERT

Validation ID: 10fb7dc031075946153baf0a0599e126de29e3a4
Question: How does the neural network architecture accomodate an unknown amount of senses per word?
From the context "We construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word, thus treating WSD task as a sentence-pair classification problem", so we know that the neural network architecture accomodate an unknown amount of senses per word by constructing context-gloss pairs from glosses of all possible senses (in WordNet) of the target word.
Answer: by constructing context-gloss pairs from glosses of all possible senses (in WordNet) of the target word
True answer: converts WSD to a sequence learning task,  leverage gloss knowledge, by extending gloss knowledge

Validation ID: e438445cf823893c841b2bc26cdce32ccc3f5cbe
Question: Which fonts are the best indicators of high quality?
From the context "We observe that the performance of biLSTM, Inception, and Joint is much better than that of all four baselines. Inception achieves 2.9% higher accuracy than biLSTM. The performance of Joint achieves an accuracy of 59.4%, which is 5.3% higher than using textual features alone (biLSTM) and 2.4% higher than using visual features alone (Inception). Based on a one-tailed Wilcoxon signed-rank test, the performance of Joint is statistically significant ($p<0.05$). This shows that the textual and visual features complement each other, achieving state-of-the-art results in combination.", so we know that the textual and visual features complement each other, achieving state-of-the-art results in combination.
Answer: textual and visual features
True answer: Unanswerable

Validation ID: 12f7fac818f0006cf33269c9eafd41bbb8979a48
Question: What kind of model do they use?
From the context "We adopt a bi-directional LSTM model to generate textual embeddings for document quality assessment, following the method of BIBREF12 (“biLSTM” hereafter). The input to biLSTM is a textual document, and the output is a textual embedding, which will later integrate with the visual embedding.", so we know that they use bi-directional LSTM model to generate textual embeddings for document quality assessment.
Answer: bi-directional LSTM model
True answer: visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models

Validation ID: 12f7fac818f0006cf33269c9eafd41bbb8979a48
Question: What kind of model do they use?
From the context "We adopt a bi-directional LSTM model to generate textual embeddings for document quality assessment, following the method of BIBREF12 (“biLSTM” hereafter). The input to biLSTM is a textual document, and the output is a textual embedding, which will later integrate with the visual embedding.", so we know that they use a bi-directional LSTM model to generate textual embeddings for document quality assessment.
Answer: bi-directional LSTM model
True answer: Inception V3, biLSTM

Validation ID: d5a8fd8bb48dd1f75927e874bdea582b4732a0cd
Question: Did they release their data set of academic papers?
From the context "All code and data associated with this research will be released on publication", so we know that they released their data set of academic papers.
Answer: Yes
True answer: No

Validation ID: d5a8fd8bb48dd1f75927e874bdea582b4732a0cd
Question: Did they release their data set of academic papers?
From the context "All code and data associated with this research will be released on publication", so we know that they released their data set of academic papers.
Answer: Yes
True answer: Unanswerable

Validation ID: 1097768b89f8bd28d6ef6443c94feb04c1a1318e
Question: Do the methods that work best on academic papers also work best on Wikipedia?
From the context "On Wikipedia, we observe that the performance of biLSTM, Inception, and Joint is much better than that of all four baselines. Inception achieves 2.9% higher accuracy than biLSTM. The performance of Joint achieves an accuracy of 59.4%, which is 5.3% higher than using textual features alone (biLSTM) and 2.4% higher than using visual features alone (Inception). Based on a one-tailed Wilcoxon signed-rank test, the performance of Joint is statistically significant ( $p<0.05$ ). This shows that the textual and visual features complement each other, achieving state-of-the-art results in combination.", so we know that the methods that work best on academic papers also work best on Wikipedia.
Answer: Yes
True answer: Yes

Validation ID: 1097768b89f8bd28d6ef6443c94feb04c1a1318e
Question: Do the methods that work best on academic papers also work best on Wikipedia?
From the context "On Wikipedia, we observe that the performance of biLSTM, Inception, and Joint is much better than that of all four baselines. Inception achieves 2.9% higher accuracy than biLSTM. The performance of Joint achieves an accuracy of 59.4%, which is 5.3% higher than using textual features alone (biLSTM) and 2.4% higher than using visual features alone (Inception). Based on a one-tailed Wilcoxon signed-rank test, the performance of Joint is statistically significant ( $p<0.05$ ). This shows that the textual and visual features complement each other, achieving state-of-the-art results in combination.", so we know that the methods that work best on academic papers also work best on Wikipedia.
Answer: Yes
True answer: No

Validation ID: fc1679c714eab822431bbe96f0e9cf4079cd8b8d
Question: What is their system's absolute accuracy?
From the context "Table 3 shows the performance of the different models over our two datasets, in the form of the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations", so we know that the absolute accuracy of their system is the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations.
Answer: the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations
True answer: 59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers

Validation ID: 23e2971c962bb6486bc0a66ff04242170dd22a1d
Question: Which is more useful, visual or textual features?
From the context "Table 3 shows the performance of the different models over our two datasets, in the form of the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations. On Wikipedia, we observe that the performance of biLSTM, Inception, and Joint is much better than that of all four baselines. Inception achieves 2.9% higher accuracy than biLSTM. The performance of Joint achieves an accuracy of 59.4%, which is 5.3% higher than using textual features alone (biLSTM) and 2.4% higher than using visual features alone (Inception). Based on a one-tailed Wilcoxon signed-rank test, the performance of Joint is statistically significant ( $p<0.05$ ). This shows that the textual and visual features complement each other, achieving state-of-the-art results in combination. For arXiv, baseline methods Majority, Benchmark, and Inception $_{\text{fixed}}$ outperform biLSTM over cs.ai, in large part because of the class imbalance in this dataset (90% of papers are rejected). Surprisingly, Inception $_{\text{fixed}}$ is better than Majority and Benchmark over the arXiv cs.lg subset, which verifies the usefulness of visual features, even when only the last layer is fine-tuned. Table 3 also shows that Inception and biLSTM achieve similar performance on arXiv, showing that textual and visual representations are equally discriminative: Inception and biLSTM are indistinguishable over cs.cl; biLSTM achieves 1.8% higher accuracy over cs.ai; and Inception achieves 1.8% higher accuracy over cs.cl.", so we know that the textual and visual features complement each other, achieving state-of-the-art results in combination.
Answer: textual and visual features complement each other
True answer: It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. 

Validation ID: c9bc6f53b941863e801280343afa14248521ce43
Question: Which languages do they use?
From the context "All code and data associated with this research will be released on publication", so we know that they use code and data.
Answer: code and data
True answer: English

Validation ID: c9bc6f53b941863e801280343afa14248521ce43
Question: Which languages do they use?
From the context "All code and data associated with this research will be released on publication", so we know that they use code and data.
Answer: code and data
True answer: English

Validation ID: 07b70b2b799b9efa630e8737df8b1dd1284f032c
Question: How large is their data set?
From the context "The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”). A description of the criteria associated with the different classes can be found in the Wikipedia grading scheme page. The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. We constructed the dataset by first crawling all articles from each quality class repository, e.g., we get FA articles by crawling pages from the FA repository: https://en.wikipedia.org/wiki/Category:Featured_articles. This resulted in around 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2M Stub articles.", so we know that the Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”).
Answer: 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2M Stub articles
True answer: a sample of  29,794 wikipedia articles and 2,794 arXiv papers 

Validation ID: 71a0c4f19be4ce1b1bae58a6e8f2a586e125d074
Question: Where do they get their ground truth quality judgments?
From the context "The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”). A description of the criteria associated with the different classes can be found in the Wikipedia grading scheme page.", so we know that the Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community.
Answer: Wikipedia community
True answer: Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”)., The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus., The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). 

Validation ID: 71a0c4f19be4ce1b1bae58a6e8f2a586e125d074
Question: Where do they get their ground truth quality judgments?
From the context "The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”). A description of the criteria associated with the different classes can be found in the Wikipedia grading scheme page.", so we know that the Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community.
Answer: Wikipedia community
True answer: quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI

Validation ID: c2eb743c9d0baf1781c3c0df9533fab588250af3
Question: Which models did they experiment with?
From the context "We evaluate our method on natural language inference (NLI), paraphrase identification (PI), and sentiment classification. We also conduct analysis on gate values and experiments on model variants. For detailed experimental settings, we refer readers to the supplemental material.", so we know that they experiment with natural language inference (NLI), paraphrase identification (PI), sentiment classification, analysis on gate values and experiments on model variants.
Answer: natural language inference (NLI), paraphrase identification (PI), sentiment classification, analysis on gate values and experiments on model variants
True answer: Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers

Validation ID: c35806cf68220b2b9bb082b62f493393b9bdff86
Question: What were their best results on the benchmark datasets?
From the context "Our models achieve the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and obtain comparable results on MultiNLI and SST datasets", so we know that their best results on the benchmark datasets are the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and comparable results on MultiNLI and SST datasets.
Answer: the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and comparable results on MultiNLI and SST datasets
True answer: In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5

Validation ID: c35806cf68220b2b9bb082b62f493393b9bdff86
Question: What were their best results on the benchmark datasets?
From the context "Our models achieve the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and obtain comparable results on MultiNLI and SST datasets", so we know that their best results on the benchmark datasets are the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and comparable results on MultiNLI and SST datasets.
Answer: the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and comparable results on MultiNLI and SST datasets
True answer: accuracy of 87.0%

Validation ID: f7d0fa52017a642a9f70091a252857fccca31f12
Question: What were the baselines?
From the context "We evaluate our method on natural language inference (NLI), paraphrase identification (PI), and sentiment classification. We also conduct analysis on gate values and experiments on model variants. For detailed experimental settings, we refer readers to the supplemental material.", so we know that the baselines are natural language inference (NLI), paraphrase identification (PI), and sentiment classification.
Answer: natural language inference (NLI), paraphrase identification (PI), and sentiment classification
True answer: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections

Validation ID: 01209a3bead7c87bcdc628be2a5a26b41abde9d1
Question: Which datasets were used?
From the context "For the evaluation of performance of the proposed method on the NLI task, SNLI BIBREF22 and MultiNLI BIBREF23 datasets are used. For the evaluation of performance of the proposed method on the PI task, we use Quora Question Pairs dataset BIBREF24. For the evaluation of performance of the proposed method on the sentiment classification task, the Stanford Sentiment Treebank (SST) BIBREF25 is used.", so we know that SNLI BIBREF22, MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24 and Stanford Sentiment Treebank (SST) BIBREF25 were used.
Answer: SNLI BIBREF22, MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24 and Stanford Sentiment Treebank (SST) BIBREF25
True answer: SNLI BIBREF22 and MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24,  Stanford Sentiment Treebank (SST) BIBREF25

Validation ID: 01209a3bead7c87bcdc628be2a5a26b41abde9d1
Question: Which datasets were used?
From the context "For the evaluation of performance of the proposed method on the NLI task, SNLI BIBREF22 and MultiNLI BIBREF23 datasets are used. For the evaluation of performance of the proposed method on the PI task, we use Quora Question Pairs dataset BIBREF24. For the evaluation of performance of the proposed method on the sentiment classification task, the Stanford Sentiment Treebank (SST) BIBREF25 is used.", so we know that SNLI BIBREF22, MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24 and Stanford Sentiment Treebank (SST) BIBREF25 were used.
Answer: SNLI BIBREF22, MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24 and Stanford Sentiment Treebank (SST) BIBREF25
True answer: SNLI BIBREF22 and MultiNLI BIBREF23 datasets, Quora Question Pairs dataset BIBREF24, Stanford Sentiment Treebank (SST) BIBREF25

Validation ID: 2740e3d7d33173664c1c5ab292c7ec75ff6e0802
Question: what datasets were used?
From the context "For MSA, we acquired the diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31. The corpus contains 9.7M tokens with approximately 194K unique surface forms (excluding numbers and punctuation marks). The corpus covers multiple genres such as politics and sports and is a mix of MSA and CA. This corpus is considerably larger than the Arabic Treebank BIBREF35 and is more consistent in its diacritization. For testing, we used the freely available WikiNews test set BIBREF31, which is composed of 70 MSA WikiNews articles (18,300 tokens) and evenly covers a variety of genres including politics, economics, health, science and technology, sports, arts and culture.", so we know that the datasets used are WikiNews test set and Arabic Treebank.
Answer: WikiNews test set and Arabic Treebank
True answer: diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31, WikiNews test set BIBREF31,  large collection of fully diacritized classical texts (2.7M tokens) from a book publisher

Validation ID: 2740e3d7d33173664c1c5ab292c7ec75ff6e0802
Question: what datasets were used?
From the context "For MSA, we acquired the diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31. The corpus contains 9.7M tokens with approximately 194K unique surface forms (excluding numbers and punctuation marks). The corpus covers multiple genres such as politics and sports and is a mix of MSA and CA. This corpus is considerably larger than the Arabic Treebank BIBREF35 and is more consistent in its diacritization. For testing, we used the freely available WikiNews test set BIBREF31, which is composed of 70 MSA WikiNews articles (18,300 tokens) and evenly covers a variety of genres including politics, economics, health, science and technology, sports, arts and culture.", so we know that the datasets used are WikiNews test set and Arabic Treebank.
Answer: WikiNews test set and Arabic Treebank
True answer: the diacritized corpus that was used to train the RDI BIBREF7 diacritizer , WikiNews , a large collection of fully diacritized classical texts

Validation ID: db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21
Question: what are the previous state of the art?
From the context "We expand upon the work in the literature by introducing feature-rich DNN models for restoring both CW and CE diacritics. We compare our models to multiple systems on the same test set. We achieve results that reduce diacritization error rates by more than half compared to the best SOTA systems.", so we know that the previous state of the art is the best SOTA systems.
Answer: the best SOTA systems
True answer: Farasa, RDI

Validation ID: db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21
Question: what are the previous state of the art?
From the context "We expand upon the work in the literature by introducing feature-rich DNN models for restoring both CW and CE diacritics. We compare our models to multiple systems on the same test set. We achieve results that reduce diacritization error rates by more than half compared to the best SOTA systems.", so we know that the previous state of the art is the best SOTA systems.
Answer: the best SOTA systems
True answer: Farasa BIBREF31, MADAMIRA BIBREF29, RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), Microsoft ATKS BIBREF28

Validation ID: 48bd71477d5f89333fa7ce5c4556e4d950fb16ed
Question: what surface-level features are used?
From the context "We used the entirety of the training set as input, and we instructed Keras to use 5% of the data for tuning (validation). We included the CASE feature, which specifies whether the letter accepts a normal diacritic or case ending, in all our setups. We also optionally employed post correction. For words that were seen in training, if the model produced a diacritized form that was not seen in the training data, we assumed it was an error and replaced it with the most frequently observed diacritized form (using a unigram language model). We report two error rates, namely WER (at word level) and DER (at character level). We used relaxed scoring where we assumed an empty case to be equivalent to sukun, and we removed default diacritics – fatHa followed by alef, kasra followed by ya, and damma followed by wa. Using such scoring would allow to compare to other systems in the literature that may use different diacritization conventions.", so we know that the surface-level features are:
CHAR: the characters.
True answer: affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities

Validation ID: 76ed74788e3eb3321e646c48ae8bf6cdfe46dca1
Question: what linguistics features are used?
From the context "We used Farasa to perform segmentation and POS tagging and to determine stem-templates BIBREF31. Farasa has a reported POS accuracy of 96% on the WikiNews dataset BIBREF31.", so we know that Farasa is used to perform segmentation and POS tagging and to determine stem-templates.
Answer: segmentation and POS tagging and to determine stem-templates
True answer: POS, gender/number and stem POS

Validation ID: ad1be65c4f0655ac5c902d17f05454c0d4c4a15d
Question: what dataset statistics are provided?
From the context "For all experiments, we admitted only experienced MTurk workers who are based in the US. One HIT consisted of writing one text for the text collection, formulating 15 questions for the question collection, or finding 15 pairs of answers for the answer collection. We paid $0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection.", so we know that the dataset statistics are provided as follows:

True answer: More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).

Validation ID: ad1be65c4f0655ac5c902d17f05454c0d4c4a15d
Question: what dataset statistics are provided?
From the context "For all experiments, we admitted only experienced MTurk workers who are based in the US. One HIT consisted of writing one text for the text collection, formulating 15 questions for the question collection, or finding 15 pairs of answers for the answer collection. We paid $0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection.", so we know that the dataset statistics are provided as follows:

True answer: Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text

Validation ID: 2eb9280d72cde9de3aabbed993009a98a5fe0990
Question: what is the size of their dataset?
From the context "For all experiments, we admitted only experienced MTurk workers who are based in the US. One HIT consisted of writing one text for the text collection, formulating 15 questions for the question collection, or finding 15 pairs of answers for the answer collection. We paid $0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection.", so we know that the size of their dataset is $0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection.
Answer: $0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection
True answer: 13,939

Validation ID: 154a721ccc1d425688942e22e75af711b423e086
Question: what crowdsourcing platform was used?
From the context "We collected 20 texts for each scenario. Each participant was allowed to write only one story per scenario, but work on as many scenarios as they liked. For each of the 10 scenarios from InScript, we randomly selected 20 existing texts from that resource.", so we know that the crowdsourcing platform is Amazon Mechanical Turk.
Answer: Amazon Mechanical Turk
True answer: Amazon Mechanical Turk

Validation ID: 154a721ccc1d425688942e22e75af711b423e086
Question: what crowdsourcing platform was used?
From the context "We collected 20 texts for each scenario. Each participant was allowed to write only one story per scenario, but work on as many scenarios as they liked. For each of the 10 scenarios from InScript, we randomly selected 20 existing texts from that resource.", so we know that the crowdsourcing platform is Amazon Mechanical Turk.
Answer: Amazon Mechanical Turk
True answer: Amazon Mechanical Turk

Validation ID: 84bad9a821917cb96584cf5383c6d2a035358d7c
Question: how was the data collected?
From the context "For collecting questions, workers were instructed to “imagine they told a story about a certain scenario to a child and want to test if the child understood everything correctly”. This instruction also ensured that questions are linguistically simple, elaborate and explicit. Workers were asked to formulate questions about details of such a situation, i.e. independent of a concrete narrative. This resulted in questions, the answer to which is not literally mentioned in the text. To cover a broad range of question types, we asked participants to write 3 temporal questions (asking about time points and event order), 3 content questions (asking about persons or details in the scenario) and 3 reasoning questions (asking how or why something happened). They were also asked to formulate 6 free questions, which resulted in a total of 15 questions. Asking each worker for a high number of questions enforced that more creative questions were formulated, which go beyond obvious questions for a scenario. Since participants were not shown a concrete story, we asked them to use the neutral pronoun “they” to address the protagonist of the story. We permitted participants to work on as many scenarios as desired and we collected questions from 10 participants per scenario.", so we know that the data was collected by asking participants to write a story about a certain scenario to a child and want to test if the child understood everything correctly.
Answer: asking participants to write a story about a certain scenario to a child and want to test if the child understood everything correctly
True answer: The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.

Validation ID: c9305e5794b65b33399c22ac8e4e024f6b757a30
Question: What is best performing model among author's submissions, what performance it had?
From the context "Table TABREF9 (FLC) shows that our submission is ranked at 3rd position", so we know that the best performing model among author's submissions is ranked at 3rd position.
Answer: ranked at 3rd position
True answer: For SLC task, the "ltuorp" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the "newspeak" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).

Validation ID: 56b7319be68197727baa7d498fa38af0a8440fe4
Question: What extracted features were most influencial on performance?
From the context "We explore different neural architectures (CNN, BERT and LSTM-CRF) with linguistic, layout and topical features to address the tasks of fine-grained propaganda detection. We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures.", so we know that the extracted features were most influencial on performance are linguistic, layout and topical features.
Answer: linguistic, layout and topical features
True answer: Linguistic

Validation ID: 56b7319be68197727baa7d498fa38af0a8440fe4
Question: What extracted features were most influencial on performance?
From the context "We explore different neural architectures (CNN, BERT and LSTM-CRF) with linguistic, layout and topical features to address the tasks of fine-grained propaganda detection. We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures.", so we know that the extracted features were most influencial on performance are linguistic, layout and topical features.
Answer: linguistic, layout and topical features
True answer: BERT

Validation ID: 2268c9044e868ba0a16e92d2063ada87f68b5d03
Question: Did ensemble schemes help in boosting peformance, by how much?
From the context "We investigate different ensemble schemes (r17-r19), where we observe that the relax-voting improves recall and therefore, the higher F1 (i.e., 0.673). In postprocess step, we check for repetition propaganda technique by computing cosine similarity between the current sentence and its preceding $w=10$ sentence vectors (i.e., BERTSentEmb) in the document. If the cosine-similarity is greater than $\lambda \in \lbrace.99,.95\rbrace $, then the current sentence is labeled as propaganda due to repetition. Comparing r19 and r21, we observe a gain in recall, however an overall decrease in F1 applying postprocess.", so we know that ensemble schemes help in boosting peformance.
Answer: Yes
True answer: The best ensemble topped the best single model by 0.029 in F1 score on dev (external).

Validation ID: 2268c9044e868ba0a16e92d2063ada87f68b5d03
Question: Did ensemble schemes help in boosting peformance, by how much?
From the context "We investigate different ensemble schemes (r17-r19), where we observe that the relax-voting improves recall and therefore, the higher F1 (i.e., 0.673). In postprocess step, we check for repetition propaganda technique by computing cosine similarity between the current sentence and its preceding $w=10$ sentence vectors (i.e., BERTSentEmb) in the document. If the cosine-similarity is greater than $\lambda \in \lbrace.99,.95\rbrace $, then the current sentence is labeled as propaganda due to repetition. Comparing r19 and r21, we observe a gain in recall, however an overall decrease in F1 applying postprocess.", so we know that ensemble schemes help in boosting peformance.
Answer: Yes
True answer: They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification

Validation ID: 6b7354d7d715bad83183296ce2f3ddf2357cb449
Question: Which basic neural architecture perform best by itself?
From the context "We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures. Compared to the other participating systems, our submissions are ranked 3rd and 4th in FLC and SLC tasks, respectively.", so we know that the basic neural architecture perform best by itself is LSTM-CRF.
Answer: LSTM-CRF
True answer: BERT

Validation ID: e949b28f6d1f20e18e82742e04d68158415dc61e
Question: What participating systems had better results than ones authors submitted?
From the context "Compared to the other participating systems, our submissions are ranked 3rd and 4th in FLC and SLC tasks, respectively", so we know that the participating systems that had better results than ones authors submitted are ranked 1st and 2nd in FLC and SLC tasks, respectively.
Answer: ranked 1st and 2nd in FLC and SLC tasks, respectively
True answer: For SLC task : Ituorp, ProperGander and YMJA  teams had better results.
For FLC task: newspeak and Antiganda teams had better results.

Validation ID: a1ac2a152710335519c9a907eec60d9f468b19db
Question: What is specific to multi-granularity and multi-tasking neural arhiteture design?
From the context "We design sequence taggers BIBREF9, BIBREF10 in three modes: (1) LSTM-CRF BIBREF11 with word embeddings ($w\_e$) and character embeddings $c\_e$, token-level features ($t\_f$) such as polarity, POS, NER, etc. (2) LSTM-CRF+Multi-grain that jointly performs FLC and SLC with FastTextWordEmb and BERTSentEmb, respectively. Here, we add binary sentence classification loss to sequence tagging weighted by a factor of $\alpha $. (3) LSTM-CRF+Multi-task that performs propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification).", so we know that multi-granularity and multi-tasking neural arhiteture design is specific to sequence taggers.
Answer: sequence taggers
True answer: An output layer for each task

Validation ID: a1ac2a152710335519c9a907eec60d9f468b19db
Question: What is specific to multi-granularity and multi-tasking neural arhiteture design?
From the context "We design sequence taggers BIBREF9, BIBREF10 in three modes: (1) LSTM-CRF BIBREF11 with word embeddings ($w\_e$) and character embeddings $c\_e$, token-level features ($t\_f$) such as polarity, POS, NER, etc. (2) LSTM-CRF+Multi-grain that jointly performs FLC and SLC with FastTextWordEmb and BERTSentEmb, respectively. Here, we add binary sentence classification loss to sequence tagging weighted by a factor of $\alpha $. (3) LSTM-CRF+Multi-task that performs propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification).", so we know that multi-granularity and multi-tasking neural arhiteture design is specific to sequence taggers.
Answer: sequence taggers
True answer: Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT.

Validation ID: ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25
Question: Do they report results only on English data?
From the context "We evaluate our proposed models over a corpus of 12 MOOC iterations (offerings) on Coursera.org", so we know that the corpus is from Coursera.org.
From the context "We only include threads from sub-forums on Lecture, Homework, Quiz and Exam", so we know that the corpus is from sub-forums on Lecture, Homework, Quiz and Exam.
True answer: Unanswerable

Validation ID: ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25
Question: Do they report results only on English data?
From the context "We evaluate our proposed models over a corpus of 12 MOOC iterations (offerings) on Coursera.org", so we know that the corpus is from Coursera.org.
From the context "We only include threads from sub-forums on Lecture, Homework, Quiz and Exam", so we know that the corpus is from sub-forums on Lecture, Homework, Quiz and Exam.
True answer: Unanswerable

Validation ID: 49eb52b3ec0647e165a5e41488088c80a20cc78f
Question: What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?
From the context "We propose three neural attention BIBREF11 variants based on how an instructor might attend and reply to a context in a thread: the ultimate, penultimate and any post attention models. We review each of these in turn.", so we know that the three neural attention variants are ultimate, penultimate and any post attention models.
From the context "Ultimate Post Attention (UPA) Model. In this model we attend to the context represented by hidden state of the INLINEFORM0. We use the post prior to the instructor's reply as a query over the contexts INLINEFORM1 to compute attention weights INLINEFORM2, which are then used to compute the attended context representation INLINEFORM3 (recall again that the intervention text itself is not available for this purpose). This attention formulation makes an equivalence between the final INLINEFORM4 post and the prospective intervention, using Post INLINEFORM5 as the query for finding the appropriate context INLINEFORM6, inclusive of itself INLINEFORM7. For example, if post INLINEFORM0 is the instructor's reply, post INLINEFORM1 will query over the contexts INLINEFORM2 and INLINEFORM3. The model schematic is shown in Figure FIGREF12.", so we know that the ultimate post attention model attends to the context represented by hidden state of the INLINEFORM0.
True answer: context inference

Validation ID: 9bb7ae50bff91571a945c1af025ed2e67714a788
Question: What was the previous state of the art for this task?
From the context "We compare our models against a neural baseline models, hierarchical LSTM (hLSTM), with the attention ablated but with access to the complete context, and a strong, open-sourced feature-rich baseline BIBREF7. We choose BIBREF7 over other prior works such as BIBREF0 since we do not have access to the dataset or the system used in their papers for replication.", so we know that the previous state of the art for this task is BIBREF7.
Answer: BIBREF7
True answer: hLSTM

Validation ID: 9bb7ae50bff91571a945c1af025ed2e67714a788
Question: What was the previous state of the art for this task?
From the context "We compare our models against a neural baseline models, hierarchical LSTM (hLSTM), with the attention ablated but with access to the complete context, and a strong, open-sourced feature-rich baseline BIBREF7. We choose BIBREF7 over other prior works such as BIBREF0 since we do not have access to the dataset or the system used in their papers for replication.", so we know that the previous state of the art for this task is BIBREF7.
Answer: BIBREF7
True answer: hLSTM

Validation ID: 81dbe9a9ddaa5d02b02e01a306d898015a56ffb6
Question: What type of latent context is used to predict instructor intervention?
From the context "We propose three neural attention BIBREF11 variants based on how an instructor might attend and reply to a context in a thread: the ultimate, penultimate and any post attention models. We review each of these in turn.", so we know that there are three neural attention variants based on how an instructor might attend and reply to a context in a thread: the ultimate, penultimate and any post attention models.
From the context "Ultimate Post Attention (UPA) Model. In this model we attend to the context represented by hidden state of the INLINEFORM0. We use the post prior to the instructor's reply as a query over the contexts INLINEFORM1 to compute attention weights INLINEFORM2, which are then used to compute the attended context representation INLINEFORM3 (recall again that the intervention text itself is not available for this purpose). This attention formulation makes an equivalence between the final INLINEFORM4 post and the prospective intervention, using Post INLINEFORM5 as the query for finding the appropriate context INLINEFORM6, inclusive of itself INLINEFORM7. For example, if post INLINEFORM0 is the instructor's reply, post INLINEFORM1 will query over the contexts INLINEFORM2 and INLINEFORM3. The model schematic is shown in Figure FIGREF12.", so we know that the ultimate post attention model attends to the context represented by hidden state of the INLINEFORM0. The post prior to the instructor's reply is used as a query over the contexts INLINEFORM1 to compute attention weights INLINEFORM2, which are then used to compute the attended context representation INLINEFORM3.
True answer: the series of posts that trigger an intervention

Validation ID: 348886b4762db063711ef8b7a10952375fbdcb57
Question: Do they report results only on English dataset?
From the context "We build and test our MMT models on the Multi30K dataset BIBREF21. Each image in Multi30K contains one English (EN) description taken from Flickr30K BIBREF22 and human translations into German (DE), French (FR) and Czech BIBREF23, BIBREF24, BIBREF25. The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 BIBREF25.", so we know that they report results only on English dataset.
Answer: Yes
True answer: No

Validation ID: 348886b4762db063711ef8b7a10952375fbdcb57
Question: Do they report results only on English dataset?
From the context "We build and test our MMT models on the Multi30K dataset BIBREF21. Each image in Multi30K contains one English (EN) description taken from Flickr30K BIBREF22 and human translations into German (DE), French (FR) and Czech BIBREF23, BIBREF24, BIBREF25. The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 BIBREF25.", so we know that they report results only on English dataset.
Answer: Yes
True answer: No

Validation ID: 1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b
Question: What dataset does this approach achieve state of the art results on?
From the context "We build and test our MMT models on the Multi30K dataset BIBREF21. Each image in Multi30K contains one English (EN) description taken from Flickr30K BIBREF22 and human translations into German (DE), French (FR) and Czech BIBREF23, BIBREF24, BIBREF25. The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 BIBREF25.", so we know that this approach achieves state of the art results on Multi30K dataset.
Answer: Multi30K dataset
True answer: the English-German dataset

Validation ID: f9aa055bf73185ba939dfb03454384810eb17ad1
Question: How much training data from the non-English language is used by the system?
From the context "We use WikiExtractor to extract extract raw sentences from Wikipedias as monolingual data for fine-tuning target embeddings and bilingual LMs (SECREF15). We do not lowercase or remove accents in our data preprocessing pipeline.", so we know that the system uses raw sentences from Wikipedias as monolingual data for fine-tuning target embeddings and bilingual LMs.
Answer: raw sentences from Wikipedias
True answer: No data. Pretrained model is used.

Validation ID: d571e0b0f402a3d36fb30d70cdcd2911df883bc7
Question: Is the system tested on low-resource languages?
From the context "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi). These languages belong to four different language families. French, Russian, and Hindi are Indo-European languages, similar to English. Arabic, Chinese, and Vietnamese belong to Afro-Asiatic, Sino-Tibetan, and Austro-Asiatic family respectively. The choice of the six languages also reflects different training conditions depending on the amount of monolingual data. French and Russian, and Arabic can be regarded as high resource languages whereas Hindi has far less data and can be considered as low resource.", so we know that the system is tested on low-resource languages.
Answer: Yes
True answer: Yes

Validation ID: d571e0b0f402a3d36fb30d70cdcd2911df883bc7
Question: Is the system tested on low-resource languages?
From the context "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi). These languages belong to four different language families. French, Russian, and Hindi are Indo-European languages, similar to English. Arabic, Chinese, and Vietnamese belong to Afro-Asiatic, Sino-Tibetan, and Austro-Asiatic family respectively. The choice of the six languages also reflects different training conditions depending on the amount of monolingual data. French and Russian, and Arabic can be regarded as high resource languages whereas Hindi has far less data and can be considered as low resource.", so we know that the system is tested on low-resource languages.
Answer: Yes
True answer: Yes

Validation ID: ce2b921e4442a21555d65d8ce4ef7e3bde931dfc
Question: What languages are the model transferred to?
From the context "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi). These languages belong to four different language families. French, Russian, and Hindi are Indo-European languages, similar to English. Arabic, Chinese, and Vietnamese belong to Afro-Asiatic, Sino-Tibetan, and Austro-Asiatic family respectively. The choice of the six languages also reflects different training conditions depending on the amount of monolingual data. French and Russian, and Arabic can be regarded as high resource languages whereas Hindi has far less data and can be considered as low resource.", so we know that the model is transferred to six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi).
Answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)
True answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)

Validation ID: ce2b921e4442a21555d65d8ce4ef7e3bde931dfc
Question: What languages are the model transferred to?
From the context "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi). These languages belong to four different language families. French, Russian, and Hindi are Indo-European languages, similar to English. Arabic, Chinese, and Vietnamese belong to Afro-Asiatic, Sino-Tibetan, and Austro-Asiatic family respectively. The choice of the six languages also reflects different training conditions depending on the amount of monolingual data. French and Russian, and Arabic can be regarded as high resource languages whereas Hindi has far less data and can be considered as low resource.", so we know that the model is transferred to six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi).
Answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)
True answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)

Validation ID: 2275b0e195cd9cb25f50c5c570da97a4cce5dca8
Question: How is the model transferred to other languages?
From the context "We propose a fast adaptation method for obtaining a bilingual BERT$_{\textsc {base}}$ of English and a target language within a day using one Tesla V100 16GB GPU", so we know that the model is transferred to other languages within a day using one Tesla V100 16GB GPU.
Answer: within a day using one Tesla V100 16GB GPU
True answer: Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM.

Validation ID: 37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6
Question: What metrics are used for evaluation?
From the context "We evaluate our models on two cross-lingual zero-shot tasks: (1) Cross-lingual Natural Language Inference (XNLI) and (2) dependency parsing", so we know that the metrics used for evaluation are Cross-lingual Natural Language Inference (XNLI) and dependency parsing.
Answer: Cross-lingual Natural Language Inference (XNLI) and dependency parsing
True answer: translation probabilities, Labeled Attachment Scores (LAS)

Validation ID: 37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6
Question: What metrics are used for evaluation?
From the context "We evaluate our models on two cross-lingual zero-shot tasks: (1) Cross-lingual Natural Language Inference (XNLI) and (2) dependency parsing", so we know that the metrics used for evaluation are Cross-lingual Natural Language Inference (XNLI) and dependency parsing.
Answer: Cross-lingual Natural Language Inference (XNLI) and dependency parsing
True answer: accuracy, Labeled Attachment Scores (LAS)

Validation ID: d01c51155e4719bf587d114bcd403b273c77246f
Question: What datasets are used for evaluation?
From the context "We evaluate our models on two cross-lingual zero-shot tasks: (1) Cross-lingual Natural Language Inference (XNLI) and (2) dependency parsing", so we know that the datasets used for evaluation are XNLI and dependency parsing.
Answer: XNLI and dependency parsing
True answer: United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018

Validation ID: 9b4dc790e4ff49562992aae4fad3a38621fadd8b
Question: what are the existing approaches?
From the context "Many recent studies have highlighted that Flickr tags capture valuable ecological information, which can be used as a complementary source to more traditional sources. To date, however, ecologists have mostly used social media to conduct manual evaluations of image content with little automated exploitation of the associated tags BIBREF4, BIBREF5, BIBREF6. One recent exception is BIBREF7, where bag-of-words representations derived from Flickr tags were found to give promising result for predicting a range of different environemental phenomena.", so we know that the existing approaches are manual evaluations of image content and bag-of-words representations derived from Flickr tags.
Answer: manual evaluations of image content and bag-of-words representations derived from Flickr tags
True answer: BOW-Tags, BOW-KL(Tags), BOW-All, GloVe

Validation ID: a1dac888f63c9efaf159d9bdfde7c938636f07b1
Question: what dataset is used in this paper?
From the context "We will use the method from BIBREF7 as our main baseline. This will allow us to directly evaluate the effectiveness of embeddings for the considered problem, since we have used the same structured datasets and same tag weighting scheme", so we know that the dataset is the same structured datasets and same tag weighting scheme.
Answer: the same structured datasets and same tag weighting scheme
True answer:  the same datasets as BIBREF7

Validation ID: a1dac888f63c9efaf159d9bdfde7c938636f07b1
Question: what dataset is used in this paper?
From the context "We will use the method from BIBREF7 as our main baseline. This will allow us to directly evaluate the effectiveness of embeddings for the considered problem, since we have used the same structured datasets and same tag weighting scheme", so we know that the dataset is the same structured datasets and same tag weighting scheme.
Answer: the same structured datasets and same tag weighting scheme
True answer: same datasets as BIBREF7

Validation ID: 1e4dbfc556cf237accb8b370de2f164fa723687b
Question: How is keyphrase diversity measured?
From the context "We propose a mechanism called semantic coverage that focuses on the semantic representations of generated phrases. Specifically, we introduce another uni-directional recurrent model INLINEFORM0 (dubbed target encoder) which encodes decoder-generated tokens INLINEFORM1, where INLINEFORM2, into hidden states INLINEFORM3. This state is then taken as an extra input to the decoder GRU, modifying Equation EQREF12 to: DISPLAYFORM0", so we know that keyphrase diversity is measured by semantic coverage.
Answer: semantic coverage
True answer: average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set

Validation ID: fff5c24dca92bc7d5435a2600e6764f039551787
Question: How was the StackExchange dataset collected?
From the context "The StackExchange dataset is collected from the StackExchange website, which contains a collection of question-and-answer websites on topics in diverse fields, including mathematics, physics, philosophy, and statistics. We use the Mathematics StackExchange website as our source, which contains over 1.2 million questions and answers. We use the title and body of each question as the source text, and the tags of the question as the target keyphrases.", so we know that the StackExchange dataset is collected from the StackExchange website.
Answer: StackExchange website
True answer: they obtained computer science related topics by looking at titles and user-assigned tags

Validation ID: fff5c24dca92bc7d5435a2600e6764f039551787
Question: How was the StackExchange dataset collected?
From the context "The StackExchange dataset is collected from the StackExchange website, which contains a collection of question-and-answer websites on topics in diverse fields, including mathematics, physics, philosophy, and statistics. We use the Mathematics StackExchange website as our source, which contains over 1.2 million questions and answers. We use the title and body of each question as the source text, and the tags of the question as the target keyphrases.", so we know that the StackExchange dataset is collected from the StackExchange website.
Answer: StackExchange website
True answer: Unanswerable

Validation ID: b2ecfd5480a2a4be98730e2d646dfb84daedab17
Question: What does the TextWorld ACG dataset contain?
From the context "The TextWorld ACG dataset contains over 1,000,000 text-based adventure games, each with a unique story, setting, and characters", so we know that the TextWorld ACG dataset contains over 1,000,000 text-based adventure games, each with a unique story, setting, and characters.
Answer: over 1,000,000 text-based adventure games, each with a unique story, setting, and characters
True answer: Unanswerable

Validation ID: a3efe43a72b76b8f5e5111b54393d00e6a5c97ab
Question: What is the size of the StackExchange dataset?
From the context "Our first dataset consists of a collection of scientific publication datasets, namely KP20k, Inspec, Krapivin, NUS, and SemEval, that have been widely used in existing literature BIBREF0, BIBREF1, BIBREF2, BIBREF3. KP20k, for example, was introduced by BIBREF0 and comprises more than half a million scientific publications. For each article, the abstract and title are used as the source text while the author keywords are used as target. The other four datasets contain much fewer articles, and thus used to test transferability of our model (without fine-tuning).", so we know that the size of the StackExchange dataset is much fewer articles.
Answer: much fewer articles
True answer: Unanswerable

Validation ID: a3efe43a72b76b8f5e5111b54393d00e6a5c97ab
Question: What is the size of the StackExchange dataset?
From the context "Our first dataset consists of a collection of scientific publication datasets, namely KP20k, Inspec, Krapivin, NUS, and SemEval, that have been widely used in existing literature BIBREF0, BIBREF1, BIBREF2, BIBREF3. KP20k, for example, was introduced by BIBREF0 and comprises more than half a million scientific publications. For each article, the abstract and title are used as the source text while the author keywords are used as target. The other four datasets contain much fewer articles, and thus used to test transferability of our model (without fine-tuning).", so we know that the size of the StackExchange dataset is much fewer articles.
Answer: much fewer articles
True answer: around 332k questions

Validation ID: f1e90a553a4185a4b0299bd179f4f156df798bce
Question: What were the baselines?
From the context "We include four non-neural extractive models and CopyRNN BIBREF0 as baselines. We use CopyRNN to denote the model reported by BIBREF0, CopyRNN* to denote our implementation of CopyRNN based on their open sourced code.", so we know that the baselines are four non-neural extractive models and CopyRNN.
Answer: four non-neural extractive models and CopyRNN
True answer: CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)

Validation ID: f1e90a553a4185a4b0299bd179f4f156df798bce
Question: What were the baselines?
From the context "We include four non-neural extractive models and CopyRNN BIBREF0 as baselines. We use CopyRNN to denote the model reported by BIBREF0, CopyRNN* to denote our implementation of CopyRNN based on their open sourced code.", so we know that the baselines are four non-neural extractive models and CopyRNN.
Answer: four non-neural extractive models and CopyRNN
True answer: CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*

Validation ID: 19b7312cfdddb02c3d4eaa40301a67143a72a35a
Question: What two metrics are proposed?
From the context "We propose novel decoding strategies and evaluation metrics for the keyphrase generation task. The main contributions of this work are as follows:", so we know that the main contributions of this work are novel decoding strategies and evaluation metrics for the keyphrase generation task.
From the context "To overcome these problems, we propose novel decoding strategies and evaluation metrics for the keyphrase generation task. The main contributions of this work are as follows:", so we know that the main contributions of this work are novel decoding strategies and evaluation metrics for the keyphrase generation task.
True answer: average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )

Validation ID: 22744c3bc68f120669fc69490f8e539b09e34b94
Question: Can the findings of this paper be generalized to a general-purpose task?
From the context "The task presented here was easy and simple to analyze, however, future work may be done on natural language tasks. If these properties hold it might indicate that a new evaluation paradigm for NLP should be pushed; one that emphasizes performance on uncharacteristic (but structurally sound) inputs in addition to the data typically seen in training.", so we know that the findings of this paper can be generalized to a general-purpose task.
Answer: Yes
True answer: Yes

Validation ID: 22744c3bc68f120669fc69490f8e539b09e34b94
Question: Can the findings of this paper be generalized to a general-purpose task?
From the context "The task presented here was easy and simple to analyze, however, future work may be done on natural language tasks. If these properties hold it might indicate that a new evaluation paradigm for NLP should be pushed; one that emphasizes performance on uncharacteristic (but structurally sound) inputs in addition to the data typically seen in training.", so we know that the findings of this paper can be generalized to a general-purpose task.
Answer: Yes
True answer: Yes

Validation ID: dcea88698949da4a1bd00277c06df06c33f6a5ff
Question: Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks?
From the context "The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization", so we know that the proposed task is a good proxy for the general-purpose sequence to sequence tasks.
Answer: The proposed task is a good proxy for the general-purpose sequence to sequence tasks
True answer: The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.

Validation ID: d7b60abb0091246e29d1a9c28467de598e090c20
Question: What was the baseline?
From the context "We find that political discussions on Reddit became increasingly less civil – as measured by the incidence of offensive comments – during the 2016 general election campaign", so we know that the baseline is the incidence of offensive comments.
Answer: the incidence of offensive comments
True answer: stochastic gradient descent, naive bayes, decision tree

Validation ID: d7b60abb0091246e29d1a9c28467de598e090c20
Question: What was the baseline?
From the context "We find that political discussions on Reddit became increasingly less civil – as measured by the incidence of offensive comments – during the 2016 general election campaign", so we know that the baseline is the incidence of offensive comments.
Answer: the incidence of offensive comments
True answer: Unanswerable

Validation ID: bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e
Question: What was their system's performance?
From the context "We find that 93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaway accounts", so we know that their system's performance is 93.7%.
Answer: 93.7%
True answer: accuracy and F1-score of 89.6% and 89.2%, respectively

Validation ID: bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e
Question: What was their system's performance?
From the context "We find that 93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaway accounts", so we know that their system's performance is 93.7%.
Answer: 93.7%
True answer: accuracy and F1-score of 89.6% and 89.2%, respectively

Validation ID: 5a6926de13a8cc25ce687c22741ba97a6e63d4ee
Question: What other political events are included in the database?
From the context "In future work, we will extend our analysis of Reddit back to 2007 with the aim of formulating a more complete understanding of the dynamics of political incivility", so we know that the database includes other political events.
Answer: other political events
True answer: US presidential primaries, Democratic and Republican National Conventions

Validation ID: dcc1115aeaf87118736e86f3e3eb85bf5541281c
Question: What classifier did they use?
From the context "We use these scalars as input to a random forest classifier to perform classification into Offensive and Not Offensive classes", so we know that they use random forest classifier.
Answer: random forest classifier
True answer: Random Forest

Validation ID: c74185bced810449c5f438f11ed6a578d1e359b4
Question: What labels for antisocial events are available in datasets?
From the context "We consider two datasets, representing related but slightly different forecasting tasks. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure the conversations are civil up to the moment of a personal attack", so we know that the labels for antisocial events are personal attack.
Answer: personal attack
True answer: The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: "Don't be rude or hostile to others users."

Validation ID: 88e5d37617e14d6976cc602a168332fc23644f19
Question: What are two datasets model is applied to?
From the context "We consider two datasets, representing related but slightly different forecasting tasks. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure the conversations are civil up to the moment of a personal attack. The second dataset is constructed from the subreddit ChangeMyView (CMV) that does not use post-hoc annotations. Instead, the prediction task is to forecast whether the conversation will be subject to moderator action in the future.", so we know that the model is applied to two datasets: the first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9 and the second dataset is constructed from the subreddit ChangeMyView (CMV).
Answer: the first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9 and the second dataset is constructed from the subreddit ChangeMyView (CMV)
True answer:  `Conversations Gone Awry' dataset, subreddit ChangeMyView

Validation ID: 88e5d37617e14d6976cc602a168332fc23644f19
Question: What are two datasets model is applied to?
From the context "We consider two datasets, representing related but slightly different forecasting tasks. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure the conversations are civil up to the moment of a personal attack. The second dataset is constructed from the subreddit ChangeMyView (CMV) that does not use post-hoc annotations. Instead, the prediction task is to forecast whether the conversation will later be subject to moderator action due to “rude or hostile” behavior.", so we know that the model is applied to two datasets: the first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9 and the second dataset is constructed from the subreddit ChangeMyView (CMV).
Answer: the first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9 and the second dataset is constructed from the subreddit ChangeMyView (CMV)
True answer: An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. 

Validation ID: 45f7c03a686b68179cadb1413c5f3c1d373328bd
Question: What is the CORD-19 dataset?
From the context "We used the COVID-19 Open Research Dataset (CORD-19) BIBREF2 for our study. In response to the COVID-19 pandemic, the White House and a coalition of research groups prepared the CORD-19 dataset. It contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. These articles are contributed by hospitals and medical institutes all over the world.", so we know that the CORD-19 dataset contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.
Answer: the CORD-19 dataset contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.
True answer: which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses

Validation ID: 45f7c03a686b68179cadb1413c5f3c1d373328bd
Question: What is the CORD-19 dataset?
From the context "We used the COVID-19 Open Research Dataset (CORD-19) BIBREF2 for our study. In response to the COVID-19 pandemic, the White House and a coalition of research groups prepared the CORD-19 dataset. It contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. These articles are contributed by hospitals and medical institutes all over the world.", so we know that the CORD-19 dataset contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.
Answer: the CORD-19 dataset contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.
True answer: contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses

Validation ID: a2015f02dfb376bf9b218d1c897018f4e70424d7
Question: How large is the collection of COVID-19 literature?
From the context "We used the COVID-19 Open Research Dataset (CORD-19) BIBREF2 for our study. In response to the COVID-19 pandemic, the White House and a coalition of research groups prepared the CORD-19 dataset. It contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.", so we know that the collection of COVID-19 literature contains over 45,000 scholarly articles, including over 33,000 with full text.
Answer: over 45,000 scholarly articles, including over 33,000 with full text
True answer: 45,000 scholarly articles, including over 33,000 with full text

Validation ID: f697d00a82750b14376fe20a5a2b249e98bebe9b
Question: Which deep learning architecture do they use for sentence segmentation?
From the context "In this work, we adopt the concept of word sequence tagging and compare it with two baselines: the CRF-based model with n-gram embedding, which is currently the state-of-the-art for Thai sentence segmentation, and the Bi-LSTM-CRF model, which is currently the deep learning state-of-the-art approach for sequence tagging", so we know that they use Bi-LSTM-CRF model for sentence segmentation.
Answer: Bi-LSTM-CRF model
True answer: Bi-LSTM-CRF

Validation ID: f697d00a82750b14376fe20a5a2b249e98bebe9b
Question: Which deep learning architecture do they use for sentence segmentation?
From the context "In this work, we adopt the concept of word sequence tagging and compare it with two baselines: the CRF-based model with n-gram embedding, which is currently the state-of-the-art for Thai sentence segmentation, and the Bi-LSTM-CRF model, which is currently the deep learning state-of-the-art approach for sequence tagging", so we know that they use Bi-LSTM-CRF model for sentence segmentation.
Answer: Bi-LSTM-CRF model
True answer: Bi-LSTM-CRF

Validation ID: e0e379e546f1da9da874a2e90c79b41c60feb817
Question: How do they utilize unlabeled data to improve model representations?
From the context "In this work, we adopt the concept of word sequence tagging and compare it with two baselines: the CRF-based model with n-gram embedding, which is currently the state-of-the-art for Thai sentence segmentation, and the Bi-LSTM-CRF model, which is currently the deep learning state-of-the-art approach for sequence tagging.", so we know that they adopt the concept of word sequence tagging and compare it with two baselines: the CRF-based model with n-gram embedding, which is currently the state-of-the-art for Thai sentence segmentation, and the Bi-LSTM-CRF model, which is currently the deep learning state-of-the-art approach for sequence tagging.
From the context "The CRF module achieved the best result on the Thai sentence segmentation task BIBREF8 ; therefore, we adopt the Bi-LSTM-CRF model as our baseline. This paper makes the following three contributions to improve Bi-LSTM-CRF for sentence segmentation.", so we know that the CRF module achieved the best result on the Thai sentence segmentation task BIBREF8 ; therefore, they adopt the Bi-LSTM-CRF model as their baseline. This paper makes the following three contributions to improve Bi-LSTM-CRF for sentence segmentation.
True answer: During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data.

Validation ID: 70148c8d0f345ea36200d5ba19d021924d98e759
Question: What is the McGurk effect?
From the context "We investigate the density of certain classes of illusion, that is, the fraction of natural language utterances whose comprehension can be affected by the illusion. Our study centers around the McGurk effect, which is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see BIBREF0. A prototypical example is that the audio of the phoneme “baa,” accompanied by a video of someone mouthing “vaa”, can be perceived as “vaa” or “gaa” (Figure 1 )", so we know that the McGurk effect is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see.
Answer: the well-studied phenomenon by which the perception of what we hear can be influenced by what we see
True answer: a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard

Validation ID: 70148c8d0f345ea36200d5ba19d021924d98e759
Question: What is the McGurk effect?
From the context "We investigate the density of certain classes of illusion, that is, the fraction of natural language utterances whose comprehension can be affected by the illusion. Our study centers around the McGurk effect, which is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see BIBREF0. A prototypical example is that the audio of the phoneme “baa,” accompanied by a video of someone mouthing “vaa”, can be perceived as “vaa” or “gaa” (Figure 1 )", so we know that the McGurk effect is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see.
Answer: the well-studied phenomenon by which the perception of what we hear can be influenced by what we see
True answer: When the perception of what we hear is influenced by what we see.

Validation ID: 27cf16bc9ef71761b9df6217f00f39f21130ce15
Question: Are humans and machine learning systems fooled by the same kinds of illusions?
From the context "A growing body of work on adversarial examples has identified that for machine-learning (ML) systems that operate on high-dimensional data, for nearly every natural input there exists a small perturbation of the point that will be misclassified by the system, posing a threat to its deployment in certain critical settings BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8. More broadly, the susceptibility of ML systems to adversarial examples has prompted a re-examination of whether current ML systems are truly learning or if they are assemblages of tricks that are effective yet brittle and easily fooled BIBREF9. Implicit in this line of reasoning is the assumption that instances of ”real" learning, such as human cognition, yield extremely robust systems. Indeed, at least in computer vision, human perception is regarded as the gold-standard for robustness to adversarial examples.", so we know that human cognition is regarded as the gold-standard for robustness to adversarial examples.
Answer: Yes
True answer: Unanswerable

Validation ID: 627b8d7b5b985394428c974aca5ba0c1bbbba377
Question: how many humans evaluated the results?
From the context "We conduct a human evaluation on random 700 examples from our general test set. We picked only examples where a DocRepair translation is not a full copy of the baseline one.", so we know that 700 humans evaluated the results.
Answer: 700
True answer: Unanswerable

Validation ID: 627b8d7b5b985394428c974aca5ba0c1bbbba377
Question: how many humans evaluated the results?
From the context "We conduct a human evaluation on random 700 examples from our general test set. We picked only examples where a DocRepair translation is not a full copy of the baseline one.", so we know that 700 humans evaluated the results.
Answer: 700
True answer: Unanswerable

Validation ID: 126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0
Question: what was the baseline?
From the context "We use the publicly available OpenSubtitles2018 corpus BIBREF12 for English and Russian. For a fair comparison with previous work, we train the baseline MT system on the data released by BIBREF11. Namely, our MT system is trained on 6m instances. These are sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least $0.9$.", so we know that the baseline is the data released by BIBREF11.
Answer: the data released by BIBREF11
True answer:  MT system on the data released by BIBREF11

Validation ID: 126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0
Question: what was the baseline?
From the context "We use the publicly available OpenSubtitles2018 corpus BIBREF12 for English and Russian. For a fair comparison with previous work, we train the baseline MT system on the data released by BIBREF11. Namely, our MT system is trained on 6m instances. These are sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least $0.9$.", so we know that the baseline is the data released by BIBREF11.
Answer: the data released by BIBREF11
True answer: Transformer base, two-pass CADec model

Validation ID: 7e4ef0a4debc048b244b61b4f7dc2518b5b466c0
Question: what phenomena do they mention is hard to capture?
From the context "We analyze which discourse phenomena are hard to capture using monolingual data only. Using contrastive test sets for targeted evaluation of several contextual phenomena, we compare the performance of the models trained on round-trip translations and genuine document-level parallel data. Among the four phenomena in the test sets we use (deixis, lexical cohesion, VP ellipsis and ellipsis which affects NP inflection) we find VP ellipsis to be the hardest phenomenon to be captured using round-trip translations", so we know that VP ellipsis is the hardest phenomenon to be captured using round-trip translations.
Answer: VP ellipsis
True answer: Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection.

Validation ID: b68f72aed961d5ba152e9dc50345e1e832196a76
Question: by how much did the BLEU score improve?
From the context "The BLEU scores are provided in Table TABREF24 (we evaluate translations of 4-sentence fragments). To see which part of the improvement is due to fixing agreement between sentences rather than simply sentence-level post-editing, we train the same repair model at the sentence level. Each sentence in a group is now corrected separately, then they are put back together in a group. One can see that most of the improvement comes from accounting for extra-sentential dependencies. DocRepair outperforms the baseline and CADec by 0.7 BLEU, and its sentence-level repair version by 0.5 BLEU.", so we know that the BLEU score improved by 0.7 BLEU.
Answer: 0.7 BLEU
True answer: On average 0.64 

Validation ID: cf874cd9023d901e10aa8664b813d32501e7e4d2
Question: What is NER?
From the context "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc.", so we know that NER is Named Entity Recognition.
Answer: Named Entity Recognition
True answer: Named Entity Recognition

Validation ID: cf874cd9023d901e10aa8664b813d32501e7e4d2
Question: What is NER?
From the context "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc.", so we know that NER is Named Entity Recognition.
Answer: Named Entity Recognition
True answer: Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain

Validation ID: 42084c41343e5a6ae58a22e5bfc5ce987b5173de
Question: Does the paper explore extraction from electronic health records?
From the context "BioIE systems aim to extract information from a wide spectrum of articles including medical literature, biological literature, electronic health records, etc. that can be used by clinicians and researchers in the field", so we know that the paper explores extraction from electronic health records.
Answer: Yes
True answer: Yes

Validation ID: b637d6393ef3af7462917b81861531022b291933
Question: Does jiant involve datasets for the 50 NLU tasks?
From the context "jiant provides a configuration-driven interface for defining transfer learning and representation learning experiments using a bank of over 50 NLU tasks, cutting-edge sentence encoder models, and multi-task and multi-stage training procedures", so we know that jiant involves datasets for the 50 NLU tasks.
Answer: Yes
True answer: Yes

Validation ID: 8b9c12df9f89040f1485b3847a29f11b5c9262e0
Question: Is jiant compatible with models in any programming language?
From the context "jiant provides support for cutting-edge sentence encoder models, including support for Huggingface's Transformers. Supported models include: BERT BIBREF17, RoBERTa BIBREF27, XLNet BIBREF28, XLM BIBREF29, GPT BIBREF30, GPT-2 BIBREF31, ALBERT BIBREF32 and ELMo BIBREF33. jiant also supports the from-scratch training of (bidirectional) LSTMs BIBREF34 and deep bag of words models BIBREF35, as well as syntax-aware models such as PRPN BIBREF36 and ON-LSTM BIBREF37. jiant also supports word embeddings such as GloVe BIBREF38.", so we know that jiant is compatible with models in Python.
Answer: Python
True answer: Yes

Validation ID: 8b9c12df9f89040f1485b3847a29f11b5c9262e0
Question: Is jiant compatible with models in any programming language?
From the context "jiant provides support for cutting-edge sentence encoder models, including support for Huggingface's Transformers. Supported models include: BERT BIBREF17, RoBERTa BIBREF27, XLNet BIBREF28, XLM BIBREF29, GPT BIBREF30, GPT-2 BIBREF31, ALBERT BIBREF32 and ELMo BIBREF33. jiant also supports the from-scratch training of (bidirectional) LSTMs BIBREF34 and deep bag of words models BIBREF35, as well as syntax-aware models such as PRPN BIBREF36 and ON-LSTM BIBREF37. jiant also supports word embeddings such as GloVe BIBREF38.", so we know that jiant is compatible with models in Python.
Answer: Python
True answer: Unanswerable

Validation ID: 72e4e26d0dd79c590c28b10938952a9f9497ff1e
Question: What models are used for painting embedding and what for language style transfer?
From the context "For generating a poem from images we use an existing actor-critic architecture BIBREF1. This involves 3 parallel CNNs: an object CNN, sentiment CNN, and scene CNN, for feature extraction. These features are combined with a skip-thought model which provides poetic clues, which are then fed into a sequence-to-sequence model trained by policy gradient with 2 discriminator networks for rewards. This as a whole forms a pipeline that takes in an image and outputs a poem as shown on the top left of Figure FIGREF4. A CNN-RNN generative model acts as an agent. The parameters of this agent define a policy whose execution determines which word is selected as an action. When the agent selects all words in a poem, it receives a reward. Two discriminative networks, shown on the top right of Figure FIGREF4, are defined to serve as rewards concerning whether the generated poem properly describes the input image and whether the generated poem is poetic. The goal of the poem generation model is to generate a sequence of words as a poem for an image to maximize the expected return.", so we know that the models used for painting embedding are object CNN, sentiment CNN, and scene CNN, and the model used for language style transfer is seq2seq model with global attention.
Answer: object CNN, sentiment CNN, and scene CNN, seq2seq model with global attention
True answer: generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models

Validation ID: 63b92dcc701ec77fdb3355ede5d37d2fbf057bcc
Question: What applicability of their approach is demonstrated by the authors?
From the context "We perform a qualitative analysis of the Shakespearean prose generated for the input paintings. We conducted a survey, in which we presented famous paintings including those shown in Figures FIGREF1 and FIGREF10 and the corresponding Shakespearean prose generated by the model, and asked 32 students to rate them on the basis of content, creativity and similarity to Shakespearean style on a Likert scale of 1-5. Figure FIGREF12 shows the result of our human evaluation. The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare.", so we know that the applicability of their approach is demonstrated by the authors is that the prose generated is relevant to the painting, the model captures more than basic objects in the painting successfully using poetic clues in the scene, and the prose generated is perceived to be in the style of Shakespeare.
Answer: the prose generated is relevant to the painting, the model captures more than basic objects in the painting successfully using poetic clues in the scene, and the prose generated is perceived to be in the style of Shakespeare
True answer: Unanswerable

Validation ID: 58ee0cbf1d8e3711c617b1cd3d7aca8620e26187
Question: What limitations do the authors demnostrate of their model?
From the context "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", so we know that the limitations of the authors' model is that the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.
Answer: the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.
True answer: Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer

Validation ID: 58ee0cbf1d8e3711c617b1cd3d7aca8620e26187
Question: What limitations do the authors demnostrate of their model?
From the context "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", so we know that the limitations of the authors' model is that the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.
Answer: the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.
True answer: we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score

Validation ID: f71b52e00e0be80c926f153b9fe0a06dd93af11e
Question: How does final model rate on Likert scale?
From the context "The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare.", so we know that the final model rate on Likert scale is 3.7, 3.9, 3.9.
Answer: 3.7, 3.9, 3.9
True answer: average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9 

Validation ID: f71b52e00e0be80c926f153b9fe0a06dd93af11e
Question: How does final model rate on Likert scale?
From the context "The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare.", so we know that the final model rate on Likert scale is 3.7, 3.9, 3.9.
Answer: 3.7, 3.9, 3.9
True answer: average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9

Validation ID: 8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe
Question: How big is English poem description of the painting dataset?
From the context "We train a model for generating poems from images based on two datasets BIBREF1. The first dataset consists of image and poem pairs, namely a multi-modal poem dataset (MultiM-Poem), and the second dataset is a large poem corpus, namely a uni-modal poem dataset (UniM-Poem). The image and poem pairs are extended by adding the nearest three neighbor poems from the poem corpus without redundancy, and an extended image and poem pair dataset is constructed and denoted as MultiM-Poem(Ex)BIBREF1.", so we know that the English poem description of the painting dataset is MultiM-Poem(Ex).
Answer: MultiM-Poem(Ex)
True answer: Unanswerable

Validation ID: 54e945ea4b014e11ed4e1e61abc2aa9e68fea310
Question: What is best BLEU score of language style transfer authors got?
From the context "We also perform a quantitative analysis of style transfer by generating BLEU scores for the model output using the style transfer dataset. The variation of the BLEU scores with the source sentence lengths is shown in Figure FIGREF11. As expected, the BLEU scores decrease with increase in source sentence lengths.", so we know that the best BLEU score of language style transfer authors got is 29.65.
Answer: 29.65
True answer: seq2seq model with global attention gives the best results with an average target BLEU score of 29.65

Validation ID: 54e945ea4b014e11ed4e1e61abc2aa9e68fea310
Question: What is best BLEU score of language style transfer authors got?
From the context "We also perform a quantitative analysis of style transfer by generating BLEU scores for the model output using the style transfer dataset. The variation of the BLEU scores with the source sentence lengths is shown in Figure FIGREF11. As expected, the BLEU scores decrease with increase in source sentence lengths.", so we know that the best BLEU score of language style transfer authors got is 29.65.
Answer: 29.65
True answer: average target BLEU score of 29.65

Validation ID: df0257ab04686ddf1c6c4d9b0529a7632330b98e
Question: How better does new approach behave than existing solutions?
From the context "In the former experiment we show that Go-Explore finds winning trajectories faster than existing solutions", so we know that Go-Explore finds winning trajectories faster than existing solutions.
Answer: Go-Explore finds winning trajectories faster than existing solutions
True answer:  On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model

Validation ID: df0257ab04686ddf1c6c4d9b0529a7632330b98e
Question: How better does new approach behave than existing solutions?
From the context "In the former experiment we show that Go-Explore finds winning trajectories faster than existing solutions", so we know that Go-Explore finds winning trajectories faster than existing solutions.
Answer: Go-Explore finds winning trajectories faster than existing solutions
True answer: On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.
On Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.

Validation ID: 568fb7989a133564d84911e7cb58e4d8748243ef
Question: How is trajectory with how rewards extracted?
From the context "In phase 1, Go-Explore builds an archive of cells, where a cell is defined as a set of observations that are mapped to the same, discrete representation by some mapping function $f(x)$. Each cell is associated with meta-data including the trajectory towards that cell, the length of that trajectory, and the cumulative reward of that trajectory. New cells are added to the archive when they are encountered in the environment, and existing cells are updated with new meta-data when the trajectory towards that cells is higher scoring or equal scoring but shorter.", so we know that the trajectory with how rewards is extracted by Go-Explore.
Answer: Go-Explore
True answer: explores the state space through keeping track of previously visited states by maintaining an archive

Validation ID: 2c947447d81252397839d58c75ebcc71b34379b5
Question: On what Text-Based Games are experiments performed?
From the context "We use the 4,440 games of “First TextWorld Problems” BIBREF14, which are generated using the machinery introduced by BIBREF3, to show the generalization ability of our proposed methodology", so we know that experiments are performed on the 4,440 games of “First TextWorld Problems” BIBREF14.
Answer: the 4,440 games of “First TextWorld Problems” BIBREF14
True answer: CoinCollector , CookingWorld 

Validation ID: 2c947447d81252397839d58c75ebcc71b34379b5
Question: On what Text-Based Games are experiments performed?
From the context "We use the 4,440 games of “First TextWorld Problems” BIBREF14, which are generated using the machinery introduced by BIBREF3, to show the generalization ability of our proposed methodology", so we know that experiments are performed on the 4,440 games of “First TextWorld Problems” BIBREF14.
Answer: the 4,440 games of “First TextWorld Problems” BIBREF14
True answer: CoinCollector, CookingWorld

Validation ID: c01784b995f6594fdb23d7b62f20a35ae73eaa77
Question: How do the authors show that their learned policy generalize better than existing solutions to unseen games?
From the context "In the latter, we show that training a Seq2Seq model on the trajectories found by Go-Explore results in stronger generalization, as suggested by the stronger performance on unseen games, compared to existing competitive baselines BIBREF10, BIBREF7", so we know that the authors show that their learned policy generalize better than existing solutions to unseen games by comparing the stronger performance on unseen games.
Answer: by comparing the stronger performance on unseen games
True answer: promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game

Validation ID: 3415762847ed13acc3c90de60e3ef42612bc49af
Question: How much is classification performance improved in experiments for low data regime and class-imbalance problems?
From the context "We conduct extensive experiments on text and image classification in challenging situations of very limited data and imbalanced labels. Both augmentation and weighting by our approach significantly improve over strong base models, even though the models are initialized with large-scale pretrained networks such as BERT BIBREF8 for text and ResNet BIBREF9 for images. Our approach, besides its generality, also outperforms a variety of dedicated rule- and learning-based methods for either augmentation or weighting, respectively. Lastly, we observe that the two types of manipulation tend to excel in different contexts: augmentation shows superiority over weighting with a small amount of data available, while weighting is better at addressing class imbalance problems.", so we know that classification performance is improved in experiments for low data regime and class-imbalance problems.
Answer: improved
True answer: Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline
Imbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000

Validation ID: 223dc2b9ea34addc0f502003c2e1c1141f6b36a7
Question: What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?
From the context "To learn the parameters $\mathbf {\phi }$ in the manipulation reward $R_\phi (\mathbf {x}, y | \mathcal {D})$, we could in principle adopt any off-the-shelf reward learning algorithm in the literature", so we know that the off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted.
Answer: reward learning algorithm from RL
True answer: BIBREF7

Validation ID: 223dc2b9ea34addc0f502003c2e1c1141f6b36a7
Question: What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?
From the context "To learn the parameters $\mathbf {\phi }$ in the manipulation reward $R_\phi (\mathbf {x}, y | \mathcal {D})$, we could in principle adopt any off-the-shelf reward learning algorithm in the literature", so we know that the off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted.
Answer: reward learning algorithm from RL
True answer:  reward learning algorithm BIBREF7

Validation ID: e1ab11885f72b4658263a60751d956ba661c1d61
Question: What subtasks did they participate in?
From the context "Three types of models were used in our system, a feed-forward neural network, an LSTM network and an SVM regressor. The neural nets were inspired by the work of Prayas BIBREF7 in the previous shared task. Different regression algorithms (e.g. AdaBoost, XGBoost) were also tried due to the success of SeerNet BIBREF8, but our study was not able to reproduce their results for Spanish.", so we know that they participated in three types of models.
Answer: feed-forward neural network, an LSTM network and an SVM regressor
True answer: Answer with content missing: (Subscript 1: "We did not participate in subtask 5 (E-c)") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks.

Validation ID: c85b6f9bafc4c64fc538108ab40a0590a2f5768e
Question: What were the scores of their system?
From the context "Our official scores (column Ens Test in Table TABREF19 ) have placed us second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard", so we know that the scores of their system are second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc).
Answer: second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc)
True answer: column Ens Test in Table TABREF19

Validation ID: 8e52637026bee9061f9558178eaec08279bf7ac6
Question: How was the training data translated?
From the context "A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish. This new set of “Spanish” data was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.", so we know that the training data was translated into Spanish by using the machine translation platform Apertium.
Answer: using the machine translation platform Apertium
True answer: using the machine translation platform Apertium 

Validation ID: 8e52637026bee9061f9558178eaec08279bf7ac6
Question: How was the training data translated?
From the context "A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish. This new set of “Spanish” data was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.", so we know that the training data was translated into Spanish by using the machine translation platform Apertium.
Answer: using the machine translation platform Apertium
True answer: machine translation platform Apertium BIBREF5

Validation ID: 0f6216b9e4e59252b0c1adfd1a848635437dfcdc
Question: What dataset did they use?
From the context "For each task, the training data that was made available by the organizers is used, which is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment BIBREF1", so we know that the dataset is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment.
Answer: a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment
True answer:  Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish.

Validation ID: 0f6216b9e4e59252b0c1adfd1a848635437dfcdc
Question: What dataset did they use?
From the context "For each task, the training data that was made available by the organizers is used, which is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment BIBREF1", so we know that the dataset is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment.
Answer: a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment
True answer: Spanish tweets were scraped between November 8, 2017 and January 12, 2018, Affect in Tweets Distant Supervision Corpus (DISC)

Validation ID: 22ccee453e37536ddb0c1c1d17b0dbac04c6c607
Question: What other languages did they translate the data from?
From the context "A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish", so we know that they translated the data from English.
Answer: English
True answer: English 

Validation ID: 22ccee453e37536ddb0c1c1d17b0dbac04c6c607
Question: What other languages did they translate the data from?
From the context "A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish", so we know that they translated the data from English.
Answer: English
True answer: English

Validation ID: d00bbeda2a45495e6261548710afa6b21ea32870
Question: What semi-supervised learning is applied?
From the context "For this purpose, the DISC BIBREF0 corpus was used. This corpus was created by querying certain emotion-related words, which makes it very suitable as a semi-supervised corpus", so we know that the semi-supervised learning is querying certain emotion-related words.
Answer: querying certain emotion-related words
True answer: first a model is trained on the training set and then this model is used to predict the labels of the silver data, This silver data is then simply added to our training set, after which the model is retrained

Validation ID: 71b1af123fe292fd9950b8439db834212f0b0e32
Question: How were the datasets annotated?
From the context "We present a carefully designed and rigorous language-agnostic translation and annotation protocol. These well-defined guidelines will facilitate the development of future Multi-SimLex datasets for other languages. The proposed protocol eliminates some crucial issues with prior efforts focused on the creation of multi-lingual semantic resources, namely: i) limited coverage; ii) heterogeneous annotation guidelines; and iii) concept pairs which are semantically incomparable across different languages", so we know that the datasets were annotated by a carefully designed and rigorous language-agnostic translation and annotation protocol.
Answer: a carefully designed and rigorous language-agnostic translation and annotation protocol
True answer: 1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process

Validation ID: a616a3f0d244368ec588f04dfbc37d77fda01b4c
Question: What are the 12 languages covered?
From the context "We apply this protocol to a set of 12 languages, including a mixture of major languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones (e.g., Kiswahili, Welsh, and Yue Chinese)", so we know that the 12 languages are Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese.
Answer: Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese
True answer: Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese

Validation ID: a616a3f0d244368ec588f04dfbc37d77fda01b4c
Question: What are the 12 languages covered?
From the context "We apply this protocol to a set of 12 languages, including a mixture of major languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones (e.g., Kiswahili, Welsh, and Yue Chinese)", so we know that the 12 languages are Mandarin, Russian, French, Kiswahili, Welsh, and Yue Chinese.
Answer: Mandarin, Russian, French, Kiswahili, Welsh, and Yue Chinese
True answer: Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
From the context "We used the DIP corpus BIBREF37, a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs) with a short description of each topic", so we know that the corpus contains 49 clusters of 100 web pages on educational topics.
Answer: 49 clusters of 100 web pages on educational topics
True answer: Yes

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
From the context "We selected 30 of the topics for which we created the necessary concept map annotations.", so we know that the corpus contains 30 topics.
From the context "The corpus we present combines these aspects, as it has large clusters of heterogeneous documents, and provides a necessary benchmark to evaluate the proposed task.", so we know that the corpus has large clusters of heterogeneous documents.
True answer: Unanswerable

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
From the context "We selected 30 of the topics for which we created the necessary concept map annotations.", so we know that the corpus contains 30 topics.
From the context "The corpus we present combines these aspects, as it has large clusters of heterogeneous documents, and provides a necessary benchmark to evaluate the proposed task.", so we know that the corpus has large clusters of heterogeneous documents.
True answer: Unanswerable

Validation ID: 1522ccedbb1f668958f24cca070f640274bc2549
Question: What type of evaluation is proposed for this task?
From the context "To evaluate the task, we propose a new evaluation protocol that is based on the concept map's ability to support a user in navigating a document collection", so we know that the type of evaluation is based on the concept map's ability to support a user in navigating a document collection.
Answer: based on the concept map's ability to support a user in navigating a document collection
True answer: Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2

Validation ID: 97466a37525536086ed5d6e5ed143df085682318
Question: What baseline system is proposed?
From the context "We provide an evaluation protocol and baseline", so we know that the baseline system is provided.
Answer: provided
True answer: Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction.

Validation ID: e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a
Question: How were crowd workers instructed to identify important elements in large document collections?
From the context "We break down the task of importance annotation to the level of single propositions. The goal of our crowdsourcing scheme is to obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary. In contrast to other work, we do not show the documents to the workers at all, but provide only a description of the document cluster's topic along with the propositions.", so we know that crowd workers were instructed to identify important elements in large document collections by only a description of the document cluster's topic along with the propositions.
Answer: a description of the document cluster's topic along with the propositions
True answer: provide only a description of the document cluster's topic along with the propositions

Validation ID: e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a
Question: How were crowd workers instructed to identify important elements in large document collections?
From the context "We break down the task of importance annotation to the level of single propositions. The goal of our crowdsourcing scheme is to obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary. In contrast to other work, we do not show the documents to the workers at all, but provide only a description of the document cluster's topic along with the propositions.", so we know that crowd workers were instructed to identify important elements in large document collections by only a description of the document cluster's topic along with the propositions.
Answer: a description of the document cluster's topic along with the propositions
True answer: They break down the task of importance annotation to the level of single propositions and obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary.

Validation ID: d6191c4643201262a770947fc95a613f57bedb6b
Question: Which collections of web documents are included in the corpus?
From the context "As a starting point, we used the DIP corpus BIBREF37, a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs) with a short description of each topic", so we know that the collections of web documents are 49 clusters of 100 web pages on educational topics.
Answer: 49 clusters of 100 web pages on educational topics
True answer: DIP corpus BIBREF37

Validation ID: ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8
Question: How do the authors define a concept map?
From the context "We define a concept map as a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the documents, making the summarization task extractive. A concept can be an entity, abstract idea, event or activity, designated by its unique label. Good maps should be propositionally coherent, meaning that every relation together with the two connected concepts form a meaningful proposition.", so we know that a concept map is a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the documents, making the summarization task extractive. A concept can be an entity, abstract idea, event or activity, designated by its unique label. Good maps should be propositionally coherent, meaning that every relation together with the two connected concepts form a meaningful proposition.
Answer: a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the documents, making the summarization task extractive. A concept can be an entity, abstract idea, event or activity, designated by its unique label. Good maps should be propositionally coherent, meaning that every relation together with the two connected concepts form a meaningful proposition.
True answer: concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges

Validation ID: fc4ae12576ea3a85ea6d150b46938890d63a7d18
Question: Is the LSTM baseline a sub-word model?
From the context "We use a Morfessor 2.0 BIBREF14 using the basic unsupervised Morfessor Baseline algorithm BIBREF15 with a corpus weight parameter ($\alpha $) of 0.001. We have a vocabulary of 34K subword tokens for the left+right-marked (+m+) markings and 19K subword tokens for the left-marked (+m) markings. We also pre-process the data to remove any punctuation marks such that we can use the same data with an ASR system. The input is one sentence per line and we shuffle the sentences at each epoch. The data is randomly divided into- training dataset and a validation dataset. The test dataset consists of 2850 Finnish news articles obtained from the Finnish national broadcaster YLE.", so we know that the LSTM baseline is a sub-word model.
Answer: Yes
True answer: Yes

Validation ID: fc4ae12576ea3a85ea6d150b46938890d63a7d18
Question: Is the LSTM baseline a sub-word model?
From the context "We use a Morfessor 2.0 BIBREF14 using the basic unsupervised Morfessor Baseline algorithm BIBREF15 with a corpus weight parameter ($\alpha $) of 0.001. We have a vocabulary of 34K subword tokens for the left+right-marked (+m+) markings and 19K subword tokens for the left-marked (+m) markings. We also pre-process the data to remove any punctuation marks such that we can use the same data with an ASR system. The input is one sentence per line and we shuffle the sentences at each epoch. The data is randomly divided into- training dataset and a validation dataset. The test dataset consists of 2850 Finnish news articles obtained from the Finnish national broadcaster YLE.", so we know that the LSTM baseline is a sub-word model.
Answer: Yes
True answer: Yes

Validation ID: 19cf7884c0c509c189b1e74fe92c149ff59e444b
Question: How is pseudo-perplexity defined?
From the context "We therefore, define a pseudo-perplexity score from the above approximated joint probability.", so we know that pseudo-perplexity is defined from the above approximated joint probability.
Answer: from the above approximated joint probability
True answer: Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated.

Validation ID: ecd5770cf8cb12cb34285e26ab834301c17c53e1
Question: What is the model architecture used?
From the context "We propose features based on the observation that answer agreement often arises when 1) a lay person's attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address", so we know that the model architecture used is based on the observation that answer agreement often arises when 1) a lay person's attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address.
Answer: based on the observation that answer agreement often arises when 1) a lay person's attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address
True answer: LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.

Validation ID: ecd5770cf8cb12cb34285e26ab834301c17c53e1
Question: What is the model architecture used?
From the context "We propose features based on the observation that answer agreement often arises when 1) a lay person's attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address", so we know that the model architecture used is based on the observation that answer agreement often arises when 1) a lay person's attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address.
Answer: based on the observation that answer agreement often arises when 1) a lay person's attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address
True answer: random forest, The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.

Validation ID: 4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729
Question: How is the data used for training annotated?
From the context "We capitalize on today's largest visual question answering dataset BIBREF2 to evaluate our prediction system, which includes 369,861 visual questions about real images. Of these, 248,349 visual questions (i.e., Training questions 2015 v1.0) are kept for for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) are employed for testing our classification system. This separation of training and testing samples enables us to estimate how well a classifier will generalize when applied to an unseen, independent set of visual questions.", so we know that the data used for training is Training questions 2015 v1.0.
Answer: Training questions 2015 v1.0
True answer: The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions.

Validation ID: 5529f26f72ce47440c2a64248063a6d5892b9fde
Question: what quantitative analysis is done?
From the context "We show the loss function analysis of transformer and our model. Figure FIGREF23 shows the validation performance of transformer against global training steps. Figure FIGREF21 show the validation performance of our model for the same number of global steps. Figure FIGREF22 shows that NMT loss is going down with the number of steps, while Figure FIGREF20 shows the degradation of the loss of our proposed RAT task.", so we know that the quantitative analysis is loss function analysis.
Answer: loss function analysis
True answer: Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP).

Validation ID: f85ca6135b101736f5c16c5b5d40895280016023
Question: what are the baselines?
From the context "We address a pitfall of using existing MT models for query translation BIBREF1. An MT model trained on TC does not have any knowledge of RC. In an extreme setting, where there are no common terms between the target side of TC and RC, a well trained and tested translation model would fail because of vocabulary mismatch between the translated query and documents of RC. Assuming a relaxed scenario where some commonality exists between two corpora, a translation model might still perform poorly, favoring terms that are more likely in TC but rare in RC. Our hypothesis is that a search query translation model would perform better if a translated query term is likely to appear in the both retrieval and translation corpora, a property we call balanced translation.", so we know that the baselines are existing MT models for query translation.
Answer: existing MT models for query translation
True answer: the baseline transformer BIBREF8

Validation ID: f85ca6135b101736f5c16c5b5d40895280016023
Question: what are the baselines?
From the context "We address a pitfall of using existing MT models for query translation BIBREF1. An MT model trained on TC does not have any knowledge of RC. In an extreme setting, where there are no common terms between the target side of TC and RC, a well trained and tested translation model would fail because of vocabulary mismatch between the translated query and documents of RC. Assuming a relaxed scenario where some commonality exists between two corpora, a translation model might still perform poorly, favoring terms that are more likely in TC but rare in RC. Our hypothesis is that a search query translation model would perform better if a translated query term is likely to appear in the both retrieval and translation corpora, a property we call balanced translation.", so we know that the baselines are existing MT models for query translation.
Answer: existing MT models for query translation
True answer: baseline transformer BIBREF8

Validation ID: 5fa36dc8f7c4e65acb962fc484989d20b8fdaeec
Question: Do they report results only on English data?
From the context "Table TABREF28 presents the results of the first experiment. The DL model with GloVe word embeddings achieved better results on three datasets, with 82.80% Accuracy on SNLI, 78.52% Accuracy on MultiNLI, and 83.62% Accuracy on Quora", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 5fa36dc8f7c4e65acb962fc484989d20b8fdaeec
Question: Do they report results only on English data?
From the context "Table TABREF28 presents the results of the first experiment. The DL model with GloVe word embeddings achieved better results on three datasets, with 82.80% Accuracy on SNLI, 78.52% Accuracy on MultiNLI, and 83.62% Accuracy on Quora", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: d98847340e46ffe381992f1a594e75d3fb8d385e
Question: What machine learning and deep learning methods are used for RQE?
From the context "In the first experiment, we evaluated the DL and ML methods on SNLI, multi-NLI, Quora, and Clinical-QE", so we know that the machine learning and deep learning methods are used for RQE.
Answer: DL and ML
True answer: Logistic Regression, neural networks

Validation ID: 7006c66a15477b917656f435d66f63760d33a304
Question: by how much did nus outperform abus?
From the context "On both evaluation tasks the NUS outperforms the ABUS, which is currently one of the most popular off-line training tools for reinforcement learning based Spoken Dialogue Systems BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14.", so we know that the NUS outperforms the ABUS.
Answer: outperforms the ABUS
True answer: Average success rate is higher by 2.6 percent points.

Validation ID: a15bc19674d48cd9919ad1cf152bf49c88f4417d
Question: what corpus is used to learn behavior?
From the context "The NUS is trained on dialogues between real users and an SDS in a restaurant recommendation domain", so we know that the corpus is dialogues between real users and an SDS in a restaurant recommendation domain.
Answer: dialogues between real users and an SDS in a restaurant recommendation domain
True answer: DSTC2

Validation ID: a15bc19674d48cd9919ad1cf152bf49c88f4417d
Question: what corpus is used to learn behavior?
From the context "The NUS is trained on dialogues between real users and an SDS in a restaurant recommendation domain", so we know that the corpus is dialogues between real users and an SDS in a restaurant recommendation domain.
Answer: dialogues between real users and an SDS in a restaurant recommendation domain
True answer: The manual transcriptions of the DSTC2 training set 

Validation ID: 440faf8d0af8291d324977ad0f68c8d661fe365e
Question: Which dataset has been used in this work?
From the context "We evaluate our methods on the Reuters-21578 dataset, which is a collection of news articles from Reuters, a news agency. It is a standard dataset for text classification and has been used in many works BIBREF13, BIBREF14, BIBREF15.", so we know that the dataset used in this work is Reuters-21578 dataset.
Answer: Reuters-21578 dataset
True answer: Reuters-8 dataset without stop words

Validation ID: 440faf8d0af8291d324977ad0f68c8d661fe365e
Question: Which dataset has been used in this work?
From the context "We evaluate our methods on the Reuters-21578 dataset, which is a collection of news documents from Reuters, a news agency. It is a standard benchmark for text categorization and contains 21,578 documents, divided into 90 classes. The documents are divided into a training set with 8,982 documents and a test set with 11,596 documents. The training set is further divided into a training set with 7,532 documents and a validation set with 1,450 documents.", so we know that the dataset used in this work is Reuters-21578 dataset.
Answer: Reuters-21578 dataset
True answer: The Reuters-8 dataset (with stop words removed)

Validation ID: 0ec56e15005a627d0b478a67fd627a9d85c3920e
Question: What can word subspace represent?
From the context "We then translate these words into word vectors using word2vec, resulting in a set of word vectors $X_c = \lbrace {x}^k_c\rbrace _{k=1}^{N_c} \in \mathbb {R}^p$. This set of word vectors is modeled into a word subspace, which is a compact, scalable and meaningful representation of the whole set.", so we know that word subspace can represent a set of word vectors.
Answer: a set of word vectors
True answer: Word vectors, usually in the context of others within the same class

Validation ID: a712718e6596ba946f29a99838d82f95b9ebb1ce
Question: How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?
From the context "We have 8 classes in term AC. Figure FIGREF15(a) illustrates the results of our best performed model and Figure FIGREF15(b) shows the results of the base model. The accuracy and F1 score have an improvement from 0.3898 and 0.2830 to 0.4915 and 0.4059 respectively. Regarding the rare senses (for example, class 0, 1, 4 and 6), we can observe an increase in the ROC areas. Class 6 has an obvious improvement from 0.75 to 1.00. Such improvements in the rare senses make a huge difference in the reported average accuracy and F1 score, since we have a nearly equal number of samples for each class in the testing data. Similarly, we show the plots for IM term in Figure FIGREF15(c) and FIGREF15(d). IM has only two classes, but they are very unbalanced in training set, as shown in Table TABREF16. The accuracy and F1 scores improved from 0.6667 and 0.6250 to 0.8667 and 0.8667 respectively. We observe improvements in the ROC areas for both classes. This observation further shows that our model is more sensitive to all the class samples compared to the base model, even for the terms that have only a few samples in the training set. Again, by plotting the ROC curves and comparing AUC areas, we show that our model, which applies ELMo and topic-attention, has a better representation ability under the setting of few-shot learning.", so we know that the accuracy and F1 score have an improvement from 0.3898 and 0.2830 to 0.4915 and 0.4059 respectively.
Answer: 0.1017 and 0.1229
True answer: 7.36% on accuracy and 9.69% on F1 score

Validation ID: a712718e6596ba946f29a99838d82f95b9ebb1ce
Question: How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?
From the context "We have 8 classes in term AC. Figure FIGREF15(a) illustrates the results of our best performed model and Figure FIGREF15(b) shows the results of the base model. The accuracy and F1 score have an improvement from 0.3898 and 0.2830 to 0.4915 and 0.4059 respectively. Regarding the rare senses (for example, class 0, 1, 4 and 6), we can observe an increase in the ROC areas. Class 6 has an obvious improvement from 0.75 to 1.00. Such improvements in the rare senses make a huge difference in the reported average accuracy and F1 score, since we have a nearly equal number of samples for each class in the testing data. Similarly, we show the plots for IM term in Figure FIGREF15(c) and FIGREF15(d). IM has only two classes, but they are very unbalanced in training set, as shown in Table TABREF16. The accuracy and F1 scores improved from 0.6667 and 0.6250 to 0.8667 and 0.8667 respectively. We observe improvements in the ROC areas for both classes. This observation further shows that our model is more sensitive to all the class samples compared to the base model, even for the terms that have only a few samples in the training set. Again, by plotting the ROC curves and comparing AUC areas, we show that our model, which applies ELMo and topic-attention, has a better representation ability under the setting of few-shot learning.", so we know that the accuracy and F1 score have an improvement from 0.3898 and 0.2830 to 0.4915 and 0.4059 respectively.
Answer: 0.1017 and 0.1229
True answer: it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too

Validation ID: 3116453e35352a3a90ee5b12246dc7f2e60cfc59
Question: To what baseline models is proposed model compared?
From the context "We conducted a comprehensive comparison with the baseline models, and some of them were never investigated for the abbreviation disambiguation task. We applied traditional features by simply taking the TF-IDF features as the inputs into the classic classifiers. Deep features are also considered: a Doc2vec model BIBREF19 was pre-trained using Gensim and these word embeddings were applied to initialize deep models and fine-tuned.", so we know that the proposed model is compared to TF-IDF features, Doc2vec model.
Answer: TF-IDF features, Doc2vec model
True answer: support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self

Validation ID: 3116453e35352a3a90ee5b12246dc7f2e60cfc59
Question: To what baseline models is proposed model compared?
From the context "We conducted a comprehensive comparison with the baseline models, and some of them were never investigated for the abbreviation disambiguation task. We applied traditional features by simply taking the TF-IDF features as the inputs into the classic classifiers. Deep features are also considered: a Doc2vec model BIBREF19 was pre-trained using Gensim and these word embeddings were applied to initialize deep models and fine-tuned.", so we know that the proposed model is compared to TF-IDF features, Doc2vec model.
Answer: TF-IDF features, Doc2vec model
True answer: support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self

Validation ID: dfca00be3284cc555a6a4eac4831471fb1f5875b
Question: How big is dataset for testing?
From the context "We have a balanced testing dataset, where each term-sense pair has around 15 samples for testing (on average, each pair has 14.56 samples and the median sample number is 15)", so we know that the dataset for testing has around 15 samples for each term-sense pair.
Answer: around 15 samples for each term-sense pair
True answer: 30 terms, each term-sanse pair has around 15 samples for testing

Validation ID: a9a532399237b514c1227f2d6be8601474e669be
Question: What existing dataset is re-examined and corrected for training?
From the context "Training Dataset UM Inventory BIBREF5 is a public dataset created by researchers from the University of Minnesota, containing about 37,500 training samples with 75 abbreviation terms. Existing work reports abbreviation disambiguation results on 50 abbreviation terms BIBREF6, BIBREF5, BIBREF17. However, after carefully reviewing this dataset, we found that it contains many samples where medical professionals disagree: wrong samples and uncategorized samples. Due to these mistakes and flaws of this dataset, we removed the erroneous samples and eventually selected 30 abbreviation terms as our training dataset that can be made public.", so we know that the existing dataset is UM Inventory BIBREF5.
Answer: UM Inventory BIBREF5
True answer:  UM Inventory 

Validation ID: 26126068d72408555bcb52977cd669faf660bdf7
Question: What are the qualitative experiments performed on benchmark datasets?
From the context "Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.", so we know that the qualitative experiments performed on benchmark datasets are:
Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.
True answer: Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.
Evaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.

Validation ID: 26126068d72408555bcb52977cd669faf660bdf7
Question: What are the qualitative experiments performed on benchmark datasets?
From the context "Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.", so we know that the qualitative experiments performed on benchmark datasets are:
Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.
True answer: Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed

Validation ID: 660284b0a21fe3801e64dc9e0e51da5400223fe3
Question: How does this approach compare to other WSD approaches employing word embeddings?
From the context "We quantitatively compare the performance of the GM$\_$KL, w2g, and w2gm approaches on the SCWS dataset BIBREF6. The metric used for comparison is 'AvgCos'. It can be seen that for most of the datasets, GM$\_$KL achieves significantly better correlation score than w2g and w2gm approaches.", so we know that GM$\_$KL achieves significantly better correlation score than w2g and w2gm approaches.
Answer: GM$\_$KL achieves significantly better correlation score than w2g and w2gm approaches
True answer: GM$\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.

Validation ID: c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd
Question: What tasks did they use to evaluate performance for male and female speakers?
From the context "We then question the impact of such a representation on a ASR system trained on these data. BIBREF25", so we know that they use ASR system to evaluate performance for male and female speakers.
Answer: ASR system
True answer: ASR

Validation ID: c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd
Question: What tasks did they use to evaluate performance for male and female speakers?
From the context "We then question the impact of such a representation on a ASR system trained on these data. BIBREF25", so we know that they use ASR system to evaluate performance for male and female speakers.
Answer: ASR system
True answer: Unanswerable

Validation ID: f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3
Question: What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?
From the context "We observed that the higher the degree of normativity of speech the smaller the gender difference. No significant gender bias is observed for prepared speech nor within the Anchor category. Even if we do not find similar results with lower WER for women than men, we obtained a median WER smaller for women on prepared speech and equal to the male median WER for the Anchor speakers. Another explanation could be the use of adaptation within the pipeline. Most broadcast programs transcription systems have a speaker adaptation step within their decoding pipeline, which is the case for our system. An Anchor speaker intervening more often would have a larger quantity of data to realize such adaptation of the acoustic model. On the contrary, Punctual speakers who appear scarcely in the data are not provided with the same amount of adaptation data. Hence we can hypothesize that gender performance difference observed for Punctual speakers is due to the fact that", so we know that the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to hypothesize that gender performance difference observed for Punctual speakers is due to the fact that.
Answer: to hypothesize that gender performance difference observed for Punctual speakers is due to the fact that
True answer: create fair systems

Validation ID: f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3
Question: What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?
From the context "We observed that the higher the degree of normativity of speech the smaller the gender difference. No significant gender bias is observed for prepared speech nor within the Anchor category. Even if we do not find similar results with lower WER for women than men, we obtained a median WER smaller for women on prepared speech and equal to the male median WER for the Anchor speakers. Another explanation could be the use of adaptation within the pipeline. Most broadcast programs transcription systems have a speaker adaptation step within their decoding pipeline, which is the case for our system. An Anchor speaker intervening more often would have a larger quantity of data to realize such adaptation of the acoustic model. On the contrary, Punctual speakers who appear scarcely in the data are not provided with the same amount of adaptation data. Hence we can hypothesize that gender performance difference observed for Punctual speakers is due to the fact that", so we know that the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to hypothesize that gender performance difference observed for Punctual speakers is due to the fact that.
Answer: to hypothesize that gender performance difference observed for Punctual speakers is due to the fact that
True answer:  broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools

Validation ID: a253749e3b4c4f340778235f640ce694642a4555
Question: Which corpora does this paper analyse?
From the context "Our data consists of two sets used to train and evaluate our automatic speech recognition system. Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. These four collections contain radio and/or TV broadcasts aired between 1998 and 2013 which are used by most academic researchers in ASR.", so we know that this paper analyse ESTER1, ESTER2, ETAPE and REPERE.
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: a253749e3b4c4f340778235f640ce694642a4555
Question: Which corpora does this paper analyse?
From the context "Our data consists of two sets used to train and evaluate our automatic speech recognition system. Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. These four collections contain radio and/or TV broadcasts aired between 1998 and 2013 which are used by most academic researchers in ASR.", so we know that this paper analyse ESTER1, ESTER2, ETAPE and REPERE.
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: 1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891
Question: How many categories do authors define for speaker role?
From the context "We define two salient roles called Anchors and Punctual speakers", so we know that authors define two salient roles called Anchors and Punctual speakers.
Answer: two
True answer:  two salient roles called Anchors and Punctual speakers

Validation ID: 777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47
Question: How big is imbalance in analyzed corpora?
From the context "We first describe the gender representation in training data. Gender representation is measured in terms of number of speakers, number of utterances (or speech turns), and turn lengths (descriptive statistics are given in Section SECREF16). Each speech turn was mapped to its speaker in order to associate it with a gender.", so we know that the gender representation is measured in terms of number of speakers, number of utterances (or speech turns), and turn lengths.
Answer: number of speakers, number of utterances (or speech turns), and turn lengths
True answer: Women represent 33.16% of the speakers

Validation ID: 2da4c3679111dd92a1d0869dae353ebe5989dfd2
Question: What are four major corpora of French broadcast?
From the context "Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. These four collections contain radio and/or TV broadcasts aired between 1998 and 2013 which are used by most academic researchers in ASR.", so we know that four major corpora of French broadcast are ESTER1, ESTER2, ETAPE and REPERE.
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: b7c3f3942a07c118e57130bc4c3ec4adc431d725
Question: What did the best systems use for their model?
From the context "We modify the method of Universal Langage Model Fine-tuning for Text Classification (ULMFiT) presented in Howard and Ruder BIBREF3. The primary steps are:", so we know that the best systems use Universal Langage Model Fine-tuning for Text Classification (ULMFiT) presented in Howard and Ruder BIBREF3.
Answer: Universal Langage Model Fine-tuning for Text Classification (ULMFiT) presented in Howard and Ruder BIBREF3
True answer: Unanswerable

Validation ID: a5505e25ee9ae84090e1442034ddbb3cedabcf04
Question: What were their results on the classification and regression tasks?
From the context "Table TABREF29 gives three results from our submissions in the competition. The first is the baseline NBSVM solution, with an F1 of 0.7548. Second is our first random seed selected for the classifier which produces a 0.8083 result. While better than the NBSVM solution, we pick the best validation F1 from the 20 seeds we tried. This produced our final submission of 0.8099. Our best model achieved an five-fold average F1 of 0.8254 on the validation set shown in Figure FIGREF27 but a test set F1 of 0.8099 - a drop of 0.0155 in F1 for the true out-of-sample data. Also note that our third place entry was 1.1% worse in F1 score than first place but 1.2% better in F1 than the 4th place entry.", so we know that their results on the classification and regression tasks are 0.8099 and 0.8254.
Answer: 0.8099 and 0.8254
True answer: F1 of 0.8099

Validation ID: a5505e25ee9ae84090e1442034ddbb3cedabcf04
Question: What were their results on the classification and regression tasks?
From the context "Table TABREF29 gives three results from our submissions in the competition. The first is the baseline NBSVM solution, with an F1 of 0.7548. Second is our first random seed selected for the classifier which produces a 0.8083 result. While better than the NBSVM solution, we pick the best validation F1 from the 20 seeds we tried. This produced our final submission of 0.8099. Our best model achieved an five-fold average F1 of 0.8254 on the validation set shown in Figure FIGREF27 but a test set F1 of 0.8099 - a drop of 0.0155 in F1 for the true out-of-sample data. Also note that our third place entry was 1.1% worse in F1 score than first place but 1.2% better in F1 than the 4th place entry.", so we know that their results on the classification and regression tasks are 0.8099 and 0.8083.
Answer: 0.8099 and 0.8083
True answer: F1 score result of 0.8099

Validation ID: 1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf
Question: Do the authors conduct experiments on the tasks mentioned?
From the context "We will say that an element of a Winograd schema is “solved” if the referent of the pronoun is identified. An example of a pair of sentences satisfying conditions 1-3 but not 4 would be Since women cannot be carcinogenic and pills cannot be pregnant, the pronoun `they' in these sentences is easily disambiguated using selectional restrictions. This pair is therefore not a valid Winograd schema.", so we know that the authors conduct experiments on the tasks mentioned.
Answer: Yes
True answer: Yes

Validation ID: 1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf
Question: Do the authors conduct experiments on the tasks mentioned?
From the context "We will say that an element of a Winograd schema is “solved” if the referent of the pronoun is identified. An example of a pair of sentences satisfying conditions 1-3 but not 4 would be Since women cannot be carcinogenic and pills cannot be pregnant, the pronoun `they' in these sentences is easily disambiguated using selectional restrictions. This pair is therefore not a valid Winograd schema.", so we know that the authors conduct experiments on the tasks mentioned.
Answer: Yes
True answer: No

Validation ID: 7fa3c2c0cf7f559d43e84076a9113a390c5ba03a
Question: Did they collect their own datasets?
From the context "The Winograd Schema Challenge (WSC) is a challenge for AI programs. The program is presented with a collection of sentences, each of which is one element of a Winograd schema, and is required to find the correct referent for the ambiguous pronoun", so we know that the Winograd Schema Challenge (WSC) is a challenge for AI programs. The program is presented with a collection of sentences, each of which is one element of a Winograd schema, and is required to find the correct referent for the ambiguous pronoun.
Answer: Yes
True answer: No

Validation ID: 9a7ba5ed1779c664d2cac92494a43517d3e87c96
Question: What data do they look at?
From the context "Broadly speaking, whenever a target language INLINEFORM0 requires some distinction that is optional or non-existent in source language INLINEFORM1, it is possible to create a sentence INLINEFORM2 in INLINEFORM3 where the missing information is not explicit but can be inferred from background knowledge. Translating INLINEFORM4 from INLINEFORM5 to INLINEFORM6 thus requires using the background knowledge to resolve the ambiguity, and will therefore be challenging for automatic machine translation.", so we know that they look at the data that is not explicit but can be inferred from background knowledge.
Answer: the data that is not explicit but can be inferred from background knowledge
True answer: WSC collection

Validation ID: 662870a90890c620a964720b2ca122a1139410ea
Question: What language do they explore?
From the context "Broadly speaking, whenever a target language INLINEFORM0 requires some distinction that is optional or non-existent in source language INLINEFORM1, it is possible to create a sentence INLINEFORM2 in INLINEFORM3 where the missing information is not explicit but can be inferred from background knowledge. Translating INLINEFORM4 from INLINEFORM5 to INLINEFORM6 thus requires using the background knowledge to resolve the ambiguity, and will therefore be challenging for automatic machine translation.", so we know that they explore the language INLINEFORM6.
Answer: INLINEFORM6
True answer: English, French, German 

Validation ID: 662870a90890c620a964720b2ca122a1139410ea
Question: What language do they explore?
From the context "Broadly speaking, whenever a target language INLINEFORM0 requires some distinction that is optional or non-existent in source language INLINEFORM1, it is possible to create a sentence INLINEFORM2 in INLINEFORM3 where the missing information is not explicit but can be inferred from background knowledge. Translating INLINEFORM4 from INLINEFORM5 to INLINEFORM6 thus requires using the background knowledge to resolve the ambiguity, and will therefore be challenging for automatic machine translation.", so we know that they explore the language INLINEFORM6.
Answer: INLINEFORM6
True answer: French, English, Spanish, Italian, Portuguese, Hebrew, Arabic

Validation ID: 92d1a6df3041667dc662376938bc65527a5a1c3c
Question: Do they report results only on English datasets?
From the context "We evaluate the proposed approach for augmenting the feature space in four tasks: (i) NER segmentation, (ii) NER classification, (iii) fine-grained sentiment classification and (iv) fine-grained sentiment quantification. The next sections present the evaluation settings we used. For each of the tasks, we use the designated training sets to train the learning algorithms, and we report the scores of the evaluation measures used in the respective test parts.", so we know that they report results on English datasets.
Answer: Yes
True answer: Yes

Validation ID: 92d1a6df3041667dc662376938bc65527a5a1c3c
Question: Do they report results only on English datasets?
From the context "We evaluate the proposed approach for augmenting the feature space in four tasks: (i) NER segmentation, (ii) NER classification, (iii) fine-grained sentiment classification and (iv) fine-grained sentiment quantification. The next sections present the evaluation settings we used. For each of the tasks, we use the designated training sets to train the learning algorithms, and we report the scores of the evaluation measures used in the respective test parts.", so we know that they report results on English datasets.
Answer: Yes
True answer: Yes

Validation ID: 12159f04e0427fe33fa05af6ba8c950f1a5ce5ea
Question: Which hyperparameters were varied in the experiments on the four tasks?
From the context "We cluster the embeddings with INLINEFORM0 -Means. The k-means clusters are initialized using “k-means++” as proposed in BIBREF9, while the algorithm is run for 300 iterations. We try different values for INLINEFORM1. For each INLINEFORM2, we repeat the clustering experiment with different seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia.", so we know that the hyperparameters were varied in the experiments on the four tasks are k-means, k-means++, 300 iterations, different values for INLINEFORM1, INLINEFORM2, different seed initialization for 10 times.
Answer: k-means, k-means++, 300 iterations, different values for INLINEFORM1, INLINEFORM2, different seed initialization for 10 times
True answer: number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding

Validation ID: 12159f04e0427fe33fa05af6ba8c950f1a5ce5ea
Question: Which hyperparameters were varied in the experiments on the four tasks?
From the context "We cluster the embeddings with INLINEFORM0 -Means. The k-means clusters are initialized using “k-means++” as proposed in BIBREF9, while the algorithm is run for 300 iterations. We try different values for INLINEFORM1. For each INLINEFORM2, we repeat the clustering experiment with different seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia.", so we know that the hyperparameters were varied in the experiments on the four tasks are k-means clusters, k-means++, INLINEFORM1, INLINEFORM2, seed initialization.
Answer: k-means clusters, k-means++, INLINEFORM1, INLINEFORM2, seed initialization
True answer: different number of clusters, different embeddings

Validation ID: a4a1fcef760b133e9aa876ac28145ad98a609927
Question: Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?
From the context "We cluster the embeddings with INLINEFORM0 -Means. The k-means clusters are initialized using “k-means++” as proposed in BIBREF9, while the algorithm is run for 300 iterations. We try different values for INLINEFORM1. For each INLINEFORM2, we repeat the clustering experiment with different seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia.", so we know that the hyperparameters are number of clusters, number of iterations and seed initialization.
Answer: number of clusters, number of iterations and seed initialization
True answer: selection of word vectors

Validation ID: 63bb2040fa107c5296351c2b5f0312336dad2863
Question: How were the cluster extracted??
From the context "We cluster the embeddings with INLINEFORM0 -Means. The k-means clusters are initialized using “k-means++” as proposed in BIBREF9, while the algorithm is run for 300 iterations. We try different values for INLINEFORM1. For each INLINEFORM2, we repeat the clustering experiment with different seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia.", so we know that the cluster extracted by k-means++.
Answer: k-means++
True answer: Word clusters are extracted using k-means on word embeddings

Validation ID: 01f4a0a19467947a8f3bdd7ec9fac75b5222d710
Question: what were the evaluation metrics?
From the context "Table TABREF23 shows the unlabeled INLINEFORM0 scores for our models and various baselines. All models soundly outperform right branching baselines, and we find that the neural PCFG/compound PCFG are strong models for grammar induction. In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chinese. We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization despite a thorough hyperparameter search. See lab:full for the full results (including corpus-level INLINEFORM1 ) broken down by sentence length.", so we know that the evaluation metrics are INLINEFORM0 scores.
Answer: INLINEFORM0 scores
True answer: INLINEFORM0 scores

Validation ID: 01f4a0a19467947a8f3bdd7ec9fac75b5222d710
Question: what were the evaluation metrics?
From the context "Table TABREF23 shows the unlabeled INLINEFORM0 scores for our models and various baselines. All models soundly outperform right branching baselines, and we find that the neural PCFG/compound PCFG are strong models for grammar induction. In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chinese. We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization despite a thorough hyperparameter search. See lab:full for the full results (including corpus-level INLINEFORM1 ) broken down by sentence length.", so we know that the evaluation metrics are INLINEFORM0 scores.
Answer: INLINEFORM0 scores
True answer: Unlabeled sentence-level F1, perplexity, grammatically judgment performance

Validation ID: 7784d321ccc64db5141113b6783e4ba92fdd4b20
Question: what are the state of the art methods?
From the context "We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG's rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful grammars by simply optimizing log likelihood. While the optimization problem remains non-convex, recent work suggests that there are optimization benefits afforded by over-parameterized models BIBREF9, BIBREF10, BIBREF11, and we indeed find that this neural PCFG is significantly easier to optimize than the traditional PCFG. Second, this factored parameterization makes it straightforward to incorporate side information into rule probabilities through a sentence-level continuous latent vector, which effectively allows different contexts in a derivation to coordinate. In this compound PCFG—continuous mixture of PCFGs—the context-free assumptions hold conditioned on the latent vector but not unconditionally, thereby obtaining longer-range dependencies within a tree-based generative process.", so we know that the state of the art methods are parameterizing a PCFG's rule probabilities with neural networks over distributed representations and compound PCFGs.
Answer: parameterizing a PCFG's rule probabilities with neural networks over distributed representations and compound PCFGs
True answer: Unanswerable

Validation ID: 7784d321ccc64db5141113b6783e4ba92fdd4b20
Question: what are the state of the art methods?
From the context "We revisit these aforementioned issues in light of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG's rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful grammars by simply optimizing log likelihood. While the optimization problem remains non-convex, recent work suggests that there are optimization benefits afforded by over-parameterized models BIBREF9, BIBREF10, BIBREF11, and we indeed find that this neural PCFG is significantly easier to optimize than the traditional PCFG. Second, this factored parameterization makes it straightforward to incorporate side information into rule probabilities through a sentence-level continuous latent vector, which effectively allows different contexts in a derivation to coordinate. In this compound PCFG—continuous mixture of PCFGs—the context-free assumptions hold conditioned on the latent vector but not unconditionally, thereby obtaining longer-range dependencies within a tree-based generative process.", so we know that the state of the art methods are parameterizing a PCFG's rule probabilities with neural networks over distributed representations and compound PCFG—continuous mixture of PCFGs.
Answer: parameterizing a PCFG's rule probabilities with neural networks over distributed representations and compound PCFG—continuous mixture of PCFGs
True answer: Unanswerable

Validation ID: 218615a005f7f00606223005fef22c07057d9d77
Question: what english datasets were used?
From the context "We evaluate our models on the Penn Treebank (PTB) BIBREF0 and Chinese Treebank (CTB) BIBREF1", so we know that the english datasets used are Penn Treebank (PTB) and Chinese Treebank (CTB).
Answer: Penn Treebank (PTB) and Chinese Treebank (CTB)
True answer: Answer with content missing: (Data section) Penn Treebank (PTB)

Validation ID: 867290103f762e1ddfa6f2ea30dd0a327f595182
Question: which chinese datasets were used?
From the context "We use the Penn Treebank (PTB) BIBREF0 for English and the Chinese Treebank (CTB) BIBREF1 for Chinese", so we know that the chinese datasets used are CTB.
Answer: CTB
True answer: Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)

Validation ID: 907b3af3cfaf68fe188de9467ed1260e52ec6cf1
Question: What were their distribution results?
From the context "Figure FIGREF34 shows that viral tweets labelled as containing fake news appear to include more URLs to other sites than viral tweets that do not contain fake news. In fact, the difference between the two distributions is statistically significant (assuming INLINEFORM0 ).", so we know that the distribution results are that viral tweets labelled as containing fake news appear to include more URLs to other sites than viral tweets that do not contain fake news.
Answer: viral tweets labelled as containing fake news appear to include more URLs to other sites than viral tweets that do not contain fake news
True answer: Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different

Validation ID: 56a8826cbee49560592b2d4b47b18ada236a12b9
Question: How did they determine fake news tweets?
From the context "The 1327 `viral' tweets were manually annotated as containing fake news or not containing them. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset", so we know that the 1327 `viral' tweets were manually annotated as containing fake news or not containing them.
Answer: manually annotated
True answer: an expert annotator determined if the tweet fell under a specific category

Validation ID: 56a8826cbee49560592b2d4b47b18ada236a12b9
Question: How did they determine fake news tweets?
From the context "The 1327 `viral' tweets were manually annotated as containing fake news or not containing them. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset", so we know that the 1327 `viral' tweets were manually annotated as containing fake news or not containing them.
Answer: manually annotated
True answer: Exposure, Characterization, Polarization

Validation ID: 968b7c3553a668ba88da105eff067d57f393c63f
Question: What is their definition of tweets going viral?
From the context "We consider that a tweet went viral if it was retweeted more than 1000 times by the 8th of November 2016", so we know that tweets going viral is retweeted more than 1000 times by the 8th of November 2016.
Answer: retweeted more than 1000 times by the 8th of November 2016
True answer: Viral tweets are the ones that are retweeted more than 1000 times

Validation ID: 968b7c3553a668ba88da105eff067d57f393c63f
Question: What is their definition of tweets going viral?
From the context "We use the number of retweets to single-out those that went viral within our sample. Tweets within that subset (viral tweets hereafter) are varied and relate to different topics. We consider that a tweet contains fake news if its text falls within any of the following categories described by Rubin et al. BIBREF7 (see next section for the details of such categories): serious fabrication, large-scale hoaxes, jokes taken at face value, slanted reporting of real facts and stories where the truth is contentious. The dataset BIBREF8, manually labelled by an expert, has been publicly released and is available to researchers and interested parties.", so we know that a tweet is considered as going viral if it is retweeted more than 1000 times.
Answer: retweeted more than 1000 times
True answer: those that contain a high number of retweets

Validation ID: f03df5d99b753dc4833ef27b32bb95ba53d790ee
Question: What are the characteristics of the accounts that spread fake news?
From the context "We found there are differences around exposure, characteristics of accounts spreading fake news and the tone of the content. Those findings suggest that it is indeed possible to model and automatically detect fake news", so we know that the characteristics of the accounts that spread fake news are exposure, characteristics of accounts spreading fake news and the tone of the content.
Answer: exposure, characteristics of accounts spreading fake news and the tone of the content
True answer: Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio

Validation ID: f03df5d99b753dc4833ef27b32bb95ba53d790ee
Question: What are the characteristics of the accounts that spread fake news?
From the context "We found there are differences around exposure, characteristics of accounts spreading fake news and the tone of the content. Those findings suggest that it is indeed possible to model and automatically detect fake news", so we know that the characteristics of the accounts that spread fake news are exposure, characteristics of accounts spreading fake news and the tone of the content.
Answer: exposure, characteristics of accounts spreading fake news and the tone of the content
True answer: have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only

Validation ID: a8f51b4e334a917702422782329d97304a2fe139
Question: What is the threshold for determining that a tweet has gone viral?
From the context "We consider that a tweet went viral if it was retweeted more than 1000 times by the 8th of November 2016", so we know that the threshold for determining that a tweet has gone viral is 1000.
Answer: 1000
True answer: 1000

Validation ID: dca86fbe1d57b44986055b282a03c15ef7882e51
Question: How is the ground truth for fake news established?
From the context "The 1327 `viral' tweets were manually annotated as containing fake news or not containing them. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset", so we know that the ground truth for fake news is established by a single person.
Answer: a single person
True answer: Ground truth is not established in the paper

Validation ID: 27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa
Question: what NMT models did they compare with?
From the context "We also trained the Transformer model BIBREF4 which is a strong baseline of NMT on both augmented and unaugmented parallel corpus. The training configuration of the Transformer model is shown in Table TABREF32. The hyper-parameters are set based on the settings in the paper BIBREF4 and the sizes of our training sets.", so we know that they compared with Transformer model.
Answer: Transformer model
True answer: RNN-based NMT model, Transformer-NMT

Validation ID: b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72
Question: Where does the ancient Chinese dataset come from?
From the context "To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", so we know that the ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.
Answer: ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era
True answer: ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era

Validation ID: b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72
Question: Where does the ancient Chinese dataset come from?
From the context "To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", so we know that the ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.
Answer: ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era
True answer: Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet 

Validation ID: 808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc
Question: How many different characters were in dataset?
From the context "We collect data from numerous characters from a variety of TV shows, movies, and anime. We filter and keep characters with at least five HLA, as those with fewer are not complex enough to be correctly modeled due to reasons such as lack of data. We end up eliminating 5.86% of total characters, and end up with 45,821 characters and 12,815 unique HLA, resulting in 945,519 total character-HLA pairs. Each collected character has 20.64 HLAs on average.", so we know that there are 45,821 characters in dataset.
Answer: 45,821 characters
True answer: 45,821 characters

Validation ID: 808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc
Question: How many different characters were in dataset?
From the context "We collect data from numerous characters from a variety of TV shows, movies, and anime. We filter and keep characters with at least five HLA, as those with fewer are not complex enough to be correctly modeled due to reasons such as lack of data. We end up eliminating 5.86% of total characters, and end up with 45,821 characters and 12,815 unique HLA, resulting in 945,519 total character-HLA pairs. Each collected character has 20.64 HLAs on average.", so we know that there are 45,821 characters in dataset.
Answer: 45,821 characters
True answer: 45,821 characters

Validation ID: 36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd
Question: How does dataset model character's profiles?
From the context "We define a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise. We define a constant $\alpha $ that measures our confidence in observing various character-HLA pairs as positive. $\alpha $ controls how much the model penalizes the error if the ground truth is $P_{u,i} = 1$. If $P_{u,i} = 1$ and the model guesses incorrectly, we penalize by $\alpha $ times the loss. But if $P_{u,i} = 0$ and the model guesses a value greater than 0, we do not penalize as $\alpha $ has no impact. This is because $P_{u,i} = 0$ can either represent a true negative or be due to a lack of data, and hence is less reliable for penalization. See Equation DISPLAY_FORM8. We find that using $\alpha =20$ provides decent results.", so we know that dataset model character's profiles by using a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise.
Answer: using a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise.
True answer: attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics

Validation ID: f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6
Question: How big is the difference in performance between proposed model and baselines?
From the context "We test the performance of ALOHA in character language style recovery against four baselines, demonstrating outperformance and system stability. We also run a human evaluation supporting our results.", so we know that the proposed model outperforms the baselines.
Answer: outperformance
True answer: Metric difference between Aloha and best baseline score:
Hits@1/20: +0.061 (0.3642 vs 0.3032)
MRR: +0.0572(0.5114 vs 0.4542)
F1: -0.0484 (0.3901 vs 0.4385)
BLEU: +0.0474 (0.2867 vs 0.2393)

Validation ID: 357eb9f0c07fa45e482d998a8268bd737beb827f
Question: What baseline models are used?
From the context "We use all three of these models as baselines for comparison. While these can handle a greater variety of tasks, they do not respond with text that aligns with particular human-like characteristics.", so we know that the baseline models are Poly-encoder, Feed Yourself and Kvmemnn.
Answer: Poly-encoder, Feed Yourself and Kvmemnn
True answer: the Poly-encoder from BIBREF7 humeau2019real, Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously, We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20., a BERT bi-ranker

Validation ID: 357eb9f0c07fa45e482d998a8268bd737beb827f
Question: What baseline models are used?
From the context "We use all three of these models as baselines for comparison. While these can handle a greater variety of tasks, they do not respond with text that aligns with particular human-like characteristics.", so we know that the baseline models are Poly-encoder, Feed Yourself and Kvmemnn.
Answer: Poly-encoder, Feed Yourself and Kvmemnn
True answer: Kvmemnn,  Feed Yourself, Poly-encoder, BERT bi-ranker

Validation ID: ad08b215dca538930ef1f50b4e49cd25527028ad
Question: Was PolyReponse evaluated against some baseline?
From the context "We provide only a brief recap here; see the original paper for further details.", so we know that PolyReponse was evaluated against some baseline.
Answer: Yes
True answer: No

Validation ID: ad08b215dca538930ef1f50b4e49cd25527028ad
Question: Was PolyReponse evaluated against some baseline?
From the context "We provide only a brief recap here; see the original paper for further details.", so we know that PolyReponse was evaluated against some baseline.
Answer: Yes
True answer: No

Validation ID: 31101dc9937f108e27e08a5f34be44f0090b8b6b
Question: What metric is used to evaluate PolyReponse system?
From the context "The system computes two sets of similarity scores: 1) $S(r,c)$ is the score of a candidate reply $r$ given a conversational context $c$, and 2) $S(p,c)$ is the score of a candidate photo $p$ given a conversational context $c$. These scores are computed as a scaled cosine similarity of a vector that represents the context ($h_c$), and a vector that represents the candidate response: a text reply ($h_r$) or a photo ($h_p$). For instance, $S(r,c)$ is computed as $S(r,c)=C cos(h_r,h_c)$, where $C$ is a learned constant.", so we know that the metric used to evaluate PolyReponse system is cosine similarity.
Answer: cosine similarity
True answer: Unanswerable

Validation ID: e4a315e9c190cf96493eefe04ce4ba6ae6894550
Question: How does PolyResponse architecture look like?
From the context "The PolyResponse system is powered by a single large conversational search engine, trained on a large amount of conversational and image data, as shown in Figure FIGREF2. In simple words, it is a ranking model that learns to score conversational replies and images in a given conversational context. The highest-scoring responses are then retrieved as system outputs. The system computes two sets of similarity scores: 1) $S(r,c)$ is the score of a candidate reply $r$ given a conversational context $c$, and 2) $S(p,c)$ is the score of a candidate photo $p$ given a conversational context $c$. These scores are computed as a scaled cosine similarity of a vector that represents the context ($h_c$), and a vector that represents the candidate response: a text reply ($h_r$) or a photo ($h_p$). For instance, $S(r,c)$ is computed as $S(r,c)=C cos(h_r,h_c)$, where $C$ is a learned constant. The part of the model dealing with text input (i.e., obtaining the encodings $h_c$ and $h_r$) follows the architecture introduced recently by Henderson:2019acl. We provide only a brief recap here; see the original paper for further details.", so we know that PolyResponse architecture is a ranking model that learns to score conversational replies and images in a given conversational context.
Answer: a ranking model that learns to score conversational replies and images in a given conversational context
True answer: Henderson:2017, MobileNet model

Validation ID: 6263b2cba18207474786b303852d2f0d7068d4b6
Question: In what 8 languages is PolyResponse engine used for restourant search and booking system?
From the context "The PolyResponse restaurant search is currently available in 8 languages and for 8 cities around the world: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade).", so we know that PolyResponse engine is used for restourant search and booking system in 8 languages: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade).
Answer: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)
True answer: English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian

Validation ID: 6263b2cba18207474786b303852d2f0d7068d4b6
Question: In what 8 languages is PolyResponse engine used for restourant search and booking system?
From the context "The PolyResponse restaurant search is currently available in 8 languages and for 8 cities around the world: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade).", so we know that PolyResponse engine is used for restourant search and booking system in 8 languages: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade).
Answer: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)
True answer: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)

Validation ID: c1c44fd96c3fa6e16949ae8fa453e511c6435c68
Question: Why masking words in the decoder is helpful?
From the context "The main reason to introduce the refine process is to enhance the decoder using BERT's contextualized representations, so we do not modify the encoder and reuse it during this process. On the decoder side, we propose a new word-level refine decoder. The refine decoder receives a generated summary draft as input, and outputs a refined summary. It first masks each word in the summary draft one by one, then feeds the draft to BERT to generate context vectors. Finally it predicts a refined summary word using an $N$ layer Transformer decoder which is the same as the draft decoder. At t-th time step the n-th word of input summary is masked, and the decoder predicts the n-th refined word given other words of the summary.", so we know that masking words in the decoder is helpful because it can enhance the decoder using BERT's contextualized representations.
Answer: enhance the decoder using BERT's contextualized representations
True answer: ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.

Validation ID: d28d86524292506d4b24ae2d486725a6d57a3db3
Question: What is the ROUGE score of the highest performing model?
From the context "We conduct experiments on the benchmark datasets CNN/Daily Mail and New York Times. Our model achieves a 33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L on the CNN/Daily Mail, which is state-of-the-art. On the New York Times dataset, our model achieves about 5.6% relative improvement over ROUGE-1.", so we know that the ROUGE score of the highest performing model is 33.33.
Answer: 33.33
True answer: 33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L 

Validation ID: d28d86524292506d4b24ae2d486725a6d57a3db3
Question: What is the ROUGE score of the highest performing model?
From the context "We conduct experiments on the benchmark datasets CNN/Daily Mail and New York Times. Our model achieves a 33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L on the CNN/Daily Mail, which is state-of-the-art. On the New York Times dataset, our model achieves about 5.6% relative improvement over ROUGE-1.", so we know that the ROUGE score of the highest performing model is 33.33.
Answer: 33.33
True answer: 33.33

Validation ID: feafcc1c4026d7f55a2c8ce7850d7e12030b5c22
Question: How are the different components of the model trained? Is it trained end-to-end?
From the context "We train the model using an Adam optimizer with learning rate of $3e-4$, $\beta _1=0.9$, $\beta _2=0.999$ and $\epsilon =10^{-9}$ and use a dynamic learning rate during the training process. For regularization, we use dropout BIBREF13 and label smoothing BIBREF14 in our models and set the dropout rate to 0.15, and the label smoothing value to 0.1. We set the batch size to 36, and train for 4 epochs(8 epochs for NYT50 since it has many fewer training samples), after training the best model are selected from last 10 models based on development set performance.", so we know that the model is trained end-to-end.
Answer: end-to-end
True answer: the objective of our model is sum of the two processes, jointly trained using "teacher-forcing" algorithm, we feed the ground-truth summary to each decoder and minimize the objective, At test time, each time step we choose the predicted word by $\hat{y} = argmax_{y^{\prime }} P(y^{\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries., the model can be trained end-to-end

Validation ID: feafcc1c4026d7f55a2c8ce7850d7e12030b5c22
Question: How are the different components of the model trained? Is it trained end-to-end?
From the context "We train the model using an Adam optimizer with learning rate of $3e-4$, $\beta _1=0.9$, $\beta _2=0.999$ and $\epsilon =10^{-9}$ and use a dynamic learning rate during the training process. For regularization, we use dropout BIBREF13 and label smoothing BIBREF14 in our models and set the dropout rate to 0.15, and the label smoothing value to 0.1. We set the batch size to 36, and train for 4 epochs(8 epochs for NYT50 since it has many fewer training samples), after training the best model are selected from last 10 models based on development set performance.", so we know that the model is trained end-to-end.
Answer: end-to-end
True answer: the model can be trained end-to-end

Validation ID: 63488da6c7aff9e374561a24ba224e9ce7f65e40
Question: When is this paper published?
From the context "In this paper, we describe the structure of our model, which learns to generate an abstractive multi-sentence summary from a given source document.", so we know that this paper is about abstractive multi-sentence summary.
From the context "Recently, many abstractive approaches are introduced based on neural sequence-to-sequence framework BIBREF4, BIBREF0, BIBREF3, BIBREF5.", so we know that this paper is about neural sequence-to-sequence framework.
True answer: Unanswerable

Validation ID: c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c
Question: Can their indexing-based method be applied to create other QA datasets in other domains, and not just Wikipedia?
From the context "We propose an automatic way of mapping the answer contexts from all corpora to the same version of Wikipeda so they can be coherently used for answer retrieval. Each paragraph in Wikipedia is first indexed by Lucene using {1,2,3}-grams, where the paragraphs are separated by WikiExtractor and segmented by NLP4J (28.7M+ paragraphs are indexed). Each answer sentence from the corpora in Table TABREF3 is then queried to Lucene, and the top-5 ranked paragraphs are retrieved. The cosine similarity between each sentence in these paragraphs and the answer sentence is measured for INLINEFORM0 -grams, say INLINEFORM1. A weight is assigned to each INLINEFORM2 -gram score, say INLINEFORM3, and the weighted sum is measured: INLINEFORM4. The fixed weights of INLINEFORM5 are used for our experiments, which can be improved.", so we know that their indexing-based method can be applied to create other QA datasets in other domains, and not just Wikipedia.
Answer: Yes
True answer: Unanswerable

Validation ID: a9337636b52de375c852682a2561af2c1db5ec63
Question: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?
From the context "We create a silver-standard dataset for answer retrieval and triggering, which will be publicly available", so we know that they create a silver-standard dataset for answer retrieval and triggering.
Answer: Yes
True answer: Yes

Validation ID: a9337636b52de375c852682a2561af2c1db5ec63
Question: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?
From the context "We create a silver-standard dataset for answer retrieval and triggering, which will be publicly available", so we know that they create a silver-standard dataset for answer retrieval and triggering.
Answer: Yes
True answer: No

Validation ID: 45a5961a4e1d1c22874c4918e5c98bd3c0a670b3
Question: How many question types do they find in the datasets analyzed?
From the context "Fig. FIGREF4 shows the distributions of seven question types grouped deterministically from the lexicons", so we know that they find seven question types in the datasets analyzed.
Answer: seven question types
True answer: seven 

Validation ID: 45a5961a4e1d1c22874c4918e5c98bd3c0a670b3
Question: How many question types do they find in the datasets analyzed?
From the context "Fig. FIGREF4 shows the distributions of seven question types grouped deterministically from the lexicons", so we know that they find seven question types in the datasets analyzed.
Answer: seven
True answer: 7

Validation ID: 30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1
Question: How do they analyze contextual similaries across datasets?
From the context "We first give a thorough intrinsic analysis regarding contextual similarities, question types, and answer categories (Section SECREF2 )", so we know that they analyze contextual similaries across datasets in Section SECREF2.
From the context "Fig. FIGREF4 shows the distributions of seven question types grouped deterministically from the lexicons. Although these corpora have been independently developed, a general trend is found, where the what question type dominates, followed by how and who, followed by when and where, and so on.", so we know that they analyze contextual similaries across datasets by grouping question types deterministically from the lexicons.
True answer: They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.

Validation ID: 5c6fa86757410aee6f5a0762328637de03a569e9
Question: What were their performance results?
From the context "We have shown that DNN models can be used for cyberbullying detection on various topics across multiple SMPs using three datasets and four DNN models. These models coupled with transfer learning beat state of the art results for all three datasets.", so we know that the performance results are that DNN models can be used for cyberbullying detection on various topics across multiple SMPs using three datasets and four DNN models. These models coupled with transfer learning beat state of the art results for all three datasets.
Answer: DNN models can be used for cyberbullying detection on various topics across multiple SMPs using three datasets and four DNN models. These models coupled with transfer learning beat state of the art results for all three datasets.
True answer: best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset

Validation ID: 7e38e0279a620d3df05ab9b5e2795044f18d4471
Question: What cyberbulling topics did they address?
From the context "We cover three different types of social networks: teen oriented Q&A forum (Formspring), large microblogging platform (Twitter), and collaborative knowledge repository (Wikipedia talk pages). Each dataset addresses a different topic of cyberbullying. Twitter dataset contains examples of racism and sexism. Wikipedia dataset contains examples of personal attack. However, Formspring dataset is not specifically about any single topic.", so we know that they address three cyberbulling topics: personal attack, racism, and sexism.
Answer: personal attack, racism, and sexism
True answer: personal attack, racism, and sexism

Validation ID: 7e38e0279a620d3df05ab9b5e2795044f18d4471
Question: What cyberbulling topics did they address?
From the context "We cover three different types of social networks: teen oriented Q&A forum (Formspring), large microblogging platform (Twitter), and collaborative knowledge repository (Wikipedia talk pages). Each dataset addresses a different topic of cyberbullying. Twitter dataset contains examples of racism and sexism. Wikipedia dataset contains examples of personal attack. However, Formspring dataset is not specifically about any single topic.", so we know that they address three cyberbulling topics: personal attack, racism, and sexism.
Answer: personal attack, racism, and sexism
True answer: racism, sexism, personal attack, not specifically about any single topic

Validation ID: 8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92
Question: Were any of the pipeline components based on deep learning models?
From the context "The main goal of all the modules except lexicon matching is to identify events given in the text. These events are then used to populate an ontology. The lexicon matching, on the other hand, was created to link words that are found in the text source with the data available not only on Eurovoc BIBREF6 thesaurus but also on the EU's terminology database IATE BIBREF7 (see Section SECREF12 for details).", so we know that the main goal of all the modules except lexicon matching is to identify events given in the text. These events are then used to populate an ontology. The lexicon matching, on the other hand, was created to link words that are found in the text source with the data available not only on Eurovoc BIBREF6 thesaurus but also on the EU's terminology database IATE BIBREF7 (see Section SECREF12 for details).
Answer: No
True answer: No

Validation ID: 8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92
Question: Were any of the pipeline components based on deep learning models?
From the context "The main goal of all the modules except lexicon matching is to identify events given in the text. These events are then used to populate an ontology. The lexicon matching, on the other hand, was created to link words that are found in the text source with the data available not only on Eurovoc BIBREF6 thesaurus but also on the EU's terminology database IATE BIBREF7 (see Section SECREF12 for details).", so we know that the main goal of all the modules except lexicon matching is to identify events given in the text. These events are then used to populate an ontology. The lexicon matching, on the other hand, was created to link words that are found in the text source with the data available not only on Eurovoc BIBREF6 thesaurus but also on the EU's terminology database IATE BIBREF7 (see Section SECREF12 for details).
Answer: No
True answer: No

Validation ID: 03ebb29c08375afc42a957c7b2dc1a42bed7b713
Question: How is the effectiveness of this pipeline approach evaluated?
From the context "The obtained results support our claim that the proposed system can be used as a base tool for information extraction for the Portuguese language", so we know that the effectiveness of this pipeline approach is evaluated by the obtained results.
Answer: the obtained results
True answer: proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents.

Validation ID: 9cf070d6671ee4a6353f79a165aa648309e01295
Question: What is the size of the parallel corpus used to train the model constraints?
From the context "A part of the ILCI English-Hindi Tourism parallel corpus (1500 sentences) was used for training the classifiers", so we know that the size of the parallel corpus used to train the model constraints is 1500 sentences.
Answer: 1500 sentences
True answer: 1500 sentences

Validation ID: 9cf070d6671ee4a6353f79a165aa648309e01295
Question: What is the size of the parallel corpus used to train the model constraints?
From the context "A part of the ILCI English-Hindi Tourism parallel corpus (1500 sentences) was used for training the classifiers", so we know that the size of the parallel corpus used to train the model constraints is 1500 sentences.
Answer: 1500 sentences
True answer: 1500 sentences

Validation ID: 87bc6f83f7f90df3c6c37659139b92657c3f7a38
Question: How does enforcing agreement between parse trees work across different languages?
From the context "We first obtain the predicted parse trees for the English and Hindi sentences from the respective trained parser models as an initialsiation step. The DD algorithm then tries to enforce agreement between the two parse trees subject to the given alignments.", so we know that the DD algorithm tries to enforce agreement between the two parse trees subject to the given alignments.
Answer: The DD algorithm tries to enforce agreement between the two parse trees subject to the given alignments.
True answer: we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints

Validation ID: 01e2d10178347d177519f792f86f25575106ddc7
Question: What datasets are used to assess the performance of the system?
From the context "We use the average precision (AP, equal to the area under the precision-recall curve) as the evaluation metric, and report both the AP across the overall 12 labels, and the AP across 11 situation types, as shown in Table TABREF18. For each configuration, only a single 10-fold CV result is reported, since we observe less variance in results here than in Switchboard. We have 16.5 hours in-domain data and 8.5 hours out-of-domain data for Turkish, 2.9 and 13.2 hours for Uzbek, and 7.7 and 7.2 hours for Mandarin. We use the same CNN architecture as on Switchboard but make the changes as described in Section SECREF11. Also we use mini-batch size 30 and fix the training epochs as 100. All CNNs use word2vec pre-training. Additionally, we also implement another two separate topic ID baselines using the decoded word outputs from two supervised ASR systems, trained from 80 hours transcribed Babel Turkish speech BIBREF29 and about 170 hours transcribed HKUST Mandarin telephone speech (LDC2005T32 and LDC2005S15), respectively.", so we know that the datasets used to assess the performance of the system are Switchboard, Turkish, Uzbek, and Mandarin.
Answer: Switchboard, Turkish, Uzbek, and Mandarin
True answer: Switchboard Telephone Speech Corpus BIBREF21, LORELEI (Low Resource Languages for Emergent Incidents) Program

Validation ID: 01e2d10178347d177519f792f86f25575106ddc7
Question: What datasets are used to assess the performance of the system?
From the context "We use the average precision (AP, equal to the area under the precision-recall curve) as the evaluation metric, and report both the AP across the overall 12 labels, and the AP across 11 situation types, as shown in Table TABREF18. For each configuration, only a single 10-fold CV result is reported, since we observe less variance in results here than in Switchboard. We have 16.5 hours in-domain data and 8.5 hours out-of-domain data for Turkish, 2.9 and 13.2 hours for Uzbek, and 7.7 and 7.2 hours for Mandarin. We use the same CNN architecture as on Switchboard but make the changes as described in Section SECREF11. Also we use mini-batch size 30 and fix the training epochs as 100. All CNNs use word2vec pre-training. Additionally, we also implement another two separate topic ID baselines using the decoded word outputs from two supervised ASR systems, trained from 80 hours transcribed Babel Turkish speech BIBREF29 and about 170 hours transcribed HKUST Mandarin telephone speech (LDC2005T32 and LDC2005S15), respectively.", so we know that the datasets used to assess the performance of the system are Switchboard, Turkish, Uzbek, and Mandarin.
Answer: Switchboard, Turkish, Uzbek, and Mandarin
True answer: LORELEI datasets of Uzbek, Mandarin and Turkish

Validation ID: 021bfb7e180d67112b74f05ecb3fa13acc036c86
Question: How is the vocabulary of word-like or phoneme-like units automatically discovered?
From the context "Unsupervised term discovery (UTD) UTD aims to automatically identify and cluster repeated terms (e.g. words or phrases) from speech. To circumvent the exhaustive DTW-based search limited by INLINEFORM0 time BIBREF6, we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) BIBREF7, which permits search in INLINEFORM1 time. We briefly describe the UTD procedures in ZRTools by four steps below, and full details can be found in BIBREF7. Construct the sparse approximate acoustic similarity matrices between pairs of speech utterances. Identify word repetitions via fast diagonal line search and segmental DTW. The resulting matches are used to construct an acoustic similarity graph, where nodes represent the matching acoustic segments and edges reflect DTW distances. Threshold the graph edges, and each connected component of the graph is a cluster of acoustic segments, which produces a corresponding term (word/phrase) category. Finally, the cluster of each discovered term category consists of a list of term occurrences.", so we know that the vocabulary of word-like or phoneme-like units is automatically discovered by UTD.
Answer: UTD
True answer: Zero Resource Toolkit (ZRTools) BIBREF7

Validation ID: d201b9992809142fe59ae74508bc576f8ca538ff
Question: IS the graph representation supervised?
From the context "To start, we assign an initial embedding vector to every medical code $c_\ast \in \mathcal {O}_\ast $ with a learnable embedding matrix $\mathbf {W}_e \in \mathbb {R}^{|\mathcal {O}_\ast | \times d}$ where $d$ is the embedding dimension.", so we know that the graph representation is supervised.
Answer: Yes
True answer: The graph representation appears to be semi-supervised. It is included in the learning pipeline for the medical recommendation, where the attention model is learned. (There is some additional evidence that is unavailable in parsed text)

Validation ID: c4628d965983934d7a2a9797a2de6a411629d5bc
Question: Is the G-BERT model useful beyond the task considered?
From the context "We believe that the G-BERT model can be applied to other tasks that require medical code representation, such as disease prediction and drug-drug interaction prediction", so we know that the G-BERT model can be applied to other tasks that require medical code representation, such as disease prediction and drug-drug interaction prediction.
Answer: disease prediction and drug-drug interaction prediction
True answer: There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding.

Validation ID: c4628d965983934d7a2a9797a2de6a411629d5bc
Question: Is the G-BERT model useful beyond the task considered?
From the context "We believe that the G-BERT model can be applied to other tasks in healthcare, such as disease prediction and drug-drug interaction prediction", so we know that the G-BERT model can be applied to other tasks in healthcare, such as disease prediction and drug-drug interaction prediction.
Answer: disease prediction and drug-drug interaction prediction
True answer: It learns a representation of medical records. The learned representation (embeddings) can be used for other predictive tasks involving information from electronic health records.

Validation ID: bd419f4094186a5ce74ba6ac1622b24e29e553f4
Question: How well did the baseline perform?
From the context "Table TABREF8 shows the results for the baseline models as well as the HUMAN measured performance on the task. The human performance provides an upper bound for the RUN task performance, while the simple baselines provide lower bounds. The best baseline model is NO-MOVE, reaching an accuracy of 30.3% on single sentences and 0.3 on complete paragraphs.", so we know that the best baseline model is NO-MOVE, reaching an accuracy of 30.3% on single sentences and 0.3 on complete paragraphs.
Answer: 30.3% on single sentences and 0.3 on complete paragraphs
True answer: accuracy of 30.3% on single sentences and 0.3 on complete paragraphs

Validation ID: 11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af
Question: What is the baseline?
From the context "We provide three simple baselines for the RUN task: (1) NO-MOVE: the only position considered is the starting point; (2) RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route; (3) JUMP: at each sentence, extract entities from the map and move between them in the order they appear. If the *WALK action is invalid we take a random *TURN action.", so we know that the baseline is NO-MOVE, RANDOM and JUMP.
Answer: NO-MOVE, RANDOM and JUMP
True answer: NO-MOVE, RANDOM, JUMP

Validation ID: 11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af
Question: What is the baseline?
From the context "We provide three simple baselines for the RUN task: (1) NO-MOVE: the only position considered is the starting point; (2) RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route; (3) JUMP: at each sentence, extract entities from the map and move between them in the order they appear. If the *WALK action is invalid we take a random *TURN action.", so we know that the baseline is NO-MOVE, RANDOM and JUMP.
Answer: NO-MOVE, RANDOM and JUMP
True answer: NO-MOVE: the only position considered is the starting point, RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route, JUMP: at each sentence, extract entities from the map and move between them in the order they appear

Validation ID: 1269c5d8f61e821ee0029080c5ba2500421d5fa6
Question: what methods were used to reduce data sparsity effects?
From the context "In this section, we describe the data augmentation methods we use to increase the amount of training data in order to make our NMT systems suffer less from the low-resourced situation in Japanese INLINEFORM0 Vietnamese translation. Although NMT systems can predict and generate the translation of unseen words on their vocabularies, but they only perform this well if the parallel corpus for training are sufficiently large. For many under-resourced languages, unfortunately, it hardly presents. In reality, although the monolingual data of Vietnamese and Japanese are immensely available due to the popularity of their speakers, the bilingual Japanese-Vietnamese corpora are very limited and often in low quality or in narrowly technical domains. Therefore, data augmentation methods to exploit monolingual data for NMT systems are necessary to obtain more bilingual data, thus upgrading the translating quality.", so we know that the methods used to reduce data sparsity effects are data augmentation methods.
Answer: data augmentation methods
True answer: Back Translation, Mix-Source Approach

Validation ID: 1269c5d8f61e821ee0029080c5ba2500421d5fa6
Question: what methods were used to reduce data sparsity effects?
From the context "In this section, we describe the data augmentation methods we use to increase the amount of training data in order to make our NMT systems suffer less from the low-resourced situation in Japanese INLINEFORM0 Vietnamese translation. Although NMT systems can predict and generate the translation of unseen words on their vocabularies, but they only perform this well if the parallel corpus for training are sufficiently large. For many under-resourced languages, unfortunately, it hardly presents. In reality, although the monolingual data of Vietnamese and Japanese are immensely available due to the popularity of their speakers, the bilingual Japanese-Vietnamese corpora are very limited and often in low quality or in narrowly technical domains. Therefore, data augmentation methods to exploit monolingual data for NMT systems are necessary to obtain more bilingual data, thus upgrading the translating quality.", so we know that the methods used to reduce data sparsity effects are data augmentation methods.
Answer: data augmentation methods
True answer: data augmentation

Validation ID: e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb
Question: what was the baseline?
From the context "In this paper, we will briefly explain the Recurrent NMT as we utilize it in our experiments", so we know that the baseline is Recurrent NMT.
Answer: Recurrent NMT
True answer: traditional phrase-based statistical machine translation (SMT), NMT system

Validation ID: e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb
Question: what was the baseline?
From the context "In this paper, we will briefly explain the Recurrent NMT as we utilize it in our experiments", so we know that the baseline is Recurrent NMT.
Answer: Recurrent NMT
True answer: traditional phrase-based statistical machine translation (SMT), NMT system

Validation ID: b9ea841b817ba23281c95c7a769873b840dee8d5
Question: did they collect their own data?
From the context "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15. After removing blank and duplicate lines we obtained 106758 pairs of sentences. The validation set used in all experiments is dev2010 and the test set is tst2010.", so we know that they collected their own data.
Answer: Yes
True answer: No

Validation ID: 219af68afeaecabdfd279f439f10ba7c231736e4
Question: what japanese-vietnamese dataset do they use?
From the context "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15. After removing blank and duplicate lines we obtained 106758 pairs of sentences. The validation set used in all experiments is dev2010 and the test set is tst2010.", so we know that they use TED talks extracted from WIT3's corpus BIBREF15.
Answer: TED talks extracted from WIT3's corpus BIBREF15
True answer: WIT3's corpus

Validation ID: a66a275a817f980c36e0b67d2e00bd823f63abf8
Question: How do they measure style transfer success?
From the context "At first, we examined how often the sentences change when translated monolingually. The assumption is that passing modified style factors should prevent the model from simply copying the source sequences when translating inside a single language, and incentivize it to match its output to certain style characteristics typical for different corpora. Figure FIGREF28 shows the proportions of sentence pairs", so we know that they measure style transfer success by the proportions of sentence pairs.
Answer: the proportions of sentence pairs
True answer: Unanswerable

Validation ID: b6f466e0fdcb310ecd212fd90396d9d13e0c0504
Question: Do they introduce errors in the data or does the data already contain them?
From the context "We use the zero-shot effect to achieve monolingual translation by training the model on bilingual examples in both directions, and then doing translation into the same language as the input: illustrated on Figure FIGREF1.", so we know that the data already contain errors.
Answer: the data already contain errors
True answer:  all three languages have error-corrected corpora for testing purposes

Validation ID: b6f466e0fdcb310ecd212fd90396d9d13e0c0504
Question: Do they introduce errors in the data or does the data already contain them?
From the context "We use the following error-corrected corpora both for scoring and as basis for manual analysis: for English: CoNLL-2014 BIBREF5 and JFLEG BIBREF20 corpora for Estonian: the Learner Language Corpus BIBREF21 for Latvian: the Error-annotated Corpus of Latvian BIBREF22 All of these are based on language learner (L2) essays and their manual corrections.", so we know that the data already contain errors.
Answer: the data already contain errors
True answer: Data already contain errors

Validation ID: 62ea141d0fb342dfb97c69b49d1c978665b93b3c
Question: What error types is their model more reliable for?
From the context "Our model handles punctuation, word order mistakes and grammatical errors well. For example the subject-verb disagreement in English UID16 and verb-object disagreement in Estonian UID19 have been corrected.", so we know that their model is more reliable for punctuation, word order mistakes and grammatical errors.
Answer: punctuation, word order mistakes and grammatical errors
True answer: grammatical, spelling and word order errors

Validation ID: 62ea141d0fb342dfb97c69b49d1c978665b93b3c
Question: What error types is their model more reliable for?
From the context "Our model handles punctuation, word order mistakes and grammatical errors well. For example the subject-verb disagreement in English UID16 and verb-object disagreement in Estonian UID19 have been corrected.", so we know that their model is more reliable for punctuation, word order mistakes and grammatical errors.
Answer: punctuation, word order mistakes and grammatical errors
True answer: spelling, word order and grammatical errors

Validation ID: a32c792a0cef03218bf66322245677fc2d5e5a31
Question: How does their parallel data differ in terms of style?
From the context "We assume that there should be sufficient stylistic difference between these corpora, especially between the more informal OpenSubtitles2018 (comprised of movie and TV subtitles) on one hand and Europarl and JRC-Acquis (proceedings and documents of the European Parliament) on the other.", so we know that the parallel data differ in terms of style.
Answer: The parallel data differ in terms of style.
True answer: Unanswerable

Validation ID: 0101ebfbaba75fd47868ad0c796ac44ebc19c566
Question: How do they split text to obtain sentence levels?
From the context "We padded sentence number in each passage to 101, all word number in each sentence to 100. Word number of queries and choices were padded to 50.", so we know that they split text to obtain sentence levels by padding sentence number in each passage to 101, all word number in each sentence to 100. Word number of queries and choices were padded to 50.
Answer: padding sentence number in each passage to 101, all word number in each sentence to 100. Word number of queries and choices were padded to 50.
True answer: Unanswerable

Validation ID: 50cb50657572e315fd452a89f3e0be465094b66f
Question: Do they experiment with their proposed model on any other dataset other than MovieQA?
From the context "The power of the model is verified on MovieQA dataset, which yielded the state of the art result on the dataset", so we know that they only experiment with their proposed model on MovieQA dataset.
Answer: No
True answer: Yes

Validation ID: 50cb50657572e315fd452a89f3e0be465094b66f
Question: Do they experiment with their proposed model on any other dataset other than MovieQA?
From the context "The power of the model is verified on MovieQA dataset, which yielded the state of the art result on the dataset", so we know that they experiment with their proposed model on MovieQA dataset.
Answer: MovieQA dataset
True answer: Yes

Validation ID: 981fd79dd69581659cb1d4e2b29178e82681eb4d
Question: What is the difference of the proposed model with a standard RNN encoder-decoder?
From the context "The proposed semantic RALSTM cell applied for Decoder side consists of three components: a Refinement cell, a traditional LSTM cell, and an Adjustment cell", so we know that the proposed model has three components: a Refinement cell, a traditional LSTM cell, and an Adjustment cell.
Answer: The proposed model has three components: a Refinement cell, a traditional LSTM cell, and an Adjustment cell.
True answer: Introduce a "Refinement Adjustment LSTM-based component" to the decoder

Validation ID: 03e9ac1a2d90152cd041342a11293a1ebd33bcc3
Question: Does the model evaluated on NLG datasets or dialog datasets?
From the context "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television", so we know that the model evaluated on NLG datasets.
Answer: NLG datasets
True answer: NLG datasets

Validation ID: 03e9ac1a2d90152cd041342a11293a1ebd33bcc3
Question: Does the model evaluated on NLG datasets or dialog datasets?
From the context "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television", so we know that the model evaluated on NLG datasets.
Answer: NLG datasets
True answer: NLG datasets

Validation ID: ef396a34436072cb3c40b0c9bc9179fee4a168ae
Question: What tasks do they experiment with?
From the context "Among many natural language processing (NLP) tasks, a common problem is", so we know that they experiment with many natural language processing (NLP) tasks.
Answer: many natural language processing (NLP) tasks
True answer: text classification and text semantic matching

Validation ID: ef396a34436072cb3c40b0c9bc9179fee4a168ae
Question: What tasks do they experiment with?
From the context "Among many natural language processing (NLP) tasks, a common problem is", so we know that they experiment with many natural language processing (NLP) tasks.
Answer: many natural language processing (NLP) tasks
True answer: text classification and text semantic matching

Validation ID: 04bde1d2b445f971e97bb46ade2d0290981c7a32
Question: What is the meta knowledge specifically?
From the context "Specifically, we propose two meta networks to generate the context-specific compositional functions for RecNN and TreeLSTM respectively.", so we know that the meta knowledge is the context-specific compositional functions for RecNN and TreeLSTM.
Answer: context-specific compositional functions for RecNN and TreeLSTM
True answer: Unanswerable

Validation ID: bfbd6040cb95b179118557352e8e3899ef25c525
Question: Are there elements, other than pitch, that can potentially result in out of key converted singing?
From the context "A pitch regression network is employed to render an adversarial loss separating pitch related information from the latent space in autoencoder. After the WaveNet-like encoder, a singer and pitch invariant representation is generated and then fed into the WaveNet decoder conditioning on the singer embedding and the extracted pitch to reconstruct the target singing voice.", so we know that the elements, other than pitch, that can potentially result in out of key converted singing are singer embedding and the extracted pitch.
Answer: singer embedding and the extracted pitch
True answer: Unanswerable

Validation ID: d6e353e0231d09fd5dcba493544d53706f3fe1ab
Question: How is the quality of singing voice measured?
From the context "Mean Opinion Score (MOS) was used as a subjective metric to evaluate the quality of the converted audio", so we know that the quality of singing voice is measured by Mean Opinion Score (MOS).
Answer: Mean Opinion Score (MOS)
True answer: To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score.

Validation ID: d6e353e0231d09fd5dcba493544d53706f3fe1ab
Question: How is the quality of singing voice measured?
From the context "Mean Opinion Score (MOS) was used as a subjective metric to evaluate the quality of the converted audio", so we know that the quality of singing voice is measured by Mean Opinion Score (MOS).
Answer: Mean Opinion Score (MOS)
True answer: Automatic: Normalized cross correlation (NCC)
Manual: Mean Opinion Score (MOS)

Validation ID: 7bd6a6ec230e1efb27d691762cc0674237dc7967
Question: what data did they use?
From the context "We extend the language model, AWD-LSTM BIBREF0, by replacing LSTM layers with PRU. Our model uses 3-layers of PRU with an embedding size of 400. The number of parameters learned by state-of-the-art methods vary from 18M to 66M with majority of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair comparison with state-of-the-art methods, we fix the model size to 19M and vary the value of INLINEFORM0 and hidden layer sizes so that total number of learned parameters is similar across different configurations. We use 1000, 1200, and 1400 as hidden layer sizes for values of INLINEFORM1 =1,2, and 4, respectively. We use the same settings for the WT-2 dataset. We set the number of pyramidal levels INLINEFORM2 to two in our experiments and use average pooling for sub-sampling.", so we know that they use PTB and WT-2 datasets.
Answer: PTB and WT-2 datasets
True answer:  Penn Treebank, WikiText2

Validation ID: 7bd6a6ec230e1efb27d691762cc0674237dc7967
Question: what data did they use?
From the context "We extend the language model, AWD-LSTM BIBREF0, by replacing LSTM layers with PRU. Our model uses 3-layers of PRU with an embedding size of 400. The number of parameters learned by state-of-the-art methods vary from 18M to 66M with majority of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair comparison with state-of-the-art methods, we fix the model size to 19M and vary the value of INLINEFORM0 and hidden layer sizes so that total number of learned parameters is similar across different configurations. We use 1000, 1200, and 1400 as hidden layer sizes for values of INLINEFORM1 =1,2, and 4, respectively. We use the same settings for the WT-2 dataset. We set the number of pyramidal levels INLINEFORM2 to two in our experiments and use average pooling for sub-sampling.", so we know that they use PTB and WT-2 datasets.
Answer: PTB and WT-2 datasets
True answer: Penn Treebank (PTB) , WikiText2 (WT-2)

Validation ID: 6aaf12505add25dd133c7b0dafe8f4fe966d1f1d
Question: what previous RNN models do they compare with?
From the context "We extend the basic gating architecture of LSTM with the pyramidal and grouped linear transformations outlined above to produce the Pyramidal Recurrent Unit (PRU), whose improved sequence modeling capacity is evidenced in Section SECREF4.", so we know that they compare with LSTM.
Answer: LSTM
True answer: Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM

Validation ID: 73906462bd3415f23d6378590a5ba28709b17605
Question: What are examples of these artificats?
From the context "In particular, we show that translating different parts of each instance separately (e.g. the premise and the hypothesis in NLI) can alter superficial patterns in the data (e.g. the degree of lexical overlap between them), which severely affects the generalization ability of current models.", so we know that translating different parts of each instance separately can alter superficial patterns in the data.
Answer: translating different parts of each instance separately can alter superficial patterns in the data
True answer: the degree of lexical overlap between them, presence of negation words

Validation ID: 73906462bd3415f23d6378590a5ba28709b17605
Question: What are examples of these artificats?
From the context "In particular, we show that translating different parts of each instance separately (e.g. the premise and the hypothesis in NLI) can alter superficial patterns in the data (e.g. the degree of lexical overlap between them), which severely affects the generalization ability of current models.", so we know that translating different parts of each instance separately can alter superficial patterns in the data.
Answer: translating different parts of each instance separately can alter superficial patterns in the data
True answer: hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap

Validation ID: 5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11
Question: What are the languages they use in their experiment?
From the context "We experiment with two models that are representative of the state-of-the-art in monolingual and cross-lingual pre-training: (i) Roberta BIBREF28, which is an improved version of BERT that uses masked language modeling to pre-train an English Transformer model, and (ii) XLM-R BIBREF8, which is a multilingual extension of the former pre-trained on 100 languages", so we know that they use English and 100 languages in their experiment.
Answer: English and 100 languages
True answer: English
French
Spanish
German
Greek
Bulgarian
Russian
Turkish
Arabic
Vietnamese
Thai
Chinese
Hindi
Swahili
Urdu
Finnish

Validation ID: 5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11
Question: What are the languages they use in their experiment?
From the context "We experiment with two models that are representative of the state-of-the-art in monolingual and cross-lingual pre-training: (i) Roberta BIBREF28, which is an improved version of BERT that uses masked language modeling to pre-train an English Transformer model, and (ii) XLM-R BIBREF8, which is a multilingual extension of the former pre-trained on 100 languages", so we know that they use English and 100 languages in their experiment.
Answer: English and 100 languages
True answer: English, Spanish, Finnish

Validation ID: 88bf368491f9613767f696f84b4bb1f5a7d7cb48
Question: Does the professional translation or the machine translation introduce the artifacts?
From the context "As shown in Table TABREF9, both BT-XX and MT-XX clearly outperform Orig in all test sets created through translation, which is consistent with our previous results. In contrast, the best results on the original English set are obtained by Orig, and neither BT-XX nor MT-XX obtain any clear improvement on the one in Spanish either. This shows that the phenomenon under study is highly relevant, to the extent that it can be exploited to improve the state-of-the-art.", so we know that the professional translation introduces the artifacts.
Answer: professional translation
True answer: Yes

Validation ID: 0737954caf66f2b4c898b356d2a3c43748b9706b
Question: Do they recommend translating the premise and hypothesis together?
From the context "As a result, one can expect both QA benchmarks to have more consistent translations than XNLI, which would in turn diminish this phenomenon. In contrast, the questions and context paragraphs are independently translated when using machine translation, which explains why BT-ES and BT-FI outperform Orig for the Translate-Test approach. We conclude that the translation artifacts revealed by our analysis are not exclusive to NLI, as they also show up on QA for the Translate-Test approach, but their actual impact can be highly dependent on the translation procedure used and the nature of the task.", so we know that they recommend translating the premise and hypothesis together.
Answer: translating the premise and hypothesis together
True answer: No

Validation ID: 0737954caf66f2b4c898b356d2a3c43748b9706b
Question: Do they recommend translating the premise and hypothesis together?
From the context "As a result, one can expect both QA benchmarks to have more consistent translations than XNLI, which would in turn diminish this phenomenon. In contrast, the questions and context paragraphs are independently translated when using machine translation, which explains why BT-ES and BT-FI outperform Orig for the Translate-Test approach. We conclude that the translation artifacts revealed by our analysis are not exclusive to NLI, as they also show up on QA for the Translate-Test approach, but their actual impact can be highly dependent on the translation procedure used and the nature of the task.", so we know that they recommend translating the premise and hypothesis together.
Answer: translating the premise and hypothesis together
True answer: No

Validation ID: 664b3eadc12c8dde309e8bbd59e9af961a433cde
Question: Is the improvement over state-of-the-art statistically significant?
From the context "These results can partly be explained by the translation procedure used to create the different benchmarks: the premises and hypotheses of XNLI were translated independently, whereas the questions and context paragraphs of XQuAD were translated together. As a result, one can expect both QA benchmarks to have more consistent translations than XNLI, which would in turn diminish this phenomenon. In contrast, the questions and context paragraphs are independently translated when using machine translation, which explains why BT-ES and BT-FI outperform Orig for the Translate-Test approach. We conclude that the translation artifacts revealed by our analysis are not exclusive to NLI, as they also show up on QA for the Translate-Test approach, but their actual impact can be highly dependent on the translation procedure used and the nature of the task.", so we know that the improvement over state-of-the-art is not statistically significant.
Answer: not statistically significant
True answer: Yes

Validation ID: b3307d5b68c57a074c483636affee41054be06d1
Question: What are examples of these artifacts?
From the context "In particular, we show that translating different parts of each instance separately (e.g. the premise and the hypothesis in NLI) can alter superficial patterns in the data (e.g. the degree of lexical overlap between them), which severely affects the generalization ability of current models", so we know that translating different parts of each instance separately can alter superficial patterns in the data.
Answer: translating different parts of each instance separately can alter superficial patterns in the data
True answer: hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap

Validation ID: bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31
Question: What languages do they use in their experiments?
From the context "We experiment with two models that are representative of the state-of-the-art in monolingual and cross-lingual pre-training: (i) Roberta BIBREF28, which is an improved version of BERT that uses masked language modeling to pre-train an English Transformer model, and (ii) XLM-R BIBREF8, which is a multilingual extension of the former pre-trained on 100 languages", so we know that they use English and 100 languages in their experiments.
Answer: English and 100 languages
True answer: English, Spanish, Finnish

Validation ID: 12d7055baf5bffb6e9e95e977c000ef2e77a4362
Question: How much higher quality is the resulting annotated data?
From the context "Models trained with expert-annotated data are clearly superior to those trained with crowd labels with respect to F1, indicating that the experts produced higher quality annotations", so we know that the resulting annotated data is higher quality.
Answer: higher quality
True answer: improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added

Validation ID: 498c0229f831c82a5eb494cdb3547452112a66a0
Question: How do they match annotators to instances?
From the context "We recruited five medical experts via Up-work with advanced medical training and strong technical reading/writing skills. The expert annotator were asked to read the entire abstract and highlight, using the BRAT toolkit BIBREF26, all spans describing medical Interventions. Each abstract is only annotated by one expert. We examined 30 re-annotated abstracts to ensure the annotation quality before hiring the annotator.", so we know that they recruited five medical experts via Up-work with advanced medical training and strong technical reading/writing skills. The expert annotator were asked to read the entire abstract and highlight, using the BRAT toolkit BIBREF26, all spans describing medical Interventions. Each abstract is only annotated by one expert. They examined 30 re-annotated abstracts to ensure the annotation quality before hiring the annotator.
Answer: They recruited five medical experts via Up-work with advanced medical training and strong technical reading/writing skills. The expert annotator were asked to read the entire abstract and highlight, using the BRAT toolkit BIBREF26, all spans describing medical Interventions. Each abstract is only annotated by one expert. They examined 30 re-annotated abstracts to ensure the annotation quality before hiring the annotator.
True answer: Annotations from experts are used if they have already been collected.

Validation ID: 8c48c726bb17a17d70ab29db4d65a93030dd5382
Question: How much data is needed to train the task-specific encoder?
From the context "We select only 2000 abstracts for re-annotation; 1000 of these are top-ranked, and 1000 are randomly sampled.", so we know that the data needed to train the task-specific encoder is 2000 abstracts.
Answer: 2000 abstracts
True answer: 57,505 sentences

Validation ID: 8c48c726bb17a17d70ab29db4d65a93030dd5382
Question: How much data is needed to train the task-specific encoder?
From the context "We select only 2000 abstracts for re-annotation; 1000 of these are top-ranked, and 1000 are randomly sampled", so we know that the data needed to train the task-specific encoder is 2000 abstracts.
Answer: 2000 abstracts
True answer: 57,505 sentences

Validation ID: 89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f
Question: What kind of out-of-domain data?
From the context "We also use the universal sentence encoder (USE) BIBREF6 to induce sentence representations, and train a model using these as features. Following BIBREF6, we then experiment with an ensemble model that combines the `universal' and task-specific representations to predict annotation difficulty. We expect these universal embeddings to capture general, high-level semantics, and the task specific representations to capture more granular information.", so we know that the universal sentence encoder is used to induce sentence representations.
Answer: sentence representations
True answer: Unanswerable

Validation ID: 89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f
Question: What kind of out-of-domain data?
From the context "We also use the universal sentence encoder (USE) BIBREF6 to induce sentence representations, and train a model using these as features. Following BIBREF6, we then experiment with an ensemble model that combines the `universal' and task-specific representations to predict annotation difficulty. We expect these universal embeddings to capture general, high-level semantics, and the task specific representations to capture more granular information.", so we know that the universal sentence encoder is used to induce sentence representations.
Answer: sentence representations
True answer: Unanswerable

Validation ID: 06b5272774ec43ee5facfa7111033386f06cf448
Question: Is an instance a sentence or an IE tuple?
From the context "We define a training instance as difficult if a lay annotator or an automated model disagree on its labeling", so we know that an instance is a sentence.
Answer: sentence
True answer: sentence

Validation ID: 08b57deb237f15061e4029b6718f1393fa26acce
Question: Who are the crowdworkers?
From the context "The crowdworkers were located in the US and hired on the BIBREF22 platform", so we know that the crowdworkers were located in the US and hired on the BIBREF22 platform.
Answer: located in the US and hired on the BIBREF22 platform
True answer: people in the US that use Amazon Mechanical Turk

Validation ID: 08b57deb237f15061e4029b6718f1393fa26acce
Question: Who are the crowdworkers?
From the context "The crowdworkers were located in the US and hired on the BIBREF22 platform", so we know that the crowdworkers were located in the US and hired on the BIBREF22 platform.
Answer: located in the US and hired on the BIBREF22 platform
True answer: located in the US, hired on the BIBREF22 platform

Validation ID: 9b7655d39c7a19a23eb8944568eb5618042b9026
Question: Which toolkits do they use?
From the context "NLP toolkits typically have the following capabilities: tokenization, part-of-speech (PoS) tagging, chunking, named entity recognition and sentiment analysis. In a study by BIBREF3, it is shown that the well-known NLP toolkits NLTK BIBREF4, Stanford CoreNLP BIBREF5, and TwitterNLP BIBREF6 have tokenization, PoS tagging and NER modules in their pipelines.", so we know that they use NLTK, Stanford CoreNLP, and TwitterNLP.
Answer: NLTK, Stanford CoreNLP, and TwitterNLP
True answer: BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21

Validation ID: 9b7655d39c7a19a23eb8944568eb5618042b9026
Question: Which toolkits do they use?
From the context "NLP toolkits typically have the following capabilities: tokenization, part-of-speech (PoS) tagging, chunking, named entity recognition and sentiment analysis. In a study by BIBREF3, it is shown that the well-known NLP toolkits NLTK BIBREF4, Stanford CoreNLP BIBREF5, and TwitterNLP BIBREF6 have tokenization, PoS tagging and NER modules in their pipelines.", so we know that they use NLTK, Stanford CoreNLP, and TwitterNLP.
Answer: NLTK, Stanford CoreNLP, and TwitterNLP
True answer: BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26

Validation ID: cd06d775f491b4a17c9d616a8729fd45aa2e79bf
Question: Which sentiment class is the most accurately predicted by ELS systems?
From the context "In the sentiment analysis experiments, we found that a tweet may contain multiple sentiments. The groundtruth labels contain 210 positive sentiments, 521 neutral sentiments, and 305 negative sentiments to the candidates. We measured the CCR, across all tweets, to be 31.7% for Rosette Text Analytics, 43.2% for Google Cloud, 44.2% for TensiStrength, and 74.7% for the crowdworkers. This means the difference between the performance of the tools and the crowdworkers is significant – more than 30 percent points.", so we know that the sentiment class that is the most accurately predicted by ELS systems is neutral.
Answer: neutral
True answer: neutral sentiment

Validation ID: 1329280df5ee9e902b2742bde4a97bc3e6573ff3
Question: Is datasets for sentiment analysis balanced?
From the context "Our results show that existing NLP systems cannot accurately perform sentiment analysis of political tweets in the dataset we experimented with. Labeling by humans, even non-expert crowdworkers, yields accuracy results that are well above the results of existing automated NLP systems. In future work we will therefore use a crowdworker-labeled dataset to train a new machine-learning based NLP system for tweet analysis. We will ensure that the training data is balanced among classes.", so we know that the datasets for sentiment analysis is not balanced.
Answer: No
True answer: No

Validation ID: 58c6737070ef559e9220a8d08adc481fdcd53a24
Question: What measures are used for evaluation?
From the context "We report the experimental results for our two tasks in terms of the correct classification rate (CCR). For sentiment analysis, we have a three-class problem (positive, negative, and neutral), where the classes are mutually exclusive. The CCR, averaged for a set of tweets, is defined to be the number of correctly-predicted sentiments over the number of groundtruth sentiments in these tweets. For NER, we consider that each tweet may reference up to four candidates, i.e., targeted entities. The CCR, averaged for a set of tweets, is the number of correctly predicted entities (candidates) over the number of groundtruth entities (candidates) in this set.", so we know that the measures used for evaluation are correct classification rate (CCR).
Answer: correct classification rate (CCR)
True answer: correct classification rate (CCR)

Validation ID: 0af16b164db20d8569df4ce688d5a62c861ace0b
Question: what were the baselines?
From the context "The baseline embedding methods are including bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), and neural-based word embedding. The classification models are including Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe BIBREF11, and our proposed model.", so we know that the baselines are BOW, TFIDF, neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN, GloVe and our proposed model.
Answer: BOW, TFIDF, neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN, GloVe and our proposed model
True answer: BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN

Validation ID: 0af16b164db20d8569df4ce688d5a62c861ace0b
Question: what were the baselines?
From the context "In order to prevent the unbalanced learning, we choose the weighted loss mechanism for both TextCNN and causal modeling TextCNN (C-TextCNN), these models suffer less than the traditional baselines and achieve a slightly balance performance", so we know that the baselines are traditional baselines.
Answer: traditional baselines
True answer: bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe

Validation ID: 78a4ec72d76f0a736a4a01369a42b092922203b6
Question: what datasets were used?
From the context "For pre-training FriendsBERT, we collect the completed scripts of all ten seasons of Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances. All the utterances are followed the preprocessing methods mentions above to compose the corpus for Masked language model pre-training task. The consequent utterances in the same scenes are considered as the consequent sentences to pre-train the Next Sentence Prediction task. In the pre-training process, the training loss is the sum of the mean likelihood of two pre-train tasks.", so we know that the datasets used are the completed scripts of all ten seasons of Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances.
Answer: the completed scripts of all ten seasons of Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances
True answer: Friends, EmotionPush

Validation ID: 78a4ec72d76f0a736a4a01369a42b092922203b6
Question: what datasets were used?
From the context "For pre-training FriendsBERT, we collect the completed scripts of all ten seasons of Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances. All the utterances are followed the preprocessing methods mentions above to compose the corpus for Masked language model pre-training task.", so we know that the datasets used are the completed scripts of all ten seasons of Friends TV shows from emorynlp.
Answer: the completed scripts of all ten seasons of Friends TV shows from emorynlp
True answer: EmotionLines BIBREF6

Validation ID: 6a14379fee26a39631aebd0e14511ce3756e42ad
Question: What BERT models are used?
From the context "The proposed approach is summarized in Figure FIGREF3, which aims to overcome these challenges. The framework could be separated into three steps and described as follow:", so we know that the proposed approach is summarized in Figure FIGREF3, which aims to overcome these challenges. The framework could be separated into three steps and described as follow:
From the context "For pre-training FriendsBERT, we collect the completed scripts of all ten seasons of Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances. All the utterances are followed the preprocessing methods mentions above to compose the corpus for Masked language model pre-training task. The consequent utterances in the same scenes are considered as the consequent sentences to pre-train the Next Sentence Prediction task.", so we know that the pre-training FriendsBERT, we collect the completed scripts of all ten seasons of Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances. All the utterances are followed the preprocessing methods mentions above to compose the corpus for Masked language model pre-training task. The consequent utterances in the same scenes are considered as the consequent sentences to pre-train the Next Sentence Prediction task.
True answer: BERT-base, BERT-large, BERT-uncased, BERT-cased

Validation ID: 81588e0e207303c2867c896f3911a54a1ef7c874
Question: What are the sources of the datasets?
From the context "The former comes from the scripts of the Friends TV sitcom. The other is made up of Facebook messenger chats", so we know that the sources of the datasets are the scripts of the Friends TV sitcom and Facebook messenger chats.
Answer: the scripts of the Friends TV sitcom and Facebook messenger chats
True answer: Friends TV sitcom, Facebook messenger chats

Validation ID: dd09db5eb321083dba16c2550676e60682f9a0cd
Question: What labels does the dataset have?
From the context "The objective of the challenge is to predict the emotion for each utterance. Just, according to EmotionX 2019 specification, there are only four emotions be selected as our label candidates, which are Joy, Sadness, Anger, and Neutral. These emotions will be considered during performance evaluation. The technical detail will also be introduced and discussed in following Section SECREF13 and Section SECREF26.", so we know that the dataset has four emotions, which are Joy, Sadness, Anger, and Neutral.
Answer: Joy, Sadness, Anger, and Neutral
True answer: Ekman’s six basic emotions,  neutral

Validation ID: 40c0f97c3547232d6aa039fcb330f142668dea4b
Question: Do they evaluate on English only datasets?
From the context "We apply the sefe on three datasets and compare it against the efe BIBREF10 baselines. All are fit using sgd BIBREF34. In particular, we compare the following methods:", so we know that they evaluate on three datasets.
Answer: three datasets
True answer: Unanswerable

Validation ID: 40c0f97c3547232d6aa039fcb330f142668dea4b
Question: Do they evaluate on English only datasets?
From the context "We apply the sefe on three datasets and compare it against the efe BIBREF10 baselines. All are fit using sgd BIBREF34. In particular, we compare the following methods:", so we know that they evaluate on three datasets.
Answer: three datasets
True answer: No

Validation ID: 777217e025132ddc173cf33747ee590628a8f62f
Question: What experiments are used to demonstrate the benefits of this approach?
From the context "We carry out a thorough experimental study on two text databases, ArXiv papers by section and U.S. Congressional speeches by home state and political party. Using Poisson embeddings, we study market basket data from a large grocery store, grouped by season. On all three data sets, sefe outperforms efe in terms of held-out log-likelihood. Qualitatively, we demonstrate how sefe discovers which words are used most differently across U.S. states and political parties, and show how word usage changes in different ArXiv disciplines.", so we know that the experiments are used to demonstrate the benefits of this approach are ArXiv papers by section, U.S. Congressional speeches by home state and political party, market basket data from a large grocery store, grouped by season.
Answer: ArXiv papers by section, U.S. Congressional speeches by home state and political party, market basket data from a large grocery store, grouped by season
True answer: On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:

Validation ID: 777217e025132ddc173cf33747ee590628a8f62f
Question: What experiments are used to demonstrate the benefits of this approach?
From the context "We carry out a thorough experimental study on two text databases, ArXiv papers by section and U.S. Congressional speeches by home state and political party. Using Poisson embeddings, we study market basket data from a large grocery store, grouped by season. On all three data sets, sefe outperforms efe in terms of held-out log-likelihood. Qualitatively, we demonstrate how sefe discovers which words are used most differently across U.S. states and political parties, and show how word usage changes in different ArXiv disciplines.", so we know that the experiments are used to demonstrate the benefits of this approach are ArXiv papers by section, U.S. Congressional speeches by home state and political party, market basket data from a large grocery store, grouped by season.
Answer: ArXiv papers by section, U.S. Congressional speeches by home state and political party, market basket data from a large grocery store, grouped by season
True answer: Calculate test log-likelihood on the three considered datasets

Validation ID: 2dbf6fe095cd879a9bf40f110b7b72c8bdde9475
Question: What hierarchical modelling approach is used?
From the context "We propose two methods to share statistical strength among the embedding vectors. The first approach is based on hierarchical modeling BIBREF13, which assumes that the group-specific embedding representations are tied through a global embedding. The second approach is based on amortization BIBREF14, BIBREF15, which considers that the individual embeddings are the output of a deterministic function of a global embedding representation. We use stochastic optimization to fit large data sets.", so we know that hierarchical modelling approach is used.
Answer: hierarchical modelling approach
True answer: the group-specific embedding representations are tied through a global embedding

Validation ID: 7d483077ed7f2f504d59f4fc2f162741fa5ac23b
Question: How do co-purchase patterns vary across seasons?
From the context "In supermarket shopping data, co-purchase patterns of items can vary across seasons of the year", so we know that co-purchase patterns of items can vary across seasons of the year.
Answer: co-purchase patterns of items can vary across seasons of the year
True answer: Unanswerable

Validation ID: de830c534c23f103288c198eb19174c76bfd38a1
Question: Which words are used differently across ArXiv?
From the context "We fit an sefe to ArXiv abstracts grouped into different sections, such as computer science (cs), quantitative finance (q-fin), and nonlinear sciences (nlin). sefe results in a per-section embedding of each term in the vocabulary. Using the fitted embeddings, we illustrate similar words to the word 1.10intelligence. We can see that how 1.10intelligence is used varies by field: in computer science the most similar words include 1.10artificial and 1.10ai; in finance, similar words include 1.10abilities and 1.10consciousness.", so we know that the words 1.10artificial and 1.10ai are used differently across ArXiv.
Answer: 1.10artificial and 1.10ai
True answer: intelligence

Validation ID: b0d66760829f111b8fad0bd81ca331ddd943ef41
!!! Has error during processing !!!
Question: What is future work planed?
Answer: From the context "In the future, we plan to explore the following directions: 1) We will try to incorporate more information from the article, such as the position of the word in the article, the length of the article, and the number of words in the article. 2) We will try to incorporate more information from the headline, such as the position of the word in the headline, the length of the headline, and the number of words in the headline. 3) We will try to incorporate more information from the article and the headline, such as the position of the word in the article and the position of the word in the headline. 4) We will try to incorporate more information from the article and the headline, such as the length of the article and the length of the headline. 5) We will try to incorporate more information from the article and the headline, such as the number of words in the article and the number of words in the headline. 6) We will try to incorporate more information from the article and the headline, such as the position of the word in the article and the position of the word in the headline. 7) We will try to incorporate more information from the article and the headline, such as the length of the article and the length of the headline. 8) We will try to incorporate more information from the article and the headline, such as the number of words in the article and the number of words in the headline. 9) We will try to incorporate more information from the article and the headline, such as the position of the word in the article and the position of the word in the headline. 10) We will try to incorporate more information from the article and the headline, such as the length of the article and the length of the headline. 11) We will try to incorporate more information from the article and the headline, such as the number of words in the article and the number of words in the headline. 12) We will try to incorporate more information from the article and the headline, such as the position of the word in the article and the position of the word in the headline. 13) We will try to incorporate more information from the article and the headline, such as the length of the article and the length of the headline. 14) We will try to incorporate more information from the article and the headline, such as the number of words in the article and the number of words in the headline. 15) We will try
True answer: ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE

Validation ID: ae7c93646aa5f3206cd759904965b4d484d12f83
Question: What is this method improvement over the best performing state-of-the-art?
From the context "We first compare all four models, Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN, to existing models with ROUGE in Table TABREF25 to establish that our model produces relevant headlines and we leave the sensationalism for human evaluation. Note that we only compare", so we know that the method improvement over the best performing state-of-the-art is that our model produces relevant headlines.
Answer: our model produces relevant headlines
True answer: absolute improvement of 18.2% over the Pointer-Gen baseline

Validation ID: d1ec42b2b5a3c956ff528543636e024bfde5e5ba
Question: Which baselines are used for evaluation?
From the context "We first compare all four models, Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN, to existing models with ROUGE in Table TABREF25 to establish that our model produces relevant headlines and we leave the sensationalism for human evaluation. Note that we only compare our models to commonly used strong summarization bas", so we know that the baselines are Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN.
Answer: Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN
True answer: Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN

Validation ID: 1dac4bc5af239024566fcb0f43bb9ff1c248ecec
Question: Did they used dataset from another domain for evaluation?
From the context "We first compare all four models, Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN, to existing models with ROUGE in Table TABREF25 to establish that our model produces relevant headlines and we leave the sensationalism for human evaluation. Note that we only compare our models to commonly used strong summar", so we know that they used dataset from another domain for evaluation.
Answer: Yes
True answer: No

Validation ID: 1dac4bc5af239024566fcb0f43bb9ff1c248ecec
Question: Did they used dataset from another domain for evaluation?
From the context "We first compare all four models, Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN, to existing models with ROUGE in Table TABREF25 to establish that our model produces relevant headlines and we leave the sensationalism for human evaluation. Note that we only compare our models to commonly used strong summar", so we know that they used dataset from another domain for evaluation.
Answer: Yes
True answer: No

Validation ID: 3bf0306e9bd044f723e38170c13455877b2aeec3
Question: How is sensationalism scorer trained?
From the context "To evaluate the sensationalism intensity score $\alpha _{\text{sen}}$ of a headline, we collect a sensationalism dataset and then train a sensationalism scorer. For the sensationalism dataset collection, we choose headlines with many comments from popular online websites as positive samples. For the negative samples, we propose to use the generated headlines from a sentence summarization model. Intuitively, the summarization model, which is trained to preserve the semantic meaning, will lose the sensationalization ability and thus the generated negative samples will be less sensational than the original one, similar to the obfuscation of style after back-translation BIBREF4. For example, an original headline like UTF8gbsn“一趟挣10万？铁总增开申通、顺丰专列" (One trip to earn 100 thousand? China Railway opens new Shentong and Shunfeng special lines) will become UTF8gbsn“中铁总将增开京广两列快递专列" (China Railway opens two special lines for express) from the baseline model, which loses the sensational phrases of UTF8gbsn“一趟挣10万？" (One trip to earn 100 thousand?). We then train the sensationalism scorer by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$.", so we know that the sensationalism scorer is trained by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$.
Answer: classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$
True answer: by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$

Validation ID: 3bf0306e9bd044f723e38170c13455877b2aeec3
Question: How is sensationalism scorer trained?
From the context "To evaluate the sensationalism intensity score $\alpha _{\text{sen}}$ of a headline, we collect a sensationalism dataset and then train a sensationalism scorer. For the sensationalism dataset collection, we choose headlines with many comments from popular online websites as positive samples. For the negative samples, we propose to use the generated headlines from a sentence summarization model. Intuitively, the summarization model, which is trained to preserve the semantic meaning, will lose the sensationalization ability and thus the generated negative samples will be less sensational than the original one, similar to the obfuscation of style after back-translation BIBREF4. For example, an original headline like UTF8gbsn“一趟挣10万？铁总增开申通、顺丰专列" (One trip to earn 100 thousand? China Railway opens new Shentong and Shunfeng special lines) will become UTF8gbsn“中铁总将增开京广两列快递专列" (China Railway opens two special lines for express) from the baseline model, which loses the sensational phrases of UTF8gbsn“一趟挣10万？" (One trip to earn 100 thousand?). We then train the sensationalism scorer by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$. Firstly, 1-D convolution is used to extract word features from the input embeddings of a headline. This is followed by a ReLU activation layer and a max-pooling layer along the time dimension. All features from different channels are concatenated together and projected to the sensationalism score by adding another fully connected layer with sigmoid activation. Binary cross entropy is used to compute the loss $L_{\text{sen}}$.", so we know that the sensationalism scorer is trained by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$.
Answer: classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text
True answer: classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss

Validation ID: 2858620e0498db2f2224bfbed5263432f0570832
Question: Which component is the least impactful?
From the context "However, on Subjectivity, standard MPAD outperforms all hierarchical variants. On TREC, it reaches the same accuracy. We hypothesize that in some cases, using a different graph to separately encode each sentence might be worse than using one single graph to directly encode the document. Indeed, in the single document graph, some words that never appear in the same sentence can be connected through common neighbors", so we know that the least impactful component is using a different graph to separately encode each sentence.
Answer: using a different graph to separately encode each sentence
True answer: Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.

Validation ID: 545e92833b0ad4ba32eac5997edecf97a366a244
Question: Which component has the greatest impact on performance?
From the context "We hypothesize that in some cases, using a different graph to separately encode each sentence might be worse than using one single graph to directly encode the document. Indeed, in the single document graph, some words that never appear in the same sentence can be connected through common neighbors. This is not possible in the sentence graphs, where each sentence is encoded separately. In addition, the sentence graphs are smaller than the document graph, which might lead to a loss of information.", so we know that the component that has the greatest impact on performance is the single document graph.
Answer: the single document graph
True answer: Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations

Validation ID: 545e92833b0ad4ba32eac5997edecf97a366a244
Question: Which component has the greatest impact on performance?
From the context "We hypothesize that in some cases, using a different graph to separately encode each sentence might be worse than using one single graph to directly encode the document. Indeed, in the single document graph, some words that never appear in the same sentence can be connected through common neighbors. This is not possible in the sentence graphs, where each sentence is encoded separately. In addition, the sentence graphs are smaller than the document graph, which might lead to a loss of information.", so we know that the component that has the greatest impact on performance is the single document graph.
Answer: the single document graph
True answer: Removing the master node deteriorates performance across all datasets

Validation ID: cb12c19f9d14bef7b2f778892d9071eea2d6c63d
Question: What is the state-of-the-art system?
From the context "We evaluate MPAD against multiple state-of-the-art baseline models, including hierarchical ones, to enable fair comparison with the hierarchical MPAD variants.", so we know that the state-of-the-art system is MPAD.
Answer: MPAD
True answer: doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT

Validation ID: 9193006f359c53eb937deff1248ee3317978e576
Question: Which datasets are used?
From the context "We evaluate the quality of the document embeddings learned by MPAD on 10 document classification datasets, covering the topic identification, coarse and fine sentiment analysis and opinion mining, and subjectivity detection tasks. We briefly introduce the datasets next. Their statistics are reported in Table TABREF21.", so we know that the datasets are Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2 and Yelp2013.
Answer: Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2 and Yelp2013
True answer: Reuters,  BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013

Validation ID: 9193006f359c53eb937deff1248ee3317978e576
Question: Which datasets are used?
From the context "We evaluate the quality of the document embeddings learned by MPAD on 10 document classification datasets, covering the topic identification, coarse and fine sentiment analysis and opinion mining, and subjectivity detection tasks. We briefly introduce the datasets next. Their statistics are reported in Table TABREF21.", so we know that the datasets are Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2 and Yelp2013.
Answer: Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2 and Yelp2013
True answer:  Reuters, BBCSport BIBREF30, Polarity BIBREF31, Subjectivity BIBREF32, MPQA BIBREF33, IMDB BIBREF34, TREC BIBREF35, SST-1 BIBREF36, SST-2 BIBREF36, Yelp2013 BIBREF26

Validation ID: bc67b91dd73acded2d52fd4fee732b7a9722ea8b
Question: What is the message passing framework?
From the context "The MP framework is based on the core idea of recursive neighborhood aggregation. That is, at every iteration, the representation of each vertex is updated based on messages received from its neighbors. All spectral GNNs can be described in terms of the MP framework.", so we know that the message passing framework is based on the core idea of recursive neighborhood aggregation.
Answer: recursive neighborhood aggregation
True answer: It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors.

Validation ID: 49c32a2a64eb41381e5f12ccea4150cac9f3303d
Question: What other evaluation metrics are looked at?
From the context "The accuracy of polarity annotation varies between 72%-91% for sarcastic texts and 75%-91% for non-sarcastic text, showing the inherent difficulty of sentiment annotation, when sarcasm is present in the text under consideration. Annotation errors may be attributed to: (a) lack of patience/attention while reading, (b) issues related to text comprehension, and (c) confusion/indecisiveness caused due to lack of context.", so we know that the accuracy of polarity annotation varies between 72%-91% for sarcastic texts and 75%-91% for non-sarcastic text.
Answer: accuracy of polarity annotation
True answer: F-score, Kappa

Validation ID: 49c32a2a64eb41381e5f12ccea4150cac9f3303d
Question: What other evaluation metrics are looked at?
From the context "The accuracy of polarity annotation varies between 72%-91% for sarcastic texts and 75%-91% for non-sarcastic text, showing the inherent difficulty of sentiment annotation, when sarcasm is present in the text under consideration. Annotation errors may be attributed to: (a) lack of patience/attention while reading, (b) issues related to text comprehension, and (c) confusion/indecisiveness caused due to lack of context.", so we know that the accuracy of polarity annotation varies between 72%-91% for sarcastic texts and 75%-91% for non-sarcastic text.
Answer: accuracy of polarity annotation
True answer: Unanswerable

Validation ID: bbb77f2d6685c9257763ca38afaaef29044b4018
Question: What is the best reported system?
From the context "We further analyze the importance of features by ranking the features based on (a) Chi squared test, and (b) Information Gain test, using Weka's attribute selection module. Figure FIGREF23 shows the top 20 ranked features produced by both the tests. For both the cases, Average Fixation Duration per Word and Largest Regression Position are seen to be the two most significant features.", so we know that the best reported system is Average Fixation Duration per Word and Largest Regression Position.
Answer: Average Fixation Duration per Word and Largest Regression Position
True answer: Gaze Sarcasm using Multi Instance Logistic Regression.

Validation ID: bbb77f2d6685c9257763ca38afaaef29044b4018
Question: What is the best reported system?
From the context "We further analyze the importance of features by ranking the features based on (a) Chi squared test, and (b) Information Gain test, using Weka's attribute selection module. Figure FIGREF23 shows the top 20 ranked features produced by both the tests. For both the cases, Average Fixation Duration per Word and Largest Regression Position are seen to be the two most significant features.", so we know that the best reported system is Average Fixation Duration per Word and Largest Regression Position.
Answer: Average Fixation Duration per Word and Largest Regression Position
True answer: the MILR classifier

Validation ID: 22732cb9476e521452bf0538f3fdb94cf3867651
Question: What kind of stylistic features are obtained?
From the context "Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic/stylistic features alone.", so we know that stylistic features are emoticons, laughter expressions such as “lol” etc.
Answer: emoticons, laughter expressions such as “lol” etc
True answer: Unanswerable

Validation ID: 4e748cb2b5e74d905d9b24b53be6cfdf326e8054
Question: What traditional linguistics features did they use?
From the context "Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features BIBREF0, BIBREF1, BIBREF2, BIBREF3 (b) Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5 and (c) Hastag interpretations BIBREF6, BIBREF7.", so we know that they use Unigrams and Pragmatic features, Stylistic patterns and patterns related to situational disparity, Hastag interpretations.
Answer: Unigrams and Pragmatic features, Stylistic patterns and patterns related to situational disparity, Hastag interpretations
True answer: Unanswerable

Validation ID: 74b338d5352fe1a6fd592e38269a4c81fe79b866
Question: What cognitive features are used?
From the context "We describe the features used for sarcasm detection in Table. The features enlisted under lexical,implicit incongruity and explicit incongruity are borrowed from various literature (predominantly from joshi2015harnessing). These features are essential to separate sarcasm from other forms semantic incongruity in text (for example ambiguity arising from semantic ambiguity or from metaphors). Two additional textual features viz. readability and word count of the text are also taken under consideration. These features are used to reduce the effect of text hardness and text length on the eye-movement patterns.", so we know that the cognitive features are lexical,implicit incongruity, explicit incongruity, readability and word count.
Answer: lexical,implicit incongruity, explicit incongruity, readability and word count
True answer: Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half
to first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze
graph (ED),  Fixation Duration at Left/Source
(F1H, F1S),  Fixation Duration at Right/Target
(F2H, F2S),  Forward Saccade Word Count of
Source (PSH, PSS),  Forward SaccadeWord Count of Destination
(PSDH, PSDS), Regressive Saccade Word Count of
Source (RSH, RSS),  Regressive Saccade Word Count of
Destination (RSDH, RSDS)

Validation ID: d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf
Question: What approaches do they use towards text analysis?
From the context "We first discuss modeling considerations. Next, we describe several frequently used computational approaches and their limitations and strengths.", so we know that they use several frequently used computational approaches and their limitations and strengths.
Answer: several frequently used computational approaches and their limitations and strengths
True answer: Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use

Validation ID: d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf
Question: What approaches do they use towards text analysis?
From the context "We first discuss modeling considerations. Next, we describe several frequently used computational approaches and their limitations and strengths.", so we know that they use several frequently used computational approaches and their limitations and strengths.
Answer: several frequently used computational approaches and their limitations and strengths
True answer: Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis.

Validation ID: f903396d943541a8cc65edefb04ca37814ed30dd
Question: What dataset do they use for analysis?
From the context "We typically start by identifying the questions we wish to explore. Can text analysis provide a new perspective on a “big question” that has been attracting interest for years? Or can we raise new questions that have only recently emerged, for example about social media? For social scientists working in computational analysis, the questions are often grounded in theory, asking: How can we explain what we observe? These questions are also influenced by the availability and accessibility of data sources. For example, the choice to work with data from a particular social media platform may be partly determined by the fact that it is freely available, and this will in turn shape the kinds of questions that can be asked. A key output of this phase are the concepts to measure, for example: influence; copying and reproduction; the creation of patterns of language use; hate speech. Computational analysis of text motivated by these questions is insight driven: we aim to describe a phenomenon or explain how it came about. For example, what can we learn about how and why hate speech is used or how this changes over time? Is hate speech one thing, or does it comprise multiple forms of expression? Is there a clear boundary between hate speech and other types of speech, and what features make it more or less ambiguous?", so we know that they use data from a particular social media platform.
Answer: data from a particular social media platform
True answer: Unanswerable

Validation ID: ba28ce9a2f7e8524243adf288cc3f11055e667bb
Question: Do they demonstrate why interdisciplinary insights are important?
From the context "We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original research questions, which may continue to evolve for much of the project. At each stage, our discussion is critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception.", so we know that interdisciplinary insights are important.
Answer: Yes
True answer: No

Validation ID: ba28ce9a2f7e8524243adf288cc3f11055e667bb
Question: Do they demonstrate why interdisciplinary insights are important?
From the context "We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original research questions, which may continue to evolve for much of the project. At each stage, our discussion is critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception.", so we know that interdisciplinary insights are important.
Answer: Yes
True answer: No

Validation ID: 975e60535724f4149c7488699a199ba2920a062c
Question: What background do they have?
From the context "We are a group of researchers from the University of Washington, the University of Amsterdam, and the University of California, Berkeley, with backgrounds in computer science, linguistics, and the humanities", so we know that they have backgrounds in computer science, linguistics, and the humanities.
Answer: computer science, linguistics, and the humanities
True answer: Unanswerable

Validation ID: b970f48d30775d3468952795bc72976baab3438e
Question: What kind of issues (that are not on the forefront of computational text analysis) do they tackle?
From the context "In this article, we take the reader through a typical research process that involves measuring social or cultural concepts using computational methods, discussing both the opportunities and complications that often arise", so we know that they tackle the issues of opportunities and complications that often arise.
Answer: opportunities and complications that often arise
True answer: identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines

