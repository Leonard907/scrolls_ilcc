{
    "b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54": "MNMT",
    "f5e6f43454332e0521a778db0b769481e23e7682": "pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16",
    "9a05a5f4351db75da371f7ac12eb0b03607c4b87": "Europarl and MultiUN",
    "5eda469a8a77f028d0c5f1acd296111085614537": "source to pivot and pivot to target",
    "18c5d366b1da8447b5404eab71f4cc658ba12e6f": "Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15",
    "b5e4866f0685299f1d7af267bbcc4afe2aab806f": "ilur.am",
    "1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590": "yes",
    "b6ae8e10c6a0d34c834f18f66ab730b670fb528c": "politics, technology, science, business, AskReddit, worldnews, IAmA, atheism, Bitcoin, and aww",
    "a87a009c242d57c51fc94fe312af5e02070f898b": "a statistical model of dogmatism",
    "ef4dba073d24042f24886580ae77add5326f2130": "",
    "2df4a045a9cd7b44874340b6fdf9308d3c55327a": "Amazon Mechanical Turk",
    "a313e98994fc039a82aa2447c411dda92c65a470": "CFILT-preorder system",
    "37861be6aecd9242c4fdccdfcd06e48f3f1f8f81": "Indian languages",
    "7e62a53823aba08bc26b2812db016f5ce6159565": "IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus",
    "9eabb54c2408dac24f00f92cf1061258c7ea2e1a": "text structure, typography, and images",
    "3d013f15796ae7fed5272183a166c45f16e24e39": "Information on text structure, typography, and images",
    "9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc": "both single-relation and multi-relation KBQA tasks",
    "d3aa0449708cc861a51551b128d73e11d62207d2": "HR-BiLSTM model",
    "cfbec1ef032ac968560a7c76dec70faf1269b27c": "Knowledge Base Question Answering",
    "c0e341c4d2253eb42c8840381b082aae274eddad": "relation detection",
    "1ec152119cf756b16191b236c85522afeed11f59": "upper layers produce context-specific embeddings",
    "891c2001d6baaaf0da4e65b647402acac621a7d2": "taking the first principal component of a word's contextualized representations",
    "66c96c297c2cffdf5013bab5e95b59101cb38655": "recall of 0.988",
    "6b53e1f46ae4ba9b75117fc6e593abded89366be": "CRF classifier, spaCy entity recogniser and NLNDE",
    "c0bee6539eb6956a7347daa9d2419b367bd02064": "Yes",
    "3de0487276bb5961586acc6e9f82934ef8cb668c": "NUBes-PHI and MEDDOCAN",
    "113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab": "phoneme embeddings",
    "0752d71a0a1f73b3482a888313622ce9e9870d6e": "wFST",
    "55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3": "PER, WER and WER 100",
    "4eaf9787f51cd7cdc45eb85cf223d752328c6ee4": "the multilingual pronunciation corpus collected by deri2016grapheme",
    "fb2b536dc8e442dffab408db992b971e86548158": "49% for CS and 41% for Stat2015",
    "31735ec3d83c40b79d11df5c34154849aeb3fb47": "recruited from our institution",
    "10d450960907091f13e0be55f40bcb96f44dd074": "Yes",
    "b5608076d91450b0d295ad14c3e3a90d7e168d0e": "Yes",
    "c21b87c97d1afac85ece2450ee76d01c946de668": "neural seq2seq models",
    "d087539e6a38c42f0a521ff2173ef42c0733878e": "the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.",
    "efe9bad55107a6be7704ed97ecce948a8ca7b1d2": "",
    "71e4ba4e87e6596aeca187127c0d088df6570c57": "two methods",
    "7561a968470a8936d10e1ba722d2f38b5a9a4d38": "over 30,000 images with 5 crowdsourced descriptions each",
    "6d4400f45bd97b812e946b8a682b018826e841f1": "to suppress or to emphasize biases in human language",
    "26c2e1eb12143d985e4fb50543cf0d1eb4395e67": "linguistic bias and unwarranted inferences",
    "f17ca24b135f9fe6bb25dc5084b13e1637ec7744": "implicit relations",
    "bd5bd1765362c2d972a762ca12675108754aa437": "significantly",
    "d9b6c61fc6d29ad399d27b931b6cb7b1117b314a": "generate a question $\\hat{q}$ based on the semantics of a candidate answer",
    "d27438b11bc70e706431dda0af2b1c0b0d209f96": "the BERT models are trained on different (and larger) data, are allowed to access the suffix of the sentence in addition to its prefix, and are evaluated on somewhat different data due to discarding OOV items",
    "8d4ac4afbf5b14f412171729ceb5e822afcfa3f4": "Yes",
    "3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f": "gender and location information",
    "07d15501a599bae7eb4a9ead63e9df3d55b3dc35": "Meaning Extraction Method (MEM) BIBREF10",
    "99e78c390932594bd833be0f5c890af5c605d808": "the first approach",
    "861187338c5ad445b9acddba8f2c7688785667b1": "Yes",
    "f161e6d5aecf8fae3a26374dcb3e4e1b40530c95": "ELMo, BERT and ClinicalBERT embeddings",
    "12c50dea84f9a8845795fa8b8c1679328bd66246": "CSAT, 20newsgroups and Fisher",
    "0810b43404686ddfe4ca84783477ae300fdd2ea4": "",
    "455d4ef8611f62b1361be4f6387b222858bb5e56": "1900 dialogs and 8533 turns. Topics discussed in dialogs are questions randomly chosen from training examples of Simple questions BIBREF7 dataset. From this dataset we also took the correct answers in form of Freebase entities.",
    "bc16ce6e9c61ae13d46970ebe6c4728a47f8f425": "4.48",
    "1ff0fccf0dca95a6630380c84b0422bed854269a": "the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ is measured as the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.",
    "3d7d865e905295d11f1e85af5fa89b210e3e9fdf": "100",
    "2ad4d3d222f5237ed97923640bc8e199409cbe52": "completion times and accuracies",
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "Unif and Stopword",
    "ee417fea65f9b1029455797671da0840c8c1abbe": "handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API",
    "ca5a82b54cb707c9b947aa8445aac51ea218b23a": "accumulates labeled dialogues in the background and stores them in a structured form",
    "da55bd769721b878dd17f07f124a37a0a165db02": "complex mathematical questions",
    "feb448860918ef5b905bb25d7b855ba389117c1f": "news All India Radio news channel",
    "4bc2784be43d599000cb71d31928908250d4cef3": "GhostVLAD is an extension of the NetVLAD approach",
    "75df70ce7aa714ec4c6456d0c51f82a16227f2cb": "7 Indian languages",
    "6424e442b34a576f904d9649d63acf1e4fdefdfc": "",
    "5eabfc6cc8aa8a99e6e42514ef9584569cb75dec": "English datasets",
    "887c6727e9f25ade61b4853a869fe712fe0b703d": "the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied",
    "6236762b5631d9e395f81e1ebccc4bf3ab9b24ac": "they show on which examples how conflict works better than attention",
    "31d695ba855d821d3e5cdb7bea638c7dbb7c87c7": "two stacked GRU layers",
    "b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab": "Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask",
    "a99fdd34422f4231442c220c97eafc26c76508dd": "Yes",
    "2c78993524ca62bf1f525b60f2220a374d0e3535": "articles annotated with monolingual and crosslingual cluster labels",
    "d604f5fb114169f75f9a38fab18c1e866c5ac28b": "F INLINEFORM0 score",
    "1d3e914d0890fc09311a70de0b20974bf7f0c9fe": "eight NER tasks",
    "16535db1d73a9373ffe9d6eedaa2369cefd91ac4": "https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json",
    "de0b650022ad8693465242ded169313419eed7d9": "No",
    "2b3cac7af10d358d4081083962d03ea2798cf622": "Yes",
    "897ba53ef44f658c128125edd26abf605060fb13": "Yes",
    "41ac23e32bf208b69414f4b687c4f324c6132464": "English and German",
    "e97186c51d4af490dba6faaf833d269c8256426c": "Yes",
    "5bb3c27606c59d73fd6944ba7382096de4fa58d8": "multiple choice question answering",
    "8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b": "2 hops",
    "85590bb26fed01a802241bc537d85ba5ef1c6dc2": "using definitions from entries without example sentences as distractors",
    "75ff6e425ce304a35f18c0230c0d13d3913a31a9": "Yes",
    "5cb610d3d5d7d447b4cd5736d6a7d8262140af58": "randomly alternating between languages for every new minibatch",
    "c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a": "Track 1 and Track 2",
    "b9d168da5321a7d7b812c52bb102a05210fe45bd": "Yes",
    "0c234db3b380c27c4c70579a5d6948e1e3b24ff1": "LSTM encoder and an attention mechanism",
    "fa527becb8e2551f4fd2ae840dbd4a68971349e0": "LSTM",
    "32a3c248b928d4066ce00bbb0053534ee62596e7": "to predict the MSD tag of the target form",
    "c9b8d3858c112859eabee54248b874331c48f71b": "universal morphological reinflection",
    "45e9533586199bde19313cd43b3d0ecadcaf7a33": "Yes",
    "d3dbb5c22ef204d85707d2d24284cc77fa816b6c": "spaCy tool to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. The word embeddings are initialized with pre-trained 300-dimensional GloVe BIBREF10. A 2-layer BiLSTM is used encoding the contextual information of both questions and passages.",
    "a5e49cdb91d9fd0ca625cc1ede236d3d4672403c": "a multi-turn answer module",
    "aefa333b2cf0a4000cd40566149816f5b36135e7": "",
    "c5abe97625b9e1c8de8208e15d59c704a597b88c": "From the context \"Figure FIGREF10 shows that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. The knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass the bottleneck. A2C-chained\u2014which explores without a knowledge graph\u2014fails to even outperform the baseline A2C. We hypothesize that this is due to the knowledge graph aiding implicitly in the sample efficiency of bottleneck detection and subsequent exploration. That is, exploring after backtracking from a potentially detected bottleneck is much more efficient in the knowledge graph based agent. Comparing the advanced exploration methods when using the knowledge graph, we see that both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. KG-A2C-chained is significantly more sample efficient and converges faster. We can infer that chaining policies by explicitly detecting bottlenecks lets us pass it more quickly than attempting to find promising cell representations with Go-Explore. This form of chained exploration with backtracking is particularly suited to sequential decision making problems that can be represented as acyclic directed graphs as in Figure FIGREF1.\", so we know that the results from these proposed strategies are that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. The knowledge graph appears to be critical; it is theorized to help with partial observability. However the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass the bottleneck. A2C-chained\u2014which explores without a knowledge graph\u2014fails to even outperform the baseline A2C. We hypothesize that this is due to the knowledge graph aiding implicitly in the sample efficiency of bottleneck detection and subsequent exploration. That",
    "eb2d5edcdfe18bd708348283f92a32294bb193a5": "KG-A2C, A2C, A2C-chained and A2C-Explore",
    "88ab7811662157680144ed3fdd00939e36552672": "1) a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state; 2) leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9",
    "cb196725edc9cdb2c54b72364f3bbf7c76471490": "Yes",
    "286078813136943dfafb5155ee15d2429e7601d9": "highly effective",
    "8f16dc7d7be0d284069841e456ebb2c69575b32b": "",
    "a7d020120a45c39bee624f65443e09b895c10533": "",
    "585626d18a20d304ae7df228c2128da542d248ff": "predictive performance and strategy formulation ability",
    "bfc2dc913e7b78f3bd45e5449d71383d0aa4a890": "Knowledge Graph, Relation-Entity Matrix,",
    "6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de": "3",
    "b46c0015a122ee5fb95c2a45691cb97f80de1bb6": "a neural-based feature encoder that maps documents from both domains to a shared feature space, and a fully connected layer with softmax activation serving as the sentiment classifier",
    "5b7a4994bfdbf8882f391adf1cd2218dbc2255a0": "strong baselines",
    "9176d2ba1c638cdec334971c4c7f1bb959495a8e": "INLINEFORM2 and INLINEFORM6",
    "0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c": "coarse-grained",
    "5e324846a99a5573cd2e843d1657e87f4eb22fa6": "heuristics",
    "2ccc26e11df4eb26fcccdd1f446dc749aff5d572": "Yes",
    "f318a2851d7061f05a5b32b94251f943480fbd15": "From the context \"The results suggest that certain topics play a particularly important roles in ISIS propaganda targeting women. These relate to the role of women in early Islam, Islamic ideology, marriage/divorce, motherhood, spousal relationships, and hijrah (moving to a new land). Comparing these topics with those that appeared on a Catholic women forum, it seems that the two groups are not that different in their approach to women. The main difference is that ISIS propaganda is more focused on the role of women in early Islam and Islamic ideology. This is not surprising given that ISIS is a radical group that seeks to establish a caliphate and return to the early days of Islam. The Catholic forum, on the other hand, is more focused on marriage/divorce and motherhood. This is not surprising given that the Catholic forum is a non-violent group that is not seeking to establish a new state. The two groups are also similar in their focus on spousal relationships. This is not surprising given that both groups are religious and have similar views on marriage. The only topic that is unique to ISIS is hijrah. This is not surprising given that ISIS is a radical group that seeks to establish a caliphate and return to the early days of Islam. The Catholic forum, on the other hand, is more focused on marriage/divorce and motherhood. This is not surprising given that the Catholic forum is a non-violent group that is not seeking to establish a new state. The two groups are also similar in their focus on spousal relationships. This is not surprising given that both groups are religious and have similar views on marriage. The only topic that is unique to ISIS is hijrah. This is not surprising given that ISIS is a radical group that seeks to establish a caliphate and return to the early days of Islam. The Catholic forum, on the other hand, is more focused on marriage/divorce and motherhood. This is not surprising given that the Catholic forum is a non-violent group that is not seeking to establish a new state. The two groups are also similar in their focus on spousal relationships. This is not surprising given that both groups are religious and have similar views on marriage. The only topic that is unique to ISIS is hijrah. This is not surprising given that ISIS is a radical group that seeks to establish a caliphate and return to the early days of Islam. The Catholic forum, on the other hand, is more focused on marriage/",
    "6bbbb9933aab97ce2342200447c6322527427061": "words with their part of speech (POS) tags",
    "2007bfb8f66e88a235c3a8d8c0a3b3dd88734706": "word frequency and topic modeling with NMF",
    "d859cc37799a508bbbe4270ed291ca6394afce2c": "word frequency and topic modeling with NMF",
    "50e80cfa84200717921840fddcf3b051a9216ad8": "Yes",
    "b1bc9ae9d40e7065343c12f860a461c7c730a612": "ShapeWorldICE datasets",
    "63a1cbe66fd58ff0ead895a8bac1198c38c008aa": "Show&Tell model and LRCN1u model",
    "509af1f11bd6f3db59284258e18fdfebe86cae47": "caption variability in the dataset itself",
    "23e16c1173b7def2c5cb56053b57047c9971e3bb": "LSTM",
    "d78f7f84a76a07b777d4092cb58161528ca3803c": "a backward greedy search over each sentence's label sequence to identify word boundaries",
    "9da1e124d28b488b0d94998d32aa2fa8a5ebec51": "BIBREF15, BIBREF19, BIBREF20",
    "37be0d479480211291e068d0d3823ad0c13321d3": "degraded by translation",
    "a3d9b101765048f4b61cbd3eaa2439582ebb5c77": "English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese",
    "009ce6f2bea67e7df911b3f93443b23467c9f4a1": "BIBREF12",
    "55569d0a4586d20c01268a80a7e31a17a18198e2": "handle code-switching data",
    "7cd22ca9e107d2b13a7cc94252aaa9007976b338": "Yes",
    "adbf33c6144b2f5c40d0c6a328a92687a476f371": "Yes",
    "f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24": "PER, LOC, ORG, MISC",
    "a0543b4afda15ea47c1e623c7f00d4aaca045be0": "Yes",
    "1591068b747c94f45b948e12edafe74b5e721047": "10K user-generated image (snap) and textual caption pairs",
    "193ee49ae0f8827a6e67388a10da59e137e7769f": "a task that learns to recover a document with a masked span of tokens",
    "ed2eb4e54b641b7670ab5a7060c7b16c628699ab": "Masked Document Generation",
    "beac555c4aea76c88f19db7cc901fa638765c250": "other relevant information",
    "91e326fde8b0a538bc34d419541b5990d8aae14b": "",
    "044f922604b4b3f42ae381419fd5cd5624fa0637": "1) attention mechanism; 2) the type of the word being generated; 3) the POS tag of the target word",
    "f94b53db307685d572aefad52cd55f53d23769c2": "by selecting a subset of the training data",
    "aa7d327ef98f9f9847b447d4def04889b4508d7a": "190 hours ( INLINEFORM1 100K instances) of transcribed speech data",
    "b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0": "confidence scores",
    "551457ed34ca7fc0878c85bc664b135c21059b58": "a base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances. The trained model is then tested on a 156-hour ( INLINEFORM3 100K instances) test set and we report CTC loss, Character Error Rate (CER) and Word Error Rate (WER).",
    "0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8": "several baselines",
    "4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94": "the same models as those they used for causality",
    "a4d115220438c0ded06a91ad62337061389a6747": "Facebook status update messages",
    "2c7e94a65f5f532aa31d3e538dcab0468a43b264": "crowdsourcing task",
    "149da739b1c19a157880d9d4827f0b692006aa2c": "SVM, MLP, FastText, CNN, BERT, Platforms",
    "27de1d499348e17fec324d0ef00361a490659988": "23,700 queries",
    "cfcdd73e712caf552ba44d0aa264d8dace65a589": "crowdsourcing",
    "23b2901264bda91045258b5d4120879ae292e950": "+0.97",
    "b5bc34e1e381dbf972d0b594fe8c66ff75305d71": "+0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets",
    "72f7ef55e150e16dcf97fe443aff9971a32414ef": "+0.97 and +2.36",
    "20e38438471266ce021817c6364f6a46d01564f2": "associate each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds",
    "28067da818e3f61f8b5152c0d42a531bf0f987d4": "INLINEFORM1",
    "bf3b27a4f4be1f9ae31319877fd0c75c03126fd5": "around 500 different workers",
    "ffa7f91d6406da11ddf415ef094aaf28f3c3872d": "more",
    "b634ff1607ce5756655e61b9a6f18bc736f84c83": "the one that has the highest concentration of news released before the market opens",
    "2f901dab6b757e12763b23ae8b37ae2e517a2271": "German and English",
    "b591853e938984e6069d738371500ebdec50d256": "IMDb movie review dataset",
    "a130306c6662ff489df13fb3f8faa7cba8c52a21": "f-pooling, fo-pooling, and ifo-pooling",
    "b1cf5739467ba90059add58d11b73d075a11ec86": "WikiQA dataset",
    "2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd": "Embedding Layer, Neural Network Layers, Loss Function, Metrics",
    "4f253dfced6a749bf57a1b4984dc962ce9550184": "",
    "dc1cec824507fc85ac1ba87882fe1e422ff6cffb": "3500 questions from the Internet and other sources such as books of general knowledge questions, history etc",
    "f428618ca9c017e0c9c2a23515dab30a7660f65f": "different types of classifiers",
    "8ce11515634236165cdb06ba80b9a36a8b9099a2": "Yes",
    "6024039bbd1118c5dab86c41cce1175d99f10a25": "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1",
    "de5b6c25e35b3a6c5e40e350fc5e52c160b33490": "outperforms existing models",
    "b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f": "global context is the whole document and local context is the section/topic",
    "6bfba3ddca5101ed15256fca75fcdc95a53cece7": "From the context \"Propaganda Techniques ::",
    "df5a4505edccc0ee11349ed6e7958cf6b84c9ed4": "A Data Pro",
    "fd753ab5177d7bd27db0e0afc12411876ee607df": "a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature",
    "88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42": "a similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above",
    "4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3": "four medical residents",
    "8b3d3953454c88bde88181897a7a2c0c8dd87e23": "word embeddings",
    "784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f": "Yes",
    "7705dd04acedaefee30d8b2c9978537afb2040dc": "word level and character level model baselines",
    "44497509fdf5e87cff05cdcbe254fbd288d857ad": "",
    "0ee73909ac638903da4a0e5565c8571fc794ab96": "a group of 50 native people who were well-versed in both English and Tamil languages",
    "1f07e837574519f2b696f3d6fa3230af0b931e5d": "one language pair",
    "5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76": "existing systems",
    "729694a9fe1e05d329b7a4078a596fe606bc5a95": "88%, 53%",
    "1c997c268c68149ae6fb43d83ffcd53f0e7fe57e": "use a softmax layer as a tag decoder to predict the entity types",
    "5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde": "Transformer architecture BIBREF14 with the toolkit of BIBREF15",
    "f9bf6bef946012dd42835bf0c547c0de9c1d229f": "annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses",
    "6a633811019e9323dc8549ad540550d27aa6d972": "Yes",
    "6b9b9e5d154cb963f6d921093539490daa5ebbae": "",
    "bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2": "semantic and syntactic tasks",
    "d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7": "not clear what is lost",
    "6844683935d0d8f588fa06530f5068bf3e1ed0c0": "$\\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus",
    "8acab64ba72831633e8cc174d5469afecccf3ae9": "telephone calls",
    "53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa": "low coverage of audio and difficulty in cross-speaker clustering",
    "72755c2d79210857cfff60bfbcb55f83c71ada51": "11 hours",
    "7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569": "LAN and P600, ELAN and P600",
    "bd6dc38a9ac8d329114172194b0820766458dacc": "BIBREF0",
    "3ddff6b707767c3dd54d7104fe88b628765cae58": "Universal Dependencies v1.2 treebanks",
    "0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7": "the languages in UD1.2",
    "06be47e2f50b902b05ebf1ff1c66051925f5c247": "Yes",
    "003d6f9722ddc2ee13e879fefafc315fb8e87cb9": "a country",
    "c88a846197b72d25e04ec55f00ee3e72f655504c": "UN General Debate Corpus",
    "4d28c99750095763c81bcd5544491a0ba51d9070": "top-scoring and bottom-scoring tweets written by the user",
    "78292bc57ee68fdb93ed45430d80acca25a9e916": "manually inserting a negation element in each template or statement",
    "443d2448136364235389039cbead07e80922ec5c": "summarization algorithms provided by the Sumy package",
    "aa6d956c2860f58fc9baea74c353c9d985b05605": "ROUGE BIBREF22 unigram score",
    "4c18081ae3b676cc7831403d11bc070c10120f8e": "Sumy package",
    "fb3d30d59ed49e87f63d3735b876d45c4c6b8939": "INLINEFORM0",
    "197b276d0610ebfacd57ab46b0b29f3033c96a40": "SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, AdaBoost, Gradient Boosting, Pattern-based approach",
    "e025061e199b121f2ac8f3d9637d9bf987d65cd5": "15.5",
    "61652a3da85196564401d616d251084a25ab4596": "4528 employees",
    "14b74ad5a6f5b0506511c9b454e9c464371ef8c4": "De-En, Ja-En and Ro-En",
    "5c88d601e8fca96bffebfa9ef22331ecf31c6d75": "Yes",
    "71bd5db79635d48a0730163a9f2e8ef19a86cd66": "the exploitation of verb symmetry, the use of dative and genitive cases or ambiguous prepositions and coordination scope",
    "9ecde59ffab3c57ec54591c3c7826a9188b2b270": "SQuAD, NewsQA, MultiRC, HotpotQA, ReCoRd and MsMarco",
    "005cca3c8ab6c3a166e315547a2259020f318ffb": "Figure FIGREF23",
    "af34051bf3e628c1e2a00b110bb84e5f018b419f": "",
    "022c365a14fdec406c7a945a1a18e7e79df37f08": "MT",
    "5260cb56b7d127772425583c5c28958c37cb9bea": "the dialog context state for language modeling",
    "9b97805a0c093df405391a85e4d3ab447671c86a": "Exact Match (EM) and Macro-averaged F1 scores (F1)",
    "38f58f13c7f23442d5952c8caf126073a477bac0": "2% EM score and over 1.5% F1 score",
    "7ee5c45b127fb284a4a9e72bb9b980a602f7445a": "the model that achieves superior performance",
    "ddf5e1f600b9ce2e8f63213982ef4209bab01fd8": "Spoken-SQuAD",
    "27275fe9f6a9004639f9ac33c3a5767fea388a98": "dimension size, training epochs, window size and vocabulary size",
    "ef3567ce7301b28e34377e7b62c4ec9b496f00bf": "GMB",
    "7595260c5747aede0b32b7414e13899869209506": "IMDb dataset",
    "c2d1387e08cf25cb6b1f482178cca58030e85b70": "Yes",
    "5a22293b055f5775081d6acdc0450f7bd5f5de04": "Wiseman's work",
    "03c967763e51ef2537793db7902e2c9c17e43e95": "Seq2Seq model with attention BIBREF5 and conditional copy BIBREF3",
    "26327ccebc620a73ba37a95aabe968864e3392b2": "self-coverage and opponent-coverage",
    "ababb79dd3c301f4541beafa181f6a6726839a10": "Intelligence Squared Debates",
    "c2b8ee872b99f698b3d2082d57f9408a91e1b4c1": "the configuration of letter-trigram word vectors and bidirectional LSTM",
    "8eefa116e3c3d3db751423cc4095d1c4153d3a5f": "CoNLL2003 and GENIA data sets",
    "133eb4aa4394758be5f41744c60c99901b2bc01c": "The nature of the content can vary as its provided by different people",
    "3fff37b9f68697d080dbd9d9008a63907137644e": "90.58%",
    "a778b8204a415b295f73b93623d09599f242f202": "a method to map data vectors to a space where linear separation is possible",
    "642e8cf1d39faa1cd985d16750cdc6696c52db2f": "attentional encoder-decoder networks",
    "493e971ee3f57a821ef1f67ef3cd47ade154e7c4": "Bernoulli embeddings (b-emb), continuous bag-of-words (CBOW), Distributed Memory version of Paragraph Vector (PV-DM) and the Global Vectors (GloVe)",
    "8dd8e5599fc56562f2acbc16dd8544689cddd938": "they can capture the semantic similarity between equations and the words in the collection",
    "abe2393415e533cb06311e74ed1c5674cff8571f": "minimum edit evaluation and word error rate",
    "00c57e45ac6afbdfa67350a57e81b4fad0ed2885": "Twitter tweets",
    "22714f6cad2d5c54c28823e7285dc85e8d6bc109": "Reduction, Selection and Rank",
    "82642d3111287abf736b781043d49536fe48c350": "",
    "5a81732d52f64e81f1f83e8fd3514251227efbc7": "",
    "9a8b9ea3176d30da2453cac6e9347737c729a538": "$0.995$ on synthesized queries and $0.948$ on clinical notes for the hybrid NER model, and $0.441$ on synthesized queries and $0.927$ on clinical notes for the i2b2 NER model",
    "4477bb513d56e57732fba126944073d414d1f75f": "They used the BiLSTM-CRF implementation provided by the flair package BIBREF16. They set the hidden size value to be 256 in the LSTM structure and left everything else at default values for the SequenceTagger model on flair. For word embeddings, they used the ELMo embeddings fine-tuned on PubMed articles and flair embeddings BIBREF13 trained on $5\\%$ of PubMed abstracts, respectively. They trained models for 10 epochs and experimented with different learning rate, mini batch size, and dropouts. They ran hyperparameter optimization tests to find the best combination. $S_c$ is set to be 0.6 in their experiment.",
    "1b23c4535a6c10eb70bbc95313c465e4a547db5e": "convolutional layers, NIN layers, Bi-LSTM and MLP for the encoder and unidirectional LSTM with global attention for the decoder",
    "0a75a52450ed866df3a304077769e1725a995bb7": "the decoder task predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4",
    "fd0a3e9c210163a55d3ed791e95ae3875184b8f8": "WSJ dataset",
    "c37f65c9f0d543a35c784263b79236ccf1c44fac": "LSTM",
    "584af673429c7f8621c6bf83362a37048daa0e5d": "encoder-decoder architecture",
    "1be54c5b3ea67d837ffba2290a40c1e720d9587f": "Yes",
    "b08f88d1facefceb87e134ba2c1fa90035018e83": "Yes",
    "b06512c17d99f9339ffdab12cedbc63501ff527e": "",
    "fd8e23947095fe2230ffe1a478945829b09c8c95": "generating 20 chains for each node in the knowledge graph, with the length of each chain being 21 (10 relations and 11 entities appear alternately)",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "No",
    "3611a72f754de1e256fbd25b012197e1c24e8470": "Yes",
    "4c07c33dfaf4f3e6db55e377da6fa69825d0ba15": "300",
    "b1ce129678e37070e69f01332f1a8587e18e06b0": "twitter data",
    "7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1": "RoBERTa",
    "0689904db9b00a814e3109fb1698086370a28fa2": "Doc2Vec and PV-DBOW",
    "cc354c952b5aaed2d4d1e932175e008ff2d801dd": "female noun phrases and white noun phrases",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "sentences involving at least one race- or gender-associated word, short and grammatically simple, and some sentences to include expressions of sentiment and emotion",
    "2ddb51b03163d309434ee403fef42d6b9aecc458": "other models trained using the same data",
    "e587559f5ab6e42f7d981372ee34aebdc92b646e": "competitive results on other benchmarks",
    "bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a": "2.4% and 0.3%",
    "7b4fb6da74e6bd1baea556788a02969134cf0800": "Yes",
    "bc31a3d2f7c608df8c019a64d64cb0ccc5669210": "BERTBase",
    "761de1610e934189850e8fda707dc5239dd58092": "(b3) in Table TABREF27, a weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data",
    "f8da63df16c4c42093e5778c01a8e7e9b270142e": "by counting the common pair intersection of the two lists",
    "c09a92e25e6a81369fcc4ae6045491f2690ccc10": "compare crowd evaluations to those of expert linguists",
    "63c3550c6fb42f41a0c93133e9fca12ac00df9b3": "Yes",
    "603fee7314fa65261812157ddfc2c544277fcf90": "the same",
    "09a1173e971e0fcdbf2fbecb1b077158ab08f497": "significant",
    "70e9210fe64f8d71334e5107732d764332a81cb1": "INLINEFORM0 WER",
    "051df74dc643498e95d16e58851701628fdfd43e": "crawling and pre-processing an OSG web forum",
    "33554065284110859a8ea3ca7346474ab2cab100": "1,873 Twitter conversation threads, roughly 14k tweets",
    "57f23dfc264feb62f45d9a9e24c60bd73d7fe563": "50 samples for each 15 year duration starting from age 30",
    "54830abe73fef4e629a36866ceeeca10214bd2c8": "LDA and Gibbs sampling",
    "2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6": "the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik",
    "ef7212075e80bf35b7889dc8dd52fcbae0d1400a": "low precision rates and design challenges in training datasets",
    "567dc9bad8428ea9a2658c88203a0ed0f8da0dc3": "BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS",
    "d51dc36fbf6518226b8e45d4c817e07e8f642003": "sentences",
    "d8627ba08b7342e473b8a2b560baa8cdbae3c7fd": "Yes",
    "cb77d6a74065cb05318faf57e7ceca05e126a80d": "character-level representation",
    "8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d": "Bi-directional LSTM",
    "a1b3e2107302c5a993baafbe177684ae88d6f505": "each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.",
    "bb2de20ee5937da7e3e6230e942bec7b6e8f61ee": "daily newspaper of the year 2015-2016",
    "1170e4ee76fa202cabac9f621e8fbeb4a6c5f094": "",
    "1462eb312944926469e7cee067dfc7f1267a2a8c": "three types of entities",
    "f59f1f5b528a2eec5cfb1e49c87699e0c536cc45": "the sentences collected from daily newspaper of the year 2015-2016",
    "9bd080bb2a089410fd7ace82e91711136116af6c": "because of the inflectional characteristics of Nepali language",
    "6d1217b3d9cfb04be7fcd2238666fa02855ce9c5": "Bi-directional LSTM",
    "1e775cf30784e6b1c2b573294a82e145a3f959bb": "harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.",
    "392fb87564c4f45d0d8d491a9bb217c4fce87f03": "LastStateRNN",
    "203337c15bd1ee05763c748391d295a1f6415b9b": "multi-attention method having a projected layer",
    "d004ca2e999940ac5c1576046e30efa3059832fa": "multi-attention and single attention models",
    "21548433abd21346659505296fb0576e78287a74": "\"First workshop on categorizing different types of online harassment languages in social media\"",
    "f0b2289cb887740f9255909018f400f028b1ef26": "harassment and non-harassment categories, indirect harassment, physical and sexual harassment",
    "51b1142c1d23420dbf6d49446730b0e82b32137c": "LastStateRNN",
    "58355e2a782bf145b61ee2a3e0e426119985c179": "train set, validation set and test set",
    "25c1c4a91f5dedd4e06d14121af3b5921db125e9": "Yes",
    "f88036174b4a0dbf4fe70ddad884d16082c5748d": "No",
    "a267d620af319b48e56c191aa4c433ea3870f6fb": "the car it reviews",
    "899ed05c460bf2aa0aa65101cad1986d4f622652": "$10,867$",
    "d53299fac8c94bd0179968eb868506124af407d1": "F1 micro and F1 macro scores",
    "29f2954098f055fb19d9502572f085862d75bf61": "K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)",
    "6bf93968110c6e3e3640360440607744007a5228": "car-speak pertains to a car's physical attributes",
    "37a79be0148e1751ffb2daabe4c8ec6680036106": "anti-nuclear-power",
    "518dae6f936882152c162058895db4eca815e649": "",
    "e44a6bf67ce3fde0c6608b150030e44d87eb25e3": "multiple",
    "6a31db1aca57a818f36bba9002561724655372a7": "2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users",
    "e330e162ec29722f5ec9f83853d129c9e0693d65": "Yes",
    "d3093062aebff475b4deab90815004051e802aa6": "1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN); 5",
    "4944cd597b836b62616a4e37c045ce48de8c82ca": "MR, CR, SUBJ, MPQA, SST, TREC, MRPC",
    "a29c071065d26e5ee3c3bcd877e7f215c59d1d33": "Spearman's rank correlation",
    "7f207549c75f5c4388efc15ed28822672b845663": "less than 20 minutes",
    "596aede2b311deb8cb0a82d2e7de314ef6e83e4e": "one",
    "2e89ebd2e4008c67bb2413699589ee55f59c4f36": "on the combination of the SNLI and the Multi-Genre NLI dataset",
    "e2db361ae9ad9dbaa9a85736c5593eb3a471983d": "InferSent, Universal Sentence Encoder, average BERT embeddings, average GloVe embeddings",
    "252a645af9876241fb166e5822992ce17fec6eb6": "the average length of the string of characters representing article title",
    "ed67359889cf61fa11ee291d6c378cccf83d599d": "publicly available GloVe word vectors BIBREF16 pre-trained on two datasets",
    "425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82": "accuracy",
    "955de9f7412ba98a0c91998919fa048d339b1d48": "Bag-of-Words and Support Vector Machine with linear kernel",
    "3b371ea554fa6639c76a364060258454e4b931d4": "NowThisNews Facebook page",
    "ddb23a71113cbc092cbc158066d891cae261e2c6": "NowThisNews Facebook page",
    "e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38": "hotel reviews",
    "c7486d039304ca9d50d0571236429f4f6fbcfcf7": "Russian",
    "f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9": "Dutch, Turkish, Spanish, Russian",
    "a103636c8d1dbfa53341133aeb751ffec269415c": "majority baseline and lexicon-based baseline",
    "55139fcfe04ce90aad407e2e5a0067a45f31e07e": "machine translation",
    "fbaf060004f196a286fef67593d2d76826f0304e": "The first set consists of English reviews and the second set contains restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian).",
    "7ae38f51243cb80b16a1df14872b72a1f8a2048f": "the combination of these two networks could provide a more powerful discriminative spatial and temporal representation of the data than each independent network",
    "deb89bca0925657e0f91ab5daca78b9e548de2bd": "",
    "9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570": "CNN",
    "e6583c60b13b87fc37af75ffc975e7e316d4f4e0": "composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)",
    "c7b6e6cb997de1660fd24d31759fe6bb21c7863f": "the number of electrodes",
    "f9f59c171531c452bd2767dc332dc74cadee5120": "14 subjects",
    "4ac2c3c259024d7cd8e449600b499f93332dab60": "Yes",
    "bc730e4d964b6a66656078e2da130310142ab641": "Logistic Regression (LR) BIBREF1 and Multilayer Perceptron (MLP) BIBREF2",
    "3941401a182a3d6234894a5c8a75d48c6116c45c": "CyberAttack and PoliticianDeath",
    "67e9e147b2cab5ba43572ce8a17fc863690172f0": "Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords. By doing so, our approach identifies new keywords which convey more relevant information with respect to existing ones, thus effectively boosting model performance. By exploiting the disagreement between the model and the crowd, our approach can make efficient use of the crowd, which is of critical importance in a human-in-the-loop context BIBREF9, BIBREF10.",
    "a74190189a6ced2a2d5b781e445e36f4e527e82a": "the experiments with two predetermined event categories",
    "43f074bacabd0a355b4e0f91a1afd538c0a6244f": "taking into account both the crowd-contributed labels and the model prediction",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "Yes",
    "78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d": "conversational search, conversational question answering, conversational recommendation, and conversational natural language interface to structured and semi-structured data",
    "375b281e7441547ba284068326dd834216e55c07": "a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research",
    "05c49b9f84772e6df41f530d86c1f7a1da6aa489": "File IO, Standard IO, and Telegram interfaces",
    "6ecb69360449bb9915ac73c0a816c8ac479cbbfc": "multi-modal interactions",
    "68df324e5fa697baed25c761d0be4c528f7f5cf7": "",
    "77c34f1033702278f7f044806c1eba0c6ecb8b04": "Yes",
    "2ee715c7c6289669f11a79743a6b2b696073805d": "the ones in Dunietz and Gillick BIBREF11",
    "61a9ea36ddc37c60d1a51dabcfff9445a2225725": "the news external references in Wikipedia from 73,734 entity pages",
    "cc850bc8245a7ae790e1f59014371d4f35cd46d7": "determines the correct section for the triple INLINEFORM4",
    "984fc3e726848f8f13dfe72b89e3770d00c3a1af": "novelty of news articles w.r.t the already existing entity profile",
    "fb1227b3681c69f60eb0539e16c5a8cd784177a7": "entity authority, news authority and novelty",
    "8df35c24af9efc3348d3b8d746df116480dfe661": "Yes",
    "277a7e916e65dfefd44d2d05774f95257ac946ae": "four baselines",
    "2916bbdb95ef31ab26527ba67961cf5ec94d6afe": "53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total",
    "f2e8497aa16327aa297a7f9f7d156e485fe33945": "The annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation. The annotators could choose between a pre-annotated version or a blank version of each text. The pre-annotated versions contained suggested entity spans based on string matches from lists of conditions and findings synonym lists. Their quality varied widely throughout the corpus. The blank version was preferred by the annotators. We distribute the corpus in BioC JSON format. BioC was chosen as it allows us to capture the complexities of the annotations in the biomedical domain. It represented each documents properties ranging from full text, individual passages/sentences along with captured annotations and relationships in an organized manner. BioC is based on character offsets of annotations and allows the stacking of different layers.",
    "9b76f428b7c8c9fc930aa88ee585a03478bff9b3": "53 documents",
    "dd6b378d89c05058e8f49e48fd48f5c458ea2ebc": "four baseline systems",
    "e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851": "two lists annotated in previous works",
    "c00ce1e3be14610fb4e1f0614005911bb5ff0302": "$relu$, $selu$, $tanh$",
    "71fe5822d9fccb1cb391c11283b223dc8aa1640c": "",
    "97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3": "by the posting date in ascending way",
    "1062a0506c3691a93bb914171c2701d2ae9621cb": "the features are extracted from each chunk and fed into a recurrent neural network to model the sequential flow of the chunks' tweets.",
    "8e12b5c459fa963b3e549deadb864c244879fe82": "two layers",
    "483a699563efcb8804e1861b18809279f21c7610": "Yes",
    "d3ff2986ca8cb85a9a5cec039c266df756947b43": "words embeddings, style, and morality features",
    "3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4": "180 Twitter accounts from BIBREF1 and 32 Twitter accounts from BIBREF19",
    "2317ca8d475b01f6632537b95895608dc40c4415": "a sorted sequence of tweets labeled by the label of its corresponding account",
    "3e88fb3d28593309a307eb97e875575644a01463": "LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets",
    "0767ca8ff1424f7a811222ca108a33b6411aaa8a": "the scores given by the humans do not correlate with ROGUE. Human evaluators gives almost similarly scores to summary generated by the Lead-1 and Lead-1-AMR with Lead-1-AMR actually performing better on readability though it dropped some information as clear from the scores on information contained. On the other hand, ROGUE gives very high score to Lead-1 while models 1,2 and 4 get almost same scores",
    "e8f969ffd637b82d04d3be28c51f0f3ca6b3883e": "human evaluation",
    "46227b4265f1d300a5ed71bf40822829de662bc2": "AMR Bank BIBREF10 and CNN-Dailymail ( BIBREF11 BIBREF12 )",
    "a6a48de63c1928238b37c2a01c924b852fe752f8": "ROGUE",
    "b65a83a24fc66728451bb063cf6ec50134c8bfb0": "finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR",
    "8c852fc29bda014d28c3ee5b5a7e449ab9152d35": "linear SVM, BiLSTM, and CNN",
    "682e26262abba473412f68cbeb5f69aa3b9968d7": "",
    "5daeb8d4d6f3b8543ec6309a7a35523e160437eb": "English",
    "74fb77a624ea9f1821f58935a52cca3086bb0981": "14,100 tweets",
    "d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751": "aggression identification, bullying detection, hate speech, toxic comments, and offensive language",
    "55bd59076a49b19d3283af41c5e3ccb875f3eb0c": "CNN-based sentence classifier",
    "521280a87c43fcdf9f577da235e7072a23f0673e": "more than 2",
    "5a8cc8f80509ea77d8213ed28c5ead501c68c725": "",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "A, B and C",
    "1b72aa2ec3ce02131e60626639f0cf2056ec23ca": "shown in Table TABREF15",
    "c49ee6ac4dc812ff84d255886fd5aff794f53c39": "Yes",
    "3f856097be2246bde8244add838e83a2c793bd17": "information retrieval",
    "bf52c01bf82612d0c7bbf2e6a5bb2570c322936f": "The proposed metric is more effective than Rouge in evaluating the quality of scientific summaries.",
    "74e866137b3452ec50fb6feaf5753c8637459e62": "the manual Pyramid scores",
    "184b0082e10ce191940c1d24785b631828a9f714": "Rouge is the best metric for all summarization tasks",
    "c59078efa7249acfb9043717237c96ae762c0a8c": "the bias regularization method of BIBREF5",
    "73bddaaf601a4f944a3182ca0f4de85a19cdc1d2": "Daily Mail news articles released by BIBREF9",
    "d4e5e3f37679ff68914b55334e822ea18e60a6cf": "",
    "5f60defb546f35d25a094ff34781cddd4119e400": "",
    "90d946ccc3abf494890e147dd85bd489b8f3f0e8": "",
    "b962cc817a4baf6c56150f0d97097f18ad6cd9ed": "human-generated questions",
    "fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f": "logistic regression, LSTM, MemN2N and a model of our own design",
    "52f8a3e3cd5d42126b5307adc740b71510a6bdf5": "relational reasoning skills through natural language questions",
    "2236386729105f5cf42f73cc055ce3acdea2d452": "English",
    "18942ab8c365955da3fd8fc901dfb1a3b65c1be1": "TripAdvisor",
    "7b4992e2d26577246a16ac0d1efc995ab4695d24": "using only manual annotation",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection.",
    "9a9d225f9ac35ed35ea02f554f6056af3b42471d": "(incorrect phrase, correct phrase)",
    "ea56148a8356a1918bedcf0a99ae667c27792cfe": "the original and corrected sentences in the corpus",
    "cd32a38e0f33b137ab590e1677e8fb073724df7f": "English",
    "2c6b50877133a499502feb79a682f4023ddab63e": "English",
    "f651cd144b7749e82aa1374779700812f64c8799": "",
    "4625cfba3083346a96e573af5464bc26c34ec943": "substantial gains in all tasks, and new SOTA results",
    "326588b1de9ba0fd049ab37c907e6e5413e14acd": "PBMT-R, Hybrid, SBMT-SARI, NMT and Dress",
    "ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb": "89,042 sentence pairs and 296,402 sentence pairs",
    "55507f066073b29c1736b684c09c045064053ba9": "From the context \"We analyzed all tweets labeled as offensive to better understand how Arabic speakers use offensive language. Here is a breakdown of usage",
    "e838275bb0673fba0d67ac00e4307944a2c17be3": "by analyzing the annotated tweets",
    "8dda1ef371933811e2a25a286529c31623cca0c6": "",
    "b3de9357c569fb1454be8f2ac5fcecaea295b967": "1,915 offensive tweets, including 225 vulgar tweet and 506 hate speech tweets, and 8,085 clean tweets",
    "59e58c6fc63cf5b54b632462465bfbd85b1bf3dd": "not biased by topic, dialect or target",
    "5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83": "demonstrate that our model achieves a high irony accuracy with well-preserved sentiment and content",
    "3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f": "the harmonic mean of irony reward and sentiment reward",
    "14b8ae5656e7d4ee02237288372d9e682b24fdb8": "ironic style is hard to model and describe",
    "e3a2d8886f03e78ed5e138df870f48635875727e": "crawled over 2M tweets from twitter",
    "62f27fe08ddb67f16857fab2a8a721926ecbb6fb": "",
    "9ca447c8959a693a3f7bdd0a2c516f4b86f95718": "Favor or Against",
    "05887a8466e0a2f0df4d6a5ffc5815acd7d9066a": "unigram-based and hashtag-related features in SVM classifiers for the stance detection problem in Turkish tweets",
    "c87fcc98625e82fdb494ff0f5309319620d69040": "unigram-based features in SVM classifiers for the stance detection problem in Turkish tweets",
    "500a8ec1c56502529d6e59ba6424331f797f31f0": "2 million tweets",
    "ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc": "Galatasaray and Fenerbah\u00e7e",
    "f2155dc4aeab86bf31a838c8ff388c85440fce6e": "frozen strategy performs almost as well as fine-tuning",
    "ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc": "7",
    "4d706ce5bde82caf40241f5b78338ea5ee5eb01e": "linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases",
    "86bf75245358f17e35fc133e46a92439ac86d472": "mSynC does slightly worse on 7 out of 9 tasks",
    "9132d56e26844dc13b3355448d0f14b95bd2178a": "chunk boundary information",
    "f3c204723da53c7c8ef4dc1018ffbee545e81056": "Yes",
    "0602a974a879e6eae223cdf048410b5a0111665e": "K-means, LEM and DPEMM",
    "56b034c303983b2e276ed6518d6b080f7b8abe6a": "FSD BIBREF12, Twitter, and Google datasets",
    "15e481e668114e4afe0c78eefb716ffe1646b494": "Gibbs sampler",
    "3d7a982c718ea6bc7e770d8c5da564fbb9d11951": "The generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).",
    "692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1": "all the users that had the industry profile element completed",
    "935d6a6187e6a0c9c0da8e53a42697f853f5c248": "the aggregate of enterprises in a particular field",
    "3b77b4defc8a139992bd0b07b5cf718382cb1a5f": "content-based classifiers",
    "01a41c0a4a7365cd37d28690735114f2ff5229f2": "social media platform",
    "cd2878c5a52542ddf080b20bec005d9a74f2d916": "Table TABREF1",
    "fd2c6c26fd0ab3c10aae4f2550c5391576a77491": "Yes",
    "6b6d498546f856ac20958f666fc3fd55811347e2": "max-pooling layer and average pooling layer",
    "de3b1145cb4111ea2d4e113f816b537d052d9814": "CrowdFlower dataset annotated for emotions",
    "132f752169adf6dc5ade3e4ca773c11044985da4": "the tweet dataset created by Wang et al",
    "1d9aeeaa6efa1367c22be0718f5a5635a73844bd": "the most relevant features and the text as a whole",
    "012b8a89aea27485797373adbcda32f16f9d7b54": "bidirectional recurrent neural networks or ensembles of recurrent neural networks",
    "c598028815066089cc1e131b96d6966d2610467a": "Yes",
    "ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed": "The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.",
    "0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50": "classification accuracy",
    "92dfacbbfa732ecea006e251be415a6f89fb4ec6": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.",
    "c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9": "NCHLT text corpora",
    "307e8ab37b67202fe22aedd9a98d9d06aaa169c5": "Yes",
    "6415f38a06c2f99e8627e8ba6251aa4b364ade2d": "14 languages over 6 language groups",
    "e5c8e9e54e77960c8c26e8e238168a603fcdfcc6": "No",
    "50be4a737dc0951b35d139f51075011095d77f2a": "prior knowledge to label unlabeled instances and then apply a standard learning algorithm",
    "6becff2967fe7c5256fe0b00231765be5b9db9f1": "(1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution",
    "76121e359dfe3f16c2a352bd35f28005f2a40da3": "text categorization and sentiment classification",
    "02428a8fec9788f6dc3a86b5d5f3aa679935678d": "the ability to leverage labeled features as prior knowledge",
    "7793805982354947ea9fc742411bec314a6998f6": "automatic",
    "007b13f05d234d37966d1aa7d85b5fd78564ff45": "No",
    "2ceced87af4c8fdebf2dc959aa700a5c95bd518f": "No",
    "72ed5fed07ace5e3ffe9de6c313625705bc8f0c7": "150 to 250 tokens",
    "2e37e681942e28b5b05639baaff4cd5129adb5fb": "valuable semantic information",
    "b49598b05358117ab1471b8ebd0b042d2f04b2a4": "NBOW, RNN and CNN",
    "932b39fd6c47c6a880621a62e6a978491d881d60": "TransE, TransH, TransR, TransD, TransA, TransG, TransM, TransE-N, TransE-A, TransE-N-A, TransE-N-A-G, TransE-N-A-G-M",
    "b36f867fcda5ad62c46d23513369337352aa01d2": "WordNet and Freebase",
    "c6a0b9b5dabcefda0233320dd1548518a0ae758e": "CJFA encoder",
    "1e185a3b8cac1da939427b55bf1ba7e768c5dae4": "VAE baseline, the CJFS encoder and the CJFA encoder",
    "26e2d4d0e482e6963a76760323b8e1c26b6eee91": "blocks (D+H)",
    "b80a3fbeb49a8968e149955bdcf199556478eeff": "The CJFA encoder obtains significantly better phone classification accuracy than the VAE baseline and also than the CJFS encoder. These results are replicated for speaker recognition tasks. The CJFA encoder performs better on all tasks than the VAE baseline by a significant margin.",
    "badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9": "new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location",
    "67b66fe67a3cb2ce043070513664203e564bdcbd": "CRF",
    "f56d07f73b31a9c72ea737b40103d7004ef6a079": "homographic dataset and heterographic dataset",
    "38e4aaeabf06a63a067b272f8950116733a7895c": "{ INLINEFORM0 }",
    "1d197cbcac7b3f4015416f0152a6692e881ada6c": "OpenIE",
    "92294820ac0d9421f086139e816354970f066d8a": "significant improvements",
    "477d9d3376af4d938bb01280fe48d9ae7c9cf7f7": "BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19",
    "f225a9f923e4cdd836dd8fe097848da06ec3e0cc": "SQuAD dataset",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "the system that did not pay attention to naming",
    "e807d347742b2799bc347c0eff19b4c270449fee": "the aggregate of all train data sets till the 5th version of BioASQ competition",
    "31b92c03d5b9be96abcc1d588d10651703aff716": "",
    "9ec1f88ceec84a10dc070ba70e90a792fba8ce71": "0.6103",
    "384bf1f55c34b36cb03f916f50bbefade6c86a75": "No",
    "aef607d2ac46024be17b1ddd0ed3f13378c563a6": "using the estimated word importance to detect the under-translated words",
    "93beae291b455e5d3ecea6ac73b83632a3ae7ec7": "integrated gradients",
    "6c91d44d5334a4ac80100eead4e105d34e99a284": "Transformer and RNN-Search model",
    "a69a59b6c0ab27bcee1a780d6867df21e30aec08": "Yes",
    "b3d01ac226ee979e188a4141877a6d2a5482de98": "the shortcomings of current models; the data categorized by inference type to examine weaknesses; the good performance on NLI stress tests",
    "af5730d82535464cedfa707a03415ac2e7a21295": "SNLI BIBREF1, MNLI BIBREF22, HotpotQA training set BIBREF23 and Wikipedia",
    "ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2": "Yes",
    "b249b60a8c94d0e40d65f1ffdfcac527dab57516": "Yes",
    "0f567251a6566f65170a1329eeeb5105932036b2": "BiLSTM+Lex BIBREF42",
    "4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f": "(a) gazeteer and rule based, (b) word boundary detection, and (c) ranking with language model and other features",
    "60ce4868af45753c9e124e64e518c32376f12694": "Stanford Sentiment Analysis Dataset",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "353 conversations from 40 speakers",
    "2c85865a65acd429508f50b5e4db9674813d67f2": "recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital",
    "73a7acf33b26f5e9475ee975ba00d14fd06f170f": "templates and expression pools",
    "dd53baf26dad3d74872f2d8956c9119a27269bd5": "linguistic analysis followed by manual verification",
    "218bc82796eb8d91611996979a4a42500131a936": "",
    "b21bc09193699dc9cfad523f3d5542b0b2ff1b8e": "MLP",
    "352bc6de5c5068c6c19062bad1b8f644919b1145": "INLINEFORM5 samples",
    "d667731ea20605580c398a1224a0094d1155ebbb": "Yes",
    "8bb0011ad1d63996d5650770f3be18abdd9f7fc6": "Yes",
    "b0dbe75047310fec4d4ce787be5c32935fc4e37b": "using two of its adversarial sets, namely AddSent and AddOneSent",
    "d64383e39357bd4177b49c02eb48e12ba7ffd4fb": "end-to-end MRC model",
    "52f9cd05d8312ae3c7a43689804bac63f7cac34b": "Yes",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "support vector machines with radial basis function kernel",
    "955cbea7e5ead36fb89cd6229a97ccb3febcf8bc": "10-fold cross validation",
    "04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39": "three",
    "15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8": "twitter",
    "ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797": "BIBREF26",
    "6ca938324dc7e1742a840d0a54dc13cc207394a1": "German newscrawl and English newscrawl",
    "4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802": "BIBREF26",
    "4d47bef19afd70c10bbceafd1846516546641a2f": "bi-directional language model and uni-directional model",
    "506d21501d54a12d0c9fd3dbbf19067802439a04": "the dot-product attention of each word in the generated output",
    "701571680724c05ca70c11bc267fb1160ea1460a": "No",
    "600b097475b30480407ce1de81c28c54a0b3b2f8": "layer normalization and data-specific trained word embeddings",
    "ee7e9a948ee6888aa5830b1a3d0d148ff656d864": "Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017",
    "5fda8539a97828e188ba26aad5cda1b9dd642bc8": "the least decoding time in all four datasets",
    "709feae853ec0362d4e883db8af41620da0677fe": "by producing the Hadamard product of Gaussian weight matrix $G$ and the score matrix produced by $Q{K^{T}}$",
    "186b7978ee33b563a37139adff1da7d51a60f581": "limits all the data for learning should not be beyond the given training set",
    "fabcd71644bb63559d34b38d78f6ef87c256d475": "our model finishes the segmentation with the least decoding time in all four datasets",
    "da9c0637623885afaf023a319beee87898948fe9": "Yes",
    "8a1c0ef69b6022a0642ca131a8eacb5c97016640": "context tweets",
    "48088a842f7a433d3290eb45eb0d4c6ab1d8f13c": "Na\u00efve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)",
    "4907096cf16d506937e592c50ae63b642da49052": "humans can better understand a tweet with the reference of its context",
    "8748e8f64af57560d124c7b518b853bf2711c13e": "Yes",
    "893ec40b678a72760b6802f6abf73b8f487ae639": "the model can detect some biases in data annotation and collection",
    "c81f215d457bdb913a5bade2b4283f19c4ee826c": "Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9",
    "e101e38efaa4b931f7dd75757caacdc945bb32b4": "F1-score of 81% and 91% for datasets of Waseem and Davidson respectively",
    "afb77b11da41cd0edcaa496d3f634d18e48d7168": "four different fine-tuning approaches",
    "41b2355766a4260f41b477419d44c3fd37f3547d": "From the context \"We also include the details of our implementation and error analysis in the respective subsections. We consider 80% of each dataset as training data to update the weights in the fine-tuning phase, 10% as validation data to measure the out-of-sample performance of the model during training, and 10% as test data to measure the out-of-sample performance after training. To prevent overfitting, we use stratified sampling to select 0.8, 0.1, and 0.1 portions of tweets from each class (racism/sexism/neither or hate/offensive/neither) for train, validation, and test. Classes' distribution of train, validation, and test datasets are shown in Table TABREF16. As it is understandable from Tables TABREF16(classdistributionwaseem) and TABREF16(classdistributiondavidson), we are dealing with imbalance datasets with various classes\u2019 distribution. Since hate speech and offensive languages are real phenomena, we did not perform oversampling or undersampling techniques to adjust the classes\u2019 distribution and tried to supply the datasets as realistic as possible. We evaluate the effect of different fine-tuning strategies on the performance of our model. Table TABREF17 summarized the obtained results for fine-tuning strategies along with the official baselines. We use Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10 as baselines and compare the results with our different fine-tuning strategies using pre-trained BERTbase model. The evaluation results are reported on the test dataset and on three different metrics",
    "96a4091f681872e6d98d0efee777d9e820cb8dae": "some biases in the process of collecting or annotating datasets",
    "81a35b9572c9d574a30cc2164f47750716157fc8": "the official baselines",
    "f4496316ddd35ee2f0ccc6475d73a66abf87b611": "20 Newsgroups dataset (20NG)",
    "e8a32460fba149003566969f92ab5dd94a8754a4": "Concept Raw Context model (CRC) and Concept-Concept Context model (3C)",
    "2a6003a74d051d0ebbe62e8883533a5f5e55078b": "dataless classification",
    "1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b": "",
    "9c44df7503720709eac933a15569e5761b378046": "English",
    "b7381927764536bd97b099b6a172708125364954": "incorporating subword information (simple n-grams and unsupervised morphemes) into the LexVec word embedding model and evaluating its impact on the resulting IV and OOV word vectors",
    "df95b3cb6aa0187655fd4856ae2b1f503d533583": "n-gram subwords and unsupervised morphemes",
    "f7ed3b9ed469ed34f46acde86b8a066c52ecf430": "stochastic gradient descent",
    "c7eb71683f53ab7acffd691a36cad6edc7f5522e": "Yes",
    "17a1eff7993c47c54eddc7344e7454fbe64191cd": "the measure given in BIBREF27, BIBREF40",
    "a5e5cda1f6195ab1336855f1e39a609d61326d62": "embedding vector dimension",
    "32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9": "INLINEFORM0 and INLINEFORM1 stand for word and context vector representations, respectively, for words INLINEFORM2 and INLINEFORM3, while INLINEFORM4 represents the (possibly weighted) cooccurrence count for the word pair INLINEFORM5. Intuitively, ( SECREF4 ) represents the requirement that if some word INLINEFORM6 occurs often enough in the context (or vicinity) of another word INLINEFORM7, then the corresponding word representations should have a large enough inner product in keeping with their large INLINEFORM8 value, up to some bias terms INLINEFORM9 ; and vice versa. INLINEFORM10 in ( SECREF4 ) is used as a discounting factor that",
    "eda4869c67fe8bbf83db632275f053e7e0241e8c": "SemEval 2015 Twitter dataset",
    "2c7494d47b2a69f182e83455fe4c75ae3b2893e9": "Yes",
    "4d7ff4e5d06902de85b0e9a364dc455196d06a7d": "Skip-thought vectors, FastSent, Siamase CBOW and log-linear model",
    "ecc63972b2783ee39b3e522653cfb6dc5917d522": "by providing the first of its kind survey of the recent tweet-specific unsupervised models in an organized fashion to understand the literature",
    "8d074aabf4f51c8455618c5bf7689d3f62c4da1d": "they do not solve missing word issue",
    "fe2666ace293b4bfac3182db6d0c6f03ea799277": "the morpheme is the basic unit of languages as Leonard Bloomfield mentioned for English",
    "70a1b0f9f26f1b82c14783f1b76dfb5400444aa4": "successful",
    "d3ca5f1814860a88ff30761fec3d860d35e39167": "Neural Network, Weighted Finite State Transducer, Conditional Random Fields, Support Vector Machine",
    "dd20d93166c14f1e57644cd7fa7b5e5738025cd0": "",
    "dc2a2c177cd5df6da5d03e6e74262bf424850ec9": "by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2",
    "ae90c5567746fe25af2fcea0cc5f355751e05c71": "TP=true positives, FP=false positives, FN=false negatives",
    "d7644c674887ca9708eb12107acd964ae53b216d": "Precision, Recall, F1-score and AUROC",
    "a3bb9a936f61bafb509fa12ac0a61f91abcc5106": "ARC questions, TREC-6, TREC-50, GARD, MLBioMedLAT",
    "df6d327e176740da9edcc111a06374c54c8e809c": "5 common models in previous work",
    "49764eee7fb523a6a28375cc699f5e0220b81766": "Yes",
    "3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1": "training a series of one-vs-all binary classifiers BIBREF34, one for each label in the taxonomy",
    "bb3267c3f0a12d8014d51105de5d81686afe5f1b": "the above benchmarks",
    "114934e1a1e818630ff33ac5c4cd4be6c6f75bb2": "effective",
    "2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72": "the same hyper-parameters for all datasets",
    "b8d0e4e0e820753ffc107c1847fe1dfd48883989": "adjacent entity mentions",
    "5aa12b4063d6182a71870c98e4e1815ff3dc8a72": "lemmatizing input before training ELMo can result in different decisions being optimal or sub-optimal at the stage of deep learning models training",
    "22815878083ebd2f9e08bc33a5e733063dac7a0f": "Russian and English",
    "220d11a03897d85af91ec88a9b502815c7d2b6f3": "lemmatization is not necessary in English",
    "d509081673f5667060400eb325a8050fa5db7cc8": "two times larger",
    "c2e475adeddcdc4d637ef0d4f5065b6a9b299827": "BLEU-4, NIST-4 and ROUGE-4",
    "cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5": "Yes",
    "6cd25c637c6b772ce29e8ee81571e8694549c5ab": "WikiBio dataset",
    "1088255980541382a2aa2c0319427702172bbf84": "",
    "0d9fcc715dee0ec85132b3f4a730d7687b6a06f4": "the expected number of unique outputs it assigns to a set of adversarial perturbations",
    "8910ee2236a497c92324bbbc77c596dba39efe46": "sentiment analysis and paraphrase detection",
    "2c59528b6bc5b5dc28a7b69b33594b274908cca6": "a technique to improve word recognition",
    "6b367775a081f4d2423dc756c9b65b6eef350345": "Yes",
    "bc01853512eb3c11528e33003ceb233d7c1d7038": "the first and last characters of each word remain unperturbed",
    "67ec8ef85844e01746c13627090dc2706bb2a4f3": "They experiment with RNNs instead of transformers for this task.",
    "ba539cab80d25c3e20f39644415ed48b9e4e4185": "falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK",
    "6bf5620f295b5243230bc97b340fae6e92304595": "assigning a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head",
    "4986f420884f917d1f60d3cea04dc8e64d3b5bf1": "CLVs",
    "747b847d687f703cc20a87877c5b138f26ff137d": "CoNLL 2009 corpus and Europarl corpus",
    "111afb77cfbf4c98e0458606378fa63a0e965e36": "Yes",
    "6568a31241167f618ef5ede939053feaa2fb0d7e": "Yes",
    "50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed": "individual Bayesian models for each language BIBREF3, and crosslingual latent variables to incorporate soft role agreement between aligned constituents",
    "0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c": "(i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO.",
    "4dc268e3d482e504ca80d2ab514e68fd9b1c3af1": "2,896",
    "ab54cd2dc83141bad3cb3628b3f0feee9169a556": "combining both data sources",
    "249c805ee6f2ebe4dbc972126b3d82fb09fa3556": "similarity-based approaches, which extract Amazon search terms",
    "b4f881331b975e6e4cab1868267211ed729d782d": "48,705",
    "79413ff5d98957c31866f22179283902650b5bb6": "two sources of data, one to generate tag recommendations and another one to evaluate tag recommendations",
    "29c014baf99fb9f40b5171aab3e2c7f12a748f79": "19 different algorithms",
    "09c86ef78e567033b725fc56b85c5d2602c1a7c3": "a weighted average of the predictions of the three models",
    "d67c01d9b689c052045f3de1b0918bab18c3f174": "INLINEFORM2",
    "e5bc73974c79d96eee2b688e578a9de1d0eb38fd": "there is still space for improvement",
    "2cd37743bcc7ea3bd405ce6d91e79e5339d7642e": "Yes",
    "eac9dae3492e17bc49c842fb566f464ff18c049b": "argument components",
    "7697baf8d8d582c1f664a614f6332121061f87db": "machine learning methods",
    "1cb100182508cf55b3509283c0e2bbcd527d625e": "several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming",
    "206739417251064b910ae9e5ff096e867ee10fb8": "empirical evidence to issues that are on the spot of current argumentation research",
    "d6401cece55a14d2a35ba797a0878dfe2deabedc": "different registers and domains",
    "ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff": "the DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes)",
    "fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c": "Android application",
    "b1a068c1050e2bed12d5c9550c73e59cd5b1f78d": "Persian and English",
    "f9edd8f9c13b54d8b1253ed30e7decc1999602da": "official trial lists with different numbers of enrollment utterances per trial",
    "d93c0e78a3fe890cd534a11276e934be68583f4b": "18 to 65",
    "30af1926559079f59b0df055da76a3a34df8336f": "RedDots",
    "ceb767e33fde4b927e730f893db5ece947ffb0d8": "CUI-based upper bound on extractive summarization of discharge summaries and presented a NN architecture that jointly classifies words in history of present illness notes",
    "c2cb6c4500d9e02fc9a1bdffd22c3df69655189f": "No",
    "c571deefe93f0a41b60f9886db119947648e967c": "MIMIC-III, 55,177 discharge reports and 4,475 discharge addendums",
    "06eb9f2320451df83e27362c22eb02f4a426a018": "effectiveness of the document preprocessing",
    "e54257585cc75564341eb02bdc63ff8111992f82": "the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task BIBREF0",
    "2a3e36c220e7b47c1b652511a4fdd7238a74a68f": "244 scientific articles",
    "9658b5ffb5c56e5a48a3fea0342ad8fc99741908": "Yes",
    "46c9e5f335b2927db995a55a18b7c7621fd3d051": "15",
    "ce0e2a8675055a5468c4c54dbb099cfd743df8a7": "10 phenotypes that are annotated",
    "3a6e843c6c81244c14730295cfb8b865cd7ede46": "BIBREF9 and BIBREF37",
    "fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf": "the publicly available word2vec vectors",
    "1beb4a590fa6127a138f4ed1dd13d5d51cc96809": "the features extracted from all deep CNN based feature extraction models",
    "5c5aeee83ea3b34f5936404f5855ccb9869356c1": "machine translation tasks",
    "f8c1b17d265a61502347c9a937269b38fc3fcab1": "better",
    "5913930ce597513299e4b630df5e5153f3618038": "$\\alpha $-entmax for attention improves interpretability compared to softmax transformers.",
    "81d193672090295e687bc4f4ac1b7a9c76ea35df": "distant supervised method",
    "cf171fad0bea5ab985c53d11e48e7883c23cdc44": "nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels.",
    "2a564b092916f2fabbfe893cf13de169945ef2e1": "20,244 reviews and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5.",
    "0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d": "+1 if the row word is a positive word, and with -1 if it is a negative word",
    "73e83c54251f6a07744413ac8b8bed6480b2294f": "",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "by taking into account the minimum, maximum, and average scores of the words occurring in the same contexts as the target word",
    "e48e750743aef36529fbea4328b8253dbe928b4d": "dev data set",
    "c08aab979dcdc8f4fe8ec1337c3c8290ab13414e": "all the individual features",
    "8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f": "GloVe embeddings trained on 2 Billion tweets, Edinburgh embeddings and Emoji embeddings",
    "cc608df2884e1e82679f663ed9d9d67a4b6c03f3": "BLEU-4, METEOR, CIDEr and ROUGE-L",
    "3e432d71512ffbd790a482c716e7079ee78ce732": "over 41,250 videos and 825,000 captions in both English and Chinese",
    "dd76130ec5fac477123fe8880472d03fbafddef6": "types of interactions, types of architecture, and types of context reasoning",
    "43eecc576348411b0634611c81589f618cd4fddf": "SeqGAN, LeakGAN, IRL, MaliGAN and ARAML",
    "79f9468e011670993fd162543d1a4b3dd811ac5d": "significantly better",
    "c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf": "Yes",
    "902b3123aec0f3a39319ffa9d05ab8e08a2eb567": "skip-gram with negative sampling (SGNS)",
    "1038542243efe5ab3e65c89385e53c4831cd9981": "DTA18 and DTA19",
    "e2b0cd30cf56a4b13f96426489367024310c3a05": "The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\\rho $ is used. The higher Spearman's rank-order correlation the better the system's performance.",
    "e831041d50f3922265330fcbee5a980d0e2586dd": "a task that participants were instructed to read the sentences naturally, without any specific task other than comprehension",
    "7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93": "Yes",
    "ac7f6497be4bcca64e75f28934b207c9e8097576": "the Wikipedia corpus provided by culotta2006integrating",
    "87bb3105e03ed6ac5abfde0a7ca9b8de8985663c": "they do not require the availability of a backward translation engine",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "copy",
    "9225b651e0fed28d4b6261a9f6b443b52597e401": "backtrans-nmt improves quickly in the earliest updates and then stays horizontal, whereas natural continues improving, even after 400k updates. Therefore BT does not help to avoid overfitting, it actually encourages it, which may be due \u201ceasier\u201d training examples (cf. $ SECREF15 ).",
    "565189b672efee01d22f4fc6b73cd5287b2ee72c": "the Europarl corpus (about 6M sentences for both French and German)",
    "b6f7fadaa1bb828530c2d6780289f12740229d84": "English INLINEFORM0 German and English INLINEFORM1 French",
    "7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613": "English, German and French",
    "4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f": "Yes",
    "6c96e910bd98c9fd58ba2050f99b9c9bac69840a": "80% of the questions",
    "9af3142630b350c93875441e1e1767312df76d17": "the answered questions measure for the usefulness of the answer",
    "e374169ee10f835f660ab8403a5701114586f167": "display name, description and username",
    "82595ca5d11e541ed0c3353b41e8698af40a479b": "display name and description, username and profile image",
    "d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f": "$95.7\\%$",
    "53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e": "BLEU-1, Meteor and Rouge-L",
    "869feb7f47606105005efdb6bea1c549824baea0": "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs",
    "c497e8701060583d91bb64b9f9202d40047effc4": "They crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets.",
    "8060a773f6a136944f7b59758d08cc6f2a59693b": "a small dataset",
    "1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306": "character error rate",
    "c0af8b7bf52dc15e0b33704822c4a34077e09cd1": "conventional RNNs and deep LSTM RNNs",
    "9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e": "determine text from the audio",
    "e0122fc7b0143d5cbcda2120be87a012fb987627": "68.8% to 71.8%",
    "5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4": "feed-forward neural model",
    "37edc25e39515ffc2d92115d2fcd9e6ceb18898b": "the winning system of the 2016 edition of the challenge BIBREF2",
    "e431661f17347607c3d3d9764928385a8f3d9650": "greatly",
    "876700622bd6811d903e65314ac75971bbe23dcc": "two different twitter sentiment classification problems",
    "312e9cc11b9036a6324bdcb64eca6814053ffa17": "From the context \"The inter-annotator agreement was measured using Cohen's kappa coefficient, which is a statistical measure of inter-rater agreement for categorical items. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than expected by chance. The kappa coefficient is a measure of agreement between two raters who each classify N items into C mutually exclusive categories. The kappa coefficient is defined as DISPLAYFORM0 where INLINEFORM0 is the observed agreement between the two raters, and INLINEFORM1 is the agreement expected by chance. The kappa coefficient is a number between -1 and 1. Values below 0 indicate less agreement than expected by chance, 0 indicates the agreement expected by chance, and values above 0 indicate greater agreement than",
    "1c0ba6958da09411deded4a14dfea5be55687619": "multiple patients",
    "1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b": "TF-IDF features",
    "d915b401bb96c9f104a0353bef9254672e6f5a47": "the last paragraph",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "",
    "664db503509b8236bc4d3dc39cebb74498365750": "outperforms previous models on BLEU score and is generally better on qualitative metrics",
    "64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8": "the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends",
    "b0a18628289146472aa42f992d0db85c200ec64b": "sentence level classification task and fragment level task",
    "72ce05546c81ada05885026470f4c8c218805055": "NLP",
    "5b551ba47d582f2e6467b1b91a8d4d6a30c343ec": "BLEU-1 and BLEU-4",
    "3cf1edfa6d53a236cf4258afd87c87c0a477e243": "English",
    "9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa": "plausible, personalized, and coherent recipes preferred by human evaluators for consumption",
    "34dc0838632d643f33c8dbfe7bd4b656586582a2": "name-based Nearest-Neighbor model (NN) and Encoder-Decoder baseline with ingredient attention (Enc-Dec)",
    "c77359fb9d3ef96965a9af0396b101f82a0a9de6": "scraped from Food.com",
    "1bdc990c7e948724ab04e70867675a334fdd3051": "Food.com",
    "78536da059b884d6ad04680baeb894895458055c": "Khandelwal and Sawant (BIBREF12)",
    "96b07373756d7854bccc3c12e8d41454ab8741f5": "No",
    "511517efc96edcd3e91e7783821c9d6d5a6562af": "BF and BA datasets",
    "9122de265577e8f6b5160cd7d28be9e22da752b2": "BIBREF12",
    "e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d": "SFU",
    "45be665a4504f0c7f458cf3f75a95d5a75eefd42": "BA and BF",
    "22b740cc3c8598247ee102279f96575bdb10d53f": "Yes",
    "74b4779de437c697fe702e51f23e2b0538b0f631": "by computing the likelihoods of surrounding context words given a phrase representation",
    "435570723b37ee1f5898c1a34ef86a0b2e8701bb": "translation systems with non-terminals only or the lexical",
    "aa2948209cc33b071dbf294822e72bb136678345": "AutoJudge consistently and significantly outperforms all the baselines, including RC models and other neural text classification models",
    "d9412dda3279729e95fcb35cbed09e61577a896e": "precision, recall, F1 and accuracy",
    "41b70699514703820435b00efbc3aac4dd67560a": "divorce proceedings",
    "e3c9e4bc7bb93461856e1f4354f33010bc7d28d5": "r-net BIBREF5 and AoA BIBREF6",
    "06cc8fcafc0880cf69a2514bb7341642b9833041": "INLINEFORM0",
    "d650101712e36594bd77b45930a990402a455222": "INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available",
    "cb384dc5366b693f28680374d31ff45356af0461": "Yes",
    "d41e20ec716b5904a272938e5a8f5f3f15a7779e": "by extracting all subject-verb-object tuples containing at least one target group label using the Spacy dependency parser",
    "0682bf049f96fa603d50f0fdad0b79a5c55f6c97": "Yes",
    "97d1ac71eed13d4f51f29aac0e1a554007907df8": "They use interval segment embeddings to distinguish multiple sentences within a document.",
    "c17b609b0b090d7e8f99de1445be04f8f66367d4": "ROUGE-1, ROUGE-2, and ROUGE-L F1 scores",
    "53014cfb506f6fffb22577bf580ae6f4d5317ce5": "ROUGE-1, ROUGE-2, and ROUGE-L F1 scores",
    "fa30a938b58fc05131c3854f12efe376cbad887f": "competitive results",
    "f875337f2ecd686cd7789e111174d0f14972638d": "the development portion of the Affective Text, the Affective test, the Fairy Tales dataset and the ISEAR dataset",
    "de53af4eddbc30c808d90b8a11a29217d377569e": "different pages (and therefore domains and stances), aiming at a balanced and varied dataset",
    "dac087e1328e65ca08f66d8b5307d6624bf3943f": "Yes",
    "a1645d0ba50e4c29f0feb806521093e7b1459081": "Social Honeypot dataset and it is high quality",
    "3cd185b7adc835e1c4449eff81222f5fc15c8500": "a novel feature extraction method",
    "f03112b868b658c954db62fc64430bebbaa7d9e0": "ROUGE",
    "5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4": "better than state-of-the-art extractive and abstractive systems",
    "a6d3e57de796172c236e33a6ceb4cca793dc2315": "LEAD",
    "395b61d368e8766014aa960fde0192e4196bcb85": "three datasets based on IMDB reviews and Yelp reviews",
    "92bb41cf7bd1f7886784796a8220ed5aa07bc49b": "the number of training examples for target classifier decreases from yelp200 (2M) to yelp50 (407K) to imdb400 (18K)",
    "4ef11518b40cc55d86c485f14e24732123b0d907": "FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4",
    "6a219d7c58451842aa5d6819a7cdf51c55e9fc0f": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese",
    "cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad": "a Transformer base architecture with 3 encoder layers, 3 decoder layers and 0.3 dropout",
    "f8f4e4a50d2b3fbd193327e79ea32d8d057e1414": "using the official train-development-test partition of the validated data",
    "bc84c5a58c57038910f7720d7a784560054d3e1a": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese",
    "29923a824c98b3ba85ced964a0e6a2af35758abe": "sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint",
    "559c68802ee2bb8b11e2188127418ca3a6155ba7": "Yes",
    "8dc707a0daf7bff61a97d9d854283e65c0c85064": "",
    "ffde866b1203a01580eb33237a0bb9da71c75ecf": "From the context \"We augment LibriSpeech with the TED-LIUM corpus BIBREF1, which contains 2,048 hours of speech from TED talks. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech and the official train-dev-test split of TED-LIUM. We use the official train-dev-test split of LibriSpeech",
    "6cd8bad8a031ce6d802ded90f9754088e0c8d653": "new state-of-the-art results",
    "30eacb4595014c9c0e5ee9669103d003cfdfe1e5": "the relation classification dataset of the SemEval 2010 task 8",
    "0f7867f888109b9e000ef68965df4dde2511a55f": "",
    "e2e977d7222654ee8d983fd8ba63b930e9a5a691": "",
    "0cfe0e33fbb100751fc0916001a5a19498ae8cb5": "From the context \"One of our contributions is a new input representation especially designed for relation classification. The contexts are split into three disjoint regions based on the two relation arguments",
    "35b3ce3a7499070e9b280f52e2cb0c29b0745380": "Yes",
    "71ba1b09bb03f5977d790d91702481cc406b3767": "the accuracy of the global majority class baseline and the target-wise majority class baseline",
    "612c3675b6c55b60ae6d24265ed8e20f62cb117e": "Yes",
    "bd40f33452da7711b65faaa248aca359b27fddb6": "state of the art",
    "787c4d4628eac00dbceb1c96020bff0090edca46": "`favor' and `against'",
    "3c3807f226ba72fc41f59f0338f12a49a0c35605": "Yes",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "double annotated",
    "81d607fc206198162faa54a796717c2805282d9b": "seven experts with legal training",
    "51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad": "BERT-based baselines",
    "f0848e7a339da0828278f6803ed7990366c975f0": "Yes",
    "b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8": "Yes",
    "792f6d76d2befba2af07198584aac1b189583ae4": "established task",
    "127d5ddfabec5c58832e5865cbd8ed0978c25a13": "",
    "b91671715ad4fad56c67c28ce6f29e180fe08595": "the effect of model size on the performance of these models",
    "a6d37b5975050da0b1959232ae756fc09e5f87e8": "",
    "e82fa03f1638a8c59ceb62bb9a6b41b498950e1f": "previous traditional supervised methods and recent neural-based methods",
    "7ab9c0b4ceca1c142ff068f85015a249b14282d0": "Yes",
    "00050f7365e317dc0487e282a4c33804b58b1fb3": "No",
    "c5b0ed5db65051eebd858beaf303809aa815e8e5": "BERT$_\\mathrm {BASE}$",
    "10fb7dc031075946153baf0a0599e126de29e3a4": "by constructing context-gloss pairs from glosses of all possible senses (in WordNet) of the target word",
    "e438445cf823893c841b2bc26cdce32ccc3f5cbe": "textual and visual features",
    "12f7fac818f0006cf33269c9eafd41bbb8979a48": "bi-directional LSTM model",
    "d5a8fd8bb48dd1f75927e874bdea582b4732a0cd": "Yes",
    "1097768b89f8bd28d6ef6443c94feb04c1a1318e": "Yes",
    "fc1679c714eab822431bbe96f0e9cf4079cd8b8d": "the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations",
    "23e2971c962bb6486bc0a66ff04242170dd22a1d": "textual and visual features complement each other",
    "c9bc6f53b941863e801280343afa14248521ce43": "code and data",
    "07b70b2b799b9efa630e8737df8b1dd1284f032c": "5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2M Stub articles",
    "71a0c4f19be4ce1b1bae58a6e8f2a586e125d074": "Wikipedia community",
    "c2eb743c9d0baf1781c3c0df9533fab588250af3": "natural language inference (NLI), paraphrase identification (PI), sentiment classification, analysis on gate values and experiments on model variants",
    "c35806cf68220b2b9bb082b62f493393b9bdff86": "the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and comparable results on MultiNLI and SST datasets",
    "f7d0fa52017a642a9f70091a252857fccca31f12": "natural language inference (NLI), paraphrase identification (PI), and sentiment classification",
    "01209a3bead7c87bcdc628be2a5a26b41abde9d1": "SNLI BIBREF22, MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24 and Stanford Sentiment Treebank (SST) BIBREF25",
    "2740e3d7d33173664c1c5ab292c7ec75ff6e0802": "WikiNews test set and Arabic Treebank",
    "db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21": "the best SOTA systems",
    "48bd71477d5f89333fa7ce5c4556e4d950fb16ed": "",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "segmentation and POS tagging and to determine stem-templates",
    "ad1be65c4f0655ac5c902d17f05454c0d4c4a15d": "",
    "2eb9280d72cde9de3aabbed993009a98a5fe0990": "$0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection",
    "154a721ccc1d425688942e22e75af711b423e086": "Amazon Mechanical Turk",
    "84bad9a821917cb96584cf5383c6d2a035358d7c": "asking participants to write a story about a certain scenario to a child and want to test if the child understood everything correctly",
    "c9305e5794b65b33399c22ac8e4e024f6b757a30": "ranked at 3rd position",
    "56b7319be68197727baa7d498fa38af0a8440fe4": "linguistic, layout and topical features",
    "2268c9044e868ba0a16e92d2063ada87f68b5d03": "Yes",
    "6b7354d7d715bad83183296ce2f3ddf2357cb449": "LSTM-CRF",
    "e949b28f6d1f20e18e82742e04d68158415dc61e": "ranked 1st and 2nd in FLC and SLC tasks, respectively",
    "a1ac2a152710335519c9a907eec60d9f468b19db": "sequence taggers",
    "ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25": "",
    "49eb52b3ec0647e165a5e41488088c80a20cc78f": "",
    "9bb7ae50bff91571a945c1af025ed2e67714a788": "BIBREF7",
    "81dbe9a9ddaa5d02b02e01a306d898015a56ffb6": "",
    "348886b4762db063711ef8b7a10952375fbdcb57": "Yes",
    "1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b": "Multi30K dataset",
    "f9aa055bf73185ba939dfb03454384810eb17ad1": "raw sentences from Wikipedias",
    "d571e0b0f402a3d36fb30d70cdcd2911df883bc7": "Yes",
    "ce2b921e4442a21555d65d8ce4ef7e3bde931dfc": "French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)",
    "2275b0e195cd9cb25f50c5c570da97a4cce5dca8": "within a day using one Tesla V100 16GB GPU",
    "37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6": "Cross-lingual Natural Language Inference (XNLI) and dependency parsing",
    "d01c51155e4719bf587d114bcd403b273c77246f": "XNLI and dependency parsing",
    "9b4dc790e4ff49562992aae4fad3a38621fadd8b": "manual evaluations of image content and bag-of-words representations derived from Flickr tags",
    "a1dac888f63c9efaf159d9bdfde7c938636f07b1": "the same structured datasets and same tag weighting scheme",
    "1e4dbfc556cf237accb8b370de2f164fa723687b": "semantic coverage",
    "fff5c24dca92bc7d5435a2600e6764f039551787": "StackExchange website",
    "b2ecfd5480a2a4be98730e2d646dfb84daedab17": "over 1,000,000 text-based adventure games, each with a unique story, setting, and characters",
    "a3efe43a72b76b8f5e5111b54393d00e6a5c97ab": "much fewer articles",
    "f1e90a553a4185a4b0299bd179f4f156df798bce": "four non-neural extractive models and CopyRNN",
    "19b7312cfdddb02c3d4eaa40301a67143a72a35a": "",
    "22744c3bc68f120669fc69490f8e539b09e34b94": "Yes",
    "dcea88698949da4a1bd00277c06df06c33f6a5ff": "The proposed task is a good proxy for the general-purpose sequence to sequence tasks",
    "d7b60abb0091246e29d1a9c28467de598e090c20": "the incidence of offensive comments",
    "bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e": "93.7%",
    "5a6926de13a8cc25ce687c22741ba97a6e63d4ee": "other political events",
    "dcc1115aeaf87118736e86f3e3eb85bf5541281c": "random forest classifier",
    "c74185bced810449c5f438f11ed6a578d1e359b4": "personal attack",
    "88e5d37617e14d6976cc602a168332fc23644f19": "the first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9 and the second dataset is constructed from the subreddit ChangeMyView (CMV)",
    "45f7c03a686b68179cadb1413c5f3c1d373328bd": "the CORD-19 dataset contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.",
    "a2015f02dfb376bf9b218d1c897018f4e70424d7": "over 45,000 scholarly articles, including over 33,000 with full text",
    "f697d00a82750b14376fe20a5a2b249e98bebe9b": "Bi-LSTM-CRF model",
    "e0e379e546f1da9da874a2e90c79b41c60feb817": "",
    "70148c8d0f345ea36200d5ba19d021924d98e759": "the well-studied phenomenon by which the perception of what we hear can be influenced by what we see",
    "27cf16bc9ef71761b9df6217f00f39f21130ce15": "Yes",
    "627b8d7b5b985394428c974aca5ba0c1bbbba377": "700",
    "126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0": "the data released by BIBREF11",
    "7e4ef0a4debc048b244b61b4f7dc2518b5b466c0": "VP ellipsis",
    "b68f72aed961d5ba152e9dc50345e1e832196a76": "0.7 BLEU",
    "cf874cd9023d901e10aa8664b813d32501e7e4d2": "Named Entity Recognition",
    "42084c41343e5a6ae58a22e5bfc5ce987b5173de": "Yes",
    "b637d6393ef3af7462917b81861531022b291933": "Yes",
    "8b9c12df9f89040f1485b3847a29f11b5c9262e0": "Python",
    "72e4e26d0dd79c590c28b10938952a9f9497ff1e": "object CNN, sentiment CNN, and scene CNN, seq2seq model with global attention",
    "63b92dcc701ec77fdb3355ede5d37d2fbf057bcc": "the prose generated is relevant to the painting, the model captures more than basic objects in the painting successfully using poetic clues in the scene, and the prose generated is perceived to be in the style of Shakespeare",
    "58ee0cbf1d8e3711c617b1cd3d7aca8620e26187": "the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.",
    "f71b52e00e0be80c926f153b9fe0a06dd93af11e": "3.7, 3.9, 3.9",
    "8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe": "MultiM-Poem(Ex)",
    "54e945ea4b014e11ed4e1e61abc2aa9e68fea310": "29.65",
    "df0257ab04686ddf1c6c4d9b0529a7632330b98e": "Go-Explore finds winning trajectories faster than existing solutions",
    "568fb7989a133564d84911e7cb58e4d8748243ef": "Go-Explore",
    "2c947447d81252397839d58c75ebcc71b34379b5": "the 4,440 games of \u201cFirst TextWorld Problems\u201d BIBREF14",
    "c01784b995f6594fdb23d7b62f20a35ae73eaa77": "by comparing the stronger performance on unseen games",
    "3415762847ed13acc3c90de60e3ef42612bc49af": "improved",
    "223dc2b9ea34addc0f502003c2e1c1141f6b36a7": "reward learning algorithm from RL",
    "e1ab11885f72b4658263a60751d956ba661c1d61": "feed-forward neural network, an LSTM network and an SVM regressor",
    "c85b6f9bafc4c64fc538108ab40a0590a2f5768e": "second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc)",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "using the machine translation platform Apertium",
    "0f6216b9e4e59252b0c1adfd1a848635437dfcdc": "a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment",
    "22ccee453e37536ddb0c1c1d17b0dbac04c6c607": "English",
    "d00bbeda2a45495e6261548710afa6b21ea32870": "querying certain emotion-related words",
    "71b1af123fe292fd9950b8439db834212f0b0e32": "a carefully designed and rigorous language-agnostic translation and annotation protocol",
    "a616a3f0d244368ec588f04dfbc37d77fda01b4c": "Mandarin, Russian, French, Kiswahili, Welsh, and Yue Chinese",
    "8e44c02c2d9fa56fb74ace35ee70a5add50b52ae": "",
    "1522ccedbb1f668958f24cca070f640274bc2549": "based on the concept map's ability to support a user in navigating a document collection",
    "97466a37525536086ed5d6e5ed143df085682318": "provided",
    "e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a": "a description of the document cluster's topic along with the propositions",
    "d6191c4643201262a770947fc95a613f57bedb6b": "49 clusters of 100 web pages on educational topics",
    "ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8": "a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the documents, making the summarization task extractive. A concept can be an entity, abstract idea, event or activity, designated by its unique label. Good maps should be propositionally coherent, meaning that every relation together with the two connected concepts form a meaningful proposition.",
    "fc4ae12576ea3a85ea6d150b46938890d63a7d18": "Yes",
    "19cf7884c0c509c189b1e74fe92c149ff59e444b": "from the above approximated joint probability",
    "ecd5770cf8cb12cb34285e26ab834301c17c53e1": "based on the observation that answer agreement often arises when 1) a lay person's attention can be easily concentrated to a single, undisputed region in an image and 2) a lay person would find the requested task easy to address",
    "4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729": "Training questions 2015 v1.0",
    "5529f26f72ce47440c2a64248063a6d5892b9fde": "loss function analysis",
    "f85ca6135b101736f5c16c5b5d40895280016023": "existing MT models for query translation",
    "5fa36dc8f7c4e65acb962fc484989d20b8fdaeec": "Yes",
    "d98847340e46ffe381992f1a594e75d3fb8d385e": "DL and ML",
    "7006c66a15477b917656f435d66f63760d33a304": "outperforms the ABUS",
    "a15bc19674d48cd9919ad1cf152bf49c88f4417d": "dialogues between real users and an SDS in a restaurant recommendation domain",
    "440faf8d0af8291d324977ad0f68c8d661fe365e": "Reuters-21578 dataset",
    "0ec56e15005a627d0b478a67fd627a9d85c3920e": "a set of word vectors",
    "a712718e6596ba946f29a99838d82f95b9ebb1ce": "0.1017 and 0.1229",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "TF-IDF features, Doc2vec model",
    "dfca00be3284cc555a6a4eac4831471fb1f5875b": "around 15 samples for each term-sense pair",
    "a9a532399237b514c1227f2d6be8601474e669be": "UM Inventory BIBREF5",
    "26126068d72408555bcb52977cd669faf660bdf7": "",
    "660284b0a21fe3801e64dc9e0e51da5400223fe3": "GM$\\_$KL achieves significantly better correlation score than w2g and w2gm approaches",
    "c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd": "ASR system",
    "f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3": "to hypothesize that gender performance difference observed for Punctual speakers is due to the fact that",
    "a253749e3b4c4f340778235f640ce694642a4555": "ESTER1, ESTER2, ETAPE and REPERE",
    "1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891": "two",
    "777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47": "number of speakers, number of utterances (or speech turns), and turn lengths",
    "2da4c3679111dd92a1d0869dae353ebe5989dfd2": "ESTER1, ESTER2, ETAPE and REPERE",
    "b7c3f3942a07c118e57130bc4c3ec4adc431d725": "Universal Langage Model Fine-tuning for Text Classification (ULMFiT) presented in Howard and Ruder BIBREF3",
    "a5505e25ee9ae84090e1442034ddbb3cedabcf04": "0.8099 and 0.8083",
    "1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf": "Yes",
    "7fa3c2c0cf7f559d43e84076a9113a390c5ba03a": "Yes",
    "9a7ba5ed1779c664d2cac92494a43517d3e87c96": "the data that is not explicit but can be inferred from background knowledge",
    "662870a90890c620a964720b2ca122a1139410ea": "INLINEFORM6",
    "92d1a6df3041667dc662376938bc65527a5a1c3c": "Yes",
    "12159f04e0427fe33fa05af6ba8c950f1a5ce5ea": "k-means clusters, k-means++, INLINEFORM1, INLINEFORM2, seed initialization",
    "a4a1fcef760b133e9aa876ac28145ad98a609927": "number of clusters, number of iterations and seed initialization",
    "63bb2040fa107c5296351c2b5f0312336dad2863": "k-means++",
    "01f4a0a19467947a8f3bdd7ec9fac75b5222d710": "INLINEFORM0 scores",
    "7784d321ccc64db5141113b6783e4ba92fdd4b20": "parameterizing a PCFG's rule probabilities with neural networks over distributed representations and compound PCFG\u2014continuous mixture of PCFGs",
    "218615a005f7f00606223005fef22c07057d9d77": "Penn Treebank (PTB) and Chinese Treebank (CTB)",
    "867290103f762e1ddfa6f2ea30dd0a327f595182": "CTB",
    "907b3af3cfaf68fe188de9467ed1260e52ec6cf1": "viral tweets labelled as containing fake news appear to include more URLs to other sites than viral tweets that do not contain fake news",
    "56a8826cbee49560592b2d4b47b18ada236a12b9": "manually annotated",
    "968b7c3553a668ba88da105eff067d57f393c63f": "retweeted more than 1000 times",
    "f03df5d99b753dc4833ef27b32bb95ba53d790ee": "exposure, characteristics of accounts spreading fake news and the tone of the content",
    "a8f51b4e334a917702422782329d97304a2fe139": "1000",
    "dca86fbe1d57b44986055b282a03c15ef7882e51": "a single person",
    "27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa": "Transformer model",
    "b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72": "ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era",
    "808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc": "45,821 characters",
    "36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd": "using a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise.",
    "f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6": "outperformance",
    "357eb9f0c07fa45e482d998a8268bd737beb827f": "Poly-encoder, Feed Yourself and Kvmemnn",
    "ad08b215dca538930ef1f50b4e49cd25527028ad": "Yes",
    "31101dc9937f108e27e08a5f34be44f0090b8b6b": "cosine similarity",
    "e4a315e9c190cf96493eefe04ce4ba6ae6894550": "a ranking model that learns to score conversational replies and images in a given conversational context",
    "6263b2cba18207474786b303852d2f0d7068d4b6": "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)",
    "c1c44fd96c3fa6e16949ae8fa453e511c6435c68": "enhance the decoder using BERT's contextualized representations",
    "d28d86524292506d4b24ae2d486725a6d57a3db3": "33.33",
    "feafcc1c4026d7f55a2c8ce7850d7e12030b5c22": "end-to-end",
    "63488da6c7aff9e374561a24ba224e9ce7f65e40": "",
    "c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c": "Yes",
    "a9337636b52de375c852682a2561af2c1db5ec63": "Yes",
    "45a5961a4e1d1c22874c4918e5c98bd3c0a670b3": "seven",
    "30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1": "",
    "5c6fa86757410aee6f5a0762328637de03a569e9": "DNN models can be used for cyberbullying detection on various topics across multiple SMPs using three datasets and four DNN models. These models coupled with transfer learning beat state of the art results for all three datasets.",
    "7e38e0279a620d3df05ab9b5e2795044f18d4471": "personal attack, racism, and sexism",
    "8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92": "No",
    "03ebb29c08375afc42a957c7b2dc1a42bed7b713": "the obtained results",
    "9cf070d6671ee4a6353f79a165aa648309e01295": "1500 sentences",
    "87bc6f83f7f90df3c6c37659139b92657c3f7a38": "The DD algorithm tries to enforce agreement between the two parse trees subject to the given alignments.",
    "01e2d10178347d177519f792f86f25575106ddc7": "Switchboard, Turkish, Uzbek, and Mandarin",
    "021bfb7e180d67112b74f05ecb3fa13acc036c86": "UTD",
    "d201b9992809142fe59ae74508bc576f8ca538ff": "Yes",
    "c4628d965983934d7a2a9797a2de6a411629d5bc": "disease prediction and drug-drug interaction prediction",
    "bd419f4094186a5ce74ba6ac1622b24e29e553f4": "30.3% on single sentences and 0.3 on complete paragraphs",
    "11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af": "NO-MOVE, RANDOM and JUMP",
    "1269c5d8f61e821ee0029080c5ba2500421d5fa6": "data augmentation methods",
    "e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb": "Recurrent NMT",
    "b9ea841b817ba23281c95c7a769873b840dee8d5": "Yes",
    "219af68afeaecabdfd279f439f10ba7c231736e4": "TED talks extracted from WIT3's corpus BIBREF15",
    "a66a275a817f980c36e0b67d2e00bd823f63abf8": "the proportions of sentence pairs",
    "b6f466e0fdcb310ecd212fd90396d9d13e0c0504": "the data already contain errors",
    "62ea141d0fb342dfb97c69b49d1c978665b93b3c": "punctuation, word order mistakes and grammatical errors",
    "a32c792a0cef03218bf66322245677fc2d5e5a31": "The parallel data differ in terms of style.",
    "0101ebfbaba75fd47868ad0c796ac44ebc19c566": "padding sentence number in each passage to 101, all word number in each sentence to 100. Word number of queries and choices were padded to 50.",
    "50cb50657572e315fd452a89f3e0be465094b66f": "MovieQA dataset",
    "981fd79dd69581659cb1d4e2b29178e82681eb4d": "The proposed model has three components",
    "03e9ac1a2d90152cd041342a11293a1ebd33bcc3": "NLG datasets",
    "ef396a34436072cb3c40b0c9bc9179fee4a168ae": "many natural language processing (NLP) tasks",
    "04bde1d2b445f971e97bb46ade2d0290981c7a32": "context-specific compositional functions for RecNN and TreeLSTM",
    "bfbd6040cb95b179118557352e8e3899ef25c525": "singer embedding and the extracted pitch",
    "d6e353e0231d09fd5dcba493544d53706f3fe1ab": "Mean Opinion Score (MOS)",
    "7bd6a6ec230e1efb27d691762cc0674237dc7967": "PTB and WT-2 datasets",
    "6aaf12505add25dd133c7b0dafe8f4fe966d1f1d": "LSTM",
    "73906462bd3415f23d6378590a5ba28709b17605": "translating different parts of each instance separately can alter superficial patterns in the data",
    "5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11": "English and 100 languages",
    "88bf368491f9613767f696f84b4bb1f5a7d7cb48": "professional translation",
    "0737954caf66f2b4c898b356d2a3c43748b9706b": "translating the premise and hypothesis together",
    "664b3eadc12c8dde309e8bbd59e9af961a433cde": "not statistically significant",
    "b3307d5b68c57a074c483636affee41054be06d1": "translating different parts of each instance separately can alter superficial patterns in the data",
    "bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31": "English and 100 languages",
    "12d7055baf5bffb6e9e95e977c000ef2e77a4362": "higher quality",
    "498c0229f831c82a5eb494cdb3547452112a66a0": "They recruited five medical experts via Up-work with advanced medical training and strong technical reading/writing skills. The expert annotator were asked to read the entire abstract and highlight, using the BRAT toolkit BIBREF26, all spans describing medical Interventions. Each abstract is only annotated by one expert. They examined 30 re-annotated abstracts to ensure the annotation quality before hiring the annotator.",
    "8c48c726bb17a17d70ab29db4d65a93030dd5382": "2000 abstracts",
    "89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f": "sentence representations",
    "06b5272774ec43ee5facfa7111033386f06cf448": "sentence",
    "08b57deb237f15061e4029b6718f1393fa26acce": "located in the US and hired on the BIBREF22 platform",
    "9b7655d39c7a19a23eb8944568eb5618042b9026": "NLTK, Stanford CoreNLP, and TwitterNLP",
    "cd06d775f491b4a17c9d616a8729fd45aa2e79bf": "neutral",
    "1329280df5ee9e902b2742bde4a97bc3e6573ff3": "No",
    "58c6737070ef559e9220a8d08adc481fdcd53a24": "correct classification rate (CCR)",
    "0af16b164db20d8569df4ce688d5a62c861ace0b": "traditional baselines",
    "78a4ec72d76f0a736a4a01369a42b092922203b6": "the completed scripts of all ten seasons of Friends TV shows from emorynlp",
    "6a14379fee26a39631aebd0e14511ce3756e42ad": "",
    "81588e0e207303c2867c896f3911a54a1ef7c874": "the scripts of the Friends TV sitcom and Facebook messenger chats",
    "dd09db5eb321083dba16c2550676e60682f9a0cd": "Joy, Sadness, Anger, and Neutral",
    "40c0f97c3547232d6aa039fcb330f142668dea4b": "three datasets",
    "777217e025132ddc173cf33747ee590628a8f62f": "ArXiv papers by section, U.S. Congressional speeches by home state and political party, market basket data from a large grocery store, grouped by season",
    "2dbf6fe095cd879a9bf40f110b7b72c8bdde9475": "hierarchical modelling approach",
    "7d483077ed7f2f504d59f4fc2f162741fa5ac23b": "co-purchase patterns of items can vary across seasons of the year",
    "de830c534c23f103288c198eb19174c76bfd38a1": "1.10artificial and 1.10ai",
    "b0d66760829f111b8fad0bd81ca331ddd943ef41": "From the context \"In the future, we plan to explore the following directions",
    "ae7c93646aa5f3206cd759904965b4d484d12f83": "our model produces relevant headlines",
    "d1ec42b2b5a3c956ff528543636e024bfde5e5ba": "Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN",
    "1dac4bc5af239024566fcb0f43bb9ff1c248ecec": "Yes",
    "3bf0306e9bd044f723e38170c13455877b2aeec3": "classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\\text",
    "2858620e0498db2f2224bfbed5263432f0570832": "using a different graph to separately encode each sentence",
    "545e92833b0ad4ba32eac5997edecf97a366a244": "the single document graph",
    "cb12c19f9d14bef7b2f778892d9071eea2d6c63d": "MPAD",
    "9193006f359c53eb937deff1248ee3317978e576": "Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2 and Yelp2013",
    "bc67b91dd73acded2d52fd4fee732b7a9722ea8b": "recursive neighborhood aggregation",
    "49c32a2a64eb41381e5f12ccea4150cac9f3303d": "accuracy of polarity annotation",
    "bbb77f2d6685c9257763ca38afaaef29044b4018": "Average Fixation Duration per Word and Largest Regression Position",
    "22732cb9476e521452bf0538f3fdb94cf3867651": "emoticons, laughter expressions such as \u201clol\u201d etc",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "Unigrams and Pragmatic features, Stylistic patterns and patterns related to situational disparity, Hastag interpretations",
    "74b338d5352fe1a6fd592e38269a4c81fe79b866": "lexical,implicit incongruity, explicit incongruity, readability and word count",
    "d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf": "several frequently used computational approaches and their limitations and strengths",
    "f903396d943541a8cc65edefb04ca37814ed30dd": "data from a particular social media platform",
    "ba28ce9a2f7e8524243adf288cc3f11055e667bb": "Yes",
    "975e60535724f4149c7488699a199ba2920a062c": "computer science, linguistics, and the humanities",
    "b970f48d30775d3468952795bc72976baab3438e": "opportunities and complications that often arise"
}