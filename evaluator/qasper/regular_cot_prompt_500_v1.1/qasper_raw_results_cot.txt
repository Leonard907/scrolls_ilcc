Validation ID: b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54
Question: which multilingual approaches do they compare with?
From the context "We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.", so we know that they compare with pivoting, multilingual NMT (MNMT) and cross-lingual transfer without pretraining.
Answer: pivoting, multilingual NMT (MNMT) and cross-lingual transfer without pretraining
True answer: BIBREF19, BIBREF20

Validation ID: b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54
Question: which multilingual approaches do they compare with?
From the context "We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.", so we know that they compare with pivoting, multilingual NMT (MNMT) and cross-lingual transfer without pretraining.
Answer: pivoting, multilingual NMT (MNMT) and cross-lingual transfer without pretraining
True answer: multilingual NMT (MNMT) BIBREF19

Validation ID: f5e6f43454332e0521a778db0b769481e23e7682
Question: what are the pivot-based baselines?
From the context "We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting.", so we know that the pivot-based baselines are pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.
Answer: pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16
True answer: pivoting, pivoting$_{\rm m}$

Validation ID: f5e6f43454332e0521a778db0b769481e23e7682
Question: what are the pivot-based baselines?
From the context "We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.", so we know that the pivot-based baselines are pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.
Answer: pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16
True answer: firstly translates a source language into the pivot language which is later translated to the target language

Validation ID: 9a05a5f4351db75da371f7ac12eb0b03607c4b87
Question: which datasets did they experiment with?
From the context "Experiments on public datasets show that our approaches significantly outperforms several strong baseline systems, and manifest the language invariance characteristics in both sentence level and word level neural representations.", so we know that they experiment with public datasets.
From the context "We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.", so we know that they experiment with Europarl and MultiUN datasets.
True answer: Europarl, MultiUN

Validation ID: 9a05a5f4351db75da371f7ac12eb0b03607c4b87
Question: which datasets did they experiment with?
From the context "Experiments on public datasets show that our approaches significantly outperforms several strong baseline systems, and manifest the language invariance characteristics in both sentence level and word level neural representations.", so we know that they experiment with public datasets.
From the context "We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.", so we know that they experiment with Europarl and MultiUN datasets.
True answer: Europarl BIBREF31, MultiUN BIBREF32

Validation ID: 5eda469a8a77f028d0c5f1acd296111085614537
Question: what language pairs are explored?
From the context "We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.", so we know that the language pairs explored are Europarl and MultiUN.
Answer: Europarl and MultiUN
True answer: De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru

Validation ID: 5eda469a8a77f028d0c5f1acd296111085614537
Question: what language pairs are explored?
From the context "We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.", so we know that the language pairs explored are Europarl and MultiUN.
Answer: Europarl and MultiUN
True answer: French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation

Validation ID: 18c5d366b1da8447b5404eab71f4cc658ba12e6f
Question: what ner models were evaluated?
From the context "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.", so we know that Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF were evaluated.
Answer: Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF
True answer: Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer

Validation ID: 18c5d366b1da8447b5404eab71f4cc658ba12e6f
Question: what ner models were evaluated?
From the context "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15.", so we know that Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF were evaluated.
Answer: Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF
True answer: Stanford NER, spaCy 2.0, recurrent model with a CRF top layer

Validation ID: b5e4866f0685299f1d7af267bbcc4afe2aab806f
Question: what is the source of the news sentences?
From the context "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am", so we know that the source of the news sentences is ilur.am.
Answer: ilur.am
True answer: ilur.am

Validation ID: b5e4866f0685299f1d7af267bbcc4afe2aab806f
Question: what is the source of the news sentences?
From the context "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am", so we know that the source of the news sentences is ilur.am.
Answer: ilur.am
True answer: links between Wikipedia articles to generate sequences of named-entity annotated tokens

Validation ID: 1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590
Question: did they use a crowdsourcing platform for manual annotations?
From the context "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am", so we know that they used a crowdsourcing platform for manual annotations.
Answer: crowdsourcing platform
True answer: No

Validation ID: 1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590
Question: did they use a crowdsourcing platform for manual annotations?
From the context "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am", so we know that they used a crowdsourcing platform for manual annotations.
Answer: crowdsourcing platform
True answer: No

Validation ID: b6ae8e10c6a0d34c834f18f66ab730b670fb528c
Question: what are the topics pulled from Reddit?
From the context "We sample 1000 Reddit users who posted at least once a year between 2007 and 2015 to construct a corpus of 10 million posts that constitute their entire post history.", so we know that the topics pulled from Reddit are politics, business, science, AskReddit, and the Reddit frontpage.
Answer: politics, business, science, AskReddit, and the Reddit frontpage
True answer: politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. 

Validation ID: b6ae8e10c6a0d34c834f18f66ab730b670fb528c
Question: what are the topics pulled from Reddit?
From the context "We sample 1000 Reddit users who posted at least once a year between 2007 and 2015 to construct a corpus of 10 million posts that constitute their entire post history.", so we know that the topics pulled from Reddit are politics, business, science, AskReddit, and the Reddit frontpage.
Answer: politics, business, science, AskReddit, and the Reddit frontpage
True answer: training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit

Validation ID: a87a009c242d57c51fc94fe312af5e02070f898b
Question: What predictive model do they build?
From the context "We apply this model to Reddit, where we discover behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users.", so we know that they build a predictive model to discover behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users.
Answer: A predictive model to discover behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users.
True answer: logistic regression models

Validation ID: a87a009c242d57c51fc94fe312af5e02070f898b
Question: What predictive model do they build?
From the context "We apply this model to Reddit, where we discover behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users.", so we know that they build a predictive model to discover behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users.
Answer: A predictive model to discover behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users.
True answer: logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.

Validation ID: ef4dba073d24042f24886580ae77add5326f2130
Question: What accuracy does the proposed system achieve?
From the context "The results show that our system outperforms very strong baseline systems", so we know that the accuracy of the proposed system is higher than the baseline systems.
Answer: higher
True answer: F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data 

Validation ID: ef4dba073d24042f24886580ae77add5326f2130
Question: What accuracy does the proposed system achieve?
From the context "The results show that our system outperforms very strong baseline systems", so we know that the accuracy of the proposed system is higher than the baseline systems.
Answer: higher
True answer: F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)

Validation ID: 2df4a045a9cd7b44874340b6fdf9308d3c55327a
Question: What crowdsourcing platform is used?
From the context "We create two Chinese NER datasets in two domains: dialog and e-commerce. We require the crowd annotators to label the types of entities, including person, song, brand, product, and so on. Identifying these entities is useful for chatbot and e-commerce platforms BIBREF8.", so we know that the crowdsourcing platform is Amazon Mechanical Turk.
Answer: Amazon Mechanical Turk
True answer: Unanswerable

Validation ID: 2df4a045a9cd7b44874340b6fdf9308d3c55327a
Question: What crowdsourcing platform is used?
From the context "We create two Chinese NER datasets in two domains: dialog and e-commerce. We require the crowd annotators to label the types of entities, including person, song, brand, product, and so on. Identifying these entities is useful for chatbot and e-commerce platforms BIBREF8.", so we know that the crowdsourcing platform is Amazon Mechanical Turk.
Answer: Amazon Mechanical Turk
True answer: They did not use any platform, instead they hired undergraduate students to do the annotation.

Validation ID: a313e98994fc039a82aa2447c411dda92c65a470
Question: How do they match words before reordering them?
From the context "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17, and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28. These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29.", so we know that they use CFILT-preorder system for reordering English sentences to match the Indian language word order.
Answer: CFILT-preorder system
True answer: Unanswerable

Validation ID: a313e98994fc039a82aa2447c411dda92c65a470
Question: How do they match words before reordering them?
From the context "We use the CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages BIBREF17, and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering BIBREF28. These Hindi-tuned rules have been found to improve reordering for many English to Indian language pairs BIBREF29.", so we know that they use CFILT-preorder system for reordering English sentences to match the Indian language word order.
Answer: CFILT-preorder system
True answer: CFILT-preorder system

Validation ID: 37861be6aecd9242c4fdccdfcd06e48f3f1f8f81
Question: On how many language pairs do they show that preordering assisting language sentences helps translation quality?
From the context "We show that pre-ordering the assisting language to match the word order of the source language significantly improves translation quality in an extremely low-resource setting. While the current work focused on Indian languages, we would like to validate the hypothesis on a more diverse set of languages.", so we know that they show that preordering assisting language sentences helps translation quality on Indian languages.
Answer: Indian languages
True answer: 5

Validation ID: 37861be6aecd9242c4fdccdfcd06e48f3f1f8f81
Question: On how many language pairs do they show that preordering assisting language sentences helps translation quality?
From the context "We show that pre-ordering the assisting language to match the word order of the source language significantly improves translation quality in an extremely low-resource setting. While the current work focused on Indian languages, we would like to validate the hypothesis on a more diverse set of languages.", so we know that they show that preordering assisting language sentences helps translation quality on Indian languages.
Answer: Indian languages
True answer: Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.

Validation ID: 7e62a53823aba08bc26b2812db016f5ce6159565
Question: Which dataset(s) do they experiment with?
From the context "For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus BIBREF22 ( INLINEFORM0 sentences from the training set) and the ILCI English-Hindi parallel corpus ( INLINEFORM1 sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus BIBREF23 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use INLINEFORM2 sentences from ILCI corpus as the test set.", so we know that they experiment with IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus.
Answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus
True answer: IITB English-Hindi parallel corpus BIBREF22, ILCI English-Hindi parallel corpus

Validation ID: 7e62a53823aba08bc26b2812db016f5ce6159565
Question: Which dataset(s) do they experiment with?
From the context "For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus BIBREF22 ( INLINEFORM0 sentences from the training set) and the ILCI English-Hindi parallel corpus ( INLINEFORM1 sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus BIBREF23 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use INLINEFORM2 sentences from ILCI corpus as the test set.", so we know that they experiment with IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus.
Answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus
True answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus

Validation ID: 9eabb54c2408dac24f00f92cf1061258c7ea2e1a
Question: Which information about text structure is included in the corpus?
From the context "Information on text structure, typography, and images can also be leveraged as part of a neural machine translation approach to text simplification", so we know that the information about text structure is included in the corpus.
Answer: Information about text structure
True answer: paragraphs, lines, Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation

Validation ID: 9eabb54c2408dac24f00f92cf1061258c7ea2e1a
Question: Which information about text structure is included in the corpus?
From the context "Information on text structure, typography, and images can also be leveraged as part of a neural machine translation approach to text simplification", so we know that the information about text structure is included in the corpus.
Answer: Information about text structure
True answer: paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))

Validation ID: 3d013f15796ae7fed5272183a166c45f16e24e39
Question: Which information about typography is included in the corpus?
From the context "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", so we know that the information about typography is font type and font style.
Answer: font type and font style
True answer: font type, font style, Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page

Validation ID: 3d013f15796ae7fed5272183a166c45f16e24e39
Question: Which information about typography is included in the corpus?
From the context "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)", so we know that the information about typography is font type and font style.
Answer: font type and font style
True answer: font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer, A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer

Validation ID: 9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc
Question: On which benchmarks they achieve the state of the art?
From the context "We compare our system with two published baselines (1) STAG", so we know that the benchmarks are STAG.
Answer: STAG
True answer: SimpleQuestions, WebQSP

Validation ID: 9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc
Question: On which benchmarks they achieve the state of the art?
From the context "We compare our system with two published baselines (1) STAG", so we know that the benchmarks are STAG.
Answer: STAG
True answer: WebQSP, SimpleQuestions

Validation ID: d3aa0449708cc861a51551b128d73e11d62207d2
Question: What they use in their propsoed framework?
From the context "We use the same entity linker as BIBREF4, which is a state-of-the-art entity linker on WebQSP", so we know that they use the same entity linker as BIBREF4.
Answer: The same entity linker as BIBREF4
True answer: break the relation names into word sequences,  relation-level and word-level relation representations, bidirectional LSTMs (BiLSTMs),  residual learning method

Validation ID: d3aa0449708cc861a51551b128d73e11d62207d2
Question: What they use in their propsoed framework?
From the context "We use the same entity linker as BIBREF4, which is a state-of-the-art entity linker on WebQSP", so we know that they use the same entity linker as BIBREF4.
Answer: The same entity linker as BIBREF4
True answer: break the relation names into word sequences for question-relation matching, build both relation-level and word-level relation representations, use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations, residual learning method for sequence matching, a simple KBQA implementation composed of two-step relation detection

Validation ID: cfbec1ef032ac968560a7c76dec70faf1269b27c
Question: What does KBQA abbreviate for?
From the context "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples", so we know that KBQA abbreviates for Knowledge Base Question Answering.
Answer: Knowledge Base Question Answering
True answer: Knowledge Base Question Answering

Validation ID: cfbec1ef032ac968560a7c76dec70faf1269b27c
Question: What does KBQA abbreviate for?
From the context "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples", so we know that KBQA abbreviates for Knowledge Base Question Answering.
Answer: Knowledge Base Question Answering
True answer: Knowledge Base Question Answering 

Validation ID: c0e341c4d2253eb42c8840381b082aae274eddad
Question: What is te core component for KBQA?
From the context "Our KBQA system takes an existing entity linker to produce the top- $K$ linked entities, $EL_K(q)$, for a question $q$ (“initial entity linking”). Then we generate the KB queries for $q$ following the four steps illustrated in Algorithm "KBQA Enhanced by Relation Detection".
[htbp] InputInput OutputOutput Top query tuple $(\hat{e},\hat{r}, \lbrace (c, r_c)\rbrace )$ Entity Re-Ranking (first-step relation detection): Use the raw question text as input for a relation detector to score all relations in the KB that are associated to the entities in $EL_K(q)$ ; use the relation scores to re-rank $EL_K(q)$ and generate a shorter list $EL^{\prime }_{K^{\prime }}(q)$ containing the top- $K^{\prime }$ entity candidates (Section "Entity Re-Ranking" ) Relation Detection: Detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token $<$ e $>$ (Section "Relation Detection" ) Query Generation: Combine the scores from step 1 and 2, and select the top pair $(\hat{e},\hat{r})$ (Section "Query Generation" ) Constraint Detection (optional): Compute similarity between $q$ and any neighbor entity $c$ of the entities along $EL_K(q)$0 (connecting by a relation $EL_K(q)$1 ), add the high scoring $EL_K(q)$2 and $EL_K(q)$3 to the query (Section "Constraint Detection" ). KBQA with two-step relation detection
True answer: answer questions by obtaining information from KB tuples 

Validation ID: c0e341c4d2253eb42c8840381b082aae274eddad
Question: What is te core component for KBQA?
From the context "Our KBQA system takes an existing entity linker to produce the top- $K$ linked entities, $EL_K(q)$, for a question $q$ (“initial entity linking”). Then we generate the KB queries for $q$ following the four steps illustrated in Algorithm "KBQA Enhanced by Relation Detection".
[htbp] InputInput OutputOutput Top query tuple $(\hat{e},\hat{r}, \lbrace (c, r_c)\rbrace )$ Entity Re-Ranking (first-step relation detection): Use the raw question text as input for a relation detector to score all relations in the KB that are associated to the entities in $EL_K(q)$ ; use the relation scores to re-rank $EL_K(q)$ and generate a shorter list $EL^{\prime }_{K^{\prime }}(q)$ containing the top- $K^{\prime }$ entity candidates (Section "Entity Re-Ranking" ) Relation Detection: Detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token $<$ e $>$ (Section "Relation Detection" ) Query Generation: Combine the scores from step 1 and 2, and select the top pair $(\hat{e},\hat{r})$ (Section "Query Generation" ) Constraint Detection (optional): Compute similarity between $q$ and any neighbor entity $c$ of the entities along $EL_K(q)$0 (connecting by a relation $EL_K(q)$1 ), add the high scoring $EL_K(q)$2 and $EL_K(q)$3 to the query (Section "Constraint Detection" ). KBQA with two-step relation detection
True answer: hierarchical matching between questions and relations with residual learning

Validation ID: 1ec152119cf756b16191b236c85522afeed11f59
Question: What experiments are proposed to test that upper layers produce context-specific embeddings?
From the context "We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.", so we know that the experiments are self-similarity, intra-sentence similarity, and maximum explainable variance.
Answer: self-similarity, intra-sentence similarity, and maximum explainable variance
True answer: They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.

Validation ID: 1ec152119cf756b16191b236c85522afeed11f59
Question: What experiments are proposed to test that upper layers produce context-specific embeddings?
From the context "We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.", so we know that the experiments are self-similarity, intra-sentence similarity, and maximum explainable variance.
Answer: self-similarity, intra-sentence similarity, and maximum explainable variance
True answer: They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  
They plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.
They plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another.

Validation ID: 891c2001d6baaaf0da4e65b647402acac621a7d2
Question: How do they calculate a static embedding for each word?
From the context "We ultimately found that after adjusting for anisotropy, on average, less than 5% of the variance in a word's contextualized representations could be explained by a static embedding. This means that even in the best-case scenario, in all layers of all models, static word embeddings would be a poor replacement for contextualized ones.", so we know that they calculate a static embedding for each word by taking the first principal component of its contextualized representations.
Answer: By taking the first principal component of its contextualized representations
True answer: They use the first principal component of a word's contextualized representation in a given layer as its static embedding.

Validation ID: 891c2001d6baaaf0da4e65b647402acac621a7d2
Question: How do they calculate a static embedding for each word?
From the context "We ultimately found that after adjusting for anisotropy, on average, less than 5% of the variance in a word's contextualized representations could be explained by a static embedding. This means that even in the best-case scenario, in all layers of all models, static word embeddings would be a poor replacement for contextualized ones.", so we know that they calculate a static embedding for each word by taking the first principal component of its contextualized representations.
Answer: By taking the first principal component of its contextualized representations
True answer:  by taking the first principal component (PC) of its contextualized representations in a given layer

Validation ID: 66c96c297c2cffdf5013bab5e95b59101cb38655
Question: What is the performance of BERT on the task?
From the context "The results of the experiments show that, in NUBes-PHI, the BERT-based model outperforms the other systems without requiring any adaptation or domain-specific feature engineering, just by being trained on the provided labelled data. Interestingly, the BERT-based model obtains a remarkably higher recall than the other systems. High recall is a desirable outcome because, when anonymising sensible documents, the accidental leak of sensible data is likely to be more dangerous than the unintended over-obfuscation of non-sensitive text.", so we know that the performance of BERT on the task is high recall.
Answer: high recall
True answer: F1 scores are:
HUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)
Medoccan: Detection(0.972), Classification (0.967)

Validation ID: 66c96c297c2cffdf5013bab5e95b59101cb38655
Question: What is the performance of BERT on the task?
From the context "The results of the experiments show that, in NUBes-PHI, the BERT-based model outperforms the other systems without requiring any adaptation or domain-specific feature engineering, just by being trained on the provided labelled data. Interestingly, the BERT-based model obtains a remarkably higher recall than the other systems. High recall is a desirable outcome because, when anonymising sensible documents, the accidental leak of sensible data is likely to be more dangerous than the unintended over-obfuscation of non-sensitive text.", so we know that the performance of BERT on the task is high recall.
Answer: high recall
True answer: BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table 

Validation ID: 6b53e1f46ae4ba9b75117fc6e593abded89366be
Question: What are the other algorithms tested?
From the context "Apart from experimenting with a pre-trained BERT model, we have run experiments with other systems and baselines, to compare them and obtain a better perspective about BERT's performance in these datasets.", so we know that the other algorithms tested are rule-based baseline, the CRF classifier, the spaCy entity tagger.
Answer: rule-based baseline, the CRF classifier, the spaCy entity tagger
True answer: NER model, CRF classifier trained with sklearn-crfsuite, classifier has been developed that consists of regular-expressions and dictionary look-up

Validation ID: 6b53e1f46ae4ba9b75117fc6e593abded89366be
Question: What are the other algorithms tested?
From the context "Apart from experimenting with a pre-trained BERT model, we have run experiments with other systems and baselines, to compare them and obtain a better perspective about BERT's performance in these datasets.", so we know that the other algorithms tested are rule-based baseline, the CRF classifier, the spaCy entity tagger.
Answer: rule-based baseline, the CRF classifier, the spaCy entity tagger
True answer: As the simplest baseline, a sensitive data recogniser and classifier, Conditional Random Fields (CRF), spaCy 

Validation ID: c0bee6539eb6956a7347daa9d2419b367bd02064
Question: Does BERT reach the best performance among all the algorithms compared?
From the context "The results of the experiments show that, in NUBes-PHI, the BERT-based model outperforms the other systems without requiring any adaptation or domain-specific feature engineering, just by being trained on the provided labelled data. Interestingly, the BERT-based model obtains a remarkably higher recall than the other systems. High recall is a desirable outcome because, when anonymising sensible documents, the accidental leak of sensible data is likely to be more dangerous than the unintended over-obfuscation of non-sensitive text.", so we know that BERT reaches the best performance among all the algorithms compared.
Answer: Yes
True answer: No

Validation ID: c0bee6539eb6956a7347daa9d2419b367bd02064
Question: Does BERT reach the best performance among all the algorithms compared?
From the context "The results of the experiments show that, in NUBes-PHI, the BERT-based model outperforms the other systems without requiring any adaptation or domain-specific feature engineering, just by being trained on the provided labelled data. Interestingly, the BERT-based model obtains a remarkably higher recall than the other systems. High recall is a desirable outcome because, when anonymising sensible documents, the accidental leak of sensible data is likely to be more dangerous than the unintended over-obfuscation of non-sensitive text.", so we know that BERT reaches the best performance among all the algorithms compared.
Answer: Yes
True answer: No

Validation ID: 3de0487276bb5961586acc6e9f82934ef8cb668c
Question: What are the clinical datasets used in the paper?
From the context "Two datasets are exploited in this article. Both datasets consist of plain text containing clinical narrative written in Spanish, and their respective manual annotations of sensitive information in BRAT BIBREF13 standoff format.", so we know that the clinical datasets used in the paper are NUBes-PHI and MEDDOCAN.
Answer: NUBes-PHI and MEDDOCAN
True answer: MEDDOCAN, NUBes-PHI

Validation ID: 3de0487276bb5961586acc6e9f82934ef8cb668c
Question: What are the clinical datasets used in the paper?
From the context "Two datasets are exploited in this article. Both datasets consist of plain text containing clinical narrative written in Spanish, and their respective manual annotations of sensitive information in BRAT BIBREF13 standoff format.", so we know that the clinical datasets used in the paper are NUBes-PHI and MEDDOCAN.
Answer: NUBes-PHI and MEDDOCAN
True answer: MEDDOCAN, NUBes 

Validation ID: 113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab
Question: how is model compactness measured?
From the context "LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB", so we know that model compactness is measured by the size of the model.
Answer: the size of the model
True answer: Using file size on disk

Validation ID: 113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab
Question: how is model compactness measured?
From the context "LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB", so we know that model compactness is measured by the size of the model.
Answer: the size of the model
True answer: 15.4 MB

Validation ID: 0752d71a0a1f73b3482a888313622ce9e9870d6e
Question: what was the baseline?
From the context "Results on LangID and NoLangID are compared to the system presented by deri2016grapheme, which is identified in our results as wFST", so we know that the baseline is wFST.
Answer: wFST
True answer: system presented by deri2016grapheme

Validation ID: 0752d71a0a1f73b3482a888313622ce9e9870d6e
Question: what was the baseline?
From the context "Results on LangID and NoLangID are compared to the system presented by deri2016grapheme, which is identified in our results as wFST", so we know that the baseline is wFST.
Answer: wFST
True answer: wFST

Validation ID: 55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3
Question: what evaluation metrics were used?
From the context "We use the following three evaluation metrics:
Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences.
True answer: Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)

Validation ID: 55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3
Question: what evaluation metrics were used?
From the context "We use the following three evaluation metrics:
Phoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences.
True answer: PER, WER, WER 100

Validation ID: 4eaf9787f51cd7cdc45eb85cf223d752328c6ee4
Question: what datasets did they use?
From the context "We use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10.", so we know that they used the multilingual pronunciation corpus collected by deri2016grapheme.
Answer: the multilingual pronunciation corpus collected by deri2016grapheme
True answer: the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary

Validation ID: 4eaf9787f51cd7cdc45eb85cf223d752328c6ee4
Question: what datasets did they use?
From the context "We use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10.", so we know that they used the multilingual pronunciation corpus collected by deri2016grapheme.
Answer: the multilingual pronunciation corpus collected by deri2016grapheme
True answer: multilingual pronunciation corpus collected by deri2016grapheme

Validation ID: fb2b536dc8e442dffab408db992b971e86548158
Question: What is the interannotator agreement for the human evaluation?
From the context "Our evaluation study investigates whether tuning the PG-net model increases summary coherence, by asking evaluators to select which of three summaries for the same document they like most: the PG-net model trained on CNN/DM; the model trained on student reflections; and finally the model trained on CNN/DM and tuned on student reflections. 20 evaluators were recruited from our institution and asked to each perform 20 annotations. Summaries are presented to evaluators in random order. Evaluators are then asked to select the summary they feel to be most readable and coherent, regardless of the source of the summary. For both courses, the majority of selected summaries were produced by the tuned model (49% for CS and 41% for Stat2015), compared to (31% for CS and 30.9% for Stat2015) for CNN/DM model, and (19.7% for CS and 28.5% for Stat2015) for student reflections model. These results again suggest that domain transfer can remedy the size of in-domain data and improve performance.", so we know that the interannotator agreement for the human evaluation is 49% for CS and 41% for Stat2015.
Answer: 49% for CS and 41% for Stat2015
True answer: Unanswerable

Validation ID: fb2b536dc8e442dffab408db992b971e86548158
Question: What is the interannotator agreement for the human evaluation?
From the context "Our evaluation study investigates whether tuning the PG-net model increases summary coherence, by asking evaluators to select which of three summaries for the same document they like most: the PG-net model trained on CNN/DM; the model trained on student reflections; and finally the model trained on CNN/DM and tuned on student reflections. 20 evaluators were recruited from our institution and asked to each perform 20 annotations. Summaries are presented to evaluators in random order. Evaluators are then asked to select the summary they feel to be most readable and coherent, regardless of the source of the summary. For both courses, the majority of selected summaries were produced by the tuned model (49% for CS and 41% for Stat2015), compared to (31% for CS and 30.9% for Stat2015) for CNN/DM model, and (19.7% for CS and 28.5% for Stat2015) for student reflections model. These results again suggest that domain transfer can remedy the size of in-domain data and improve performance.", so we know that the interannotator agreement for the human evaluation is 49% for CS and 41% for Stat2015.
Answer: 49% for CS and 41% for Stat2015
True answer: Unanswerable

Validation ID: 31735ec3d83c40b79d11df5c34154849aeb3fb47
Question: Who were the human evaluators used?
From the context "20 evaluators were recruited from our institution and asked to each perform 20 annotations", so we know that the human evaluators were recruited from the authors' institution.
Answer: the authors' institution
True answer: 20 evaluators were recruited from our institution and asked to each perform 20 annotations

Validation ID: 31735ec3d83c40b79d11df5c34154849aeb3fb47
Question: Who were the human evaluators used?
From the context "20 evaluators were recruited from our institution and asked to each perform 20 annotations", so we know that the human evaluators were recruited from the authors' institution.
Answer: the authors' institution
True answer: 20 annotatos from author's institution

Validation ID: 10d450960907091f13e0be55f40bcb96f44dd074
Question: Is the template-based model realistic? ?
From the context "We showed that enriching the training data with this synthesized data can further increase the benefits of using domain transfer / tuning to increase ROUGE scores", so we know that the template-based model is realistic.
Answer: Yes
True answer: Yes

Validation ID: 10d450960907091f13e0be55f40bcb96f44dd074
Question: Is the template-based model realistic? ?
From the context "We showed that enriching the training data with this synthesized data can further increase the benefits of using domain transfer / tuning to increase ROUGE scores", so we know that the template-based model is realistic.
Answer: Yes
True answer: Yes

Validation ID: b5608076d91450b0d295ad14c3e3a90d7e168d0e
Question: Is the student reflection data very different from the newspaper data? ?
From the context "Our work differs in several ways from these two prior domain transfer efforts. First, our experiments involve two entirely different domains: news and student reflections. Unlike news, student reflection documents lack global structure, are repetitive, and contain many sentence fragments and grammatical mistakes.", so we know that the student reflection data is very different from the newspaper data.
Answer: Yes
True answer: Yes

Validation ID: b5608076d91450b0d295ad14c3e3a90d7e168d0e
Question: Is the student reflection data very different from the newspaper data? ?
From the context "Our work differs in several ways from these two prior domain transfer efforts. First, our experiments involve two entirely different domains: news and student reflections. Unlike news, student reflection documents lack global structure, are repetitive, and contain many sentence fragments and grammatical mistakes.", so we know that the student reflection data is very different from the newspaper data.
Answer: Yes
True answer: Yes

Validation ID: c21b87c97d1afac85ece2450ee76d01c946de668
Question: What is the recent abstractive summarization method in this paper?
From the context "Recently, with the emergence of neural seq2seq models, abstractive summarization methods have seen great performance strides BIBREF0, BIBREF1, BIBREF2. However, complex neural summarization models with thousands of parameters usually require a large amount of training data. In fact, much of the neural summarization work has been trained and tested in news domains where numerous large datasets exist. For example, the CNN/DailyMail (CNN/DM) BIBREF3, BIBREF4 and New York Times (NYT) datasets are in the magnitude of 300k and 700k documents, respectively. In contrast, in other domains such as student reflections, summarization datasets are only in the magnitude of tens or hundreds of documents (e.g., BIBREF5). We hypothesize that training complex neural abstractive summarization models in such domains will not yield good performing models, and we will indeed later show that this is the case for student reflections.", so we know that the recent abstractive summarization method in this paper is neural seq2seq models.
Answer: neural seq2seq models
True answer: pointer networks with coverage mechanism (PG-net)

Validation ID: c21b87c97d1afac85ece2450ee76d01c946de668
Question: What is the recent abstractive summarization method in this paper?
From the context "Recently, with the emergence of neural seq2seq models, abstractive summarization methods have seen great performance strides BIBREF0, BIBREF1, BIBREF2. However, complex neural summarization models with thousands of parameters usually require a large amount of training data. In fact, much of the neural summarization work has been trained and tested in news domains where numerous large datasets exist. For example, the CNN/DailyMail (CNN/DM) BIBREF3, BIBREF4 and New York Times (NYT) datasets are in the magnitude of 300k and 700k documents, respectively. In contrast, in other domains such as student reflections, summarization datasets are only in the magnitude of tens or hundreds of documents (e.g., BIBREF5). We hypothesize that training complex neural abstractive summarization models in such domains will not yield good performing models, and we will indeed later show that this is the case for student reflections.", so we know that the recent abstractive summarization method in this paper is neural seq2seq models.
Answer: neural seq2seq models
True answer:  pointer networks with coverage mechanism (PG-net)BIBREF0

Validation ID: d087539e6a38c42f0a521ff2173ef42c0733878e
Question: Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models? ?
From the context "Since we seek to train a general-purpose student language model, we elect to reuse the teacher model's original training objective to optimize the student model, i.e., masked language modeling and next sentence prediction, before any fine-tuning. In the former task, words in context are randomly masked, and the language model needs to predict those words given the masked context. In the latter task, given a pair of sentences, the language model predicts whether the pair is consistent. However, since the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.", so we know that the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces.
Answer: The student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces.
True answer: While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.

Validation ID: d087539e6a38c42f0a521ff2173ef42c0733878e
Question: Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models? ?
From the context "Since we seek to train a general-purpose student language model, we elect to reuse the teacher model's original training objective to optimize the student model, i.e., masked language modeling and next sentence prediction, before any fine-tuning. In the former task, words in context are randomly masked, and the language model needs to predict those words given the masked context. In the latter task, given a pair of sentences, the language model predicts whether the pair is consistent. However, since the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces. As a result, we explore two alternative approaches that enable implicit transfer of knowledge to the student model, which we describe below.", so we know that the student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces.
Answer: The student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces.
True answer: distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.

Validation ID: efe9bad55107a6be7704ed97ecce948a8ca7b1d2
Question: What state-of-the-art compression techniques were used in the comparison?
From the context "We compare with NoKD as well as Patient Knowledge Distillation (PKD) from BIBREF34, who distill the 12-layer BERTBASE model into 3 and 6-layer BERT models by using the teacher model's hidden states.", so we know that the state-of-the-art compression techniques used in the comparison are Patient Knowledge Distillation (PKD) and NoKD.
Answer: Patient Knowledge Distillation (PKD) and NoKD
True answer: baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)

Validation ID: efe9bad55107a6be7704ed97ecce948a8ca7b1d2
Question: What state-of-the-art compression techniques were used in the comparison?
From the context "We compare with NoKD as well as Patient Knowledge Distillation (PKD) from BIBREF34, who distill the 12-layer BERTBASE model into 3 and 6-layer BERT models by using the teacher model's hidden states.", so we know that the state-of-the-art compression techniques used in the comparison are Patient Knowledge Distillation (PKD) and NoKD.
Answer: Patient Knowledge Distillation (PKD) and NoKD
True answer: NoKD, PKD, BERTBASE teacher model

Validation ID: 71e4ba4e87e6596aeca187127c0d088df6570c57
Question: What evaluations methods do they take?
From the context "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.", so we know that they take two evaluations methods: (1) suppress or (2) emphasize biases in human language.
Answer: (1) suppress or (2) emphasize biases in human language
True answer: Unanswerable

Validation ID: 71e4ba4e87e6596aeca187127c0d088df6570c57
Question: What evaluations methods do they take?
From the context "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.", so we know that they take two evaluations methods: (1) suppress or (2) emphasize biases in human language.
Answer: (1) suppress or (2) emphasize biases in human language
True answer: Unanswerable

Validation ID: 7561a968470a8936d10e1ba722d2f38b5a9a4d38
Question: What is the size of the dataset?
From the context "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each", so we know that the size of the dataset is 30,000.
Answer: 30,000
True answer: 30,000

Validation ID: 7561a968470a8936d10e1ba722d2f38b5a9a4d38
Question: What is the size of the dataset?
From the context "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each", so we know that the size of the dataset is 30,000.
Answer: 30,000
True answer: collection of over 30,000 images with 5 crowdsourced descriptions each

Validation ID: 6d4400f45bd97b812e946b8a682b018826e841f1
Question: Which methods are considered to find examples of biases and unwarranted inferences??
From the context "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.", so we know that the methods are considered to find examples of biases and unwarranted inferences are:

True answer: spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering

Validation ID: 6d4400f45bd97b812e946b8a682b018826e841f1
Question: Which methods are considered to find examples of biases and unwarranted inferences??
From the context "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.", so we know that the methods are considered to find examples of biases and unwarranted inferences are:

True answer: Looking for adjectives marking the noun "baby" and also looking for most-common adjectives related to certain nouns using POS-tagging

Validation ID: 26c2e1eb12143d985e4fb50543cf0d1eb4395e67
Question: What biases are found in the dataset?
From the context "I have divided these descriptions into two classes: linguistic bias and unwarranted inferences. The former corresponds to the annotators' choice of words when confronted with an image that may or may not match their stereotypical expectancies. The latter corresponds to the tendency of annotators to go beyond what the physical data can tell us, and expand their descriptions based on their past experiences and knowledge of the world.", so we know that the biases found in the dataset are linguistic bias and unwarranted inferences.
Answer: linguistic bias and unwarranted inferences
True answer: Ethnic bias

Validation ID: 26c2e1eb12143d985e4fb50543cf0d1eb4395e67
Question: What biases are found in the dataset?
From the context "I have divided these descriptions into two classes: linguistic bias and unwarranted inferences. The former corresponds to the annotators' choice of words when confronted with an image that may or may not match their stereotypical expectancies. The latter corresponds to the tendency of annotators to go beyond what the physical data can tell us, and expand their descriptions based on their past experiences and knowledge of the world.", so we know that the biases found in the dataset are linguistic bias and unwarranted inferences.
Answer: linguistic bias and unwarranted inferences
True answer: adjectives are used to create “more narrow labels [or subtypes] for individuals who do not fit with general social category expectations”

Validation ID: f17ca24b135f9fe6bb25dc5084b13e1637ec7744
Question: What discourse relations does it work best/worst for?
From the context "We have presented a paragraph-level neural network model that takes a sequence of discourse units as input, models inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predicts a sequence of discourse relations in a paragraph. By building wider-context informed discourse unit representations and capturing the overall discourse structure, the paragraph-level neural network model outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset.", so we know that the paragraph-level neural network model works best for implicit discourse relation recognition.
Answer: implicit discourse relation recognition
True answer: explicit discourse relations

Validation ID: f17ca24b135f9fe6bb25dc5084b13e1637ec7744
Question: What discourse relations does it work best/worst for?
From the context "We have presented a paragraph-level neural network model that takes a sequence of discourse units as input, models inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predicts a sequence of discourse relations in a paragraph. By building wider-context informed discourse unit representations and capturing the overall discourse structure, the paragraph-level neural network model outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset.", so we know that the paragraph-level neural network model works best for implicit discourse relation recognition.
Answer: implicit discourse relation recognition
True answer: Best: Expansion (Exp). Worst: Comparison (Comp).

Validation ID: bd5bd1765362c2d972a762ca12675108754aa437
Question: How much does this model improve state-of-the-art?
From the context "We have presented a paragraph-level neural network model that takes a sequence of discourse units as input, models inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predicts a sequence of discourse relations in a paragraph. By building wider-context informed discourse unit representations and capturing the overall discourse structure, the paragraph-level neural network model outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset.", so we know that the paragraph-level neural network model outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset.
Answer: outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset
True answer: the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 )., full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent., Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. 

Validation ID: bd5bd1765362c2d972a762ca12675108754aa437
Question: How much does this model improve state-of-the-art?
From the context "We have presented a paragraph-level neural network model that takes a sequence of discourse units as input, models inter-dependencies between discourse units as well as discourse relation continuity and patterns, and predicts a sequence of discourse relations in a paragraph. By building wider-context informed discourse unit representations and capturing the overall discourse structure, the paragraph-level neural network model outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset.", so we know that the paragraph-level neural network model outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset.
Answer: outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset
True answer: 1 percent

Validation ID: d9b6c61fc6d29ad399d27b931b6cb7b1117b314a
Question: Where is a question generation model used?
From the context "We further incorporate four open KBs as external knowledge into both question answering and generative approaches, and demonstrate that incorporating additional evidence from open KBs improves total accuracy", so we know that a question generation model is used in generative approaches.
Answer: generative approaches
True answer: The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. 

Validation ID: d9b6c61fc6d29ad399d27b931b6cb7b1117b314a
Question: Where is a question generation model used?
From the context "We further incorporate four open KBs as external knowledge into both question answering and generative approaches, and demonstrate that incorporating additional evidence from open KBs improves total accuracy", so we know that a question generation model is used in generative approaches.
Answer: generative approaches
True answer: framework consisting of both a question answering model and a question generation model

Validation ID: d27438b11bc70e706431dda0af2b1c0b0d209f96
Question: Were any of these tasks evaluated in any previous work?
From the context "I adapt the evaluation protocol and stimuli of BIBREF1, BIBREF2 and BIBREF3 to the bidirectional setting required by BERT, and evaluate the pre-trained BERT models (both the Large and the Base models). Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks.", so we know that BIBREF1, BIBREF2 and BIBREF3 were evaluated in previous work.
Answer: BIBREF1, BIBREF2 and BIBREF3
True answer: Yes

Validation ID: d27438b11bc70e706431dda0af2b1c0b0d209f96
Question: Were any of these tasks evaluated in any previous work?
From the context "I adapt the evaluation protocol and stimuli of BIBREF1, BIBREF2 and BIBREF3 to the bidirectional setting required by BERT, and evaluate the pre-trained BERT models (both the Large and the Base models). Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks.", so we know that BIBREF1, BIBREF2 and BIBREF3 were evaluated in previous work.
Answer: BIBREF1, BIBREF2 and BIBREF3
True answer: Yes

Validation ID: 8d4ac4afbf5b14f412171729ceb5e822afcfa3f4
Question: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?
From the context "We also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.", so we know that they build a model to automatically detect demographic, lingustic or psycological dimensons of people.
Answer: Yes
True answer: No

Validation ID: 8d4ac4afbf5b14f412171729ceb5e822afcfa3f4
Question: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?
From the context "We also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.", so we know that they build a model to automatically detect demographic, lingustic or psycological dimensons of people.
Answer: Yes
True answer: No

Validation ID: 3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f
Question: Which demographic dimensions of people do they obtain?
From the context "We also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.", so we know that they obtain the gender distribution of people.
Answer: gender distribution
True answer: occupation, industry, profile information, language use, gender 

Validation ID: 3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f
Question: Which demographic dimensions of people do they obtain?
From the context "We also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.", so we know that they obtain the gender distribution of people.
Answer: gender distribution
True answer: density of users, gender distribution

Validation ID: 07d15501a599bae7eb4a9ead63e9df3d55b3dc35
Question: How do they obtain psychological dimensions of people?
From the context "We also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10. MEM is a topic modeling approach applied to a corpus of texts created by hundreds of survey respondents from the U.S. who were asked to freely write about their personal values.", so we know that they obtain psychological dimensions of people by using the Meaning Extraction Method (MEM).
Answer: Meaning Extraction Method (MEM)
True answer: using the Meaning Extraction Method

Validation ID: 07d15501a599bae7eb4a9ead63e9df3d55b3dc35
Question: How do they obtain psychological dimensions of people?
From the context "We also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10. MEM is a topic modeling approach applied to a corpus of texts created by hundreds of survey respondents from the U.S. who were asked to freely write about their personal values.", so we know that they obtain psychological dimensions of people by using the Meaning Extraction Method (MEM).
Answer: Meaning Extraction Method (MEM)
True answer: Unanswerable

Validation ID: 99e78c390932594bd833be0f5c890af5c605d808
Question: What is the baseline?
From the context "We also create two naive baselines, one each for the Dosage and Frequency extraction tasks. For Dosage extraction, the baseline we consider is `Nearest Number', where we take the number nearest to the Medication Name as the prediction, and `none' if no number is mentioned or if the Medication Name is not detected in the input. For Frequency extraction, the baseline we consider is `Random Top-3' where we predict a random Frequency tag, from top-3 most frequent ones from our dataset - {`none', `daily', `twice a day'}.", so we know that the baseline is `Random Top-3'.
Answer: Random Top-3
True answer: QA PGNet, Multi-decoder QA PGNet with lookup table embedding

Validation ID: 99e78c390932594bd833be0f5c890af5c605d808
Question: What is the baseline?
From the context "We also create two naive baselines, one each for the Dosage and Frequency extraction tasks. For Dosage extraction, the baseline we consider is `Nearest Number', where we take the number nearest to the Medication Name as the prediction, and `none' if no number is mentioned or if the Medication Name is not detected in the input. For Frequency extraction, the baseline we consider is `Random Top-3' where we predict a random Frequency tag, from top-3 most frequent ones from our dataset - {`none', `daily', `twice a day'}.", so we know that the baseline is `Random Top-3'.
Answer: Random Top-3
True answer: QA PGNet and Multi-decoder QA PGNet

Validation ID: 861187338c5ad445b9acddba8f2c7688785667b1
Question: Is the data de-identified?
From the context "The recordings have an average duration of 9min 28s and have a verbatim transcript of 1,500 words on average (written by the experts). Both the audio and the transcript are de-identified (by removing the identifying information) with digital zeros and [de-identified] tags, respectively.", so we know that the data is de-identified.
Answer: Yes
True answer: Yes

Validation ID: 861187338c5ad445b9acddba8f2c7688785667b1
Question: Is the data de-identified?
From the context "The recordings have an average duration of 9min 28s and have a verbatim transcript of 1,500 words on average (written by the experts). Both the audio and the transcript are de-identified (by removing the identifying information) with digital zeros and [de-identified] tags, respectively.", so we know that the data is de-identified.
Answer: Yes
True answer: Yes

Validation ID: f161e6d5aecf8fae3a26374dcb3e4e1b40530c95
Question: What embeddings are used?
From the context "We developed different variations of our models with a simple lookup table embeddings learned from scratch and using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13 (trained and provided by the authors). Refer to Table TABREF5 for the performance comparisons.", so we know that the embeddings used are ELMo, BERT and ClinicalBERT.
Answer: ELMo, BERT and ClinicalBERT
True answer:  simple lookup table embeddings learned from scratch, using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13

Validation ID: f161e6d5aecf8fae3a26374dcb3e4e1b40530c95
Question: What embeddings are used?
From the context "We developed different variations of our models with a simple lookup table embeddings learned from scratch and using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13 (trained and provided by the authors). Refer to Table TABREF5 for the performance comparisons.", so we know that the embeddings used are ELMo, BERT and ClinicalBERT.
Answer: ELMo, BERT and ClinicalBERT
True answer: ELMO BIBREF11, BERT BIBREF12 and ClinicalBERT BIBREF13

Validation ID: 12c50dea84f9a8845795fa8b8c1679328bd66246
Question: What datasets did they use for evaluation?
From the context "We evaluated our models on 3 different datasets: CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR). 20 newsgroups for topic identification task, consisting of written text; Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual);", so we know that they used CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus for evaluation.
Answer: CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus
True answer: CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus

Validation ID: 12c50dea84f9a8845795fa8b8c1679328bd66246
Question: What datasets did they use for evaluation?
From the context "We evaluated our models on 3 different datasets: CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR). 20 newsgroups for topic identification task, consisting of written text; Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual);", so we know that they used CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus for evaluation.
Answer: CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus
True answer: CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus

Validation ID: 0810b43404686ddfe4ca84783477ae300fdd2ea4
Question: On top of BERT does the RNN layer work better or the transformer layer?
From the context "We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer. Its output serves as a document embedding. Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions.", so we know that the RNN layer is used.
From the context "Given that Transformers' edge over recurrent networks is their ability to effectively capture long distance relationships between words in a sequence BIBREF0, we experiment with replacing the LSTM recurrent layer in favor of a small Transformer model (2 layers of transformer building block containing self-attention, fully connected, etc.).", so we know that the transformer layer is used.
True answer: Transformer over BERT (ToBERT)

Validation ID: 0810b43404686ddfe4ca84783477ae300fdd2ea4
Question: On top of BERT does the RNN layer work better or the transformer layer?
From the context "We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer. Its output serves as a document embedding. Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions.", so we know that the RNN layer is used.
From the context "Given that Transformers' edge over recurrent networks is their ability to effectively capture long distance relationships between words in a sequence BIBREF0, we experiment with replacing the LSTM recurrent layer in favor of a small Transformer model (2 layers of transformer building block containing self-attention, fully connected, etc.).", so we know that the transformer layer is used.
True answer: The transformer layer

Validation ID: 455d4ef8611f62b1361be4f6387b222858bb5e56
Question: How was this data collected?
From the context "For purposes of evaluation we collected a dataset from conversational dialogs with workers on crowdsourcing platform CrowdFlower. Those dialogs were annotated with expert annotators and published under Creative Commons 4.0 BY-SA license on lindat.", so we know that this data was collected from conversational dialogs with workers on crowdsourcing platform CrowdFlower.
Answer: conversational dialogs with workers on crowdsourcing platform CrowdFlower
True answer: CrowdFlower

Validation ID: 455d4ef8611f62b1361be4f6387b222858bb5e56
Question: How was this data collected?
From the context "For purposes of evaluation we collected a dataset from conversational dialogs with workers on crowdsourcing platform CrowdFlower. Those dialogs were annotated with expert annotators and published under Creative Commons 4.0 BY-SA license on lindat.", so we know that this data was collected from conversational dialogs with workers on crowdsourcing platform CrowdFlower.
Answer: conversational dialogs with workers on crowdsourcing platform CrowdFlower
True answer: The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English.

Validation ID: bc16ce6e9c61ae13d46970ebe6c4728a47f8f425
Question: What is the average length of dialog?
From the context "We collected the dataset with 1900 dialogs and 8533 turns", so we know that the average length of dialog is 8533 / 1900 = 4.5.
Answer: 4.5
True answer: 4.49 turns

Validation ID: bc16ce6e9c61ae13d46970ebe6c4728a47f8f425
Question: What is the average length of dialog?
From the context "We collected the dataset with 1900 dialogs and 8533 turns", so we know that the average length of dialog is 8533 / 1900 = 4.5.
Answer: 4.5
True answer: 4.5 turns per dialog (8533 turns / 1900 dialogs)

Validation ID: 1ff0fccf0dca95a6630380c84b0422bed854269a
Question: How are models evaluated in this human-machine communication game?
From the context "We quantify the efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", so we know that models are evaluated by the retention rate of tokens and the accuracy of a scheme.
Answer: retention rate of tokens and the accuracy of a scheme
True answer: by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews

Validation ID: 1ff0fccf0dca95a6630380c84b0422bed854269a
Question: How are models evaluated in this human-machine communication game?
From the context "We quantify the efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.", so we know that models are evaluated by the retention rate of tokens and the accuracy of a scheme.
Answer: retention rate of tokens and the accuracy of a scheme
True answer: efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence

Validation ID: 3d7d865e905295d11f1e85af5fa89b210e3e9fdf
Question: How many participants were trying this communication game?
From the context "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus", so we know that 100 participants were trying this communication game.
Answer: 100
True answer: 100 

Validation ID: 3d7d865e905295d11f1e85af5fa89b210e3e9fdf
Question: How many participants were trying this communication game?
From the context "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus", so we know that 100 participants were trying this communication game.
Answer: 100
True answer: 100 crowdworkers 

Validation ID: 2ad4d3d222f5237ed97923640bc8e199409cbe52
Question: What user variations have been tested?
From the context "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus", so we know that the user variations have been tested are 100 crowdworkers on Amazon Mechanical Turk (AMT).
Answer: 100 crowdworkers on Amazon Mechanical Turk (AMT)
True answer: completion times and accuracies 

Validation ID: 2ad4d3d222f5237ed97923640bc8e199409cbe52
Question: What user variations have been tested?
From the context "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus", so we know that the user variations have been tested are 100 crowdworkers on Amazon Mechanical Turk (AMT).
Answer: 100 crowdworkers on Amazon Mechanical Turk (AMT)
True answer: Unanswerable

Validation ID: 3fad42be0fb2052bb404b989cc7d58b440cd23a0
Question: What are the baselines used?
From the context "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword. The Unif encoder randomly keeps tokens to generate keywords with the probability $\delta $. The Stopword encoder keeps all tokens but drops stop words (e.g. `the', `a', `or') all the time ($\delta =0$) or half of the time ($\delta =0.5$). The corresponding decoders for these encoders are optimized using gradient descent to minimize the reconstruction error (i.e. $\mathrm {loss}(x, \alpha, \beta )$).", so we know that the baselines used are Unif and Stopword.
Answer: Unif and Stopword
True answer: Unif and Stopword

Validation ID: 3fad42be0fb2052bb404b989cc7d58b440cd23a0
Question: What are the baselines used?
From the context "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword. The Unif encoder randomly keeps tokens to generate keywords with the probability $\delta $. The Stopword encoder keeps all tokens but drops stop words (e.g. `the', `a', `or') all the time ($\delta =0$) or half of the time ($\delta =0.5$). The corresponding decoders for these encoders are optimized using gradient descent to minimize the reconstruction error (i.e. $\mathrm {loss}(x, \alpha, \beta )$).", so we know that the baselines used are Unif and Stopword.
Answer: Unif and Stopword
True answer: Unif and Stopword

Validation ID: ee417fea65f9b1029455797671da0840c8c1abbe
Question: Do they use off-the-shelf NLP systems to build their assitant?
From the context "We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API", so we know that they use handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API to build their assitant.
Answer: handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API
True answer: No

Validation ID: ee417fea65f9b1029455797671da0840c8c1abbe
Question: Do they use off-the-shelf NLP systems to build their assitant?
From the context "We implemented an NLU unit utilizing handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API", so we know that they use handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API to build their assitant.
Answer: handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API
True answer: No

Validation ID: ca5a82b54cb707c9b947aa8445aac51ea218b23a
Question: How does the IPA label data after interacting with users?
From the context "The system is implemented in a way such that it accumulates labeled dialogues in the background and stores them in a structured form", so we know that the IPA label data after interacting with users.
Answer: The IPA label data after interacting with users
True answer: It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).

Validation ID: ca5a82b54cb707c9b947aa8445aac51ea218b23a
Question: How does the IPA label data after interacting with users?
From the context "The system is implemented in a way such that it accumulates labeled dialogues in the background and stores them in a structured form", so we know that the IPA label data after interacting with users.
Answer: The IPA label data after interacting with users
True answer: Plain dialogues with unique dialogue indexes, Plain Information Dictionary information (e.g., extracted entities) collected for the whole dialogue, Pairs of questions (i.e., user requests) and responses (i.e., bot responses), Triples in the form of (User Request, Next Action, Response)

Validation ID: da55bd769721b878dd17f07f124a37a0a165db02
Question: What kind of repetitive and time-consuming activities does their assistant handle?
From the context "The system is intended to operate in a real-world scenario, we had to implement additional policies that control the dialogue flow and validate the system's accuracy. Below we describe these policies:
Completeness of the Information Dictionary: In this step, the system validates the completeness of the ID, which is defined by the number of obligatory slots filled in the information dictionary. There are 6 distinct cases when the ID is considered to be complete (see Section SECREF13 of the Appendix). For instance, if a user works on a final examination, the system does not has to request a sub-topic or examination level. Thus, the ID has to be filled only with data for a topic, examination mode, and question number, whereas, if the user works on a quiz, the system has to gather information about the topic, sub-topic, examination mode, and the question number.", so we know that the system handles repetitive and time-consuming activities.
True answer: Unanswerable

Validation ID: da55bd769721b878dd17f07f124a37a0a165db02
Question: What kind of repetitive and time-consuming activities does their assistant handle?
From the context "The system is intended to operate in a real-world scenario, we had to implement additional policies that control the dialogue flow and validate the system's accuracy. Below we describe these policies:
Completeness of the Information Dictionary: In this step, the system validates the completeness of the ID, which is defined by the number of obligatory slots filled in the information dictionary. There are 6 distinct cases when the ID is considered to be complete (see Section SECREF13 of the Appendix). For instance, if a user works on a final examination, the system does not has to request a sub-topic or examination level. Thus, the ID has to be filled only with data for a topic, examination mode, and question number, whereas, if the user works on a quiz, the system has to gather information about the topic, sub-topic, examination mode, and the question number.", so we know that the system handles repetitive and time-consuming activities.
True answer:  What kind of topic (or sub-topic) a student has a problem with, At which examination mode (i.e., quiz, chapter level training or exercise, section level training or exercise, or final examination) the student is working right now,  the exact question number and exact problem formulation

Validation ID: feb448860918ef5b905bb25d7b855ba389117c1f
Question: How was the audio data gathered?
From the context "We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages", so we know that the audio data was gathered from All India Radio news channel.
Answer: All India Radio news channel
True answer: Through the All India Radio new channel where actors read news.

Validation ID: feb448860918ef5b905bb25d7b855ba389117c1f
Question: How was the audio data gathered?
From the context "We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages", so we know that the audio data was gathered from All India Radio news channel.
Answer: All India Radio news channel
True answer:  $\textbf {All India Radio}$ news channel

Validation ID: 4bc2784be43d599000cb71d31928908250d4cef3
Question: What is the GhostVLAD approach?
From the context "We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into the projection layer, followed by softmax to predict the language label.", so we know that GhostVLAD is a pooling approach.
Answer: pooling approach
True answer: extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters

Validation ID: 4bc2784be43d599000cb71d31928908250d4cef3
Question: What is the GhostVLAD approach?
From the context "We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into the projection layer, followed by softmax to predict the language label.", so we know that GhostVLAD is a pooling approach.
Answer: pooling approach
True answer: An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content.

Validation ID: 75df70ce7aa714ec4c6456d0c51f82a16227f2cb
Question: Which 7 Indian languages do they experiment with?
From the context "We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages", so we know that they experiment with 7 Indian languages.
Answer: 7 Indian languages
True answer: Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam

Validation ID: 75df70ce7aa714ec4c6456d0c51f82a16227f2cb
Question: Which 7 Indian languages do they experiment with?
From the context "We collected and curated 630 hrs audio data from news All India Radio news channel for 7 Indian languages", so we know that they experiment with 7 Indian languages.
Answer: 7 Indian languages
True answer: Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)

Validation ID: 6424e442b34a576f904d9649d63acf1e4fdefdfc
Question: What datasets do they evaluate on?
From the context "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank", so we know that they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.
Answer: Wall Street Journal (WSJ) portion of the Penn Treebank
True answer:  Wall Street Journal (WSJ) portion of the Penn Treebank

Validation ID: 6424e442b34a576f904d9649d63acf1e4fdefdfc
Question: What datasets do they evaluate on?
From the context "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank", so we know that they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.
Answer: Wall Street Journal (WSJ) portion of the Penn Treebank
True answer: Unanswerable

Validation ID: 5eabfc6cc8aa8a99e6e42514ef9584569cb75dec
Question: Do they evaluate only on English datasets?
From the context "Experiments on both POS induction and unsupervised dependency parsing tasks demonstrate the effectiveness of our proposed approach", so we know that they evaluate on both POS induction and unsupervised dependency parsing tasks.
From the context "Experiments on both POS induction and unsupervised dependency parsing tasks demonstrate the effectiveness of our proposed approach. Future work might explore more sophisticated invertible projections, or recurrent projections that jointly transform the entire input sequence.", so we know that they evaluate on English datasets.
True answer: Yes

Validation ID: 5eabfc6cc8aa8a99e6e42514ef9584569cb75dec
Question: Do they evaluate only on English datasets?
From the context "Experiments on both POS induction and unsupervised dependency parsing tasks demonstrate the effectiveness of our proposed approach", so we know that they evaluate on both POS induction and unsupervised dependency parsing tasks.
From the context "Experiments on both POS induction and unsupervised dependency parsing tasks demonstrate the effectiveness of our proposed approach. Future work might explore more sophisticated invertible projections, or recurrent projections that jointly transform the entire input sequence.", so we know that they evaluate on English datasets.
True answer: Unanswerable

Validation ID: 887c6727e9f25ade61b4853a869fe712fe0b703d
Question: What is the invertibility condition?
From the context "For ease of exposition, we explain the learning algorithm in terms of Markov structure without loss of generality. As shown in Eq. ( EQREF17 ), the optimization challenge in our approach comes from the intractability of the marginalized emission factor INLINEFORM0. If we can marginalize out INLINEFORM1 and compute INLINEFORM2, then the posterior and marginal likelihood of our Markov-structured model can be computed with the forward-backward algorithm. We can apply Eq. ( EQREF14 ) and obtain : INLINEFORM3
By using the change of variable rule to the integration, which allows the integration variable INLINEFORM0 to be replaced by INLINEFORM1, the marginal emission factor can be computed in closed-form when the invertibility condition is satisfied: DISPLAYFORM0
True answer: The neural projector must be invertible.

Validation ID: 887c6727e9f25ade61b4853a869fe712fe0b703d
Question: What is the invertibility condition?
From the context "For the projection we can use an arbitrary invertible function, and given the representational power of neural networks they seem a natural choice. However, calculating the inverse and Jacobian of an arbitrary neural network can be difficult, as it requires that all component functions be invertible and also requires storage of large Jacobian matrices, which is memory intensive. To address this issue, several recent papers propose specially designed invertible networks that are easily trainable yet still powerful BIBREF16, BIBREF17, BIBREF19. Inspired by these works, we use the invertible transformation proposed by BIBREF16, which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property).", so we know that the invertibility condition is that the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied.
Answer: the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied.
True answer: we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists

Validation ID: 6236762b5631d9e395f81e1ebccc4bf3ab9b24ac
Question: Do they show on which examples how conflict works better than attention?
From the context "We also show qualitative results where we can observe that our model with attention and conflict combined does better on cases where pairs are non-duplicate and has very small difference. We have observed that the conflict model is very sensitive to even minor differences and compensates in such cases where attention poses high bias towards similarities already there in the sequences.", so we know that they show on which examples how conflict works better than attention.
Answer: Yes
True answer: Yes

Validation ID: 6236762b5631d9e395f81e1ebccc4bf3ab9b24ac
Question: Do they show on which examples how conflict works better than attention?
From the context "We also show qualitative results where we can observe that our model with attention and conflict combined does better on cases where pairs are non-duplicate and has very small difference. We have observed that the conflict model is very sensitive to even minor differences and compensates in such cases where attention poses high bias towards similarities already there in the sequences.", so we know that they show on which examples how conflict works better than attention.
Answer: Yes
True answer: Yes

Validation ID: 31d695ba855d821d3e5cdb7bea638c7dbb7c87c7
Question: Which neural architecture do they use as a base for their attention conflict mechanisms?
From the context "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11). The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.", so we know that they use GRU as a base for their attention conflict mechanisms.
Answer: GRU
True answer: GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.

Validation ID: 31d695ba855d821d3e5cdb7bea638c7dbb7c87c7
Question: Which neural architecture do they use as a base for their attention conflict mechanisms?
From the context "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. Except interaction, all the other parts are exactly identical between the two models. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11). The classifier part is simply stacked fully-connected layers. Figure 3 shows a block diagram of how our model looks like.", so we know that they use GRU as a base for their attention conflict mechanisms.
Answer: GRU
True answer: two stacked GRU layers, attention for one model while for the another one it consists of attention and conflict combined, fully-connected layers

Validation ID: b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab
Question: On which tasks do they test their conflict method?
From the context "We train all our models for roughly 2 epochs with a batch size of 64. We use a hidden dimension of 150 throughout the model. The embedding layer uses ELMO BIBREF9 which has proven to be very useful in various downstream language understanding tasks. Our FC layers consists of four dense layers with INLINEFORM0 activation after each layer. The dropout rate is kept as 0.2 for every recurrent and FC linear layers. We use Adam optimizer in our experiment with epsilon=1e-8, beta=0.9 and learning rate=1e-3.", so we know that they test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.
Answer: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask
True answer: Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions

Validation ID: b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab
Question: On which tasks do they test their conflict method?
From the context "We train all our models for roughly 2 epochs with a batch size of 64. We use a hidden dimension of 150 throughout the model. The embedding layer uses ELMO BIBREF9 which has proven to be very useful in various downstream language understanding tasks. Our FC layers consists of four dense layers with INLINEFORM0 activation after each layer. The dropout rate is kept as 0.2 for every recurrent and FC linear layers. We use Adam optimizer in our experiment with epsilon=1e-8, beta=0.9 and learning rate=1e-3.", so we know that they test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.
Answer: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask
True answer: Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask

Validation ID: a99fdd34422f4231442c220c97eafc26c76508dd
Question: Do they use graphical models?
From the context "We test two different scenarios for optimizing the similarity threshold INLINEFORM0 for the crosslingual case. Table TABREF41 shows the results for these experiments. First, we consider the simpler case of adjusting a global INLINEFORM1 parameter for the crosslingual distances, as also described for the monolingual case. As shown, this method works poorly, since the INLINEFORM2 grid-search could not find a reasonable INLINEFORM3 which worked well for every possible language pair.", so we know that they do not use graphical models.
Answer: No
True answer: No

Validation ID: a99fdd34422f4231442c220c97eafc26c76508dd
Question: Do they use graphical models?
From the context "We test two different scenarios for optimizing the similarity threshold INLINEFORM0 for the crosslingual case. Table TABREF41 shows the results for these experiments. First, we consider the simpler case of adjusting a global INLINEFORM1 parameter for the crosslingual distances, as also described for the monolingual case. As shown, this method works poorly, since the INLINEFORM2 grid-search could not find a reasonable INLINEFORM3 which worked well for every possible language pair.", so we know that they do not use graphical models.
Answer: No
True answer: No

Validation ID: 2c78993524ca62bf1f525b60f2220a374d0e3535
Question: What are the sources of the datasets?
From the context "We adapted the cluster-linking dataset from rupnik2016news to evaluate our online crosslingual clustering approach.", so we know that the sources of the datasets are rupnik2016news.
Answer: rupnik2016news
True answer: rupnik2016news

Validation ID: 2c78993524ca62bf1f525b60f2220a374d0e3535
Question: What are the sources of the datasets?
From the context "We adapted the cluster-linking dataset from rupnik2016news to evaluate our online crosslingual clustering approach.", so we know that the sources of the datasets are rupnik2016news.
Answer: rupnik2016news
True answer: rupnik2016news, Deutsche Welle's news website

Validation ID: d604f5fb114169f75f9a38fab18c1e866c5ac28b
Question: What metric is used for evaluation?
From the context "Table TABREF35 gives the final monolingual results on the three datasets. For English, we see that the significant improvement we get using our algorithm over the algorithm of aggarwal2006framework is due to an increased recall score.", so we know that the metric used for evaluation is recall score.
Answer: recall score
True answer: F1, precision, recall, accuracy

Validation ID: d604f5fb114169f75f9a38fab18c1e866c5ac28b
Question: What metric is used for evaluation?
From the context "Table TABREF35 gives the final monolingual results on the three datasets. For English, we see that the significant improvement we get using our algorithm over the algorithm of aggarwal2006framework is due to an increased recall score.", so we know that the metric used for evaluation is recall score.
Answer: recall score
True answer: Precision, recall, F1, accuracy

Validation ID: 1d3e914d0890fc09311a70de0b20974bf7f0c9fe
Question: Which eight NER tasks did they evaluate on?
From the context "We finetune GreenBioBERT on the eight publicly available NER tasks used in BIBREF2", so we know that they evaluated on eight NER tasks.
Answer: eight NER tasks
True answer: BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800

Validation ID: 1d3e914d0890fc09311a70de0b20974bf7f0c9fe
Question: Which eight NER tasks did they evaluate on?
From the context "We finetune GreenBioBERT on the eight publicly available NER tasks used in BIBREF2", so we know that they evaluated on eight NER tasks.
Answer: eight NER tasks
True answer: BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800

Validation ID: 16535db1d73a9373ffe9d6eedaa2369cefd91ac4
Question: What in-domain text did they use?
From the context "We train Word2Vec with vector size $d_\mathrm {W2V} = d_\mathrm {LM} = 1024$ on CORD-19 (Covid-19 Open Research Dataset) and/or PubMed+PMC", so we know that they use CORD-19 and/or PubMed+PMC as in-domain text.
Answer: CORD-19 and/or PubMed+PMC
True answer: PubMed+PMC

Validation ID: 16535db1d73a9373ffe9d6eedaa2369cefd91ac4
Question: What in-domain text did they use?
From the context "We train Word2Vec with vector size $d_\mathrm {W2V} = d_\mathrm {LM} = 1024$ on CORD-19 (Covid-19 Open Research Dataset) and/or PubMed+PMC", so we know that they use CORD-19 and/or PubMed+PMC as in-domain text.
Answer: CORD-19 and/or PubMed+PMC
True answer: PubMed+PMC (the data used for BioBERTv1.0) and/or CORD-19 (Covid-19 Open Research Dataset)

Validation ID: de0b650022ad8693465242ded169313419eed7d9
Question: Does their framework automatically optimize for hyperparameters?
From the context "We could conduct more detailed analyses of the various strategies under the framework to show its universarity", so we know that their framework automatically optimize for hyperparameters.
Answer: Yes
True answer: Unanswerable

Validation ID: de0b650022ad8693465242ded169313419eed7d9
Question: Does their framework automatically optimize for hyperparameters?
From the context "We could conduct more detailed analyses of the various strategies under the framework to show its universarity", so we know that their framework automatically optimize for hyperparameters.
Answer: Yes
True answer: Unanswerable

Validation ID: 2b3cac7af10d358d4081083962d03ea2798cf622
Question: Does their framework always generate purely attention-based models?
From the context "Our proposed approach alleviates the need of complicated architecture re-designing when accommodating attention mechanism. In addition, the number of free parameters to learn in our network does not go beyond that magnitute of a single NMT system. With its universality, our approach has shown its effectiveness in an under-resourced translation task with considerable improvements. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.", so we know that their framework always generate purely attention-based models.
Answer: Yes
True answer: Yes

Validation ID: 2b3cac7af10d358d4081083962d03ea2798cf622
Question: Does their framework always generate purely attention-based models?
From the context "Our proposed approach alleviates the need of complicated architecture re-designing when accommodating attention mechanism. In addition, the number of free parameters to learn in our network does not go beyond that magnitute of a single NMT system. With its universality, our approach has shown its effectiveness in an under-resourced translation task with considerable improvements. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.", so we know that their framework always generate purely attention-based models.
Answer: Yes
True answer: Unanswerable

Validation ID: 897ba53ef44f658c128125edd26abf605060fb13
Question: Do they test their framework performance on commonly used language pairs, such as English-to-German?
From the context "In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.", so we know that they test their framework performance on commonly used language pairs.
Answer: Yes
True answer: Yes

Validation ID: 897ba53ef44f658c128125edd26abf605060fb13
Question: Do they test their framework performance on commonly used language pairs, such as English-to-German?
From the context "In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.", so we know that they test their framework performance on commonly used language pairs.
Answer: Yes
True answer: Yes

Validation ID: 41ac23e32bf208b69414f4b687c4f324c6132464
Question: Which languages do they test on for the under-resourced scenario?
From the context "We employ language-specific coding and target forcing in a strategy called bridge. Unlike the strategies used in under-resourced translation task, bridge is an entire many-to-many multilingual NMT. Simulating a zero-resourced German INLINEFORM0 French translation task given the available German-English and English-French parallel corpora, after applying language-specific coding and target forcing for each corpus, we mix those data with an English-English data as a “bridge” creating some connection between German and French. We also propose a variant of this strategy that we incorporate French-French data. And we call it universal.", so we know that they test on German-French translation.
Answer: German-French
True answer: English, German

Validation ID: 41ac23e32bf208b69414f4b687c4f324c6132464
Question: Which languages do they test on for the under-resourced scenario?
From the context "We employ language-specific coding and target forcing in a strategy called bridge. Unlike the strategies used in under-resourced translation task, bridge is an entire many-to-many multilingual NMT. Simulating a zero-resourced German INLINEFORM0 French translation task given the available German-English and English-French parallel corpora, after applying language-specific coding and target forcing for each corpus, we mix those data with an English-English data as a “bridge” creating some connection between German and French. We also propose a variant of this strategy that we incorporate French-French data. And we call it universal.", so we know that they test on German-French translation.
Answer: German-French
True answer: small portion of the large parallel corpus for English-German is used as a simulation

Validation ID: e97186c51d4af490dba6faaf833d269c8256426c
Question: Are the automatically constructed datasets subject to quality control?
From the context "As we describe in the next section, semantic clusters allow us to devise new types of evaluation that reveal whether models have comprehensive and consistent knowledge of target concepts (e.g., evaluating whether a model can correctly answer several questions associated with a concept, as opposed to a few disjoint instances).", so we know that the automatically constructed datasets are subject to quality control.
Answer: Yes
True answer: No

Validation ID: e97186c51d4af490dba6faaf833d269c8256426c
Question: Are the automatically constructed datasets subject to quality control?
From the context "As we describe in the next section, semantic clusters allow us to devise new types of evaluation that reveal whether models have comprehensive and consistent knowledge of target concepts (e.g., evaluating whether a model can correctly answer several questions associated with a concept, as opposed to a few disjoint instances).", so we know that the automatically constructed datasets are subject to quality control.
Answer: Yes
True answer: No

Validation ID: 5bb3c27606c59d73fd6944ba7382096de4fa58d8
Question: Do they focus on Reading Comprehension or multiple choice question answering?
From the context "We focus on probing state-of-the-art transformer models BIBREF7, BIBREF9 in the domain of science MCQA", so we know that they focus on multiple choice question answering.
Answer: multiple choice question answering
True answer: MULTIPLE CHOICE QUESTION ANSWERING

Validation ID: 5bb3c27606c59d73fd6944ba7382096de4fa58d8
Question: Do they focus on Reading Comprehension or multiple choice question answering?
From the context "We focus on probing state-of-the-art transformer models BIBREF7, BIBREF9 in the domain of science MCQA", so we know that they focus on multiple choice question answering.
Answer: multiple choice question answering
True answer: multiple-choice

Validation ID: 8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b
Question: After how many hops does accuracy decrease?
From the context "In the latter case, the rules $(\texttt {isa}^{r},c,c^{\prime }) \in \mathcal {T}_i$ in Table TABREF8 cover only direct ISA links from $c$ in direction $r \in \lbrace \uparrow,\downarrow \rbrace $. In practice, for each $c$ and direction $r$, we construct tests that cover the set HOPS$(c,r)$ of all direct as well as derived ISA relations of $c$:", so we know that the accuracy decreases after 2 hops.
Answer: 2
True answer: 1-hop links to 2-hops

Validation ID: 8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b
Question: After how many hops does accuracy decrease?
From the context "In the latter case, the rules $(\texttt {isa}^{r},c,c^{\prime }) \in \mathcal {T}_i$ in Table TABREF8 cover only direct ISA links from $c$ in direction $r \in \lbrace \uparrow,\downarrow \rbrace $. In practice, for each $c$ and direction $r$, we construct tests that cover the set HOPS$(c,r)$ of all direct as well as derived ISA relations of $c$:", so we know that the accuracy decreases after 2 hops.
Answer: 2
True answer: one additional hop

Validation ID: 85590bb26fed01a802241bc537d85ba5ef1c6dc2
Question: How do they control for annotation artificats?
From the context "As we describe in the next section, semantic clusters allow us to devise new types of evaluation that reveal whether models have comprehensive and consistent knowledge of target concepts (e.g., evaluating whether a model can correctly answer several questions associated with a concept, as opposed to a few disjoint instances).", so we know that semantic clusters allow us to devise new types of evaluation that reveal whether models have comprehensive and consistent knowledge of target concepts.
Answer: semantic clusters
True answer:  we use several of the MCQA baseline models first introduced in BIBREF0

Validation ID: 85590bb26fed01a802241bc537d85ba5ef1c6dc2
Question: How do they control for annotation artificats?
From the context "As in BIBREF22, we found all models to be especially sensitive to different learning rates, and performed comprehensive hyper-parameters searches that also manipulate the number of iterations and random seeds used.", so we know that they control for annotation artificats by performing comprehensive hyper-parameters searches that also manipulate the number of iterations and random seeds used.
Answer: comprehensive hyper-parameters searches that also manipulate the number of iterations and random seeds used
True answer: Choice-Only model, which is a variant of the well-known hypothesis-only baseline, Choice-to-choice model, tries to single out a given answer choice relative to other choices, Question-to-choice model, in contrast, uses the contextual representations for each question and individual choice and an attention model Att model to get a score

Validation ID: 75ff6e425ce304a35f18c0230c0d13d3913a31a9
Question: Is WordNet useful for taxonomic reasoning for this task?
From the context "We build 4 individual datasets based on semantic relations native to WordNet (see BIBREF37): hypernymy (i.e., generalization or ISA reasoning up a taxonomy, ISA$^\uparrow $), hyponymy (ISA$^{\downarrow }$), synonymy, and definitions.", so we know that WordNet is useful for taxonomic reasoning for this task.
Answer: Yes
True answer: Unanswerable

Validation ID: 75ff6e425ce304a35f18c0230c0d13d3913a31a9
Question: Is WordNet useful for taxonomic reasoning for this task?
From the context "We build 4 individual datasets based on semantic relations native to WordNet (see BIBREF37): hypernymy (i.e., generalization or ISA reasoning up a taxonomy, ISA$^\uparrow $), hyponymy (ISA$^{\downarrow }$), synonymy, and definitions.", so we know that WordNet is useful for taxonomic reasoning for this task.
Answer: Yes
True answer: Yes

Validation ID: 5cb610d3d5d7d447b4cd5736d6a7d8262140af58
Question: How do they perform multilingual training?
From the context "Since a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across languages.", so we know that they perform multilingual training by randomly alternating between languages for every new minibatch.
Answer: randomly alternating between languages for every new minibatch
True answer: Multilingual training is performed by randomly alternating between languages for every new minibatch

Validation ID: 5cb610d3d5d7d447b4cd5736d6a7d8262140af58
Question: How do they perform multilingual training?
From the context "Since a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch. We do not pass any information to the auxiliary decoder as to the source language of the signal it is receiving, as we assume abstract morpho-syntactic features are shared across languages.", so we know that they perform multilingual training by randomly alternating between languages for every new minibatch.
Answer: randomly alternating between languages for every new minibatch
True answer: by randomly alternating between languages for every new minibatch

Validation ID: c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a
Question: What languages are evaluated?
From the context "We train models for 50 different random combinations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2", so we know that the languages are evaluated in Track 1 and Track 2.
Answer: Track 1 and Track 2
True answer: German, English, Spanish, Finnish, French, Russian,  Swedish.

Validation ID: c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a
Question: What languages are evaluated?
From the context "We train models for 50 different random combinations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2", so we know that the languages are evaluated in Track 1 and Track 2.
Answer: Track 1 and Track 2
True answer: Unanswerable

Validation ID: b9d168da5321a7d7b812c52bb102a05210fe45bd
Question: Does the model have attention?
From the context "The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the model has attention.
Answer: Yes
True answer: Yes

Validation ID: b9d168da5321a7d7b812c52bb102a05210fe45bd
Question: Does the model have attention?
From the context "The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the model has attention.
Answer: Yes
True answer: Yes

Validation ID: 0c234db3b380c27c4c70579a5d6948e1e3b24ff1
Question: What architecture does the decoder have?
From the context "The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the decoder has an LSTM architecture.
Answer: LSTM
True answer: LSTM

Validation ID: 0c234db3b380c27c4c70579a5d6948e1e3b24ff1
Question: What architecture does the decoder have?
From the context "The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.", so we know that the decoder has an LSTM architecture.
Answer: LSTM
True answer: LSTM

Validation ID: fa527becb8e2551f4fd2ae840dbd4a68971349e0
Question: What architecture does the encoder have?
From the context "The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder.", so we know that the encoder has an LSTM architecture.
Answer: LSTM
True answer: LSTM

Validation ID: fa527becb8e2551f4fd2ae840dbd4a68971349e0
Question: What architecture does the encoder have?
From the context "The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder.", so we know that the encoder has an LSTM architecture.
Answer: LSTM
True answer: LSTM

Validation ID: 32a3c248b928d4066ce00bbb0053534ee62596e7
Question: What is MSD prediction?
From the context "We introduce an auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form", so we know that MSD prediction is the task to predict the MSD tag of the target form.
Answer: the task to predict the MSD tag of the target form
True answer: The task of predicting MSD tags: V, PST, V.PCTP, PASS.

Validation ID: 32a3c248b928d4066ce00bbb0053534ee62596e7
Question: What is MSD prediction?
From the context "We introduce an auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form", so we know that MSD prediction is the task to predict the MSD tag of the target form.
Answer: the task to predict the MSD tag of the target form
True answer: morphosyntactic descriptions (MSD)

Validation ID: c9b8d3858c112859eabee54248b874331c48f71b
Question: What type of inflections are considered?
From the context "This paper describes our approach and results for Task 2 of the CoNLL–SIGMORPHON 2018 shared task on universal morphological reinflection", so we know that the type of inflections are universal morphological reinflection.
Answer: universal morphological reinflection
True answer: Unanswerable

Validation ID: c9b8d3858c112859eabee54248b874331c48f71b
Question: What type of inflections are considered?
From the context "This paper describes our approach and results for Task 2 of the CoNLL–SIGMORPHON 2018 shared task on universal morphological reinflection", so we know that the type of inflections are universal morphological reinflection.
Answer: universal morphological reinflection
True answer: Unanswerable

Validation ID: 45e9533586199bde19313cd43b3d0ecadcaf7a33
Question: Do they use attention?
From the context "We use the method of BIBREF13 to apply self attention to the passage: INLINEFORM6", so we know that they use attention.
Answer: Yes
True answer: Yes

Validation ID: 45e9533586199bde19313cd43b3d0ecadcaf7a33
Question: Do they use attention?
From the context "We use the method of BIBREF13 to apply self attention to the passage: INLINEFORM6", so we know that they use attention.
Answer: Yes
True answer: Yes

Validation ID: d3dbb5c22ef204d85707d2d24284cc77fa816b6c
Question: What other models do they compare to?
From the context "Table TABREF21 reports comparison results in literature published. Our model achieves state-of-the-art on development dataset in setting without pre-trained large language model (ELMo). Comparing with the much complicated model R.M.-Reader + Verifier, which includes several components, our model still outperforms by 0.7 in terms of F1 score. Furthermore, we observe that ELMo gives a great boosting on the performance, e.g., 2.8 points in terms of F1 for DocQA. This encourages us to incorporate ELMo into our model in future.", so we know that they compare to R.M.-Reader + Verifier and DocQA.
Answer: R.M.-Reader + Verifier and DocQA
True answer: SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo

Validation ID: d3dbb5c22ef204d85707d2d24284cc77fa816b6c
Question: What other models do they compare to?
From the context "Table TABREF21 reports comparison results in literature published. Our model achieves state-of-the-art on development dataset in setting without pre-trained large language model (ELMo). Comparing with the much complicated model R.M.-Reader + Verifier, which includes several components, our model still outperforms by 0.7 in terms of F1 score. Furthermore, we observe that ELMo gives a great boosting on the performance, e.g., 2.8 points in terms of F1 for DocQA. This encourages us to incorporate ELMo into our model in future.", so we know that they compare to R.M.-Reader + Verifier and DocQA.
Answer: R.M.-Reader + Verifier and DocQA
True answer: BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo

Validation ID: a5e49cdb91d9fd0ca625cc1ede236d3d4672403c
Question: What is the architecture of the span detector?
From the context "Span detector. We adopt a multi-turn answer module for the span detector BIBREF1. Formally, at time step INLINEFORM0 in the range of INLINEFORM1, the state is defined by INLINEFORM2. The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5, where INLINEFORM6. Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11. Finally, a bilinear function is used to find the begin and end point of answer spans at each reasoning step INLINEFORM12 : DISPLAYFORM0 DISPLAYFORM1
The final prediction is the average of each time step: INLINEFORM0. We randomly apply dropout on the step level in each time step during training, as done in BIBREF1.", so we know that the architecture of the span detector is multi-turn answer module.
True answer: adopt a multi-turn answer module for the span detector BIBREF1

Validation ID: a5e49cdb91d9fd0ca625cc1ede236d3d4672403c
Question: What is the architecture of the span detector?
From the context "Span detector. We adopt a multi-turn answer module for the span detector BIBREF1. Formally, at time step INLINEFORM0 in the range of INLINEFORM1, the state is defined by INLINEFORM2. The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5, where INLINEFORM6. Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11. Finally, a bilinear function is used to find the begin and end point of answer spans at each reasoning step INLINEFORM12 : DISPLAYFORM0 DISPLAYFORM1
The final prediction is the average of each time step: INLINEFORM0. We randomly apply dropout on the step level in each time step during training, as done in BIBREF1.", so we know that the architecture of the span detector is multi-turn answer module.
True answer: Unanswerable

Validation ID: aefa333b2cf0a4000cd40566149816f5b36135e7
Question: What evaluation metric do they use?
From the context "The ratio of correct `translations' (matches) was used as an evaluation measure", so we know that the evaluation metric is the ratio of correct `translations' (matches).
Answer: the ratio of correct `translations' (matches)
True answer: Accuracy

Validation ID: aefa333b2cf0a4000cd40566149816f5b36135e7
Question: What evaluation metric do they use?
From the context "The ratio of correct `translations' (matches) was used as an evaluation measure", so we know that the evaluation metric is the ratio of correct `translations' (matches).
Answer: the ratio of correct `translations' (matches)
True answer: ratio of correct `translations'

Validation ID: c5abe97625b9e1c8de8208e15d59c704a597b88c
Question: What are the results from these proposed strategies?
From the context "Figure FIGREF10 shows that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.", so we know that the results from these proposed strategies are that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.
Answer: agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.
True answer: Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.

Validation ID: c5abe97625b9e1c8de8208e15d59c704a597b88c
Question: What are the results from these proposed strategies?
From the context "Figure FIGREF10 shows that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.", so we know that the results from these proposed strategies are that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.
Answer: agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.
True answer: KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40

Validation ID: eb2d5edcdfe18bd708348283f92a32294bb193a5
Question: What are the baselines?
From the context "We compare our two exploration strategies to the following baselines and ablations:", so we know that the baselines are KG-A2C, A2C, A2C-chained, A2C-Explore.
Answer: KG-A2C, A2C, A2C-chained, A2C-Explore
True answer: a score of 40

Validation ID: eb2d5edcdfe18bd708348283f92a32294bb193a5
Question: What are the baselines?
From the context "We compare our two exploration strategies to the following baselines and ablations:", so we know that the baselines are KG-A2C, A2C, A2C-chained, A2C-Explore.
Answer: KG-A2C, A2C, A2C-chained, A2C-Explore
True answer: KG-A2C, A2C, A2C-chained, A2C-Explore

Validation ID: 88ab7811662157680144ed3fdd00939e36552672
Question: What are the two new strategies?
From the context "We first introduce a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen. The second contribution explore how to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9", so we know that the two new strategies are KG-A2C-chained and KG-A2C-Explore.
Answer: KG-A2C-chained and KG-A2C-Explore
True answer: a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state, to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space

Validation ID: 88ab7811662157680144ed3fdd00939e36552672
Question: What are the two new strategies?
From the context "We first introduce a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen. The second contribution explore how to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9", so we know that the two new strategies are KG-A2C-chained and KG-A2C-Explore.
Answer: KG-A2C-chained and KG-A2C-Explore
True answer: KG-A2C-chained, KG-A2C-Explore

Validation ID: cb196725edc9cdb2c54b72364f3bbf7c76471490
Question: Do they report results only on English data?
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: cb196725edc9cdb2c54b72364f3bbf7c76471490
Question: Do they report results only on English data?
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 286078813136943dfafb5155ee15d2429e7601d9
Question: How much better than the baseline is LiLi?
From the context "Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability", so we know that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.
Answer: highly effective
True answer: In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. 


Validation ID: 286078813136943dfafb5155ee15d2429e7601d9
Question: How much better than the baseline is LiLi?
From the context "Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability", so we know that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.
Answer: highly effective
True answer: Unanswerable

Validation ID: 8f16dc7d7be0d284069841e456ebb2c69575b32b
Question: What baseline is used in the experiments?
From the context "We compare LiLi with the following baselines: (1) C-PR BIBREF16, the state-of-the-art closed-world KB completion method, (2) C-PR-Guess, which is the same as C-PR but uses the guessing mechanism to fill the gap when the user cannot answer a system question, and (3) C-PR-NoInteraction, which is the same as C-PR but does not interact with the user and thus, cannot learn new knowledge. We also compare LiLi with the following methods: (4) NELL BIBREF23, a knowledge learning system that continuously updates its KB using facts extracted from the Web, and (5) ILL BIBREF24, BIBREF25, which is an interactive language learning system that learns new words from the user. We also compare LiLi with the following methods: (6) C-PR-NoLL, which is the same as C-PR but does not use lifelong learning, and (7) C-PR-NoInteraction-NoLL, which is the same as C-PR-NoInteraction but does not use lifelong learning. We also compare LiLi with the following methods: (8) C-PR-NoLL-NoInteraction, which is the same as C-PR-NoLL but does not interact with the user, and (9) C-PR-NoInteraction-NoLL, which is the same as C-PR-NoInteraction but does not use lifelong learning.
Answer: C-PR, C-PR-Guess, C-PR-NoInteraction, NELL, ILL, C-PR-NoLL, C-PR-NoInteraction-NoLL, C-PR-NoLL-NoInteraction
True answer: versions of LiLi

Validation ID: 8f16dc7d7be0d284069841e456ebb2c69575b32b
Question: What baseline is used in the experiments?
From the context "We compare LiLi with the following baselines: (1) C-PR BIBREF16, the state-of-the-art closed-world KB completion method, (2) C-PR-Guess, which is the same as C-PR but uses guessing mechanism to fill the gap when the user cannot answer a system question, and (3) C-PR-NoInteraction, which is the same as C-PR but does not interact with the user and thus, cannot learn new knowledge. We also compare LiLi with the following two methods: (4) C-PR-NoLL, which is the same as C-PR but does not use lifelong learning and (5) C-PR-NoInteraction-NoLL, which is the same as C-PR-NoInteraction but does not use lifelong learning. We also compare LiLi with the following two methods: (6) C-PR-NoLL-NoInteraction, which is the same as C-PR-NoLL but does not interact with the user and (7) C-PR-NoInteraction-NoLL, which is the same as C-PR-NoInteraction but does not use lifelong learning.
Answer: C-PR, C-PR-Guess, C-PR-NoInteraction, C-PR-NoLL, C-PR-NoLL-NoInteraction, C-PR-NoInteraction-NoLL
True answer: various versions of LiLi as baselines, Single, Sep, F-th, BG, w/o PTS

Validation ID: a7d020120a45c39bee624f65443e09b895c10533
Question: In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?
From the context "We solve the OKBC problem by mimicking how humans acquire knowledge and perform reasoning in an interactive conversation. Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. The process typically involves an inference strategy (a sequence of actions), which interleaves a sequence of processing and interactive actions. A processing action can be the selection of related facts, deriving inference chain, etc., that advances the inference process. An interactive action can be deciding what to ask, formulating a suitable question, etc., that enable us to interact. The process helps grow the knowledge over time and the gained knowledge enables us to communicate better in the future. We call this lifelong interactive learning and inference (LiLi). Lifelong learning is reflected by the facts that the newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning. LiLi should have the following capabilities:
This setting is ideal for many NLP applications like dialog and question-answering systems that naturally provide the scope for human interaction and demand real-time inference.", so we know that LiLi imitates how humans acquire knowledge and perform inference during an interactive conversation.
True answer: newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning

Validation ID: a7d020120a45c39bee624f65443e09b895c10533
Question: In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?
From the context "Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. The process typically involves an inference strategy (a sequence of actions), which interleaves a sequence of processing and interactive actions. A processing action can be the selection of related facts, deriving inference chain, etc., that advances the inference process. An interactive action can be deciding what to ask, formulating a suitable question, etc., that enable us to interact. The process helps grow the knowledge over time and the gained knowledge enables us to communicate better in the future.", so we know that LiLi imitates how humans acquire knowledge and perform inference during an interactive conversation by performing inference using existing knowledge and asking questions to others to acquire related knowledge and use it in inference.
Answer: performing inference using existing knowledge and asking questions to others to acquire related knowledge and use it in inference
True answer: Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. 

Validation ID: 585626d18a20d304ae7df228c2128da542d248ff
Question: What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation??
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.", so we know that the metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are predictive performance and strategy formulation ability.
Answer: predictive performance and strategy formulation ability
True answer: Coverage, Avg. MCC and avg. +ve F1 score

Validation ID: 585626d18a20d304ae7df228c2128da542d248ff
Question: What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation??
From the context "We empirically verify the effectiveness of LiLi on two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability.", so we know that the metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are predictive performance and strategy formulation ability.
Answer: predictive performance and strategy formulation ability
True answer: strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score

Validation ID: bfc2dc913e7b78f3bd45e5449d71383d0aa4a890
Question: What are the components of the general knowledge learning engine?
From the context "We design LiLi as a combination of two interconnected models: (1) a RL model that learns to formulate a query-specific inference strategy for performing the OKBC task, and (2) a lifelong prediction model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR. The framework improves its performance over time through user interaction and knowledge retention. Compared to the existing KB inference methods, LiLi overcomes the following three challenges for OKBC:", so we know that the components of the general knowledge learning engine are RL model, lifelong prediction model, user interaction and knowledge retention.
Answer: RL model, lifelong prediction model, user interaction and knowledge retention
True answer: Answer with content missing: (list)
LiLi should have the following capabilities:
1. to formulate an inference strategy for a given query that embeds processing and interactive actions.
2. to learn interaction behaviors (deciding what to ask and when to ask the user).
3. to leverage the acquired knowledge in the current and future inference process.
4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.

Validation ID: bfc2dc913e7b78f3bd45e5449d71383d0aa4a890
Question: What are the components of the general knowledge learning engine?
From the context "We design LiLi as a combination of two interconnected models: (1) a RL model that learns to formulate a query-specific inference strategy for performing the OKBC task, and (2) a lifelong prediction model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR. The framework improves its performance over time through user interaction and knowledge retention. Compared to the existing KB inference methods, LiLi overcomes the following three challenges for OKBC:", so we know that the components of the general knowledge learning engine are RL model, lifelong prediction model, user interaction and knowledge retention.
Answer: RL model, lifelong prediction model, user interaction and knowledge retention
True answer: Knowledge Store (KS) , Knowledge Graph ( INLINEFORM0 ),  Relation-Entity Matrix ( INLINEFORM2 ), Task Experience Store ( INLINEFORM15 ), Incomplete Feature DB ( INLINEFORM29 )

Validation ID: 6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de
Question: How many labels do the datasets have?
From the context "We construct new datasets by ourselves. The results on the original Amazon benchmark is qualitatively similar, and we present them in Appendix SECREF6 for completeness since most of previous works reported results on it.", so we know that the datasets have two labels.
Answer: two labels
True answer: 719313

Validation ID: 6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de
Question: How many labels do the datasets have?
From the context "We construct new datasets by ourselves. The results on the original Amazon benchmark is qualitatively similar, and we present them in Appendix SECREF6 for completeness since most of previous works reported results on it.", so we know that the datasets have two labels.
Answer: two labels
True answer: Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.

Validation ID: b46c0015a122ee5fb95c2a45691cb97f80de1bb6
Question: What is the architecture of the model?
From the context "We have left the feature encoder INLINEFORM0 unspecified, for which, a few options can be considered. In our implementation, we adopt a one-layer CNN structure from previous works BIBREF22, BIBREF4, as it has been demonstrated to work well for sentiment classification tasks. Given a review document INLINEFORM1 consisting of INLINEFORM2 words, we begin by associating each word with a continuous word embedding BIBREF23 INLINEFORM3 from an embedding matrix INLINEFORM4, where INLINEFORM5 is the vocabulary size and INLINEFORM6 is the embedding dimension. INLINEFORM7 is jointly updated with other network parameters during training.", so we know that the architecture of the model is a one-layer CNN structure.
Answer: one-layer CNN structure
True answer: one-layer CNN structure from previous works BIBREF22 , BIBREF4

Validation ID: b46c0015a122ee5fb95c2a45691cb97f80de1bb6
Question: What is the architecture of the model?
From the context "We attempt to explicitly minimize the distance between the source and target feature representations ( INLINEFORM0 and INLINEFORM1 ). A few methods from literature can be applied such as Maximum Mean Discrepancy (MMD) BIBREF14 or adversary training BIBREF15, BIBREF16. The main idea of MMD is to estimate the distance between two distributions as the distance between sample means of the projected embeddings in Hilbert space. MMD is implicitly computed through a characteristic kernel, which is used to ensure that the sample mean is injective, leading to the MMD being zero if and only if the distributions are identical. In our implementation, we skip the mapping procedure induced by a characteristic kernel for simplifying the computation and learning. We simply estimate the distribution distance as the distance between the sample means in the current embedding space. Although this approximation cannot preserve all statistical features of the underlying distributions, we find it performs comparably to MMD on our problem. The following equations formally describe the feature adaptation loss INLINEFORM2 : DISPLAYFORM0
INLINEFORM0 normalization is applied on the mean representations INLINEFORM1 and INLINEFORM2, rescaling the vectors such that all entries sum to 1. We adopt a symmetric version of KL divergence BIBREF12 as the distance function. Given two distribution vectors INLINEFORM3, INLINEFORM4.", so we know that the architecture of the model is CNN.
True answer:  one-layer CNN

Validation ID: 5b7a4994bfdbf8882f391adf1cd2218dbc2255a0
Question: What are the baseline methods?
From the context "We compare with the following baselines: (1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM classifier trained on the source domain. (2) mSDA BIBREF7 : This is the state-of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5. (3) NaiveNN: This is a non-domain-adaptive CNN trained on source domain, which is a variant of our model by setting INLINEFORM0, INLINEFORM1, and INLINEFORM2 to zeros. (4) AuxNN BIBREF4 : This is a neural model that exploits auxiliary tasks, which has achieved state-of-the-art results on cross-domain sentiment classification. The sentence encoder used in this model is the same as ours. (5) ADAN BIBREF16 : This method exploits adversarial training to reduce representation difference between domains. The original paper uses a simple feedforward network as encoder. For fair comparison, we replace it with our CNN-based encoder. We train 5 iterations on the discriminator per iteration on the encoder and sentiment classifier as suggested in their paper. (6) MMD: MMD has been widely used for minimizing domain discrepancy on images. In those works BIBREF9, BIBREF13, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized. In NLP, adding more layers of CNNs may not be very helpful and thus those models from image-related tasks can not be directly applied to our problem. To compare with MMD-based method, we train a model that jointly minimize the classification loss INLINEFORM0 on the source domain and MMD between INLINEFORM1 and INLINEFORM2.", so we know that the baseline methods are Naive, mSDA, NaiveNN, AuxNN, ADAN, and MMD.
Answer: Naive, mSDA, NaiveNN, AuxNN, ADAN, and MMD
True answer: (1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD

Validation ID: 5b7a4994bfdbf8882f391adf1cd2218dbc2255a0
Question: What are the baseline methods?
From the context "We compare with the following baselines: (1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM classifier trained on the source domain. (2) mSDA BIBREF7 : This is the state-of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5. (3) NaiveNN: This is a non-domain-adaptive CNN trained on source domain, which is a variant of our model by setting INLINEFORM0, INLINEFORM1, and INLINEFORM2 to zeros. (4) AuxNN BIBREF4 : This is a neural model that exploits auxiliary tasks, which has achieved state-of-the-art results on cross-domain sentiment classification. The sentence encoder used in this model is the same as ours. (5) ADAN BIBREF16 : This method exploits adversarial training to reduce representation difference between domains. The original paper uses a simple feedforward network as encoder. For fair comparison, we replace it with our CNN-based encoder. We train 5 iterations on the discriminator per iteration on the encoder and sentiment classifier as suggested in their paper. (6) MMD: MMD has been widely used for minimizing domain discrepancy on images. In those works BIBREF9, BIBREF13, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized. In NLP, adding more layers of CNNs may not be very helpful and thus those models from image-related tasks can not be directly applied to our problem. To compare with MMD-based method, we train a model that jointly minimize the classification loss INLINEFORM0 on the source domain and MMD between INLINEFORM1 and INLINEFORM2.", so we know that the baseline methods are Naive, mSDA, NaiveNN, AuxNN, ADAN, and MMD.
Answer: Naive, mSDA, NaiveNN, AuxNN, ADAN, and MMD
True answer: non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized

Validation ID: 9176d2ba1c638cdec334971c4c7f1bb959495a8e
Question: What are the source and target domains?
From the context "We conduct most of our experiments under an unsupervised domain adaptation setting, where we have no labeled data from the target domain. Consider two sets INLINEFORM0 and INLINEFORM1. INLINEFORM2 is from the source domain with INLINEFORM3 labeled examples, where INLINEFORM4 is a one-hot vector representation of sentiment label and INLINEFORM5 denotes the number of classes. INLINEFORM6 is from the target domain with INLINEFORM7 unlabeled examples.", so we know that the source domain is INLINEFORM2 and the target domain is INLINEFORM6.
Answer: source domain is INLINEFORM2 and the target domain is INLINEFORM6
True answer: Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen

Validation ID: 9176d2ba1c638cdec334971c4c7f1bb959495a8e
Question: What are the source and target domains?
From the context "We conduct most of our experiments under an unsupervised domain adaptation setting, where we have no labeled data from the target domain. Consider two sets INLINEFORM0 and INLINEFORM1. INLINEFORM2 is from the source domain with INLINEFORM3 labeled examples, where INLINEFORM4 is a one-hot vector representation of sentiment label and INLINEFORM5 denotes the number of classes. INLINEFORM6 is from the target domain with INLINEFORM7 unlabeled examples.", so we know that the source domain is INLINEFORM2 and the target domain is INLINEFORM6.
Answer: source domain is INLINEFORM2 and the target domain is INLINEFORM6
True answer: we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)

Validation ID: 0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c
Question: Did they use a crowdsourcing platform for annotations?
From the context "The senses used for annotation are coarse-grained – with one sense covering a range of related usages. Each word has between two and four senses. To employ the method described in previous section, the NCP senses have been manually mapped to fine-grained plWordNet synsets. As NCP senses have been created independently from wordnet senses, a substantial part of the latter remain uncovered by the former. However, we only found four cases where an NCP sense has no counterpart in wordnet; those words are excluded from the test set.", so we know that the NCP senses have been manually mapped to fine-grained plWordNet synsets.
Answer: Yes
True answer: No

Validation ID: 0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c
Question: Did they use a crowdsourcing platform for annotations?
From the context "The senses used for annotation are coarse-grained – with one sense covering a range of related usages. Each word has between two and four senses. To employ the method described in previous section, the NCP senses have been manually mapped to fine-grained plWordNet synsets. As NCP senses have been created independently from wordnet senses, a substantial part of the latter remain uncovered by the former. However, we only found four cases where an NCP sense has no counterpart in wordnet; those words are excluded from the test set.", so we know that the NCP senses have been manually mapped to fine-grained plWordNet synsets.
Answer: Yes
True answer: Unanswerable

Validation ID: 5e324846a99a5573cd2e843d1657e87f4eb22fa6
Question: How do they deal with unknown distribution senses?
From the context "To deal with the problem of unknown sense distribution, the Bayesian classifier has been modified, removing the bias towards frequent labels in the training data", so we know that they deal with unknown distribution senses by removing the bias towards frequent labels in the training data.
Answer: removing the bias towards frequent labels in the training data
True answer: The Näive-Bayes classifier is corrected so it is not biased to most frequent classes

Validation ID: 5e324846a99a5573cd2e843d1657e87f4eb22fa6
Question: How do they deal with unknown distribution senses?
From the context "To deal with the problem of unknown sense distribution, the Bayesian classifier has been modified, removing the bias towards frequent labels in the training data", so we know that they deal with unknown distribution senses by removing the bias towards frequent labels in the training data.
Answer: removing the bias towards frequent labels in the training data
True answer: Bayesian classifier has been modified, removing the bias towards frequent labels in the training data

Validation ID: 2ccc26e11df4eb26fcccdd1f446dc749aff5d572
Question: Do they report results only on English data?
From the context "We converted all the ISIS magazines to texts using pdf readers and all articles that addressed women in both magazines (20 articles) were selected for our analysis. To facilitate comparison with a mainstream, non-violent religious group, we collected articles from catholicwomensforum.org, an online resource catering to Catholic women. We scrapped 132 articles from this domain.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 2ccc26e11df4eb26fcccdd1f446dc749aff5d572
Question: Do they report results only on English data?
From the context "We converted all the ISIS magazines to texts using pdf readers and all articles that addressed women in both magazines (20 articles) were selected for our analysis. To facilitate comparison with a mainstream, non-violent religious group, we collected articles from catholicwomensforum.org, an online resource catering to Catholic women. We scrapped 132 articles from this domain.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: f318a2851d7061f05a5b32b94251f943480fbd15
Question: What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?
From the context "Overall, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community.", so we know that the authors draw the conclusion that automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community.
Answer: automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community.
True answer: both corpuses used words that aim to inspire readers while avoiding fear, actual words that lead to these effects are very different in the two contexts, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda

Validation ID: f318a2851d7061f05a5b32b94251f943480fbd15
Question: What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?
From the context "Overall, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community.", so we know that the authors draw the conclusion that automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community.
Answer: automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community
True answer: By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores

Validation ID: 6bbbb9933aab97ce2342200447c6322527427061
Question: How id Depechemood trained?
From the context "Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news BIBREF24. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words.", so we know that Depechemood is trained by crowd-annotated news.
Answer: crowd-annotated news
True answer: By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. 

Validation ID: 6bbbb9933aab97ce2342200447c6322527427061
Question: How id Depechemood trained?
From the context "Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news BIBREF24. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words.", so we know that Depechemood is trained by crowd-annotated news.
Answer: crowd-annotated news
True answer: researchers asked subjects to report their emotions after reading each article, multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words, Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories

Validation ID: 2007bfb8f66e88a235c3a8d8c0a3b3dd88734706
Question: How are similarities and differences between the texts from violent and non-violent religious groups analyzed?
From the context "To compare the contents, we used word frequency and topic modeling with NMF. Also, our results showed that NMF outperforms LDA due to the niche domain and relatively small number of documents.", so we know that the similarities and differences between the texts from violent and non-violent religious groups are analyzed by using word frequency and topic modeling with NMF.
Answer: word frequency and topic modeling with NMF
True answer: By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum

Validation ID: 2007bfb8f66e88a235c3a8d8c0a3b3dd88734706
Question: How are similarities and differences between the texts from violent and non-violent religious groups analyzed?
From the context "To compare the contents, we used word frequency and topic modeling with NMF. Also, our results showed that NMF outperforms LDA due to the niche domain and relatively small number of documents.", so we know that the similarities and differences between the texts from violent and non-violent religious groups are analyzed by using word frequency and topic modeling with NMF.
Answer: word frequency and topic modeling with NMF
True answer: A comparison of common words, We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource

Validation ID: d859cc37799a508bbbe4270ed291ca6394afce2c
Question: How are prominent topics idenified in Dabiq and Rumiyah?
From the context "To compare the contents, we used word frequency and topic modeling with NMF. Also, our results showed that NMF outperforms LDA due to the niche domain and relatively small number of documents.", so we know that the prominent topics are identified by word frequency and topic modeling with NMF.
Answer: word frequency and topic modeling with NMF
True answer: LDA, non-negative matrix factorization (NMF)

Validation ID: d859cc37799a508bbbe4270ed291ca6394afce2c
Question: How are prominent topics idenified in Dabiq and Rumiyah?
From the context "To compare the contents, we used word frequency and topic modeling with NMF. Also, our results showed that NMF outperforms LDA due to the niche domain and relatively small number of documents.", so we know that the prominent topics are identified by word frequency and topic modeling with NMF.
Answer: word frequency and topic modeling with NMF
True answer: Using NMF based topic modeling and their coherence prominent topics are identified

Validation ID: 50e80cfa84200717921840fddcf3b051a9216ad8
Question: Are the images from a specific domain?
From the context "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper. We consider three different types of captioning tasks, each of which focuses on a distinct aspect of reasoning abilities. Existential descriptions examine whether a certain object is present in an image. Spatial descriptions identify spatial relationships among visual objects. Quantification descriptions involve count-based and ratio-based statements, with an explicit focus on inspecting models for their counting ability. We develop two variants for each type of dataset to enable different levels of visual complexity or specific aspects of the same reasoning type. All the training and test captions sampled in this work are in English.", so we know that the images are from a specific domain.
Answer: Yes
True answer: Yes

Validation ID: 50e80cfa84200717921840fddcf3b051a9216ad8
Question: Are the images from a specific domain?
From the context "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper. We consider three different types of captioning tasks, each of which focuses on a distinct aspect of reasoning abilities. Existential descriptions examine whether a certain object is present in an image. Spatial descriptions identify spatial relationships among visual objects. Quantification descriptions involve count-based and ratio-based statements, with an explicit focus on inspecting models for their counting ability. We develop two variants for each type of dataset to enable different levels of visual complexity or specific aspects of the same reasoning type. All the training and test captions sampled in this work are in English.", so we know that the images are from a specific domain.
Answer: Yes
True answer: Yes

Validation ID: b1bc9ae9d40e7065343c12f860a461c7c730a612
Question: Which datasets are used?
From the context "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper.", so we know that the datasets used are ShapeWorldICE datasets.
Answer: ShapeWorldICE datasets
True answer: Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE

Validation ID: b1bc9ae9d40e7065343c12f860a461c7c730a612
Question: Which datasets are used?
From the context "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper.", so we know that the datasets used are ShapeWorldICE datasets.
Answer: ShapeWorldICE datasets
True answer: ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio

Validation ID: 63a1cbe66fd58ff0ead895a8bac1198c38c008aa
Question: Which existing models are evaluated?
From the context "We experiment with two image captioning models: the Show&Tell model BIBREF0 and the LRCN1u model BIBREF1", so we know that the existing models are Show&Tell model and LRCN1u model.
Answer: Show&Tell model and LRCN1u model
True answer: Show&Tell and LRCN1u

Validation ID: 63a1cbe66fd58ff0ead895a8bac1198c38c008aa
Question: Which existing models are evaluated?
From the context "We experiment with two image captioning models: the Show&Tell model BIBREF0 and the LRCN1u model BIBREF1", so we know that the existing models are Show&Tell model and LRCN1u model.
Answer: Show&Tell model and LRCN1u model
True answer: Show&Tell model, LRCN1u

Validation ID: 509af1f11bd6f3db59284258e18fdfebe86cae47
Question: How is diversity measured?
From the context "The diversity ratios of generated captions for different ShapeWorldICE datasets are illustrated in Figure FIGREF17. We can see that the diversity of inferred captions is largely sensitive to the caption variability in the dataset itself. For simple datasets (such as Existential-OneShape) where language constructions in the training set are less diverse, the output captions tend to have uniform sentence structures. The high diversity ratios of generated Spatial and Quantification captions suggest that caption diversity benefits from heterogeneous language constructions in complex datasets.", so we know that diversity is measured by the ratio of observed number versus optimal number.
Answer: the ratio of observed number versus optimal number
True answer: diversity score as the ratio of observed number versus optimal number

Validation ID: 509af1f11bd6f3db59284258e18fdfebe86cae47
Question: How is diversity measured?
From the context "The diversity ratios of generated captions for different ShapeWorldICE datasets are illustrated in Figure FIGREF17. We can see that the diversity of inferred captions is largely sensitive to the caption variability in the dataset itself. For simple datasets (such as Existential-OneShape) where language constructions in the training set are less diverse, the output captions tend to have uniform sentence structures. The high diversity ratios of generated Spatial and Quantification captions suggest that caption diversity benefits from heterogeneous language constructions in complex datasets.", so we know that diversity is measured by the ratio of observed number versus optimal number.
Answer: the ratio of observed number versus optimal number
True answer:  we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number

Validation ID: 23e16c1173b7def2c5cb56053b57047c9971e3bb
Question: What state-of-the-art deep neural network is used?
From the context "We choose an LSTM module for the CWS task", so we know that the state-of-the-art deep neural network is LSTM.
Answer: LSTM
True answer: LSTM model

Validation ID: 23e16c1173b7def2c5cb56053b57047c9971e3bb
Question: What state-of-the-art deep neural network is used?
From the context "We choose an LSTM module for the CWS task", so we know that the state-of-the-art deep neural network is LSTM.
Answer: LSTM
True answer: BIBREF15, BIBREF19, BIBREF20 

Validation ID: d78f7f84a76a07b777d4092cb58161528ca3803c
Question: What boundary assembling method is used?
From the context "In each sentence, Chinese characters are labeled as either Begin, Inside, End, or Singleton (BIES labeling). The likelihood of individual Chinese characters being labeled as each type is calculated by the LSTM module described in the previous section. BIBREF12 found in a Chinese corpus that the word label "End" has a better performance than "Begin". This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition. This strategy has the advantage to find named entities with long word length. It also reduces the influence caused by different segmentation criteria.", so we know that the boundary assembling method is a backward greedy search.
Answer: a backward greedy search
True answer: This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition.

Validation ID: d78f7f84a76a07b777d4092cb58161528ca3803c
Question: What boundary assembling method is used?
From the context "In each sentence, Chinese characters are labeled as either Begin, Inside, End, or Singleton (BIES labeling). The likelihood of individual Chinese characters being labeled as each type is calculated by the LSTM module described in the previous section. BIBREF12 found in a Chinese corpus that the word label "End" has a better performance than "Begin". This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition. This strategy has the advantage to find named entities with long word length. It also reduces the influence caused by different segmentation criteria.", so we know that the boundary assembling method is a backward greedy search over each sentence's label sequence to identify word boundaries.
Answer: a backward greedy search over each sentence's label sequence to identify word boundaries
True answer: backward greedy search over each sentence's label sequence to identify word boundaries

Validation ID: 9da1e124d28b488b0d94998d32aa2fa8a5ebec51
Question: What are previous state of the art results?
From the context "Our results are compared with state-of-the-art models BIBREF15, BIBREF19, BIBREF20 on the same Sina Weibo training and test datasets. Our model shows an absolute improvement of 2% for the overall F1 score.", so we know that the previous state of the art results are BIBREF15, BIBREF19, BIBREF20.
Answer: BIBREF15, BIBREF19, BIBREF20
True answer: Overall F1 score:
- He and Sun (2017) 58.23
- Peng and Dredze (2017) 58.99
- Xu et al. (2018) 59.11

Validation ID: 9da1e124d28b488b0d94998d32aa2fa8a5ebec51
Question: What are previous state of the art results?
From the context "Our results are compared with state-of-the-art models BIBREF15, BIBREF19, BIBREF20 on the same Sina Weibo training and test datasets. Our model shows an absolute improvement of 2% for the overall F1 score.", so we know that the previous state of the art results are BIBREF15, BIBREF19, BIBREF20.
Answer: BIBREF15, BIBREF19, BIBREF20
True answer: For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%

Validation ID: 37be0d479480211291e068d0d3823ad0c13321d3
Question: What is the model performance on target language reading comprehension?
From the context "The experimental results on English, Chinese and Korean corpora show that even when the languages for training and testing are different, reasonable performance can be obtained", so we know that the model performance on target language reading comprehension is reasonable.
Answer: reasonable
True answer: Table TABREF6, Table TABREF8

Validation ID: 37be0d479480211291e068d0d3823ad0c13321d3
Question: What is the model performance on target language reading comprehension?
From the context "The experimental results on English, Chinese and Korean corpora show that even when the languages for training and testing are different, reasonable performance can be obtained", so we know that the model performance on target language reading comprehension is reasonable.
Answer: reasonable
True answer: when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
From the context "We have training and testing sets in three different languages: English, Chinese and Korean. The English dataset is SQuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD.", so we know that the source-target language pairs were used in this work are English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese.
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
From the context "We have training and testing sets in three different languages: English, Chinese and Korean. The English dataset is SQuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD.", so we know that the source-target language pairs were used in this work are English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese.
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: English , Chinese

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
From the context "We have training and testing sets in three different languages: English, Chinese and Korean. The English dataset is SQuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD.", so we know that the source-target language pairs were used in this work are English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese.
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate

Validation ID: 009ce6f2bea67e7df911b3f93443b23467c9f4a1
Question: What model is used as a baseline? ?
From the context "We use the baseline model proposed in BIBREF12, which is a BiDAF model with self-attention and highway layers", so we know that the baseline model is BiDAF model with self-attention and highway layers.
Answer: BiDAF model with self-attention and highway layers
True answer: pre-trained multi-BERT

Validation ID: 009ce6f2bea67e7df911b3f93443b23467c9f4a1
Question: What model is used as a baseline? ?
From the context "We use the baseline model proposed in BIBREF12, which is a BiDAF model with self-attention and highway layers", so we know that the baseline model is BiDAF model with self-attention and highway layers.
Answer: BiDAF model with self-attention and highway layers
True answer: QANet , BIBREF14,  fine-tuned a BERT model

Validation ID: 55569d0a4586d20c01268a80a7e31a17a18198e2
Question: what does the model learn in zero-shot setting?
From the context "We observe linguistic-agnostic representations in the last subsection. If tokens are represented in a language-agnostic way, the model may be able to handle code-switching data. Because there is no code-switching data for RC, we create artificial code-switching datasets by replacing some of the words in contexts or questions with their synonyms in another language. The synonyms are found by word-by-word translation with given dictionaries. We use the bilingual dictionaries collected and released in facebookresearch/MUSE GitHub repository. We substitute the words if and only if the words are in the bilingual dictionaries.", so we know that the model learns the semantic mapping between different languages.
Answer: semantic mapping between different languages
True answer: we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged

Validation ID: 7cd22ca9e107d2b13a7cc94252aaa9007976b338
Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?
From the context "We showed that the modality attention based model outperforms other state-of-the-art baselines when text was the only modality available, by better combining word and character level information.", so we know that they inspect their model to see if their model learned to associate image parts with words related to entities.
Answer: Yes
True answer: Yes

Validation ID: 7cd22ca9e107d2b13a7cc94252aaa9007976b338
Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?
From the context "We showed that the modality attention based model outperforms other state-of-the-art baselines when text was the only modality available, by better combining word and character level information.", so we know that they inspect their model to see if their model learned to associate image parts with words related to entities.
Answer: Yes
True answer: Yes

Validation ID: adbf33c6144b2f5c40d0c6a328a92687a476f371
Question: Does their NER model learn NER from both text and images?
From the context "We demonstrated for the first time an effective MNER system, where visual information is combined with textual information to outperform traditional text-based NER baselines", so we know that their NER model learn NER from both text and images.
Answer: Yes
True answer: Yes

Validation ID: adbf33c6144b2f5c40d0c6a328a92687a476f371
Question: Does their NER model learn NER from both text and images?
From the context "We demonstrated for the first time an effective MNER system, where visual information is combined with textual information to outperform traditional text-based NER baselines", so we know that their NER model learn NER from both text and images.
Answer: Yes
True answer: Yes

Validation ID: f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24
Question: Which types of named entities do they recognize?
From the context "Task: given a caption and a paired image (if used), the goal is to label every token in a caption in BIO scheme (B: beginning, I: inside, O: outside) BIBREF27. We report the performance of the following state-of-the-art NER models as baselines, as well as several configurations of our proposed approach to examine contributions of each component (W: word, C: char, V: visual).", so we know that they recognize PER, LOC, ORG, MISC.
Answer: PER, LOC, ORG, MISC
True answer: PER, LOC, ORG, MISC

Validation ID: f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24
Question: Which types of named entities do they recognize?
From the context "Task: given a caption and a paired image (if used), the goal is to label every token in a caption in BIO scheme (B: beginning, I: inside, O: outside) BIBREF27. We report the performance of the following state-of-the-art NER models as baselines, as well as several configurations of our proposed approach to examine contributions of each component (W: word, C: char, V: visual).", so we know that they recognize PER, LOC, ORG, MISC.
Answer: PER, LOC, ORG, MISC
True answer: PER, LOC, ORG, MISC

Validation ID: a0543b4afda15ea47c1e623c7f00d4aaca045be0
Question: Can named entities in SnapCaptions be discontigious?
From the context "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories). Examples of such public crowd-sourced stories are “New York Story” or “Thanksgiving Story”, which comprise snaps that are aggregated for various public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available). We split the dataset into train (70%), validation (15%), and test sets (15%). The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings BIBREF22. Named entities annotated in the SnapCaptions dataset include many of new and emerging entities, and they are found in various surface forms (various nicknames, typos, etc.) To the best of our knowledge, SnapCaptions is the only dataset that contains natural image-caption pairs with expert-annotated named entities.", so we know that named entities in SnapCaptions can be discontigious.
Answer: Yes
True answer: No

Validation ID: a0543b4afda15ea47c1e623c7f00d4aaca045be0
Question: Can named entities in SnapCaptions be discontigious?
From the context "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Snapchat Live Stories or Our Stories). Examples of such public crowd-sourced stories are “New York Story” or “Thanksgiving Story”, which comprise snaps that are aggregated for various public events, venues, etc. All snaps were posted between year 2016 and 2017, and do not contain raw images or other associated information (only textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet are available). We split the dataset into train (70%), validation (15%), and test sets (15%). The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings BIBREF22. Named entities annotated in the SnapCaptions dataset include many of new and emerging entities, and they are found in various surface forms (various nicknames, typos, etc.) To the best of our knowledge, SnapCaptions is the only dataset that contains natural image-caption pairs with expert-annotated named entities.", so we know that named entities in SnapCaptions can be discontigious.
Answer: Yes
True answer: Unanswerable

Validation ID: 1591068b747c94f45b948e12edafe74b5e721047
Question: How large is their MNER SnapCaptions dataset?
From the context "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC)", so we know that the MNER SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs.
Answer: 10K
True answer: 10K user-generated image (snap) and textual caption pairs

Validation ID: 1591068b747c94f45b948e12edafe74b5e721047
Question: How large is their MNER SnapCaptions dataset?
From the context "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC)", so we know that the MNER SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs.
Answer: 10K
True answer: 10000

Validation ID: 193ee49ae0f8827a6e67388a10da59e137e7769f
Question: What is masked document generation?
From the context "The third task we consider is Masked Document Generation (MDG) that learns to recover a document with a masked span of tokens (see Figure FIGREF5). For simplicity, a document consisting of a sequence of tokens is denoted as $X=(x_1, x_2, \cdots, x_{|X|})$. We randomly sample the length of the span $l$ from a discrete uniform distribution $\mathcal {U}(a, b)$ and the span start position $k$ from another discrete uniform distribution $\mathcal {U}(1, |X|-l+1)$ (see Section SECREF4 for more details). Thus, $\mathcal {M}=(x_k, x_{k+1}, \cdots, x_{k+l-1})$ is the text span to be masked.", so we know that masked document generation is a task that learns to recover a document with a masked span of tokens.
Answer: a task that learns to recover a document with a masked span of tokens
True answer: A task for seq2seq model pra-training that recovers a masked document to its original form.

Validation ID: 193ee49ae0f8827a6e67388a10da59e137e7769f
Question: What is masked document generation?
From the context "The third task we consider is Masked Document Generation (MDG) that learns to recover a document with a masked span of tokens (see Figure FIGREF5). For simplicity, a document consisting of a sequence of tokens is denoted as $X=(x_1, x_2, \cdots, x_{|X|})$. We randomly sample the length of the span $l$ from a discrete uniform distribution $\mathcal {U}(a, b)$ and the span start position $k$ from another discrete uniform distribution $\mathcal {U}(1, |X|-l+1)$ (see Section SECREF4 for more details). Thus, $\mathcal {M}=(x_k, x_{k+1}, \cdots, x_{k+l-1})$ is the text span to be masked.", so we know that masked document generation is a task that learns to recover a document with a masked span of tokens.
Answer: a task that learns to recover a document with a masked span of tokens
True answer: recovers a masked document to its original form

Validation ID: ed2eb4e54b641b7670ab5a7060c7b16c628699ab
Question: Which of the three pretraining tasks is the most helpful?
From the context "Among all three pre-training tasks, SR works slightly better than the other two tasks (i.e., NSG and MDG). We also tried to randomly use all the three tasks during training with 1/3 probability each (indicated as ALL). Interesting, we observed that, in general, All outperforms all three tasks when employing unlabeled documents of training splits of CNNDM or NYT, which might be due to limited number of unlabeled documents of the training splits. After adding more data (i.e., GIAG-CM) to pre-training, SR consistently achieves highest ROUGE-2 on both", so we know that SR is the most helpful.
Answer: SR
True answer: SR

Validation ID: ed2eb4e54b641b7670ab5a7060c7b16c628699ab
Question: Which of the three pretraining tasks is the most helpful?
From the context "Among all three pre-training tasks, SR works slightly better than the other two tasks (i.e., NSG and MDG). We also tried to randomly use all the three tasks during training with 1/3 probability each (indicated as ALL). Interesting, we observed that, in general, All outperforms all three tasks when employing unlabeled documents of training splits of CNNDM or NYT, which might be due to limited number of unlabeled documents of the training splits. After adding more data (i.e., GIAG-CM) to pre-training, SR consistently achieves highest ROUGE-2 on both", so we know that SR is the most helpful.
Answer: SR
True answer: SR

Validation ID: beac555c4aea76c88f19db7cc901fa638765c250
Question: What useful information does attention capture?
From the context "However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information.", so we know that attention captures other relevant information.
Answer: other relevant information
True answer: it captures other information rather than only the translational equivalent in the case of verbs

Validation ID: beac555c4aea76c88f19db7cc901fa638765c250
Question: What useful information does attention capture?
From the context "However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information.", so we know that attention captures other relevant information.
Answer: other relevant information
True answer: Alignment points of the POS tags.

Validation ID: 91e326fde8b0a538bc34d419541b5990d8aae14b
Question: What datasets are used?
From the context "We train both of the systems on the WMT15 German-to-English training data, see Table TABREF18 for some statistics.", so we know that the datasets are WMT15 German-to-English training data.
Answer: WMT15 German-to-English training data
True answer: WMT15 German-to-English, RWTH German-English dataset

Validation ID: 91e326fde8b0a538bc34d419541b5990d8aae14b
Question: What datasets are used?
From the context "We train both of the systems on the WMT15 German-to-English training data, see Table TABREF18 for some statistics.", so we know that the datasets are WMT15 German-to-English training data.
Answer: WMT15 German-to-English training data
True answer: RWTH German-English dataset

Validation ID: 044f922604b4b3f42ae381419fd5cd5624fa0637
Question: In what cases is attention different from alignment?
From the context "We have shown that attention agrees with traditional alignment to a certain extent. However, this differs substantially by attention mechanism and the type of the word being generated. We have shown that attention has different patterns based on the POS tag of the target word. The concentrated pattern of attention and the relatively high correlations for nouns show that training the attention with explicit alignment labels is useful for generating nouns. However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information. This explains the mixed results reported when guiding attention to comply with alignments BIBREF9, BIBREF7, BIBREF8.", so we know that attention is different from alignment in the following cases:
1. Attention differs from alignment by attention mechanism
True answer: For certain POS tags, e.g. VERB, PRON.

Validation ID: 044f922604b4b3f42ae381419fd5cd5624fa0637
Question: In what cases is attention different from alignment?
From the context "We have shown that attention agrees with traditional alignment to a certain extent. However, this differs substantially by attention mechanism and the type of the word being generated. We have shown that attention has different patterns based on the POS tag of the target word. The concentrated pattern of attention and the relatively high correlations for nouns show that training the attention with explicit alignment labels is useful for generating nouns. However, this is not the case for verbs, since the large portion of attention being paid to words other than alignment points, is already capturing other relevant information. Training attention with alignments in this case will force the attention model to forget these useful information. This explains the mixed results reported when guiding attention to comply with alignments BIBREF9, BIBREF7, BIBREF8.", so we know that attention is different from alignment in the following cases:
1. Attention differs from alignment by attention mechanism
True answer: most word alignments only involve one or a few words, attention can be distributed more freely

Validation ID: f94b53db307685d572aefad52cd55f53d23769c2
Question: How do they calculate variance from the model outputs?
From the context "Assume the joint distribution of INLINEFORM0 has the following form, DISPLAYFORM0 where INLINEFORM0 is the true parameter, and INLINEFORM1 is independent of INLINEFORM2. By selecting a subset of the training data, we are essentially choosing another distribution INLINEFORM3 so that the INLINEFORM4 pairs are drawn from INLINEFORM5", so we know that they calculate variance from the model outputs by selecting a subset of the training data.
Answer: selecting a subset of the training data
True answer: reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3

Validation ID: f94b53db307685d572aefad52cd55f53d23769c2
Question: How do they calculate variance from the model outputs?
From the context "Assume the joint distribution of INLINEFORM0 has the following form, DISPLAYFORM0 where INLINEFORM0 is the true parameter, and INLINEFORM1 is independent of INLINEFORM2. By selecting a subset of the training data, we are essentially choosing another distribution INLINEFORM3 so that the INLINEFORM4 pairs are drawn from INLINEFORM5", so we know that they calculate variance from the model outputs by selecting a subset of the training data.
Answer: selecting a subset of the training data
True answer:  Fisher Information Ratio

Validation ID: aa7d327ef98f9f9847b447d4def04889b4508d7a
Question: How much data samples do they start with before obtaining the initial model labels?
From the context "A base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data", so we know that they start with 190 hours ( INLINEFORM1 100K instances) of transcribed speech data.
Answer: 190 hours ( INLINEFORM1 100K instances)
True answer: 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset

Validation ID: aa7d327ef98f9f9847b447d4def04889b4508d7a
Question: How much data samples do they start with before obtaining the initial model labels?
From the context "A base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data", so we know that they start with 190 hours ( INLINEFORM1 100K instances) of transcribed speech data.
Answer: 190 hours ( INLINEFORM1 100K instances)
True answer: INLINEFORM2 is queried for the “most informative” instance(s) INLINEFORM3

Validation ID: b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0
Question: Which model do they use for end-to-end speech recognition?
From the context "The speech recognition community has generally identified the informativeness of samples by calculating confidence scores. In particular, an utterance is considered informative if the most likely prediction has small probability BIBREF1, or if the predictions are distributed very uniformly over the labels BIBREF2. Though confidence-based measures work well in practice, less attention has been focused on gradient-based methods like Expected Gradient Length (EGL) BIBREF3, where the informativeness is measured by the norm of the gradient incurred by the instance. EGL has previously been justified as intuitively measuring the expected change in a model's parameters BIBREF3.We formalize this intuition from the perspective of asymptotic variance reduction, and experimentally, we show EGL to be superior to confidence-based methods on speech recognition tasks. Additionally, we observe that the ranking of samples scored by EGL is not correlated with that of confidence scoring, suggesting EGL identifies aspects of an instance that confidence scores cannot capture.", so we know that they use EGL for end-to-end speech recognition.
Answer: EGL
True answer: RNN

Validation ID: b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0
Question: Which model do they use for end-to-end speech recognition?
From the context "The speech recognition community has generally identified the informativeness of samples by calculating confidence scores. In particular, an utterance is considered informative if the most likely prediction has small probability BIBREF1, or if the predictions are distributed very uniformly over the labels BIBREF2. Though confidence-based measures work well in practice, less attention has been focused on gradient-based methods like Expected Gradient Length (EGL) BIBREF3, where the informativeness is measured by the norm of the gradient incurred by the instance. EGL has previously been justified as intuitively measuring the expected change in a model's parameters BIBREF3.We formalize this intuition from the perspective of asymptotic variance reduction, and experimentally, we show EGL to be superior to confidence-based methods on speech recognition tasks. Additionally, we observe that the ranking of samples scored by EGL is not correlated with that of confidence scoring, suggesting EGL identifies aspects of an instance that confidence scores cannot capture.", so we know that they use EGL for end-to-end speech recognition.
Answer: EGL
True answer:  Recurrent Neural Network (RNN)

Validation ID: 551457ed34ca7fc0878c85bc664b135c21059b58
Question: Which dataset do they use?
From the context "A base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training.", so we know that they use a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset.
Answer: a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset
True answer: 190 hours ( INLINEFORM1 100K instances)

Validation ID: 551457ed34ca7fc0878c85bc664b135c21059b58
Question: Which dataset do they use?
From the context "A base model, INLINEFORM0, is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training.", so we know that they use a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset.
Answer: a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset
True answer: trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data, selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset

Validation ID: 0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8
Question: Which baselines did they compare against?
From the context "Compared with the previous syntactic tree-based models as well as other neural models, our SATA Tree-LSTM shows superior or competitive performance on all tasks", so we know that they compared against the previous syntactic tree-based models as well as other neural models.
Answer: the previous syntactic tree-based models as well as other neural models
True answer: Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks

Validation ID: 0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8
Question: Which baselines did they compare against?
From the context "Compared with the previous syntactic tree-based models as well as other neural models, our SATA Tree-LSTM shows superior or competitive performance on all tasks", so we know that they compared against the previous syntactic tree-based models as well as other neural models.
Answer: the previous syntactic tree-based models as well as other neural models
True answer: Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). 
Stanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).

Validation ID: 4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94
Question: What baselines did they consider?
From the context "We compared off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers.", so we know that they considered off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers as baselines.
Answer: off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers
True answer: state-of-the-art PDTB taggers

Validation ID: 4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94
Question: What baselines did they consider?
From the context "We compared off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers.", so we know that they considered off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers as baselines.
Answer: off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers
True answer: Linear SVM, RBF SVM, and Random Forest

Validation ID: a4d115220438c0ded06a91ad62337061389a6747
Question: What types of social media did they consider?
From the context "We created our own causal explanation dataset by collecting 3,268 random Facebook status update messages.", so we know that they considered Facebook status update messages.
Answer: Facebook status update messages
True answer: Facebook status update messages

Validation ID: a4d115220438c0ded06a91ad62337061389a6747
Question: What types of social media did they consider?
From the context "We created our own causal explanation dataset by collecting 3,268 random Facebook status update messages.", so we know that they considered Facebook status update messages.
Answer: Facebook status update messages
True answer: Facebook status update messages

Validation ID: 2c7e94a65f5f532aa31d3e538dcab0468a43b264
Question: How was the dataset annotated?
From the context "We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents. To collect additional data for each intent, we used the rephrase and scenario crowdsourcing tasks proposed by BIBREF2. For each intent, there are 100 training queries, which is representative of what a team with a limited budget could gather while developing a task-driven dialog system. Along with the 100 training queries, there are 20 validation and 30 testing queries per intent.", so we know that the dataset was annotated by crowdsourcing task.
Answer: crowdsourcing task
True answer: intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task

Validation ID: 2c7e94a65f5f532aa31d3e538dcab0468a43b264
Question: How was the dataset annotated?
From the context "We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents. To collect additional data for each intent, we used the rephrase and scenario crowdsourcing tasks proposed by BIBREF2. For each intent, there are 100 training queries, which is representative of what a team with a limited budget could gather while developing a task-driven dialog system. Along with the 100 training queries, there are 20 validation and 30 testing queries per intent.", so we know that the dataset was annotated by crowdsourcing task.
Answer: crowdsourcing task
True answer: manually 

Validation ID: 149da739b1c19a157880d9d4827f0b692006aa2c
Question: Which classifiers are evaluated?
From the context "To quantify the challenges that our new dataset presents, we evaluated the performance of a range of classifier models and out-of-scope prediction schemes.", so we know that the classifiers are evaluated.
From the context "SVM: A linear support vector machine with bag-of-words sentence representations. MLP: A multi-layer perceptron with USE embeddings BIBREF4 as input. FastText: A shallow neural network that averages embeddings of n-grams BIBREF5. CNN: A convolutional neural network with non-static word embeddings initialized with GloVe BIBREF6. BERT: A neural network that is trained to predict elided words in text and then fine-tuned on our data BIBREF1. Platforms: Several platforms exist for the development of task-oriented agents. We consider Google's DialogFlow and Rasa NLU with spacy-sklearn.", so we know that the classifiers are SVM, MLP, FastText, CNN, BERT, DialogFlow and Rasa NLU.
True answer: SVM, MLP, FastText, CNN, BERT, Google's DialogFlow, Rasa NLU

Validation ID: 149da739b1c19a157880d9d4827f0b692006aa2c
Question: Which classifiers are evaluated?
From the context "To quantify the challenges that our new dataset presents, we evaluated the performance of a range of classifier models and out-of-scope prediction schemes.", so we know that the classifiers are evaluated.
From the context "SVM: A linear support vector machine with bag-of-words sentence representations. MLP: A multi-layer perceptron with USE embeddings BIBREF4 as input. FastText: A shallow neural network that averages embeddings of n-grams BIBREF5. CNN: A convolutional neural network with non-static word embeddings initialized with GloVe BIBREF6. BERT: A neural network that is trained to predict elided words in text and then fine-tuned on our data BIBREF1. Platforms: Several platforms exist for the development of task-oriented agents. We consider Google's DialogFlow and Rasa NLU with spacy-sklearn.", so we know that the classifiers are SVM, MLP, FastText, CNN, BERT, DialogFlow and Rasa NLU.
True answer: SVM, MLP, FastText, CNN, BERT, DialogFlow, Rasa NLU

Validation ID: 27de1d499348e17fec324d0ef00361a490659988
Question: What is the size of this dataset?
From the context "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries.", so we know that the size of this dataset is 23,700.
Answer: 23,700
True answer: 23,700 

Validation ID: 27de1d499348e17fec324d0ef00361a490659988
Question: What is the size of this dataset?
From the context "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries.", so we know that the size of this dataset is 23,700.
Answer: 23,700
True answer:  23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries.

Validation ID: cfcdd73e712caf552ba44d0aa264d8dace65a589
Question: Where does the data come from?
From the context "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries.", so we know that the data comes from crowdsourcing.
Answer: crowdsourcing
True answer: crowsourcing platform

Validation ID: cfcdd73e712caf552ba44d0aa264d8dace65a589
Question: Where does the data come from?
From the context "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries.", so we know that the data comes from crowdsourcing.
Answer: crowdsourcing
True answer: For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. 
For out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere.

Validation ID: 23b2901264bda91045258b5d4120879ae292e950
Question: What are method improvements of F1 for paraphrase identification?
From the context "Using dice loss can help narrow the gap between training objectives and evaluation metrics. Empirically, we show that the proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks.", so we know that the method improvements of F1 for paraphrase identification are significant performance boost.
Answer: significant performance boost
True answer: Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP

Validation ID: 23b2901264bda91045258b5d4120879ae292e950
Question: What are method improvements of F1 for paraphrase identification?
From the context "Using dice loss can help narrow the gap between training objectives and evaluation metrics. Empirically, we show that the proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks.", so we know that the method improvements of F1 for paraphrase identification are significant performance boost.
Answer: significant performance boost
True answer: +0.58

Validation ID: b5bc34e1e381dbf972d0b594fe8c66ff75305d71
Question: What are method's improvements of F1 for NER task for English and Chinese datasets?
From the context "We show that the proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks", so we know that the method's improvements of F1 for NER task for English and Chinese datasets are significant.
Answer: significant
True answer: English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively

Validation ID: b5bc34e1e381dbf972d0b594fe8c66ff75305d71
Question: What are method's improvements of F1 for NER task for English and Chinese datasets?
From the context "We show that the proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks", so we know that the method's improvements of F1 for NER task for English and Chinese datasets are significant.
Answer: significant
True answer: For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively

Validation ID: 72f7ef55e150e16dcf97fe443aff9971a32414ef
Question: What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?
From the context "As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by +1.86 in terms of F1 score on CTB5, +1.80 on CTB6 and +2.19 on UD1.4.", so we know that the method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets are +1.86, +1.80 and +2.19.
Answer: +1.86, +1.80 and +2.19
True answer: +1.86 in terms of F1 score on CTB5, +1.80 on CTB6, +2.19 on UD1.4

Validation ID: 72f7ef55e150e16dcf97fe443aff9971a32414ef
Question: What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?
From the context "As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by +1.86 in terms of F1 score on CTB5, +1.80 on CTB6 and +2.19 on UD1.4.", so we know that the method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets are +1.86, +1.80 and +2.19.
Answer: +1.86, +1.80 and +2.19
True answer:  +1.86

Validation ID: 20e38438471266ce021817c6364f6a46d01564f2
Question: How are weights dynamically adjusted?
From the context "To address this issue, we propose to multiply the soft probability $p$ with a decaying factor $(1-p)$, changing Eq.DISPLAY_FORM22 to the following form:", so we know that the weights are dynamically adjusted by multiplying the soft probability $p$ with a decaying factor $(1-p)$.
Answer: multiplying the soft probability $p$ with a decaying factor $(1-p)$
True answer: One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.

Validation ID: 20e38438471266ce021817c6364f6a46d01564f2
Question: How are weights dynamically adjusted?
From the context "To address this issue, we propose to multiply the soft probability $p$ with a decaying factor $(1-p)$, changing Eq.DISPLAY_FORM22 to the following form:", so we know that the weights are dynamically adjusted by multiplying the soft probability $p$ with a decaying factor $(1-p)$.
Answer: multiplying the soft probability $p$ with a decaying factor $(1-p)$
True answer: associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds

Validation ID: 28067da818e3f61f8b5152c0d42a531bf0f987d4
Question: Ngrams of which length are aligned using PARENT?
From the context "PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3", so we know that PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3.
Answer: INLINEFORM1
True answer: Unanswerable

Validation ID: 28067da818e3f61f8b5152c0d42a531bf0f987d4
Question: Ngrams of which length are aligned using PARENT?
From the context "PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3", so we know that PARENT evaluates each instance INLINEFORM0 separately, by computing the precision and recall of INLINEFORM1 against both INLINEFORM2 and INLINEFORM3.
Answer: INLINEFORM1
True answer: Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4

Validation ID: bf3b27a4f4be1f9ae31319877fd0c75c03126fd5
Question: How many people participated in their evaluation study of table-to-text models?
From the context "We collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24. However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0, for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22, which assigns a score to each model based on how many times it was preferred over an alternative.
The data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.", so we know that 500 different workers were involved in the annotation.
True answer: about 500

Validation ID: bf3b27a4f4be1f9ae31319877fd0c75c03126fd5
Question: How many people participated in their evaluation study of table-to-text models?
From the context "We collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24. However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0, for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22, which assigns a score to each model based on how many times it was preferred over an alternative.
The data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.", so we know that 500 different workers were involved in the annotation.
True answer: Unanswerable

Validation ID: ffa7f91d6406da11ddf415ef094aaf28f3c3872d
Question: By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?
From the context "We show that this method is more effective than using the table as an additional reference. Our main contributions are:", so we know that PARENT is more effective than using the table as an additional reference.
From the context "We show that existing automatic metrics, including BLEU, correlate poorly with human judgments when the evaluation sets contain divergent references ($ SECREF36 ).", so we know that existing automatic metrics, including BLEU, correlate poorly with human judgments when the evaluation sets contain divergent references.
True answer: Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.

Validation ID: ffa7f91d6406da11ddf415ef094aaf28f3c3872d
Question: By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?
From the context "We show that this method is more effective than using the table as an additional reference. Our main contributions are:", so we know that PARENT is more effective than using the table as an additional reference.
From the context "We show that existing automatic metrics, including BLEU, correlate poorly with human judgments when the evaluation sets contain divergent references ($ SECREF36 ).", so we know that existing automatic metrics, including BLEU, correlate poorly with human judgments when the evaluation sets contain divergent references.
True answer: Their average correlation tops the best other model by 0.155 on WikiBio.

Validation ID: b634ff1607ce5756655e61b9a6f18bc736f84c83
Question: Which stock market sector achieved the best performance?
From the context "The best performing sector was the Consumer Discretionary sector, which gained 1.2%", so we know that the best performing sector was the Consumer Discretionary sector.
Answer: Consumer Discretionary sector
True answer: Energy with accuracy of 0.538

Validation ID: b634ff1607ce5756655e61b9a6f18bc736f84c83
Question: Which stock market sector achieved the best performance?
From the context "The best performing sector was the Consumer Discretionary sector, which gained 1.2%", so we know that the best performing sector was the Consumer Discretionary sector.
Answer: Consumer Discretionary sector
True answer: Energy

Validation ID: 2f901dab6b757e12763b23ae8b37ae2e517a2271
Question: What languages pairs are used in machine translation?
From the context "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German–English spoken-domain translation, applying fully character-level segmentation", so we know that the languages pairs used in machine translation are German and English.
Answer: German and English
True answer: German–English

Validation ID: 2f901dab6b757e12763b23ae8b37ae2e517a2271
Question: What languages pairs are used in machine translation?
From the context "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German–English spoken-domain translation, applying fully character-level segmentation", so we know that the languages pairs used in machine translation are German and English.
Answer: German and English
True answer: German–English

Validation ID: b591853e938984e6069d738371500ebdec50d256
Question: What sentiment classification dataset is used?
From the context "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17", so we know that the sentiment classification dataset is IMDb movie review dataset.
Answer: IMDb movie review dataset
True answer: the IMDb movie review dataset BIBREF17

Validation ID: b591853e938984e6069d738371500ebdec50d256
Question: What sentiment classification dataset is used?
From the context "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17", so we know that the sentiment classification dataset is IMDb movie review dataset.
Answer: IMDb movie review dataset
True answer: IMDb movie review

Validation ID: a130306c6662ff489df13fb3f8faa7cba8c52a21
Question: What pooling function is used?
From the context "We term these three options f-pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize INLINEFORM0 or INLINEFORM1 to zero.", so we know that the pooling function is f-pooling, fo-pooling, and ifo-pooling.
Answer: f-pooling, fo-pooling, and ifo-pooling
True answer: dynamic average pooling

Validation ID: a130306c6662ff489df13fb3f8faa7cba8c52a21
Question: What pooling function is used?
From the context "We term these three options f-pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize INLINEFORM0 or INLINEFORM1 to zero.", so we know that the pooling function is f-pooling, fo-pooling, and ifo-pooling.
Answer: f-pooling, fo-pooling, and ifo-pooling
True answer:  f-pooling, fo-pooling, and ifo-pooling 

Validation ID: b1cf5739467ba90059add58d11b73d075a11ec86
Question: Do they report results only on English?
From the context "We conducted extensive experiments for common NLP tasks on public data sets including CoNLL-2003 BIBREF14, GLUE benchmark BIBREF13, and WikiQA corpus BIBREF15", so we know that they report results only on English.
Answer: Yes
True answer: Unanswerable

Validation ID: b1cf5739467ba90059add58d11b73d075a11ec86
Question: Do they report results only on English?
From the context "We conducted extensive experiments for common NLP tasks on public data sets including CoNLL-2003 BIBREF14, GLUE benchmark BIBREF13, and WikiQA corpus BIBREF15", so we know that they report results only on English.
Answer: Yes
True answer: Yes

Validation ID: 2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd
Question: What neural network modules are included in NeuronBlocks?
From the context "NeuronBlocks provides common layers like RNN, CNN, QRNN BIBREF2, Transformer BIBREF3, Highway network, Encoder Decoder architecture, etc. Furthermore, attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention BIBREF4, Bidirectional attention flow BIBREF5, etc. Meanwhile, regularization layers such as Dropout, Layer Norm, Batch Norm, etc are also supported for improving generalization ability.", so we know that NeuronBlocks provides common layers like RNN, CNN, QRNN BIBREF2, Transformer BIBREF3, Highway network, Encoder Decoder architecture, etc. Furthermore, attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention BIBREF4, Bidirectional attention flow BIBREF5, etc. Meanwhile, regularization layers such as Dropout, Layer Norm, Batch Norm, etc are also supported for improving generalization ability.
Answer: RNN, CNN, QRNN BIBREF2, Transformer BIBREF3, Highway network, Encoder Decoder architecture, etc. Furthermore, attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention BIBREF4, Bidirectional attention flow BIBREF5, etc. Meanwhile, regularization layers such as Dropout, Layer Norm, Batch Norm, etc are also supported for improving generalization ability.
True answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics

Validation ID: 2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd
Question: What neural network modules are included in NeuronBlocks?
From the context "NeuronBlocks provides common layers like RNN, CNN, QRNN BIBREF2, Transformer BIBREF3, Highway network, Encoder Decoder architecture, etc. Furthermore, attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention BIBREF4, Bidirectional attention flow BIBREF5, etc. Meanwhile, regularization layers such as Dropout, Layer Norm, Batch Norm, etc are also supported for improving generalization ability.", so we know that NeuronBlocks provides common layers like RNN, CNN, QRNN BIBREF2, Transformer BIBREF3, Highway network, Encoder Decoder architecture, etc. Furthermore, attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention BIBREF4, Bidirectional attention flow BIBREF5, etc. Meanwhile, regularization layers such as Dropout, Layer Norm, Batch Norm, etc are also supported for improving generalization ability.
Answer: RNN, CNN, QRNN BIBREF2, Transformer BIBREF3, Highway network, Encoder Decoder architecture, etc. Furthermore, attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention BIBREF4, Bidirectional attention flow BIBREF5, etc. Meanwhile, regularization layers such as Dropout, Layer Norm, Batch Norm, etc are also supported for improving generalization ability.
True answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics

Validation ID: 4f253dfced6a749bf57a1b4984dc962ce9550184
Question: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?
From the context "However, when engineers apply DNN models to address specific NLP tasks, they often face the following challenges.
The above challenges often hinder the productivity of engineers, and result in less optimal solutions to their given tasks. This motivates us to develop an NLP toolkit for DNN models, which facilitates engineers to develop DNN approaches. Before designing this NLP toolkit, we conducted a survey among engineers and identified a spectrum of three typical personas.", so we know that the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers.
True answer: By conducting a survey among engineers

Validation ID: 4f253dfced6a749bf57a1b4984dc962ce9550184
Question: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?
From the context "However, when engineers apply DNN models to address specific NLP tasks, they often face the following challenges.
The above challenges often hinder the productivity of engineers, and result in less optimal solutions to their given tasks. This motivates us to develop an NLP toolkit for DNN models, which facilitates engineers to develop DNN approaches. Before designing this NLP toolkit, we conducted a survey among engineers and identified a spectrum of three typical personas.", so we know that the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers.
True answer: Unanswerable

Validation ID: dc1cec824507fc85ac1ba87882fe1e422ff6cffb
Question: what datasets did they use?
From the context "We use 10 fold cross validation where we have 3150 and 350 questions in our training set and validation set respectively", so we know that they use 3150 and 350 questions in their training set and validation set respectively.
Answer: 3150 and 350 questions
True answer: Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.

Validation ID: dc1cec824507fc85ac1ba87882fe1e422ff6cffb
Question: what datasets did they use?
From the context "We use 10 fold cross validation where we have 3150 and 350 questions in our training set and validation set respectively", so we know that they use 3150 and 350 questions in their training set and validation set respectively.
Answer: 3150 and 350 questions
True answer: 3500 questions collected from the internet and books.

Validation ID: f428618ca9c017e0c9c2a23515dab30a7660f65f
Question: what ml based approaches were compared?
From the context "By implementing different machine learning based classifiers on our Bengali question corpus, we perform a comparative analysis among them", so we know that the ml based approaches were compared.
Answer: ml based approaches
True answer: Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)

Validation ID: f428618ca9c017e0c9c2a23515dab30a7660f65f
Question: what ml based approaches were compared?
From the context "By implementing different machine learning based classifiers on our Bengali question corpus, we perform a comparative analysis among them", so we know that the ml based approaches were compared.
Answer: ml based approaches
True answer: Multi-Layer Perceptron, Naive Bayes Classifier, Support Vector Machine, Gradient Boosting Classifier, Stochastic Gradient Descent, K Nearest Neighbour, Random Forest

Validation ID: 8ce11515634236165cdb06ba80b9a36a8b9099a2
Question: Is pre-training effective in their evaluation?
From the context "Experimental results show that the encoder-decoder-reconstructor offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task, and the encoder-decoder-reconstructor can not be trained well without pre-training, so it proves that we have to train the forward translation model in a manner similar to the conventional attention-based NMT as pre-training.", so we know that pre-training is effective in their evaluation.
Answer: Yes
True answer: Yes

Validation ID: 8ce11515634236165cdb06ba80b9a36a8b9099a2
Question: Is pre-training effective in their evaluation?
From the context "Experimental results show that the encoder-decoder-reconstructor offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task, and the encoder-decoder-reconstructor can not be trained well without pre-training, so it proves that we have to train the forward translation model in a manner similar to the conventional attention-based NMT as pre-training.", so we know that pre-training is effective in their evaluation.
Answer: Yes
True answer: Yes

Validation ID: 6024039bbd1118c5dab86c41cce1175d99f10a25
Question: What parallel corpus did they use?
From the context "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1", so we know that they used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus.
Answer: Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus
True answer: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1

Validation ID: 6024039bbd1118c5dab86c41cce1175d99f10a25
Question: What parallel corpus did they use?
From the context "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1", so we know that they used Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus.
Answer: Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus
True answer: Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus 

Validation ID: de5b6c25e35b3a6c5e40e350fc5e52c160b33490
Question: How much does their model outperform existing models?
From the context "Our model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider documents with increasing length, it becomes more competitive for longer documents.", so we know that their model outperforms existing models.
Answer: Their model outperforms existing models.
True answer: Best proposed model result vs best previous result:
Arxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)
Pubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)

Validation ID: de5b6c25e35b3a6c5e40e350fc5e52c160b33490
Question: How much does their model outperform existing models?
From the context "Our model not only achieves state-of-the-art on these two datasets, but in an additional experiment, in which we consider documents with increasing length, it becomes more competitive for longer documents.", so we know that their model outperforms existing models.
Answer: outperforms
True answer: On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.


Validation ID: b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f
Question: What do they mean by global and local context?
From the context "In this work, we propose an extractive model for long documents, incorporating local and global context information, motivated by natural topic-oriented structure of human-written long documents", so we know that global context is the context of the whole document, while local context is the context of the topic segment that sentence falls into.
Answer: global context is the context of the whole document, while local context is the context of the topic segment that sentence falls into.
True answer: global (the whole document), local context (e.g., the section/topic)

Validation ID: b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f
Question: What do they mean by global and local context?
From the context "In this work, we propose an extractive model for long documents, incorporating local and global context information, motivated by natural topic-oriented structure of human-written long documents", so we know that global context is the context of the whole document, while local context is the context of the topic segment that sentence falls into.
Answer: global context is the context of the whole document, while local context is the context of the topic segment that sentence falls into.
True answer: global (the whole document) and the local context (e.g., the section/topic) 

Validation ID: 6bfba3ddca5101ed15256fca75fcdc95a53cece7
Question: What are the 18 propaganda techniques?
From the context "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators. More details about the data collection and the annotation, as well as statistics about the corpus can be found in BIBREF10, where an earlier version of the corpus is described, which includes 450 news articles. We further annotated 47 additional articles for the purpose of the shared task using the same protocol and the same annotators.", so we know that the 18 propaganda techniques are:
1. Loaded language
True answer: Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans,  Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating cliché, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man

Validation ID: 6bfba3ddca5101ed15256fca75fcdc95a53cece7
Question: What are the 18 propaganda techniques?
From the context "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators. More details about the data collection and the annotation, as well as statistics about the corpus can be found in BIBREF10, where an earlier version of the corpus is described, which includes 450 news articles. We further annotated 47 additional articles for the purpose of the shared task using the same protocol and the same annotators.", so we know that the 18 propaganda techniques are:
1. Loaded language
True answer: 1. Loaded language, 2. Name calling or labeling, 3. Repetition, 4. Exaggeration or minimization, 5. Doubt, 6. Appeal to fear/prejudice, 7. Flag-waving, 8. Causal oversimplification, 9. Slogans, 10. Appeal to authority, 11. Black-and-white fallacy, dictatorship, 12. Thought-terminating cliché, 13. Whataboutism, 14. Reductio ad Hitlerum, 15. Red herring, 16. Bandwagon, 17. Obfuscation, intentional vagueness, confusion, 18. Straw man

Validation ID: df5a4505edccc0ee11349ed6e7958cf6b84c9ed4
Question: What dataset was used?
From the context "The corpus for the task was annotated by A Data Pro, a company that performs high-quality manual annotations.", so we know that the dataset was annotated by A Data Pro.
Answer: A Data Pro
True answer:  news articles in free-text format

Validation ID: df5a4505edccc0ee11349ed6e7958cf6b84c9ed4
Question: What dataset was used?
From the context "The corpus for the task was annotated by A Data Pro, a company that performs high-quality manual annotations.", so we know that the dataset was annotated by A Data Pro.
Answer: A Data Pro
True answer: collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators

Validation ID: fd753ab5177d7bd27db0e0afc12411876ee607df
Question: What was the baseline for this task?
From the context "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence", so we know that the baseline for this task is a very simple logistic regression classifier with default parameters.
Answer: a very simple logistic regression classifier with default parameters
True answer: The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.

Validation ID: fd753ab5177d7bd27db0e0afc12411876ee607df
Question: What was the baseline for this task?
From the context "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence", so we know that the baseline for this task is a very simple logistic regression classifier with default parameters.
Answer: a very simple logistic regression classifier with default parameters
True answer: SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly

Validation ID: 88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42
Question: What is a second order co-ocurrence matrix?
From the context "We build an INLINEFORM0 similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above", so we know that a second order co-ocurrence matrix is a matrix that contains the similarity score between the row word and column word using the similarity measures discussed above.
Answer: a matrix that contains the similarity score between the row word and column word using the similarity measures discussed above
True answer: frequencies of the other words which occur with both of them (i.e., second order co–occurrences)

Validation ID: 88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42
Question: What is a second order co-ocurrence matrix?
From the context "We build an INLINEFORM0 similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above", so we know that a second order co-ocurrence matrix is a matrix that contains the similarity score between the row word and column word using the similarity measures discussed above.
Answer: a matrix that contains the similarity score between the row word and column word using the similarity measures discussed above
True answer: The matrix containing co-occurrences of the words which occur with the both words of every given pair of words.

Validation ID: 4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3
Question: How many humans participated?
From the context "The UMNSRS was annotated for both similarity and relatedness by four medical residents", so we know that four medical residents participated.
Answer: four
True answer: Unanswerable

Validation ID: 4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3
Question: How many humans participated?
From the context "The UMNSRS was annotated for both similarity and relatedness by four medical residents", so we know that four medical residents participated.
Answer: four
True answer: 16

Validation ID: 8b3d3953454c88bde88181897a7a2c0c8dd87e23
Question: What embedding techniques are explored in the paper?
From the context "Recently, word embeddings BIBREF9 have become a popular method for measuring semantic relatedness in the biomedical domain. This is a neural network based approach that learns a representation of a word by word co–occurrence matrix. The basic idea is that the neural network learns a series of weights (the hidden layer within the neural network) that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip–gram approach.", so we know that the embedding techniques explored in the paper are CBOW and Skip–gram.
Answer: CBOW and Skip–gram
True answer: Skip–gram, CBOW

Validation ID: 8b3d3953454c88bde88181897a7a2c0c8dd87e23
Question: What embedding techniques are explored in the paper?
From the context "Recently, word embeddings BIBREF9 have become a popular method for measuring semantic relatedness in the biomedical domain. This is a neural network based approach that learns a representation of a word by word co–occurrence matrix. The basic idea is that the neural network learns a series of weights (the hidden layer within the neural network) that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip–gram approach.", so we know that the embedding techniques explored in the paper are CBOW and Skip–gram.
Answer: CBOW and Skip–gram
True answer: integrated vector-res, vector-faith, Skip–gram, CBOW

Validation ID: 784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f
Question: Do the authors also try the model on other datasets?
From the context "In the future we would like to extend our system to handle multi-relation questions.", so we know that the authors also try the model on other datasets.
Answer: Yes
True answer: No

Validation ID: 784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f
Question: Do the authors also try the model on other datasets?
From the context "In the future we would like to extend our system to handle multi-relation questions.", so we know that the authors also try the model on other datasets.
Answer: Yes
True answer: No

Validation ID: 7705dd04acedaefee30d8b2c9978537afb2040dc
Question: What word level and character level model baselines are used?
From the context "We first explore using word-level models as an alternative to character-level models to construct embeddings for questions, entities and predicates.", so we know that the word level model baseline is used.
From the context "We then explore using character-level models to construct embeddings for questions, entities and predicates.", so we know that the character level model baseline is used.
True answer: None

Validation ID: 7705dd04acedaefee30d8b2c9978537afb2040dc
Question: What word level and character level model baselines are used?
From the context "We first explore using word-level models as an alternative to character-level models to construct embeddings for questions, entities and predicates.", so we know that the word level model baseline is used.
From the context "We then explore using character-level models to construct embeddings for questions, entities and predicates.", so we know that the character level model baseline is used.
True answer: Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015)

Validation ID: 44497509fdf5e87cff05cdcbe254fbd288d857ad
Question: By how much do they improve the efficacy of the attention mechanism?
From the context "The attention mechanism is improved by the use of a feed forward neural network to compute the alignment parameter. This parameter is calculated by means of a feed forward neural network to ensure that the alignment model is free from the difficulties of contextualization of long sentences into a single vector. The feed forward network is trained along with the neural translation model to jointly improve the performance of the translation.", so we know that the attention mechanism is improved by the use of a feed forward neural network to compute the alignment parameter.
Answer: feed forward neural network
True answer: Unanswerable

Validation ID: 44497509fdf5e87cff05cdcbe254fbd288d857ad
Question: By how much do they improve the efficacy of the attention mechanism?
From the context "The attention mechanism is improved by the use of a feed forward neural network to compute the alignment parameter. This parameter is calculated by means of a feed forward neural network to ensure that the alignment model is free from the difficulties of contextualization of long sentences into a single vector. The feed forward network is trained along with the neural translation model to jointly improve the performance of the translation.", so we know that the attention mechanism is improved by the use of a feed forward neural network to compute the alignment parameter.
Answer: feed forward neural network
True answer: Unanswerable

Validation ID: 0ee73909ac638903da4a0e5565c8571fc794ab96
Question: How were the human judgements assembled?
From the context "The human evaluation metrics of adequacy, fluency and relative ranking values were used to evaluate the performance of the models. A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive).", so we know that the human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages.
Answer: A group of 50 native people who were well-versed in both English and Tamil languages
True answer: 50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.

Validation ID: 0ee73909ac638903da4a0e5565c8571fc794ab96
Question: How were the human judgements assembled?
From the context "The human evaluation metrics of adequacy, fluency and relative ranking values were used to evaluate the performance of the models. A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive).", so we know that the human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages.
Answer: A group of 50 native people who were well-versed in both English and Tamil languages
True answer: adequacy, precision and ranking values

Validation ID: 1f07e837574519f2b696f3d6fa3230af0b931e5d
Question: Did they only experiment with one language pair?
From the context "We have presented a reordering approach for phrase-based SMT, guided by sparse dependency swap features. We have contributed a new approach for learning and performing reordering in phrase-based MT by the incorporation of dependency-based features. From our experiments, we have shown that utilizing source dependency parse for reordering sentences helps to significantly improve translation quality over a phrase-based baseline system with state-of-the-art reordering orientation models.", so we know that they only experiment with one language pair.
Answer: Yes
True answer: Yes

Validation ID: 1f07e837574519f2b696f3d6fa3230af0b931e5d
Question: Did they only experiment with one language pair?
From the context "We have presented a reordering approach for phrase-based SMT, guided by sparse dependency swap features. We have contributed a new approach for learning and performing reordering in phrase-based MT by the incorporation of dependency-based features. From our experiments, we have shown that utilizing source dependency parse for reordering sentences helps to significantly improve translation quality over a phrase-based baseline system with state-of-the-art reordering orientation models.", so we know that they only experiment with one language pair.
Answer: Yes
True answer: Yes

Validation ID: 5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76
Question: Which other approaches do they compare their model with?
From the context "We can see comparisons of our model made on Wiki(gold) in Table TABREF20. We note that the model performs similarly to existing systems without being trained or tuned on that particular dataset.", so we know that they compare their model with existing systems.
Answer: existing systems
True answer: Akbik et al. (2018), Link et al. (2012)

Validation ID: 5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76
Question: Which other approaches do they compare their model with?
From the context "We can see comparisons of our model made on Wiki(gold) in Table TABREF20. We note that the model performs similarly to existing systems without being trained or tuned on that particular dataset.", so we know that they compare their model with existing systems.
Answer: existing systems
True answer: They compare to Akbik et al. (2018) and Link et al. (2012).

Validation ID: 729694a9fe1e05d329b7a4078a596fe606bc5a95
Question: What results do they achieve using their proposed approach?
From the context "The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%", so we know that they achieve 88% and 53% results using their proposed approach.
Answer: 88% and 53%
True answer: F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).

Validation ID: 729694a9fe1e05d329b7a4078a596fe606bc5a95
Question: What results do they achieve using their proposed approach?
From the context "The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%", so we know that they achieve 88% and 53% results using their proposed approach.
Answer: 88% and 53%
True answer:  total F-1 score on the OntoNotes dataset is 88%, total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%

Validation ID: 1c997c268c68149ae6fb43d83ffcd53f0e7fe57e
Question: How do they combine a deep learning model with a knowledge base?
From the context "In this paper, we present a deep neural network model for the task of fine-grained named entity classification using ELMo embeddings and Wikidata. The proposed model learns representations for entity mentions based on its context and incorporates the rich structure of Wikidata to augment these labels into finer-grained subtypes.", so we know that they combine a deep learning model with a knowledge base by using ELMo embeddings and Wikidata.
Answer: ELMo embeddings and Wikidata
True answer: Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.

Validation ID: 1c997c268c68149ae6fb43d83ffcd53f0e7fe57e
Question: How do they combine a deep learning model with a knowledge base?
From the context "In this paper, we present a deep neural network model for the task of fine-grained named entity classification using ELMo embeddings and Wikidata. The proposed model learns representations for entity mentions based on its context and incorporates the rich structure of Wikidata to augment these labels into finer-grained subtypes.", so we know that they combine a deep learning model with a knowledge base by using ELMo embeddings and Wikidata.
Answer: ELMo embeddings and Wikidata
True answer: ELMo embeddings are then used with a residual LSTM to learn informative morphological representations from the character sequence of each token

Validation ID: 5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde
Question: What are the models used for the baseline of the three NLP tasks?
From the context "For the best quality speech synthesis we need a few hours of phonetically-balanced, single-speaker, read speech. Our first step was to use the start and end points for each turn in the dialogues, and select those of the most frequent speaker, nmlch. This gave us around 18250 segments. We further automatically removed excessive silence from the start, middle and end of these turns (based on occurrence of F0). This gave us 13 hours and 48 minutes of speech.
We phonetically aligned this data and built a speech clustergen statistical speech synthesizer BIBREF9 from all of this data. We resynthesized all of the data and measured the difference between the synthesized data and the original data using Mel Cepstral Distortion, a standard method for automatically measuring quality of speech generation BIBREF10. We then ordered the segments by their generation score and took the top 2000 turns to build a new synthesizer, assuming the better scores corresponded to better alignments, following the techniques of BIBREF7.
True answer: state-of-the-art Transformer architecture, Kaldi, speech clustergen statistical speech synthesizer

Validation ID: 5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde
Question: What are the models used for the baseline of the three NLP tasks?
From the context "For the best quality speech synthesis we need a few hours of phonetically-balanced, single-speaker, read speech. Our first step was to use the start and end points for each turn in the dialogues, and select those of the most frequent speaker, nmlch. This gave us around 18250 segments. We further automatically removed excessive silence from the start, middle and end of these turns (based on occurrence of F0). This gave us 13 hours and 48 minutes of speech.
We phonetically aligned this data and built a speech clustergen statistical speech synthesizer BIBREF9 from all of this data. We resynthesized all of the data and measured the difference between the synthesized data and the original data using Mel Cepstral Distortion, a standard method for automatically measuring quality of speech generation BIBREF10. We then ordered the segments by their generation score and took the top 2000 turns to build a new synthesizer, assuming the better scores corresponded to better alignments, following the techniques of BIBREF7.
True answer: For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15.

Validation ID: f9bf6bef946012dd42835bf0c547c0de9c1d229f
Question: How is non-standard pronunciation identified?
From the context "In addition, the transcription includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses", so we know that non-standard pronunciation is identified by annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses.
Answer: annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses
True answer: Unanswerable

Validation ID: f9bf6bef946012dd42835bf0c547c0de9c1d229f
Question: How is non-standard pronunciation identified?
From the context "In addition, the transcription includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses", so we know that non-standard pronunciation is identified by annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses.
Answer: annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses
True answer: Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.

Validation ID: 6a633811019e9323dc8549ad540550d27aa6d972
Question: Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?
From the context "In this work, we aimed to provide a fine grained classification mechanism for all the quality classes of the articles of the Wikipedia Medical Portal. The idea was to propose an automatic instrument for helping the reviewers to understand which articles are the less work-demanding papers to pass to next quality stage. We focused on an actionable model, namely whose features are related to the content of the articles, so that they can also directly suggest strategies for improving a given article.", so we know that a bad medical wikipedia article should not contain much domain-specific jargon.
Answer: Yes
True answer: Unanswerable

Validation ID: 6a633811019e9323dc8549ad540550d27aa6d972
Question: Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?
From the context "In this work, we aimed to provide a fine grained classification mechanism for all the quality classes of the articles of the Wikipedia Medical Portal. The idea was to propose an automatic instrument for helping the reviewers to understand which articles are the less work-demanding papers to pass to next quality stage. We focused on an actionable model, namely whose features are related to the content of the articles, so that they can also directly suggest strategies for improving a given article.", so we know that a bad medical wikipedia article should not contain much domain-specific jargon.
Answer: Yes
True answer: No

Validation ID: 6b9b9e5d154cb963f6d921093539490daa5ebbae
Question: What novel PMI variants are introduced?
From the context "We also experiment with normalized $\mathit {PMI}$ ($\mathit {NPMI}$) BIBREF7:
such that $NPMI(w,c) = -1$ when $(w,c)$ never cooccur, $NPMI(w,c) = 0$ when they are independent, and $NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\mathit {NNEGPMI}$ which only normalizes $\mathit {\texttt {-}PMI}$:", so we know that the novel PMI variants are $\mathit {NPMI}$ and $\mathit {NNEGPMI}$.
True answer: clipped PMI; NNEGPMI

Validation ID: 6b9b9e5d154cb963f6d921093539490daa5ebbae
Question: What novel PMI variants are introduced?
From the context "We also experiment with normalized $\mathit {PMI}$ ($\mathit {NPMI}$) BIBREF7:
such that $NPMI(w,c) = -1$ when $(w,c)$ never cooccur, $NPMI(w,c) = 0$ when they are independent, and $NPMI(w,c) = 1$ when they always cooccur together. This effectively captures the entire negative spectrum, but has the downside of normalization which discards scale information. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\mathit {NNEGPMI}$ which only normalizes $\mathit {\texttt {-}PMI}$:", so we know that the novel PMI variants are $\mathit {NPMI}$ and $\mathit {NNEGPMI}$.
True answer: clipped $\mathit {PMI}$, $\mathit {NNEGPMI}$

Validation ID: bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2
Question: What semantic and syntactic tasks are used as probes?
From the context "We evaluate sentence-level semantics using averaged bag of vectors (BoV) representations on the Semantic Textual Similarity (STSB) task BIBREF21 and Word Content (WC) probing task (identify from a list of words which is contained in the sentence representation) from SentEval BIBREF22. Syntax: Similarly, we use the Google Syntactic analogies (GSyn) BIBREF9 to evaluate word-level syntactic information, and Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks from SentEval BIBREF22. Classifiers for all SentEval probing tasks are multilayer perceptrons with a single hidden layer of 100 units and dropout of $.1$.", so we know that the semantic and syntactic tasks are STSB, WC, GSyn, Dep, and TopC.
Answer: STSB, WC, GSyn, Dep, and TopC
True answer: Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks

Validation ID: bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2
Question: What semantic and syntactic tasks are used as probes?
From the context "We evaluate sentence-level semantics using averaged bag of vectors (BoV) representations on the Semantic Textual Similarity (STSB) task BIBREF21 and Word Content (WC) probing task (identify from a list of words which is contained in the sentence representation) from SentEval BIBREF22. Syntax: Similarly, we use the Google Syntactic analogies (GSyn) BIBREF9 to evaluate word-level syntactic information, and Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks from SentEval BIBREF22. Classifiers for all SentEval probing tasks are multilayer perceptrons with a single hidden layer of 100 units and dropout of $.1$.", so we know that the semantic and syntactic tasks are STSB, WC, GSyn, Dep, and TopC.
Answer: STSB, WC, GSyn, Dep, and TopC
True answer: SimLex, Rare Word, Google Semantic, Semantic Textual Similarity, Word Content (WC) probing, Google Syntactic analogies, Depth, Top Constituent, part-of-speech (POS) tagging

Validation ID: d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7
Question: What are the disadvantages to clipping negative PMI?
From the context "Understanding which type of information is captured by $\mathit {\texttt {-}PMI}$ can help in tailoring models for optimal performance. In this work, we attempt to answer this question by studying the kind of information contained in the negative and positive spectrums of $\mathit {PMI}$ ($\mathit {\texttt {-}PMI}$ and $\mathit {\texttt {+}PMI}$). We evaluate weighted factorization of different matrices which use either $\mathit {\texttt {-}PMI}$, $\mathit {\texttt {+}PMI}$, or both on various semantic and syntactic tasks. Results show that $\mathit {\texttt {+}PMI}$ alone performs quite well on most tasks, capturing both semantics and syntax, in contrast to $\mathit {\texttt {-}PMI}$, which performs poorly on nearly all tasks, except those that test for syntax. Our main contribution is deepening our understanding of distributional semantics by extending BIBREF1's paraphrase of the distributional hypothesis to “a word is not only characterized by the company that it keeps, but also by the company it rejects”. Our secondary contributions are the proposal of two $PMI$ variants that account for the spectrum of $\mathit {\texttt {-}PMI}$, and the justification of the popular $PPMI$ measure.", so we know that clipping negative PMI is not optimal.
Answer: Clipping negative PMI is not optimal.
True answer: It may lead to poor rare word representations and word analogies.

Validation ID: d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7
Question: What are the disadvantages to clipping negative PMI?
From the context "Understanding which type of information is captured by $\mathit {\texttt {-}PMI}$ can help in tailoring models for optimal performance. In this work, we attempt to answer this question by studying the kind of information contained in the negative and positive spectrums of $\mathit {PMI}$ ($\mathit {\texttt {-}PMI}$ and $\mathit {\texttt {+}PMI}$). We evaluate weighted factorization of different matrices which use either $\mathit {\texttt {-}PMI}$, $\mathit {\texttt {+}PMI}$, or both on various semantic and syntactic tasks. Results show that $\mathit {\texttt {+}PMI}$ alone performs quite well on most tasks, capturing both semantics and syntax, in contrast to $\mathit {\texttt {-}PMI}$, which performs poorly on nearly all tasks, except those that test for syntax. Our main contribution is deepening our understanding of distributional semantics by extending BIBREF1's paraphrase of the distributional hypothesis to “a word is not only characterized by the company that it keeps, but also by the company it rejects”. Our secondary contributions are the proposal of two $PMI$ variants that account for the spectrum of $\mathit {\texttt {-}PMI}$, and the justification of the popular $PPMI$ measure.", so we know that clipping negative PMI is not optimal.
Answer: Clipping negative PMI is not optimal.
True answer: Unanswerable

Validation ID: 6844683935d0d8f588fa06530f5068bf3e1ed0c0
Question: Why are statistics from finite corpora unreliable?
From the context "Unfortunately, $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora.", so we know that statistics from finite corpora are unreliable.
Answer: statistics from finite corpora are unreliable
True answer: $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus

Validation ID: 6844683935d0d8f588fa06530f5068bf3e1ed0c0
Question: Why are statistics from finite corpora unreliable?
From the context "Unfortunately, $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora.", so we know that statistics from finite corpora are unreliable.
Answer: statistics from finite corpora are unreliable
True answer: A finite corpora may entirely omit rare word combinations

Validation ID: 8acab64ba72831633e8cc174d5469afecccf3ae9
Question: what is the domain of the corpus?
From the context "We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations", so we know that the domain of the corpus is telephone calls.
Answer: telephone calls
True answer: Unanswerable

Validation ID: 8acab64ba72831633e8cc174d5469afecccf3ae9
Question: what is the domain of the corpus?
From the context "We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations", so we know that the domain of the corpus is telephone calls.
Answer: telephone calls
True answer: telephone calls

Validation ID: 53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa
Question: what challenges are identified?
From the context "Our simple system (SECREF2) builds on unsupervised speech processing BIBREF5, BIBREF6, BIBREF7, and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8, BIBREF9. The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10, a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (SECREF3). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (SECREF4). Despite these difficulties, we demonstrate that the system learns to translate some content words (SECREF5).", so we know that the challenges identified are low coverage of audio and difficulty in cross-speaker clustering.
Answer: low coverage of audio and difficulty in cross-speaker clustering
True answer: Assigning wrong words to a cluster, Splitting words across different clusters, sparse, giving low coverage

Validation ID: 53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa
Question: what challenges are identified?
From the context "Our simple system (SECREF2) builds on unsupervised speech processing BIBREF5, BIBREF6, BIBREF7, and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech BIBREF8, BIBREF9. The clusters do not account for all of the audio, but we can use them to simulate a partial, noisy transcription, or pseudotext, which we pair with translations to learn a bag-of-words translation model. We test our system on the CALLHOME Spanish-English speech translation corpus BIBREF10, a noisy multi-speaker corpus of telephone calls in a variety of Spanish dialects (SECREF3). Using the Spanish speech as the source and English text translations as the target, we identify several challenges in the use of UTD, including low coverage of audio and difficulty in cross-speaker clustering (SECREF4). Despite these difficulties, we demonstrate that the system learns to translate some content words (SECREF5).", so we know that the challenges are low coverage of audio and difficulty in cross-speaker clustering.
Answer: low coverage of audio and difficulty in cross-speaker clustering
True answer: low coverage of audio, difficulty in cross-speaker clustering

Validation ID: 72755c2d79210857cfff60bfbcb55f83c71ada51
Question: what is the size of the speech corpus?
From the context "We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations", so we know that the size of the speech corpus is 11 hours.
Answer: 11 hours
True answer: 104 telephone calls, transcripts contain 168,195 Spanish word tokens,  translations contain 159,777 English word tokens

Validation ID: 72755c2d79210857cfff60bfbcb55f83c71ada51
Question: what is the size of the speech corpus?
From the context "We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations", so we know that the size of the speech corpus is 11 hours.
Answer: 11 hours
True answer: 104 telephone calls, which pair 11 hours of audio

Validation ID: 7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569
Question: Which two pairs of ERPs from the literature benefit from joint training?
From the context "We have shown that ERP components can be predicted from neural networks pretrained as language models and fine-tuned to directly predict those components. To the best of our knowledge, prior work has not successfully used statistical models to predict all of these components. Furthermore, we have shown that multitask learning benefits the prediction of ERP components and can suggest how components relate to each other. At present, these joint-training benefit relationships are only suggestive, but if these relationships ultimately lead to insights about what drives each ERP component, then the components become more useful tools for studying human language comprehension. By using multitask learning as a method of characterization, we have found some expected relationships (LAN+P600 and ELAN+P600) and several more surprising relationships. We believe that this is exactly the kind of finding that makes multitask learning an interesting exploratory technique in this area.", so we know that LAN+P600 and ELAN+P600 are two pairs of ERPs from the literature that benefit from joint training.
Answer: LAN+P600 and ELAN+P600
True answer: Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.
Select:
- ELAN, LAN
- PNP ERP

Validation ID: 7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569
Question: Which two pairs of ERPs from the literature benefit from joint training?
From the context "We have shown that ERP components can be predicted from neural networks pretrained as language models and fine-tuned to directly predict those components. To the best of our knowledge, prior work has not successfully used statistical models to predict all of these components. Furthermore, we have shown that multitask learning benefits the prediction of ERP components and can suggest how components relate to each other. At present, these joint-training benefit relationships are only suggestive, but if these relationships ultimately lead to insights about what drives each ERP component, then the components become more useful tools for studying human language comprehension. By using multitask learning as a method of characterization, we have found some expected relationships (LAN+P600 and ELAN+P600) and several more surprising relationships. We believe that this is exactly the kind of finding that makes multitask learning an interesting exploratory technique in this area.", so we know that LAN+P600 and ELAN+P600 are two pairs of ERPs from the literature that benefit from joint training.
Answer: LAN+P600 and ELAN+P600
True answer: Unanswerable

Validation ID: bd6dc38a9ac8d329114172194b0820766458dacc
Question: What datasets are used?
From the context "In this work, we consider six ERP components that have been associated in the cognitive neuroscience and psycholinguistics literature with language processing and which we analyze in the data from BIBREF0 (see Figure FIGREF1 for spatial and temporal definitions of these ERP components)", so we know that the datasets are BIBREF0.
Answer: BIBREF0
True answer: Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.
Select:
- ERP data collected and computed by Frank et al. (2015)
- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)

Validation ID: bd6dc38a9ac8d329114172194b0820766458dacc
Question: What datasets are used?
From the context "In this work, we consider six ERP components that have been associated in the cognitive neuroscience and psycholinguistics literature with language processing and which we analyze in the data from BIBREF0 (see Figure FIGREF1 for spatial and temporal definitions of these ERP components)", so we know that the datasets are BIBREF0.
Answer: BIBREF0
True answer: the ERP data: BIBREF0

Validation ID: 3ddff6b707767c3dd54d7104fe88b628765cae58
Question: which datasets did they experiment with?
From the context "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43.", so we know that they experiment with Universal Dependencies v1.2 treebanks.
Answer: Universal Dependencies v1.2 treebanks
True answer: Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,
Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish

Validation ID: 3ddff6b707767c3dd54d7104fe88b628765cae58
Question: which datasets did they experiment with?
From the context "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43.", so we know that they experiment with Universal Dependencies v1.2 treebanks.
Answer: Universal Dependencies v1.2 treebanks
True answer: Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2

Validation ID: 0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7
Question: which languages are explored?
From the context "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43.", so we know that the languages explored are the languages in UD1.2.
Answer: UD1.2
True answer: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish

Validation ID: 0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7
Question: which languages are explored?
From the context "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21, hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags, which is an extension of the tagset proposed by BIBREF43.", so we know that the languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.
Answer: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish
True answer: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish

Validation ID: 06be47e2f50b902b05ebf1ff1c66051925f5c247
Question: Do they use number of votes as an indicator of preference?
From the context "We replicate the results of BIBREF22 using ideal point data, which is represented as the gray line. The blue line represents the modularity calculated on the speeches.", so we know that they use number of votes as an indicator of preference.
Answer: Yes
True answer: No

Validation ID: 06be47e2f50b902b05ebf1ff1c66051925f5c247
Question: Do they use number of votes as an indicator of preference?
From the context "We replicate the results of BIBREF22 using ideal point data, which is represented as the gray line. The blue line represents the modularity calculated on the speeches.", so we know that they use ideal point data as an indicator of preference.
Answer: ideal point data
True answer: Yes

Validation ID: 003d6f9722ddc2ee13e879fefafc315fb8e87cb9
Question: What does a node in the network approach repesent?
From the context "The intuition is that a community should have more or stronger ties among the actors within the community compared to ties with actors in other communities", so we know that a node in the network approach represents an actor.
Answer: an actor
True answer: Unanswerable

Validation ID: 003d6f9722ddc2ee13e879fefafc315fb8e87cb9
Question: What does a node in the network approach repesent?
From the context "The intuition is that a community should have more or stronger ties among the actors within the community compared to ties with actors in other communities", so we know that a node in the network approach represents an actor.
Answer: an actor
True answer: Unanswerable

Validation ID: c88a846197b72d25e04ec55f00ee3e72f655504c
Question: Which dataset do they use?
From the context "We draw on the newly released UN General Debate Corpus BIBREF11 which contains every country statement in the UN General Debate between 1970 and 2014", so we know that they use the UN General Debate Corpus.
Answer: UN General Debate Corpus
True answer: corpus of state speeches delivered during the annual UN General Debate

Validation ID: c88a846197b72d25e04ec55f00ee3e72f655504c
Question: Which dataset do they use?
From the context "We draw on the newly released UN General Debate Corpus BIBREF11 which contains every country statement in the UN General Debate between 1970 and 2014", so we know that they use the UN General Debate Corpus.
Answer: UN General Debate Corpus
True answer: corpus of state speeches delivered during the annual UN General Debate

Validation ID: 4d28c99750095763c81bcd5544491a0ba51d9070
Question: What kind of celebrities do they obtain tweets from?
From the context "We present examples of the most and least characteristic tweets for celebrities from three categories, along with their corresponding characterization scores computed using Approach SECREF29.", so we know that they obtain tweets from celebrities from three categories.
Answer: celebrities from three categories
True answer: Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,
Ellen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey

Validation ID: 4d28c99750095763c81bcd5544491a0ba51d9070
Question: What kind of celebrities do they obtain tweets from?
From the context "We present examples of the most and least characteristic tweets for celebrities from three categories, along with their corresponding characterization scores computed using Approach SECREF29.", so we know that they obtain tweets from celebrities from three categories.
Answer: celebrities from three categories
True answer: Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. 

Validation ID: 78292bc57ee68fdb93ed45430d80acca25a9e916
Question: How did they extend LAMA evaluation framework to focus on negation?
From the context "We created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. We did the same for a subset of ConceptNet that is easy to negate. We selected this subset by filtering for sentence length and extracting common queries.", so we know that they extended LAMA evaluation framework to focus on negation by manually inserting a negation element in each template or statement.
Answer: manually inserting a negation element in each template or statement
True answer: To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement

Validation ID: 78292bc57ee68fdb93ed45430d80acca25a9e916
Question: How did they extend LAMA evaluation framework to focus on negation?
From the context "We created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. We did the same for a subset of ConceptNet that is easy to negate. We selected this subset by filtering for sentence length and extracting common queries.", so we know that they extended LAMA evaluation framework to focus on negation by manually inserting a negation element in each template or statement.
Answer: manually inserting a negation element in each template or statement
True answer: Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions.

Validation ID: 443d2448136364235389039cbead07e80922ec5c
Question: What summarization algorithms did the authors experiment with?
From the context "We also explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries. The performance of ILP-based summarization is comparable with the other algorithms, as the two sample t-test does not show statistically significant difference. Also, human evaluators preferred phrase-based summary generated by our approach to the other sentence-based summaries.", so we know that the summarization algorithms that the authors experimented with are Sumy package, ILP-based summarization, phrase-based summary.
Answer: Sumy package, ILP-based summarization, phrase-based summary
True answer: LSA, TextRank, LexRank and ILP-based summary.

Validation ID: 443d2448136364235389039cbead07e80922ec5c
Question: What summarization algorithms did the authors experiment with?
From the context "We also explored a few summarization algorithms provided by the Sumy package. A common parameter which is required by all these algorithms is number of sentences keep in the final summary. ILP-based summarization requires a similar parameter K, which is automatically decided based on number of total candidate phrases. Assuming a sentence is equivalent to roughly 3 phrases, for Sumy algorithms, we set number of sentences parameter to the ceiling of K/3. Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries. The performance of ILP-based summarization is comparable with the other algorithms, as the two sample t-test does not show statistically significant difference. Also, human evaluators preferred phrase-based summary generated by our approach to the other sentence-based summaries.", so we know that the summarization algorithms that the authors experimented with are Sumy package, ILP-based summarization, phrase-based summary.
Answer: Sumy package, ILP-based summarization, phrase-based summary
True answer: LSA, TextRank, LexRank

Validation ID: aa6d956c2860f58fc9baea74c353c9d985b05605
Question: What evaluation metrics were used for the summarization task?
From the context "The summaries generated by our ILP-based approach were compared with the corresponding manual summaries using the ROUGE BIBREF22 unigram score", so we know that the evaluation metrics used for the summarization task is ROUGE unigram score.
Answer: ROUGE unigram score
True answer: ROUGE BIBREF22 unigram score

Validation ID: aa6d956c2860f58fc9baea74c353c9d985b05605
Question: What evaluation metrics were used for the summarization task?
From the context "We compared the summaries generated by our ILP-based approach with the corresponding manual summaries using the ROUGE BIBREF22 unigram score", so we know that the evaluation metrics used for the summarization task is ROUGE unigram score.
Answer: ROUGE unigram score
True answer: ROUGE

Validation ID: 4c18081ae3b676cc7831403d11bc070c10120f8e
Question: What clustering algorithms were used?
From the context "We also explored clustering 12742 STRENGTH sentences directly using CLUTO BIBREF19 and Carrot2 Lingo BIBREF20 clustering algorithms", so we know that the clustering algorithms used are CLUTO and Carrot2 Lingo.
Answer: CLUTO and Carrot2 Lingo
True answer: CLUTO, Carrot2 Lingo

Validation ID: 4c18081ae3b676cc7831403d11bc070c10120f8e
Question: What clustering algorithms were used?
From the context "We also explored clustering 12742 STRENGTH sentences directly using CLUTO BIBREF19 and Carrot2 Lingo BIBREF20 clustering algorithms", so we know that the clustering algorithms used are CLUTO and Carrot2 Lingo.
Answer: CLUTO and Carrot2 Lingo
True answer: simple clustering algorithm which uses the cosine similarity between word embeddings

Validation ID: fb3d30d59ed49e87f63d3735b876d45c4c6b8939
Question: What evaluation metrics are looked at for classification tasks?
From the context "We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation. The overall accuracy for a classifier is defined as INLINEFORM0, where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation.", so we know that the evaluation metrics are accuracy, precision, recall and F-measure.
Answer: accuracy, precision, recall and F-measure
True answer: Precision, Recall, F-measure, accuracy

Validation ID: fb3d30d59ed49e87f63d3735b876d45c4c6b8939
Question: What evaluation metrics are looked at for classification tasks?
From the context "We randomly selected 2000 sentences from the supervisor assessment corpus and manually tagged them (dataset D1). This labelled dataset contained 705, 103, 822 and 370 sentences having the class labels STRENGTH, WEAKNESS, SUGGESTION or OTHER respectively. We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation. The overall accuracy for a classifier is defined as INLINEFORM0, where the denominator is 2000 for dataset D1. Note that the pattern-based approach is unsupervised i.e., it did not use any training data. Hence, the results shown for it are for the entire dataset and not based on cross-validation.", so we know that the evaluation metrics are accuracy, precision, recall and F-measure.
Answer: accuracy, precision, recall and F-measure
True answer: Precision, Recall and F-measure

Validation ID: 197b276d0610ebfacd57ab46b0b29f3033c96a40
Question: What methods were used for sentence classification?
From the context "We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation.", so we know that the methods used for sentence classification are SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, Maximum Entropy and Pattern-based.
Answer: SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, Maximum Entropy and Pattern-based
True answer: Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based

Validation ID: 197b276d0610ebfacd57ab46b0b29f3033c96a40
Question: What methods were used for sentence classification?
From the context "We trained several multi-class classifiers on this dataset. Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation.", so we know that the methods used for sentence classification are SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, and Pattern-based.
Answer: SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, and Pattern-based
True answer: Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach

Validation ID: e025061e199b121f2ac8f3d9637d9bf987d65cd5
Question: What is the average length of the sentences?
From the context "The summary statistics about the number of words in a sentence is: min:4 max:217 average:15.5 STDEV:9.2 Q1:9 Q2:14 Q3:19", so we know that the average length of the sentences is 15.5.
Answer: 15.5
True answer: 15.5

Validation ID: e025061e199b121f2ac8f3d9637d9bf987d65cd5
Question: What is the average length of the sentences?
From the context "The summary statistics about the number of words in a sentence is: min:4 max:217 average:15.5 STDEV:9.2 Q1:9 Q2:14 Q3:19", so we know that the average length of the sentences is 15.5.
Answer: 15.5
True answer: average:15.5

Validation ID: 61652a3da85196564401d616d251084a25ab4596
Question: What is the size of the real-life dataset?
From the context "The corpus of supervisor assessment has 26972 sentences", so we know that the size of the real-life dataset is 26972 sentences.
Answer: 26972 sentences
True answer: 26972

Validation ID: 61652a3da85196564401d616d251084a25ab4596
Question: What is the size of the real-life dataset?
From the context "The corpus of supervisor assessment has 26972 sentences", so we know that the size of the real-life dataset is 26972 sentences.
Answer: 26972 sentences
True answer: 26972 sentences

Validation ID: 14b74ad5a6f5b0506511c9b454e9c464371ef8c4
Question: What are the language pairs explored in this paper?
From the context "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19, and the WMT 2016 dataset for Ro-En.", so we know that the language pairs explored in this paper are De-En, Ja-En, and Ro-En.
Answer: De-En, Ja-En, and Ro-En
True answer: De-En, Ja-En, Ro-En

Validation ID: 14b74ad5a6f5b0506511c9b454e9c464371ef8c4
Question: What are the language pairs explored in this paper?
From the context "We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19, and the WMT 2016 dataset for Ro-En.", so we know that the language pairs explored in this paper are De-En, Ja-En, and Ro-En.
Answer: De-En, Ja-En, and Ro-En
True answer: De-En, Ja-En, Ro-En

Validation ID: 5c88d601e8fca96bffebfa9ef22331ecf31c6d75
Question: Have they made any attempt to correct MRC gold standards according to their findings??
From the context "Finally, we observe similar distributions of linguistic features and reasoning patterns, except where there are constraints enforced by dataset design, annotation guidelines or source text choice.", so we know that they have not made any attempt to correct MRC gold standards according to their findings.
Answer: No
True answer: Yes

Validation ID: 5c88d601e8fca96bffebfa9ef22331ecf31c6d75
Question: Have they made any attempt to correct MRC gold standards according to their findings??
From the context "Finally, we observe similar distributions of linguistic features and reasoning patterns, except where there are constraints enforced by dataset design, annotation guidelines or source text choice.", so we know that they have not made any attempt to correct MRC gold standards according to their findings.
Answer: No
True answer: No

Validation ID: 71bd5db79635d48a0730163a9f2e8ef19a86cd66
Question: What features are absent from MRC gold standards that can result in potential lexical ambiguity?
From the context "We recognise features that add ambiguity to the supporting facts, for example when information is only expressed implicitly by using an Ellipsis. As opposed to redundant words, we annotate Restrictivity and Factivity modifiers, words and phrases whose presence does change the meaning of a sentence with regard to the expected answer, and occurrences of intra- or inter-sentence Coreference in supporting facts (that is relevant to the question). Lastly, we mark ambiguous syntactic features, when their resolution is required in order to obtain the answer. Concretely, we mark argument collection with con- and disjunctions (Listing) and ambiguous Prepositions, Coordination Scope and Relative clauses/Adverbial phrases/Appositions.", so we know that the features that are absent from MRC gold standards that can result in potential lexical ambiguity are Ellipsis, Restrictivity and Factivity modifiers, words and phrases whose presence does change the meaning of a sentence with regard to the expected answer, and occurrences of intra- or inter-sentence Coreference in supporting facts (that is relevant to the question).
Answer: Ellipsis, Restrictivity and Factivity modifiers, words and phrases whose presence does change the meaning of a sentence with regard to the expected answer, and occurrences of intra- or inter-sentence Coreference in supporting facts (that is relevant to the question)
True answer: Restrictivity , Factivity , Coreference 

Validation ID: 71bd5db79635d48a0730163a9f2e8ef19a86cd66
Question: What features are absent from MRC gold standards that can result in potential lexical ambiguity?
From the context "We observe similar results in the case the queries in ReCoRd are explicitly independent from the passage, the linear classifier is still capable of achieving 34% F1 score in predicting the supporting facts", so we know that the features absent from MRC gold standards that can result in potential lexical ambiguity are the queries in ReCoRd are explicitly independent from the passage.
Answer: the queries in ReCoRd are explicitly independent from the passage
True answer: semantics-altering grammatical modifiers

Validation ID: 9ecde59ffab3c57ec54591c3c7826a9188b2b270
Question: What modern MRC gold standards are analyzed?
From the context "We select contemporary MRC benchmarks to represent all four commonly used problem definitions BIBREF15. In selecting relevant datasets, we do not consider those that are considered “solved”, i.e. where the state of the art performance surpasses human performance, as is the case with SQuAD BIBREF28, BIBREF7. Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations, and bucket them according to the answer selection styles as described in Section SECREF4 We randomly draw one from each bucket and add two randomly drawn datasets from the candidate pool. This leaves us with the datasets described in Table TABREF19. For a more detailed description, we refer to Appendix.", so we know that the modern MRC gold standards are SQuAD, NewsQA, DROP, HotpotQA, ReCoRd and MultiRC.
Answer: SQuAD, NewsQA, DROP, HotpotQA, ReCoRd and MultiRC
True answer: fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations

Validation ID: 9ecde59ffab3c57ec54591c3c7826a9188b2b270
Question: What modern MRC gold standards are analyzed?
From the context "We select contemporary MRC benchmarks to represent all four commonly used problem definitions BIBREF15. In selecting relevant datasets, we do not consider those that are considered “solved”, i.e. where the state of the art performance surpasses human performance, as is the case with SQuAD BIBREF28, BIBREF7. Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations, and bucket them according to the answer selection styles as described in Section SECREF4 We randomly draw one from each bucket and add two randomly drawn datasets from the candidate pool. This leaves us with the datasets described in Table TABREF19. For a more detailed description, we refer to Appendix.", so we know that the modern MRC gold standards are SQuAD, NewsQA, DROP, HotpotQA, ReCoRd and MultiRC.
Answer: SQuAD, NewsQA, DROP, HotpotQA, ReCoRd and MultiRC
True answer: MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.

Validation ID: 005cca3c8ab6c3a166e315547a2259020f318ffb
Question: How does proposed qualitative annotation schema looks like?
From the context "The samples were annotated by the first author of this paper, using the proposed schema. In order to validate our findings, we further take 20% of the annotated samples and present them to a second annotator (second author). Since at its core, the annotation is a multi-label task, we report the inter-annotator agreement by computing the (micro-averaged) F1 score, where we treat the first annotator's labels as gold. Table TABREF21 reports the agreement scores, the overall (micro) average F1 score of the annotations is 0.82, which means that on average, more than two thirds of the overall annotated labels were agreed on by both annotators. We deem this satisfactory, given the complexity of the annotation schema.", so we know that the proposed qualitative annotation schema is a multi-label task.
Answer: multi-label task
True answer: The resulting taxonomy of the framework is shown in Figure FIGREF10

Validation ID: 005cca3c8ab6c3a166e315547a2259020f318ffb
Question: How does proposed qualitative annotation schema looks like?
From the context "The samples were annotated by the first author of this paper, using the proposed schema. In order to validate our findings, we further take 20% of the annotated samples and present them to a second annotator (second author). Since at its core, the annotation is a multi-label task, we report the inter-annotator agreement by computing the (micro-averaged) F1 score, where we treat the first annotator's labels as gold. Table TABREF21 reports the agreement scores, the overall (micro) average F1 score of the annotations is 0.82, which means that on average, more than two thirds of the overall annotated labels were agreed on by both annotators. We deem this satisfactory, given the complexity of the annotation schema.", so we know that the proposed qualitative annotation schema is a multi-label task.
Answer: multi-label task
True answer: FIGREF10

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
From the context "Table TABREF29 shows the results on four test sets as well as the average performance. Our method significantly outperforms the strong `many-to-many+pretrain' baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective that substantially improves the translation quality. Besides, both pre-training and multi-task learning can improve translation quality, and the pre-training settings (2nd-4th rows) are more effective compared to multi-task settings (5th-8th rows). We observe a performance degradation in the `triangle+pretrain' baseline", so we know that the baselines are many-to-many+pretrain, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many, many-to-one, many-to-many, and triangle+pretrain.
Answer: many-to-many+pretrain, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many, many-to-one, many-to-many, and triangle+pretrain
True answer: Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
From the context "Table TABREF29 shows the results on four test sets as well as the average performance. Our method significantly outperforms the strong `many-to-many+pretrain' baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective that substantially improves the translation quality. Besides, both pre-training and multi-task learning can improve translation quality, and the pre-training settings (2nd-4th rows) are more effective compared to multi-task settings (5th-8th rows). We observe a performance degradation in the `triangle+pretrain' baseline", so we know that the baselines are many-to-many+pretrain, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many, many-to-one, many-to-many, triangle+pretrain.
Answer: many-to-many+pretrain, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many, many-to-one, many-to-many, triangle+pretrain
True answer: Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
From the context "Table TABREF29 shows the results on four test sets as well as the average performance. Our method significantly outperforms the strong `many-to-many+pretrain' baseline by 3.6 and 2.2 BLEU scores respectively, indicating the proposed method is very effective that substantially improves the translation quality. Besides, both pre-training and multi-task learning can improve translation quality, and the pre-training settings (2nd-4th rows) are more effective compared to multi-task settings (5th-8th rows). We observe a performance degradation in the `triangle+pretrain' baseline", so we know that the baselines are many-to-many+pretrain, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many, many-to-one, many-to-many, and triangle+pretrain.
Answer: many-to-many+pretrain, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many, many-to-one, many-to-many, and triangle+pretrain
True answer: Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.

Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.

Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\alpha _{st}=0.75$ while $\alpha _{asr}=0.25$ or $\alpha _{mt}=0.25$. For many-to-many setting, we use $\alpha _{st}=0.6, \alpha _{asr}=0.2$ and $\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.

Many-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. 

Validation ID: 022c365a14fdec406c7a945a1a18e7e79df37f08
Question: What is the attention module pretrained on?
From the context "We reuse the pre-trained MT attention module in ST, so we can leverage the alignment information learned in pre-training", so we know that the attention module is pretrained on MT.
Answer: MT
True answer: the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.

Validation ID: 5260cb56b7d127772425583c5c28958c37cb9bea
Question: How long of dialog history is captured?
From the context "The proposed models also illustrate advantageous performance over several competitive contextual language models. Perplexity of the proposed dialog context language models is higher than that of the model using true dialog act tags as context by a small margin. This indicates that the proposed model may implicitly capture the dialog context state for language modeling.", so we know that the proposed model may implicitly capture the dialog context state for language modeling.
Answer: The proposed model may implicitly capture the dialog context state for language modeling.
True answer: two previous turns

Validation ID: 5260cb56b7d127772425583c5c28958c37cb9bea
Question: How long of dialog history is captured?
From the context "The proposed models also illustrate advantageous performance over several competitive contextual language models. Perplexity of the proposed dialog context language models is higher than that of the model using true dialog act tags as context by a small margin. This indicates that the proposed model may implicitly capture the dialog context state for language modeling.", so we know that the proposed model may implicitly capture the dialog context state for language modeling.
Answer: The proposed model may implicitly capture the dialog context state for language modeling.
True answer: 160

Validation ID: 9b97805a0c093df405391a85e4d3ab447671c86a
Question: What evaluation metrics were used?
From the context "The most intuitive way to evaluate the text answer is to directly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer", so we know that the evaluation metrics are Exact Match (EM) and Macro-averaged F1 scores (F1).
Answer: Exact Match (EM) and Macro-averaged F1 scores (F1)
True answer: Exact Match (EM), Macro-averaged F1 scores (F1)

Validation ID: 9b97805a0c093df405391a85e4d3ab447671c86a
Question: What evaluation metrics were used?
From the context "The most intuitive way to evaluate the text answer is to directly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer", so we know that the evaluation metrics are Exact Match (EM) and Macro-averaged F1 scores (F1).
Answer: Exact Match (EM) and Macro-averaged F1 scores (F1)
True answer: Exact Match (EM) and Macro-averaged F1 scores (F1) 

Validation ID: 38f58f13c7f23442d5952c8caf126073a477bac0
Question: What was the score of the proposed model?
From the context "The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.", so we know that the score of the proposed model is 2% EM score and over 1.5% F1 score.
Answer: 2% EM score and over 1.5% F1 score
True answer: Best results authors obtain is EM 51.10 and F1 63.11

Validation ID: 38f58f13c7f23442d5952c8caf126073a477bac0
Question: What was the score of the proposed model?
From the context "The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.", so we know that the score of the proposed model is 2% EM score and over 1.5% F1 score.
Answer: 2% EM score and over 1.5% F1 score
True answer: EM Score of 51.10

Validation ID: 7ee5c45b127fb284a4a9e72bb9b980a602f7445a
Question: What was the previous best model?
From the context "The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.", so we know that the previous best model is the model that achieves the second best performance.
Answer: the model that achieves the second best performance
True answer: (c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 

Validation ID: 7ee5c45b127fb284a4a9e72bb9b980a602f7445a
Question: What was the previous best model?
From the context "The experiments demonstrate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score.", so we know that the previous best model is the model that achieves the second best performance.
Answer: the model that achieves the second best performance
True answer: Unanswerable

Validation ID: ddf5e1f600b9ce2e8f63213982ef4209bab01fd8
Question: Which datasets did they use for evaluation?
From the context "Spoken-SQuAD is chosen as the target domain data for training and testing. Spoken-SQuAD BIBREF5 is an automatically generated corpus in which the document is in spoken form and the question is in text form. The reference transcriptions are from SQuAD BIBREF1. There are 37,111 and 5,351 question answer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%.", so we know that they use Spoken-SQuAD and SQuAD for evaluation.
Answer: Spoken-SQuAD and SQuAD
True answer: Spoken-SQuAD testing set

Validation ID: ddf5e1f600b9ce2e8f63213982ef4209bab01fd8
Question: Which datasets did they use for evaluation?
From the context "Spoken-SQuAD is chosen as the target domain data for training and testing. Spoken-SQuAD BIBREF5 is an automatically generated corpus in which the document is in spoken form and the question is in text form. The reference transcriptions are from SQuAD BIBREF1. There are 37,111 and 5,351 question answer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%.", so we know that they use Spoken-SQuAD and SQuAD for evaluation.
Answer: Spoken-SQuAD and SQuAD
True answer: Spoken-SQuAD

Validation ID: 27275fe9f6a9004639f9ac33c3a5767fea388a98
Question: What hyperparameters are explored?
From the context "Table TABREF2 describes most hyper-parameters explored for each dataset", so we know that the hyperparameters are described in Table TABREF2.
Answer: Table TABREF2
True answer: Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.

Validation ID: 27275fe9f6a9004639f9ac33c3a5767fea388a98
Question: What hyperparameters are explored?
From the context "Table TABREF2 describes most hyper-parameters explored for each dataset", so we know that the hyperparameters are described in Table TABREF2.
Answer: Table TABREF2
True answer: Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.

Validation ID: ef3567ce7301b28e34377e7b62c4ec9b496f00bf
Question: What Named Entity Recognition dataset is used?
From the context "The GMB dataset for NER was trained in an LSTM network, which had an embedding layer for input. The network diagram is shown in fig. FIGREF4. The IMDb dataset for SA was trained in a BiLSTM network, which also used an embedding layer for input. Its network diagram is given in fig. FIGREF4. It includes an additional hidden linear layer. Hyper-parameter details of the two networks for the downstream tasks are given in table TABREF3. The metrics for extrinsic evaluation include F1, precision, recall and accuracy scores. In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by BIBREF0 and ours. In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets. Batch size of 64 was used. For each task, experiments for each embedding was conducted four times and an average value calculated and reported in the next section", so we know that the Named Entity Recognition dataset is GMB.
Answer: GMB
True answer: Groningen Meaning Bank

Validation ID: ef3567ce7301b28e34377e7b62c4ec9b496f00bf
Question: What Named Entity Recognition dataset is used?
From the context "The GMB dataset for NER was trained in an LSTM network, which had an embedding layer for input. The network diagram is shown in fig. FIGREF4. The IMDb dataset for SA was trained in a BiLSTM network, which also used an embedding layer for input. Its network diagram is given in fig. FIGREF4. It includes an additional hidden linear layer. Hyper-parameter details of the two networks for the downstream tasks are given in table TABREF3. The metrics for extrinsic evaluation include F1, precision, recall and accuracy scores. In both tasks, the default pytorch embedding was tested before being replaced by pre-trained embeddings released by BIBREF0 and ours. In each case, the dataset was shuffled before training and split in the ratio 70:15:15 for training, validation (dev) and test sets. Batch size of 64 was used. For each task, experiments for each embedding was conducted four times and an average value calculated and reported in the next section", so we know that the Named Entity Recognition dataset is GMB.
Answer: GMB
True answer: Groningen Meaning Bank (GMB)

Validation ID: 7595260c5747aede0b32b7414e13899869209506
Question: What sentiment analysis dataset is used?
From the context "The IMDb dataset for SA was trained in a BiLSTM network, which also used an embedding layer for input. Its network diagram is given in fig. FIGREF4. It includes an additional hidden linear layer.", so we know that the sentiment analysis dataset is IMDb.
Answer: IMDb
True answer: IMDb dataset of movie reviews

Validation ID: 7595260c5747aede0b32b7414e13899869209506
Question: What sentiment analysis dataset is used?
From the context "The IMDb dataset for SA was trained in a BiLSTM network, which also used an embedding layer for input. Its network diagram is given in fig. FIGREF4. It includes an additional hidden linear layer.", so we know that the sentiment analysis dataset is IMDb.
Answer: IMDb
True answer: IMDb

Validation ID: c2d1387e08cf25cb6b1f482178cca58030e85b70
Question: Do they test both skipgram and c-bow?
From the context "The models were generated in a shared cluster running Ubuntu 16 with 32 CPUs of 32x Intel Xeon 4110 at 2.1GHz. Gensim (BIBREF17) python library implementation of word2vec was used with parallelization to utilize all 32 CPUs", so we know that they use Gensim python library implementation of word2vec.
From the context "To form the vocabulary, words occurring less than 5 times in the corpora were dropped, stop words removed using the natural language toolkit (NLTK) (BIBREF22) and data pre-processing carried out. Table TABREF2 describes most hyper-parameters explored for each dataset. In all, 80 runs (of about 160 minutes) were conducted for the 15MB Wiki Abstract dataset with 80 serialized models totaling 15.136GB while 80 runs (for over 320 hours) were conducted for the 711MB SW dataset, with 80 serialized models totaling over 145GB. Experiments for all combinations for 300 dimensions were conducted on the 3.9GB training set of the BW corpus and additional runs for other dimensions for the window 8 + skipgram + heirarchical softmax combination to verify the trend of quality of word vectors as dimensions are increased.", so we know that they use 80 runs for the 15MB Wiki Abstract dataset and 80 runs for the 711MB SW dataset.
True answer: Yes

Validation ID: c2d1387e08cf25cb6b1f482178cca58030e85b70
Question: Do they test both skipgram and c-bow?
From the context "The models were generated in a shared cluster running Ubuntu 16 with 32 CPUs of 32x Intel Xeon 4110 at 2.1GHz. Gensim (BIBREF17) python library implementation of word2vec was used with parallelization to utilize all 32 CPUs", so we know that they use Gensim python library implementation of word2vec.
From the context "To form the vocabulary, words occurring less than 5 times in the corpora were dropped, stop words removed using the natural language toolkit (NLTK) (BIBREF22) and data pre-processing carried out. Table TABREF2 describes most hyper-parameters explored for each dataset. In all, 80 runs (of about 160 minutes) were conducted for the 15MB Wiki Abstract dataset with 80 serialized models totaling 15.136GB while 80 runs (for over 320 hours) were conducted for the 711MB SW dataset, with 80 serialized models totaling over 145GB. Experiments for all combinations for 300 dimensions were conducted on the 3.9GB training set of the BW corpus and additional runs for other dimensions for the window 8 + skipgram + heirarchical softmax combination to verify the trend of quality of word vectors as dimensions are increased.", so we know that they use 80 runs for the 15MB Wiki Abstract dataset and 80 runs for the 711MB SW dataset.
True answer: Yes

Validation ID: 5a22293b055f5775081d6acdc0450f7bd5f5de04
Question: What is the state-of-the-art model for the task?
From the context "We compared our model with gold texts, template-based system, CC BIBREF2 and NCP+CC (NCP) BIBREF4. We adopted the same 30 examples from above and arranged every 5-tuple of summaries into 10 pairs. Then, we asked the raters to choose which system performs the best given each pair. Scores are computed as the difference between percentage of times when the model is chosen as the best and percentage of times when the model is chosen as the worst. Gold texts is significantly more grammatical than others across all three metrics. Also, our model performs significantly better than other two neural models (CC, NCP) in all three metrics. Template-based system generates significantly more grammatical and concise but significantly less coherent results, compared to all three neural models. Because the rigid structure of texts ensures the correct grammaticality and no repetition in template-based system's output. However, since the templates are stilted and lack variability compared to others, it was deemed less coherent than the others by the raters.", so we know that the state-of-the-art model for the task is our model.
Answer: our model
True answer: OpATT BIBREF6, Neural Content Planning with conditional copy (NCP+CC) BIBREF4

Validation ID: 5a22293b055f5775081d6acdc0450f7bd5f5de04
Question: What is the state-of-the-art model for the task?
From the context "We compared our model with gold texts, template-based system, CC BIBREF2 and NCP+CC (NCP) BIBREF4. We adopted the same 30 examples from above and arranged every 5-tuple of summaries into 10 pairs. Then, we asked the raters to choose which system performs the best given each pair. Scores are computed as the difference between percentage of times when the model is chosen as the best and percentage of times when the model is chosen as the worst. Gold texts is significantly more grammatical than others across all three metrics. Also, our model performs significantly better than other two neural models (CC, NCP) in all three metrics. Template-based system generates significantly more grammatical and concise but significantly less coherent results, compared to all three neural models. Because the rigid structure of texts ensures the correct grammaticality and no repetition in template-based system's output. However, since the templates are stilted and lack variability compared to others, it was deemed less coherent than the others by the raters.", so we know that the state-of-the-art model for the task is our model.
Answer: our model
True answer: Unanswerable

Validation ID: 03c967763e51ef2537793db7902e2c9c17e43e95
Question: What is the strong baseline?
From the context "We chose Conditional Copy (CC) model as our baseline, which is the best model in Wiseman", so we know that the strong baseline is Conditional Copy (CC) model.
Answer: Conditional Copy (CC) model
True answer: Conditional Copy (CC) model 

Validation ID: 03c967763e51ef2537793db7902e2c9c17e43e95
Question: What is the strong baseline?
From the context "We chose Conditional Copy (CC) model as our baseline, which is the best model in Wiseman", so we know that the strong baseline is Conditional Copy (CC) model.
Answer: Conditional Copy (CC) model
True answer: delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)

Validation ID: 26327ccebc620a73ba37a95aabe968864e3392b2
Question: what aspects of conversation flow do they look at?
From the context "We quantify the use of these strategies by comparing the airtime debaters devote to talking points. For a side INLINEFORM0, let the self-coverage INLINEFORM1 be the fraction of content words uttered by INLINEFORM2 in round INLINEFORM3 that are among their own talking points INLINEFORM4 ; and the opponent-coverage INLINEFORM5 be the fraction of its content words covering opposing talking points INLINEFORM6.", so we know that they look at the self-coverage and the opponent-coverage.
Answer: self-coverage and the opponent-coverage
True answer: The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.

Validation ID: 26327ccebc620a73ba37a95aabe968864e3392b2
Question: what aspects of conversation flow do they look at?
From the context "We quantify the use of these strategies by comparing the airtime debaters devote to talking points. For a side INLINEFORM0, let the self-coverage INLINEFORM1 be the fraction of content words uttered by INLINEFORM2 in round INLINEFORM3 that are among their own talking points INLINEFORM4 ; and the opponent-coverage INLINEFORM5 be the fraction of its content words covering opposing talking points INLINEFORM6.", so we know that they look at the self-coverage and the opponent-coverage.
Answer: self-coverage and the opponent-coverage
True answer: —promoting one's own points and attacking the opponents' points

Validation ID: ababb79dd3c301f4541beafa181f6a6726839a10
Question: what debates dataset was used?
From the context "In this study we use transcripts and results of Oxford-style debates from the public debate series “Intelligence Squared Debates” (IQ2 for short)", so we know that the debates dataset used is Intelligence Squared Debates.
Answer: Intelligence Squared Debates
True answer: Intelligence Squared Debates

Validation ID: ababb79dd3c301f4541beafa181f6a6726839a10
Question: what debates dataset was used?
From the context "In this study we use transcripts and results of Oxford-style debates from the public debate series “Intelligence Squared Debates” (IQ2 for short)", so we know that the debates dataset used is Intelligence Squared Debates.
Answer: Intelligence Squared Debates
True answer: “Intelligence Squared Debates” (IQ2 for short)

Validation ID: c2b8ee872b99f698b3d2082d57f9408a91e1b4c1
Question: what is the state of the art?
From the context "We show that the combination of letter-trigram word hashing with bidirectional LSTM yields the best results and outperforms sequence learners based on dictionaries or word2vec", so we know that the state of the art is the combination of letter-trigram word hashing with bidirectional LSTM.
Answer: the combination of letter-trigram word hashing with bidirectional LSTM
True answer: Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
From the context "We train two models with identical parameterization, each with 2000 randomly chosen labeled sentences from a standard data set", so we know that the standard dataset is used for training.
Answer: standard data set
True answer: The GENIA Corpus , CoNLL2003

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
From the context "We train two models with identical parameterization, each with 2000 randomly chosen labeled sentences from a standard data set", so we know that the standard dataset is used for training.
Answer: standard data set
True answer: GENIA Corpus BIBREF3, CoNLL2003 BIBREF14, KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
From the context "We train two models with identical parameterization, each with 2000 randomly chosen labeled sentences from a standard data set", so we know that the standard dataset is used for training.
Answer: standard data set
True answer: CoNLL2003-testA, GENIA

Validation ID: 133eb4aa4394758be5f41744c60c99901b2bc01c
Question: Do they perform error analysis?
From the context "The nature of the content can vary as its provided by different people. The current work uses the data provided in SemEval 2019 shared task A for Offensive language identification. A comparative study is provided by exploring the effectiveness of Google universal sentence encoder, Fasttext based embedding, Dynamic Mode Decomposition based features and RKS based explicit mapping approach. For the experiments, we used the machine learning methods such as SVM linear, Random Forest, Logistic regression, Navie Bayes and Regularized least-square based classification. The measures used for evaluation are accuracy, precision, recall, and f1-score. We observed that RKS approach improved the results. However, as a future work, the proposed approach cane be explored on large datasets.", so we know that they perform error analysis.
Answer: Yes
True answer: No

Validation ID: 133eb4aa4394758be5f41744c60c99901b2bc01c
Question: Do they perform error analysis?
From the context "The nature of the content can vary as its provided by different people. The current work uses the data provided in SemEval 2019 shared task A for Offensive language identification. A comparative study is provided by exploring the effectiveness of Google universal sentence encoder, Fasttext based embedding, Dynamic Mode Decomposition based features and RKS based explicit mapping approach. For the experiments, we used the machine learning methods such as SVM linear, Random Forest, Logistic regression, Navie Bayes and Regularized least-square based classification. The measures used for evaluation are accuracy, precision, recall, and f1-score. We observed that RKS approach improved the results. However, as a future work, the proposed approach cane be explored on large datasets.", so we know that they perform error analysis.
Answer: Yes
True answer: No

Validation ID: 3fff37b9f68697d080dbd9d9008a63907137644e
Question: How do their results compare to state-of-the-art?
From the context "The Table TABREF27 shows the classification report on the proposed RKS method taking word vectors generated by Google universal encoder model as features with dimension 512. For this work, such vector is explicitly mapped to dimensions 100, 200, 500 and 1000 using RKS. The maximum accuracy obtained is 90.58% for higher dimension 1000.", so we know that the maximum accuracy obtained is 90.58% for higher dimension 1000.
Answer: 90.58%
True answer: Unanswerable

Validation ID: 3fff37b9f68697d080dbd9d9008a63907137644e
Question: How do their results compare to state-of-the-art?
From the context "The Table TABREF27 shows the classification report on the proposed RKS method taking word vectors generated by Google universal encoder model as features with dimension 512. For this work, such vector is explicitly mapped to dimensions 100, 200, 500 and 1000 using RKS. The maximum accuracy obtained is 90.58% for higher dimension 1000.", so we know that the maximum accuracy obtained is 90.58% for higher dimension 1000.
Answer: 90.58%
True answer: Unanswerable

Validation ID: a778b8204a415b295f73b93623d09599f242f202
Question: What is the Random Kitchen Sink approach?
From the context "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping.", so we know that the Random Kitchen Sink approach is a method that explicitly maps data vectors to a space where linear separation is possible.
Answer: a method that explicitly maps data vectors to a space where linear separation is possible
True answer: Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.

Validation ID: a778b8204a415b295f73b93623d09599f242f202
Question: What is the Random Kitchen Sink approach?
From the context "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping.", so we know that the Random Kitchen Sink approach is a method that explicitly maps data vectors to a space where linear separation is possible.
Answer: a method that explicitly maps data vectors to a space where linear separation is possible
True answer: explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping

Validation ID: 642e8cf1d39faa1cd985d16750cdc6696c52db2f
Question: what are the baseline systems?
From the context "Our systems are attentional encoder-decoder networks BIBREF0. We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.", so we know that the baseline systems are attentional encoder-decoder networks.
Answer: attentional encoder-decoder networks
True answer: attentional encoder-decoder networks BIBREF0

Validation ID: 642e8cf1d39faa1cd985d16750cdc6696c52db2f
Question: what are the baseline systems?
From the context "Our systems are attentional encoder-decoder networks BIBREF0. We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout.", so we know that the baseline systems are attentional encoder-decoder networks.
Answer: attentional encoder-decoder networks
True answer:  the dl4mt-tutorial

Validation ID: 493e971ee3f57a821ef1f67ef3cd47ade154e7c4
Question: What word embeddings do they test?
From the context "We analyzed EqEmb models performance using a held out set of words that we generate for each equation in our collections. Held out sets are constructed using the following procedure: We traverse over the collections and for every discovered equation we randomly sample words from its context set. The held out set contains the sampled words and their context window which also includes the equation. For each held out word we also generate a set of negative samples for the given word context. We perform the same procedure to form a validation set. For each of the INLINEFORM0 equations in a collection, two held out words INLINEFORM1 are sampled. For a context window of size 4 the sampled word context is defined as INLINEFORM2.", so we know that they test CBOW, PV-DM, GloVe and EqEmb.
Answer: CBOW, PV-DM, GloVe and EqEmb
True answer: Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model

Validation ID: 493e971ee3f57a821ef1f67ef3cd47ade154e7c4
Question: What word embeddings do they test?
From the context "We analyzed EqEmb models performance using a held out set of words that we generate for each equation in our collections. Held out sets are constructed using the following procedure: We traverse over the collections and for every discovered equation we randomly sample words from its context set. The held out set contains the sampled words and their context window which also includes the equation. For each held out word we also generate a set of negative samples for the given word context. We perform the same procedure to form a validation set. For each of the INLINEFORM0 equations in a collection, two held out words INLINEFORM1 are sampled. For a context window of size 4 the sampled word context is defined as INLINEFORM2.", so we know that they test CBOW, PV-DM, GloVe and EqEmb.
Answer: CBOW, PV-DM, GloVe and EqEmb
True answer: Bernoulli embeddings, continuous bag-of-words, Distributed Memory version of Paragraph Vector, Global Vectors, equation embeddings, equation unit embeddings

Validation ID: 8dd8e5599fc56562f2acbc16dd8544689cddd938
Question: How do they define similar equations?
From the context "We also demonstrate that they can capture the semantic similarity between equations and the words in the collection", so we know that they can capture the semantic similarity between equations and the words in the collection.
Answer: They can capture the semantic similarity between equations and the words in the collection
True answer: By using Euclidean distance computed between the context vector representations of the equations

Validation ID: 8dd8e5599fc56562f2acbc16dd8544689cddd938
Question: How do they define similar equations?
From the context "We also demonstrate that they can capture the semantic similarity between equations and the words in the collection", so we know that they can capture the semantic similarity between equations and the words in the collection.
From the context "Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 )", so we know that they define similar equations using Euclidean distance computed between the context vector representations of the equations.
True answer: Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B.

Validation ID: abe2393415e533cb06311e74ed1c5674cff8571f
Question: What evaluation criteria and metrics were used to evaluate the generated text?
From the context "In Table TABREF21, we provide evaluation results using the five aforementioned metrics. We evaluate on event level using gold standard event selection, where each generated event description is compared to its existing reference text. As the model is trained to produce a tokenized sequence, we apply a detokenizer to be able to compare against the original untokenized reference text.
On the test set, the model achieves a BLEU score of 19.67. To the extent that different datasets allow comparison, the best reported score on the Rotowire basketball news corpus is 16.50 BIBREF5. Compared to our earlier E2E baseline experiment, we score lower than our closest comparable reference of 31.90 (with single references), which is understandable due to the much smaller train set size for the hockey corpus (about 13% in size).", so we know that the evaluation criteria and metrics are BLEU, METEOR, ROUGE-L, NIST and CIDEr.
True answer: BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency

Validation ID: abe2393415e533cb06311e74ed1c5674cff8571f
Question: What evaluation criteria and metrics were used to evaluate the generated text?
From the context "In Table TABREF21, we provide evaluation results using the five aforementioned metrics. We evaluate on event level using gold standard event selection, where each generated event description is compared to its existing reference text. As the model is trained to produce a tokenized sequence, we apply a detokenizer to be able to compare against the original untokenized reference text.
On the test set, the model achieves a BLEU score of 19.67. To the extent that different datasets allow comparison, the best reported score on the Rotowire basketball news corpus is 16.50 BIBREF5. Compared to our earlier E2E baseline experiment, we score lower than our closest comparable reference of 31.90 (with single references), which is understandable due to the much smaller train set size for the hockey corpus (about 13% in size).", so we know that the evaluation criteria and metrics are BLEU, METEOR, ROUGE-L, NIST and CIDEr.
True answer: BLEU, NIST, METEOR, ROUGE-L, CIDEr

Validation ID: 00c57e45ac6afbdfa67350a57e81b4fad0ed2885
Question: Do they evaluate only on English datasets?
From the context "We conducted two feature study experiments: 1) a feature ablation study to assess the contribution of feature groups and 2) a feature elimination study to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy.", so we know that they evaluate only on Twitter datasets.
Answer: Twitter datasets
True answer: Yes

Validation ID: 00c57e45ac6afbdfa67350a57e81b4fad0ed2885
Question: Do they evaluate only on English datasets?
From the context "We conducted two feature study experiments: 1) a feature ablation study to assess the contribution of feature groups and 2) a feature elimination study to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy.", so we know that they evaluate only on Twitter datasets.
Answer: Twitter datasets
True answer: Yes

Validation ID: 22714f6cad2d5c54c28823e7285dc85e8d6bc109
Question: What are the three steps to feature elimination?
From the context "Feature elimination strategies are often taken 1) to remove irrelevant or noisy features, 2) to improve classifier performance, and 3) to reduce training and run times. We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach:
Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset.
True answer: Reduction, Selection, Rank

Validation ID: 22714f6cad2d5c54c28823e7285dc85e8d6bc109
Question: What are the three steps to feature elimination?
From the context "Feature elimination strategies are often taken 1) to remove irrelevant or noisy features, 2) to improve classifier performance, and 3) to reduce training and run times. We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach:
Reduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset.
True answer: reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features.

Validation ID: 82642d3111287abf736b781043d49536fe48c350
Question: How is the dataset annotated?
From the context "The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression") or evidence of depression (e.g., “depressed over disappointment"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps"), disturbed sleep (e.g., “another restless night"), or fatigue or loss of energy (e.g., “the fatigue is unbearable") BIBREF10. For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0.", so we know that the dataset is annotated as no evidence of depression or evidence of depression.
Answer: no evidence of depression or evidence of depression
True answer: no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy

Validation ID: 82642d3111287abf736b781043d49536fe48c350
Question: How is the dataset annotated?
From the context "The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression") or evidence of depression (e.g., “depressed over disappointment"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps"), disturbed sleep (e.g., “another restless night"), or fatigue or loss of energy (e.g., “the fatigue is unbearable") BIBREF10. For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0.", so we know that the dataset is annotated as no evidence of depression or evidence of depression.
Answer: no evidence of depression or evidence of depression
True answer: The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression

Validation ID: 5a81732d52f64e81f1f83e8fd3514251227efbc7
Question: What dataset is used for this study?
From the context "We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12, BIBREF13. The dataset contains 9,473 annotations for 9,300 tweets.", so we know that the dataset used for this study is an existing, annotated Twitter dataset.
Answer: an existing, annotated Twitter dataset
True answer: BIBREF12 , BIBREF13

Validation ID: 5a81732d52f64e81f1f83e8fd3514251227efbc7
Question: What dataset is used for this study?
From the context "We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12, BIBREF13. The dataset contains 9,473 annotations for 9,300 tweets.", so we know that the dataset used for this study is an existing, annotated Twitter dataset.
Answer: an existing, annotated Twitter dataset
True answer: an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13

Validation ID: 9a8b9ea3176d30da2453cac6e9347737c729a538
Question: what were their performance results?
From the context "In this project, we tackle the problem of extracting clinical concepts from user queries on medical search engines. By training a BiLSTM-CRF model on a hybrid data consisting of synthesized user queries and sentences from clinical note, we adopt a CE framework for clinical user queries with minimal effort spent on annotating user queries. We find that the hybrid data enables the NER model perform better on both tagging the user queries and the clinical note sentences. Furthermore, our framework is built on an easy-to-use deep learning NLP Python library, which lends it more prospective value to various online medical applications that employ medical search engines.", so we know that the performance results are that the hybrid data enables the NER model perform better on both tagging the user queries and the clinical note sentences.
Answer: the hybrid data enables the NER model perform better on both tagging the user queries and the clinical note sentences.
True answer:  the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes

Validation ID: 9a8b9ea3176d30da2453cac6e9347737c729a538
Question: what were their performance results?
From the context "In this project, we tackle the problem of extracting clinical concepts from user queries on medical search engines. By training a BiLSTM-CRF model on a hybrid data consisting of synthesized user queries and sentences from clinical note, we adopt a CE framework for clinical user queries with minimal effort spent on annotating user queries. We find that the hybrid data enables the NER model perform better on both tagging the user queries and the clinical note sentences. Furthermore, our framework is built on an easy-to-use deep learning NLP Python library, which lends it more prospective value to various online medical applications that employ medical search engines.", so we know that the performance results are that the hybrid data enables the NER model perform better on both tagging the user queries and the clinical note sentences.
Answer: the hybrid data enables the NER model perform better on both tagging the user queries and the clinical note sentences.
True answer: hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes

Validation ID: 4477bb513d56e57732fba126944073d414d1f75f
Question: where did they obtain the annotated clinical notes from?
From the context "We use the same BiLSTM-CRF model proposed by Huang et al. BIBREF2. An illustration of the architecture is shown in Figure FIGREF6. Given a sequence (or sentence) of n tokens, $r = (w_1, w_2,..., w_n)$, we use a fine-tuned ELMo model to generate contextual word embeddings for all the tokens in the sentence, where a token refers to a word or punctuation. We denote the ELMo embedding, $x$, for a token $w$ in the sentence $r$ by $x = ELMo(w|r)$. The notation and the procedure described here can be adopted for Flair embeddings or other embeddings. Now, given a sequence of tokens in ELMo embeddings, $X =(x_1, x_2,..., x_n)$, the BiLSTM layer generates a matrix of scores, $P(\theta )$ of size $n \times k$, where $k$ is the number of tag types, and $\theta $ is the parameters of the BiLSTM. To simplify notation, we will omit the $\theta $ and write $P$. Then, $P_{i,j}$ denotes the score of the token, $x_i$, being assigned to the $j$th tag. Since certain constraints may exist in the transition between tags, an "O" tag should not be followed by an "I" tag, for example, a transition matrix, $A$, of dimension $(k+2)\times (k+2)$, is initialized to model the constraints. The learnable parameters, $A_{i,j}$, represent the probability of the $j$th tag follows the $i$th tag in a sequence. For example, if we index the tags by: 1:“B”, 2:“I”, and 3:“O”, then $A_{1,3}$ would be the probability that an “O” tag follows a “B” tag. A beginning transition and an end transition are inserted in $A$ and hence $A$ is of dimension $(k+2)\times (k+2)$.
Given a sequence of tags, $Y=(y_1
True answer: clinical notes from the CE task in 2010 i2b2/VA

Validation ID: 4477bb513d56e57732fba126944073d414d1f75f
Question: where did they obtain the annotated clinical notes from?
From the context "We use the same BiLSTM-CRF model proposed by Huang et al. BIBREF2. An illustration of the architecture is shown in Figure FIGREF6. Given a sequence (or sentence) of n tokens, $r = (w_1, w_2,..., w_n)$, we use a fine-tuned ELMo model to generate contextual word embeddings for all the tokens in the sentence, where a token refers to a word or punctuation. We denote the ELMo embedding, $x$, for a token $w$ in the sentence $r$ by $x = ELMo(w|r)$. The notation and the procedure described here can be adopted for Flair embeddings or other embeddings. Now, given a sequence of tokens in ELMo embeddings, $X =(x_1, x_2,..., x_n)$, the BiLSTM layer generates a matrix of scores, $P(\theta )$ of size $n \times k$, where $k$ is the number of tag types, and $\theta $ is the parameters of the BiLSTM. To simplify notation, we will omit the $\theta $ and write $P$. Then, $P_{i,j}$ denotes the score of the token, $x_i$, being assigned to the $j$th tag. Since certain constraints may exist in the transition between tags, an "O" tag should not be followed by an "I" tag, for example, a transition matrix, $A$, of dimension $(k+2)\times (k+2)$, is initialized to model the constraints. The learnable parameters, $A_{i,j}$, represent the probability of the $j$th tag follows the $i$th tag in a sequence. For example, if we index the tags by: 1:“B”, 2:“I”, and 3:“O”, then $A_{1,3}$ would be the probability that an “O” tag follows a “B” tag. A beginning transition and an end transition are inserted in $A$ and hence $A$ is of dimension $(k+2)\times (k+2)$.
Given a sequence of tags, $Y=(y_1
True answer: clinical notes from the CE task in 2010 i2b2/VA 

Validation ID: 1b23c4535a6c10eb70bbc95313c465e4a547db5e
Question: Which architecture do they use for the encoder and decoder?
From the context "Our attention-based Wav2Text architecture uses four convolutional layers, followed by two NIN layers at the lower part of the encoder module. For all the convolutional layers, we used a leaky rectifier unit (LReLU) BIBREF20 activation function with leakiness INLINEFORM0. Inside the first NIN layers, we stacked three consecutive filters with LReLU activation function. For the second NIN layers, we stacked two consecutive filters with tanh and identity activation function. For the feature transfer learning training phase, we used Momentum SGD with a learning rate of 0.01 and momentum of 0.9. Table TABREF11 summarizes the details of the layer settings for the convolutional and NIN layers.
On the top layers of the encoder after the transferred convolutional and NIN layers, we put three bidirectional LSTMs (Bi-LSTM) with 256 hidden units (total 512 units for both directions). To reduce the computational time, we used hierarchical subsampling BIBREF21, BIBREF22, BIBREF10. We applied subsampling on all the Bi-LSTM layers and reduced the length by a factor of 8.
True answer: we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part, On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)

Validation ID: 1b23c4535a6c10eb70bbc95313c465e4a547db5e
Question: Which architecture do they use for the encoder and decoder?
From the context "Our attention-based Wav2Text architecture uses four convolutional layers, followed by two NIN layers at the lower part of the encoder module. For all the convolutional layers, we used a leaky rectifier unit (LReLU) BIBREF20 activation function with leakiness INLINEFORM0. Inside the first NIN layers, we stacked three consecutive filters with LReLU activation function. For the second NIN layers, we stacked two consecutive filters with tanh and identity activation function. For the feature transfer learning training phase, we used Momentum SGD with a learning rate of 0.01 and momentum of 0.9. Table TABREF11 summarizes the details of the layer settings for the convolutional and NIN layers.
On the top layers of the encoder after the transferred convolutional and NIN layers, we put three bidirectional LSTMs (Bi-LSTM) with 256 hidden units (total 512 units for both directions). To reduce the computational time, we used hierarchical subsampling BIBREF21, BIBREF22, BIBREF10. We applied subsampling on all the Bi-LSTM layers and reduced the length by a factor of 8.
True answer: In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM 

Validation ID: 0a75a52450ed866df3a304077769e1725a995bb7
Question: How does their decoder generate text?
From the context "The decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4, can be formulated as: DISPLAYFORM0", so we know that the decoder generates text based on previous output and context information.
Answer: previous output and context information
True answer: decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information

Validation ID: 0a75a52450ed866df3a304077769e1725a995bb7
Question: How does their decoder generate text?
From the context "The decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4, can be formulated as: DISPLAYFORM0", so we know that the decoder generates text based on previous output and context information.
Answer: previous output and context information
True answer: Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy

Validation ID: fd0a3e9c210163a55d3ed791e95ae3875184b8f8
Question: Which dataset do they use?
From the context "We investigated the performance of our proposed models on WSJ BIBREF5. We used the same definitions of the training, development and test set as the Kaldi s5 recipe BIBREF18", so we know that they use WSJ dataset.
Answer: WSJ dataset
True answer: WSJ

Validation ID: fd0a3e9c210163a55d3ed791e95ae3875184b8f8
Question: Which dataset do they use?
From the context "We investigated the performance of our proposed models on WSJ BIBREF5. We used the same definitions of the training, development and test set as the Kaldi s5 recipe BIBREF18", so we know that they use WSJ dataset.
Answer: WSJ dataset
True answer: WSJ-SI84, WSJ-SI284

Validation ID: c37f65c9f0d543a35c784263b79236ccf1c44fac
Question: What model is used to encode the images?
From the context "The representation of the images was obtained through Inception V3", so we know that Inception V3 is used to encode the images.
Answer: Inception V3
True answer: a Convolutional Neural Network (CNN)

Validation ID: c37f65c9f0d543a35c784263b79236ccf1c44fac
Question: What model is used to encode the images?
From the context "The representation of the images was obtained through Inception V3", so we know that Inception V3 is used to encode the images.
Answer: Inception V3
True answer: LSTM

Validation ID: 584af673429c7f8621c6bf83362a37048daa0e5d
Question: How is the sequential nature of the story captured?
From the context "Our model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence. For instance, the first decoder will learn the opening sentences of the story while the last decoder the closing sentences.", so we know that the sequential nature of the story is captured by the last hidden state of the encoder.
Answer: the last hidden state of the encoder
True answer: we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story

Validation ID: 584af673429c7f8621c6bf83362a37048daa0e5d
Question: How is the sequential nature of the story captured?
From the context "Our model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence. For instance, the first decoder will learn the opening sentences of the story while the last decoder the closing sentences.", so we know that the sequential nature of the story is captured by the last hidden state of the encoder.
Answer: the last hidden state of the encoder
True answer: The encoder takes the images in order, one at every timestep $t$ . At time $t=5$ , we obtain the context vector through $h_e^{(t)}$ (represented by $\mathbf {Z}$ ). This vector is used to initialize each decoder's hidden state while the first input to each decoder is its corresponding image embedding $e(I_i)$ . Each decoder generates a sequence of words $\lbrace p_1,...,p_{n}\rbrace $ for each image in the sequence. 

Validation ID: 1be54c5b3ea67d837ffba2290a40c1e720d9587f
Question: Is the position in the sequence part of the input?
From the context "Having an independent decoder for each position of the image sequence, allowed our visual storyteller to build more specific language models using the context vector as its first state and the image embedding as its first input", so we know that the position in the sequence is part of the input.
Answer: Yes
True answer: No

Validation ID: 1be54c5b3ea67d837ffba2290a40c1e720d9587f
Question: Is the position in the sequence part of the input?
From the context "Having an independent decoder for each position of the image sequence, allowed our visual storyteller to build more specific language models using the context vector as its first state and the image embedding as its first input", so we know that the position in the sequence is part of the input.
Answer: Yes
True answer: Yes

Validation ID: b08f88d1facefceb87e134ba2c1fa90035018e83
Question: Do the decoder LSTMs all have the same weights?
From the context "Our model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence. For instance, the first decoder will learn the opening sentences of the story while the last decoder the closing sentences.", so we know that the decoder LSTMs all have the same weights.
Answer: Yes
True answer: No

Validation ID: b08f88d1facefceb87e134ba2c1fa90035018e83
Question: Do the decoder LSTMs all have the same weights?
From the context "Our model contains five independent decoders, one for each image in the sequence. All the 5 decoders use the last hidden state of the encoder (i.e. the context) as its first hidden state and take the corresponding image embedding as its first input. In this way, the first decoder generates the sequence of words for the first image in the sequence, the second decoder for the second image in the sequence, and so on. This allows each decoder to learn a specific language model for each position of the sequence. For instance, the first decoder will learn the opening sentences of the story while the last decoder the closing sentences.", so we know that the decoder LSTMs all have the same weights.
Answer: Yes
True answer: No

Validation ID: b06512c17d99f9339ffdab12cedbc63501ff527e
Question: Is fine-tuning required to incorporate these embeddings into existing models?
From the context "Our work also naturally suggests new directions of investigation and research into knowledge graph embeddings. One avenue of research is to investigate the utility of each layer's entity and relation embeddings in specific learning tasks. As was noted by research in computer vision, deep representations learned on one dataset are effective and very useful in transfer-learning tasks BIBREF33. A natural line of investigation thus revolves around precisely quantifying the effectiveness of these learned embeddings across models and data-sets.", so we know that fine-tuning is not required to incorporate these embeddings into existing models.
Answer: No
True answer: No

Validation ID: b06512c17d99f9339ffdab12cedbc63501ff527e
Question: Is fine-tuning required to incorporate these embeddings into existing models?
From the context "Our work also naturally suggests new directions of investigation and research into knowledge graph embeddings. One avenue of research is to investigate the utility of each layer's entity and relation embeddings in specific learning tasks. As was noted by research in computer vision, deep representations learned on one dataset are effective and very useful in transfer-learning tasks BIBREF33. A natural line of investigation thus revolves around precisely quantifying the effectiveness of these learned embeddings across models and data-sets.", so we know that fine-tuning is not required to incorporate these embeddings into existing models.
Answer: No
True answer: No

Validation ID: fd8e23947095fe2230ffe1a478945829b09c8c95
Question: How are meaningful chains in the graph selected?
From the context "We generate a large set of such paths from the knowledge graph G by performing several random walks on it which in turn yields a corpus of “sentences” S needed for “language modeling”", so we know that meaningful chains in the graph are selected by performing several random walks on it.
Answer: performing several random walks on it
True answer: No

Validation ID: fd8e23947095fe2230ffe1a478945829b09c8c95
Question: How are meaningful chains in the graph selected?
From the context "We generate a large set of such paths from the knowledge graph G by performing several random walks on it which in turn yields a corpus of “sentences” S needed for “language modeling”", so we know that meaningful chains in the graph are selected by performing several random walks on it.
Answer: performing several random walks on it
True answer: utilize the machinery of language modeling using deep neural networks to learn Dolores embeddings.

Validation ID: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0
Question: Do the authors also analyze transformer-based architectures?
From the context "We systematically compared the strengths and weaknesses of different network designs and provided insights for future work.", so we know that the authors did not analyze transformer-based architectures.
Answer: No
True answer: No

Validation ID: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0
Question: Do the authors also analyze transformer-based architectures?
From the context "We systematically compared the strengths and weaknesses of different network designs and provided insights for future work.", so we know that the authors did not analyze transformer-based architectures.
Answer: No
True answer: No

Validation ID: 3611a72f754de1e256fbd25b012197e1c24e8470
Question: Do they remove seasonality from the time series?
From the context "In this paper, we have shown that a strong correlation exists between rise/fall in stock prices of a company to the public opinions or emotions about that company expressed on twitter through tweets", so we know that they have shown that a strong correlation exists between rise/fall in stock prices of a company to the public opinions or emotions about that company expressed on twitter through tweets.
Answer: Yes
True answer: Unanswerable

Validation ID: 3611a72f754de1e256fbd25b012197e1c24e8470
Question: Do they remove seasonality from the time series?
From the context "In this paper, we have shown that a strong correlation exists between rise/fall in stock prices of a company to the public opinions or emotions about that company expressed on twitter through tweets", so we know that they have shown that a strong correlation exists between rise/fall in stock prices of a company to the public opinions or emotions about that company expressed on twitter through tweets.
Answer: Yes
True answer: No

Validation ID: 4c07c33dfaf4f3e6db55e377da6fa69825d0ba15
Question: What is the dimension of the embeddings?
From the context "Word2vec representation is far better, advanced and a recent technique which functions by mapping words to a 300 dimensional vector representations", so we know that the dimension of the embeddings is 300.
Answer: 300
True answer: 300

Validation ID: 4c07c33dfaf4f3e6db55e377da6fa69825d0ba15
Question: What is the dimension of the embeddings?
From the context "Word2vec representation is far better, advanced and a recent technique which functions by mapping words to a 300 dimensional vector representations", so we know that the dimension of the embeddings is 300.
Answer: 300
True answer: 300

Validation ID: b1ce129678e37070e69f01332f1a8587e18e06b0
Question: What dataset is used to train the model?
From the context "While training the sentiment analyzer, 3,216 tweets are used which is comparatively a less number to train a sentiment analyzer", so we know that the dataset is 3,216 tweets.
Answer: 3,216 tweets
True answer: 2,50,000 tweets, Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016

Validation ID: b1ce129678e37070e69f01332f1a8587e18e06b0
Question: What dataset is used to train the model?
From the context "While training the sentiment analyzer, 3,216 tweets are used which is comparatively a less number to train a sentiment analyzer", so we know that the dataset is 3,216 tweets.
Answer: 3,216 tweets
True answer: Collected tweets and opening and closing stock prices of Microsoft.

Validation ID: 7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1
Question: What is the previous state of the art?
From the context "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture", so we know that the previous state of the art is RoBERTa.
Answer: RoBERTa
True answer: RoBERTa

Validation ID: 7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1
Question: What is the previous state of the art?
From the context "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture", so we know that the previous state of the art is RoBERTa.
Answer: RoBERTa
True answer: RoBERTa

Validation ID: 0689904db9b00a814e3109fb1698086370a28fa2
Question: Which text embedding methodologies are used?
From the context "Computational methods for text analysis rely on a choice of a mathematical representation of the base units, such as character $n$ -grams, words or documents of any length. An important consideration for our methodology is an attempt to avoid the use of labelled data at the core of many supervised or semi-supervised classification methods BIBREF18, BIBREF19. In this work, we use a representation of text documents in vector form following recent developments in the field.
Classically, bag-of-words (Bow) methods were used to obtain representations of the documents in a corpus in terms of vectors of term frequencies weighted by inverse document frequency (TF-iDF). While such methods provide a statistical description of documents, they do not carry information about the order or proximity of words to each other since they regard word tokens in an independent manner with no semantic or syntactic relationships considered. Furthermore, BoW representations tend to be high-dimensional and sparse, due to large sizes of word dictionaries and low frequencies of many terms.
True answer: Document to Vector (Doc2Vec)

Validation ID: 0689904db9b00a814e3109fb1698086370a28fa2
Question: Which text embedding methodologies are used?
From the context "Computational methods for text analysis rely on a choice of a mathematical representation of the base units, such as character $n$ -grams, words or documents of any length. An important consideration for our methodology is an attempt to avoid the use of labelled data at the core of many supervised or semi-supervised classification methods BIBREF18, BIBREF19. In this work, we use a representation of text documents in vector form following recent developments in the field.
Classically, bag-of-words (Bow) methods were used to obtain representations of the documents in a corpus in terms of vectors of term frequencies weighted by inverse document frequency (TF-iDF). While such methods provide a statistical description of documents, they do not carry information about the order or proximity of words to each other since they regard word tokens in an independent manner with no semantic or syntactic relationships considered. Furthermore, BoW representations tend to be high-dimensional and sparse, due to large sizes of word dictionaries and low frequencies of many terms.
True answer: Doc2Vec, PV-DBOW model

Validation ID: cc354c952b5aaed2d4d1e932175e008ff2d801dd
Question: Which race and gender are given higher sentiment intensity predictions?
From the context "We found that more than 75% of the systems tend to mark sentences involving one gender/race with higher intensity scores than the sentences involving the other gender/race", so we know that the race and gender are given higher sentiment intensity predictions.
Answer: race and gender
True answer: Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.
African American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task.

Validation ID: cc354c952b5aaed2d4d1e932175e008ff2d801dd
Question: Which race and gender are given higher sentiment intensity predictions?
From the context "We found that more than 75% of the systems tend to mark sentences involving one gender/race with higher intensity scores than the sentences involving the other gender/race", so we know that the race and gender are given higher sentiment intensity predictions.
Answer: race and gender
True answer:  the number of systems consistently giving higher scores to sentences with female noun phrases, higher scores to sentences with African American names on the tasks of anger, fear, and sadness,  joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names

Validation ID: 0f12dc077fe8e5b95ca9163cea1dd17195c96929
Question: What criteria are used to select the 8,640 English sentences?
From the context "We decided to use sentences involving at least one race- or gender-associated word. The sentences were intended to be short and grammatically simple. We also wanted some sentences to include expressions of sentiment and emotion, since the goal is to test sentiment and emotion systems. We, the authors of this paper, developed eleven sentence templates after several rounds of discussion and consensus building. They are shown in Table TABREF3. The templates are divided into two groups. The first type (templates 1–7) includes emotion words. The purpose of this set is to have sentences expressing emotions. The second type (templates 8–11) does not include any emotion words. The purpose of this set is to have non-emotional (neutral) sentences.", so we know that the 8,640 English sentences are selected from the eleven sentence templates.
Answer: eleven sentence templates
True answer: Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.

Validation ID: 0f12dc077fe8e5b95ca9163cea1dd17195c96929
Question: What criteria are used to select the 8,640 English sentences?
From the context "We decided to use sentences involving at least one race- or gender-associated word. The sentences were intended to be short and grammatically simple. We also wanted some sentences to include expressions of sentiment and emotion, since the goal is to test sentiment and emotion systems. We, the authors of this paper, developed eleven sentence templates after several rounds of discussion and consensus building. They are shown in Table TABREF3. The templates are divided into two groups. The first type (templates 1–7) includes emotion words. The purpose of this set is to have sentences expressing emotions. The second type (templates 8–11) does not include any emotion words. The purpose of this set is to have non-emotional (neutral) sentences.", so we know that the 8,640 English sentences are selected from the eleven sentence templates.
Answer: eleven sentence templates
True answer: generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates, differ only in one word corresponding to gender or race

Validation ID: 2ddb51b03163d309434ee403fef42d6b9aecc458
Question: what were the baselines?
From the context "We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31", so we know that the baselines are other models trained using the same data.
Answer: other models trained using the same data
True answer: Unanswerable

Validation ID: 2ddb51b03163d309434ee403fef42d6b9aecc458
Question: what were the baselines?
From the context "We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31", so we know that the baselines are other models trained using the same data.
Answer: other models trained using the same data
True answer: LF-MMI Attention
Seq2Seq 
RNN-T 
Char E2E LF-MMI 
Phone E2E LF-MMI 
CTC + Gram-CTC

Validation ID: e587559f5ab6e42f7d981372ee34aebdc92b646e
Question: what competitive results did they obtain?
From the context "We obtain good results for SWB. However, there is work to be done to improve WER on harder tasks such as CHM.", so we know that they obtained good results for SWB.
Answer: SWB
True answer: In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.
In case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. 

Validation ID: e587559f5ab6e42f7d981372ee34aebdc92b646e
Question: what competitive results did they obtain?
From the context "We obtain good results for SWB. However, there is work to be done to improve WER on harder tasks such as CHM.", so we know that they obtained good results for SWB.
Answer: SWB
True answer: On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.
On Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.

Validation ID: bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a
Question: How much is performance improved on NLI?
From the context "Table TABREF21 illustrates the experimental results, showing that our method is beneficial for all of NLI tasks. The improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase. Besides NLI, our model also performs better than BERTBase in the STS task. The STS tasks are semantically similar to the NLI tasks, and hence able to take advantage of PSP as well. Actually, the proposed method has a positive effect whenever the input is a sentence pair. The improvements suggest that the PSP task encourages the model to learn more detailed semantics in the pre-training, which improves the model on the downstream learning tasks. Moreover, our method is surprisingly able to achieve slightly better results in the single-sentence problem. The improvement should be attributed to better semantic representation.", so we know that the performance is improved on NLI.
Answer: improved on NLI
True answer:  improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase

Validation ID: bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a
Question: How much is performance improved on NLI?
From the context "Table TABREF21 illustrates the experimental results, showing that our method is beneficial for all of NLI tasks. The improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase. Besides NLI, our model also performs better than BERTBase in the STS task. The STS tasks are semantically similar to the NLI tasks, and hence able to take advantage of PSP as well. Actually, the proposed method has a positive effect whenever the input is a sentence pair. The improvements suggest that the PSP task encourages the model to learn more detailed semantics in the pre-training, which improves the model on the downstream learning tasks. Moreover, our method is surprisingly able to achieve slightly better results in the single-sentence problem. The improvement should be attributed to better semantic representation.", so we know that the performance is improved on NLI.
Answer: improved on NLI
True answer: The average score improved by 1.4 points over the previous best result.

Validation ID: 7b4fb6da74e6bd1baea556788a02969134cf0800
Question: Do they train their model starting from a checkpoint?
From the context "We explore three method settings for comparison. BERT-PN: The NSP task in BERT is replaced by a 3-class task with IsNext, IsPrev and DiffDoc. The label distribution is 1:1:1. BERT-PN5cls: The NSP task in BERT is replaced by a 5-class task with two additional labels IsNextInadj, IsPrevInadj. The label distribution is 1:1:1:1:1. BERT-PNsmth: It uses the same data with BERT-PN5cls, except that the IsPrevInadj (IsNextInadj) label is mapped to IsPrev (IsNext) with a label smoothing factor of 0.8.", so we know that they train their model starting from a checkpoint.
Answer: Yes
True answer: No

Validation ID: 7b4fb6da74e6bd1baea556788a02969134cf0800
Question: Do they train their model starting from a checkpoint?
From the context "We explore three method settings for comparison. BERT-PN: The NSP task in BERT is replaced by a 3-class task with IsNext, IsPrev and DiffDoc. The label distribution is 1:1:1. BERT-PN5cls: The NSP task in BERT is replaced by a 5-class task with two additional labels IsNextInadj, IsPrevInadj. The label distribution is 1:1:1:1:1. BERT-PNsmth: It uses the same data with BERT-PN5cls, except that the IsPrevInadj (IsNextInadj) label is mapped to IsPrev (IsNext) with a label smoothing factor of 0.8.", so we know that they train their model starting from a checkpoint.
Answer: Yes
True answer: No

Validation ID: bc31a3d2f7c608df8c019a64d64cb0ccc5669210
Question: What BERT model do they test?
From the context "Our method is evaluated on the BERTbase model, which has 12 layers, 12 self-attention heads with a hidden size of 768.", so we know that the BERT model they test is BERTbase.
Answer: BERTbase
True answer: BERTbase

Validation ID: bc31a3d2f7c608df8c019a64d64cb0ccc5669210
Question: What BERT model do they test?
From the context "Our method is evaluated on the BERTbase model, which has 12 layers, 12 self-attention heads with a hidden size of 768.", so we know that the BERT model they test is BERTbase.
Answer: BERTbase
True answer: BERTbase

Validation ID: 761de1610e934189850e8fda707dc5239dd58092
Question: what was the baseline?
From the context "We began with evaluating standard MT paradigms, i.e., PBSMT BIBREF3 and NMT BIBREF1. As for PBSMT, we also examined two advanced methods: pivot-based translation relying on a helping language BIBREF10 and induction of phrase tables from monolingual data BIBREF14.", so we know that the baseline is PBSMT.
Answer: PBSMT
True answer: pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17

Validation ID: 761de1610e934189850e8fda707dc5239dd58092
Question: what was the baseline?
From the context "We began with evaluating standard MT paradigms, i.e., PBSMT BIBREF3 and NMT BIBREF1. As for PBSMT, we also examined two advanced methods: pivot-based translation relying on a helping language BIBREF10 and induction of phrase tables from monolingual data BIBREF14.", so we know that the baseline is PBSMT.
Answer: PBSMT
True answer: M2M Transformer

Validation ID: f8da63df16c4c42093e5778c01a8e7e9b270142e
Question: How is segmentation quality evaluated?
From the context "We have built a second $L_c$ list for the automatically identified segments, following the same criteria of $L_r$. The $L_r$ and $L_c$ lists regroup, pair by pair, the segment border. We then count the common pair intersection of the two lists. Each pair in the $L_c$ list is also present in the $L_r$ reference list and is a correctly assigned to the class pair. A word pair belonging to the $L_c$ list but not belonging to the $L_r$ reference list, will be a pair assigned to the class.", so we know that segmentation quality is evaluated by counting the common pair intersection of the two lists.
Answer: By counting the common pair intersection of the two lists
True answer: Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.

Validation ID: f8da63df16c4c42093e5778c01a8e7e9b270142e
Question: How is segmentation quality evaluated?
From the context "We have built a second $L_c$ list for the automatically identified segments, following the same criteria of $L_r$. The $L_r$ and $L_c$ lists regroup, pair by pair, the segment border. We then count the common pair intersection of the two lists. Each pair in the $L_c$ list is also present in the $L_r$ reference list and is a correctly assigned to the class pair. A word pair belonging to the $L_c$ list but not belonging to the $L_r$ reference list, will be a pair assigned to the class.", so we know that segmentation quality is evaluated by counting the common pair intersection of the two lists.
Answer: By counting the common pair intersection of the two lists
True answer: we compare the Annodis segmentation with the automatically produced segmentation

Validation ID: c09a92e25e6a81369fcc4ae6045491f2690ccc10
Question: How do they compare lexicons?
From the context "The crowd is as capable of evaluating lexicons, as experts. Linguistic element evaluation can be efficiently crowdsourced, and the evaluation of emotional or non emotional elements can be as strict as needed. The number of evaluators plays a key role in both emotional and linguistic evaluations. The crowd is strict", so we know that the crowd is as capable of evaluating lexicons, as experts.
Answer: The crowd is as capable of evaluating lexicons, as experts.
True answer: Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.

Validation ID: c09a92e25e6a81369fcc4ae6045491f2690ccc10
Question: How do they compare lexicons?
From the context "The crowd is as capable of evaluating lexicons, as experts. Linguistic element evaluation can be efficiently crowdsourced, and the evaluation of emotional or non emotional elements can be as strict as needed. The number of evaluators plays a key role in both emotional and linguistic evaluations. The crowd is strict", so we know that the crowd is as capable of evaluating lexicons, as experts.
Answer: The crowd is as capable of evaluating lexicons, as experts.
True answer: 1000 term groups based on the number of total annotations(200 term groups with 2 total annotations, 200 term groups with 3 total annotations, and so on up to term groups with 6 total annotations)

Validation ID: 63c3550c6fb42f41a0c93133e9fca12ac00df9b3
Question: Is it possible to convert a cloze-style questions to a naturally-looking questions?
From the context "We hypothesize that we can benefit from the un-annotated corpora to better answer various questions – at least ones that are lexically similar to the content in base documents and directly require factual information.", so we know that it is possible to convert a cloze-style questions to a naturally-looking questions.
Answer: Yes
True answer: Unanswerable

Validation ID: 63c3550c6fb42f41a0c93133e9fca12ac00df9b3
Question: Is it possible to convert a cloze-style questions to a naturally-looking questions?
From the context "We hypothesize that we can benefit from the un-annotated corpora to better answer various questions – at least ones that are lexically similar to the content in base documents and directly require factual information.", so we know that it is possible to convert a cloze-style questions to a naturally-looking questions.
Answer: Yes
True answer: Unanswerable

Validation ID: 603fee7314fa65261812157ddfc2c544277fcf90
Question: How larger are the training sets of these versions of ELMo compared to the previous ones?
From the context "We trained each ELMo model using default values used to train the original English ELMo (large) model.", so we know that the training sets of these versions of ELMo are larger than the previous ones.
Answer: larger
True answer: By 14 times.

Validation ID: 603fee7314fa65261812157ddfc2c544277fcf90
Question: How larger are the training sets of these versions of ELMo compared to the previous ones?
From the context "We trained each ELMo model using default values used to train the original English ELMo (large) model.", so we know that the training sets of these versions of ELMo are the same as the original English ELMo (large) model.
From the context "The original ELMo model was trained on a one billion word large English corpus, with a given vocabulary file of about 800,000 words.", so we know that the training sets of the original English ELMo (large) model are one billion word large English corpus.
True answer: up to 1.95 times larger

Validation ID: 09a1173e971e0fcdbf2fbecb1b077158ab08f497
Question: What is the improvement in performance for Estonian in the NER task?
From the context "The results are presented in Table TABREF21. We included the evaluation of the original ELMo English model in the same table. NER models have little difficulty distinguishing between types of named entities, but recognizing whether a word is a named entity or not is more difficult. For languages with the smallest NER datasets, Croatian and Lithuanian, ELMo embeddings show the largest improvement over fastText embeddings. However, we can observe significant improvements with ELMo also on English and Finnish, which are among the largest datasets (English being by far the largest). Only on Slovenian dataset did ELMo perform slightly worse than fastText, on all other EMBEDDIA languages, the ELMo embeddings improve the results.", so we know that the improvement in performance for Estonian in the NER task is significant.
Answer: significant
True answer: 5 percent points.

Validation ID: 09a1173e971e0fcdbf2fbecb1b077158ab08f497
Question: What is the improvement in performance for Estonian in the NER task?
From the context "The results are presented in Table TABREF21. We included the evaluation of the original ELMo English model in the same table. NER models have little difficulty distinguishing between types of named entities, but recognizing whether a word is a named entity or not is more difficult. For languages with the smallest NER datasets, Croatian and Lithuanian, ELMo embeddings show the largest improvement over fastText embeddings. However, we can observe significant improvements with ELMo also on English and Finnish, which are among the largest datasets (English being by far the largest). Only on Slovenian dataset did ELMo perform slightly worse than fastText, on all other EMBEDDIA languages, the ELMo embeddings improve the results.", so we know that the improvement in performance for Estonian in the NER task is significant.
Answer: significant
True answer: 0.05 F1

Validation ID: 70e9210fe64f8d71334e5107732d764332a81cb1
Question: what is the state of the art on WSJ?
From the context "Table TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches INLINEFORM0 WER on nov92.", so we know that the state of the art on WSJ is INLINEFORM0 WER.
Answer: INLINEFORM0 WER
True answer: CNN-DNN-BLSTM-HMM

Validation ID: 70e9210fe64f8d71334e5107732d764332a81cb1
Question: what is the state of the art on WSJ?
From the context "Table TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches INLINEFORM0 WER on nov92.", so we know that the state of the art on WSJ is INLINEFORM0 WER.
Answer: INLINEFORM0 WER
True answer: HMM-based system

Validation ID: 051df74dc643498e95d16e58851701628fdfd43e
Question: How did they obtain the OSG dataset?
From the context "Our data has been developed by crawling and pre-processing an OSG web forum. The forum has a great variety of different groups such as depression, anxiety, stress, relationship, cancer, sexually transmitted diseases, etc.", so we know that they obtained the OSG dataset by crawling and pre-processing an OSG web forum.
Answer: crawling and pre-processing an OSG web forum
True answer: crawling and pre-processing an OSG web forum

Validation ID: 051df74dc643498e95d16e58851701628fdfd43e
Question: How did they obtain the OSG dataset?
From the context "Our data has been developed by crawling and pre-processing an OSG web forum. The forum has a great variety of different groups such as depression, anxiety, stress, relationship, cancer, sexually transmitted diseases, etc.", so we know that they obtained the OSG dataset by crawling and pre-processing an OSG web forum.
Answer: crawling and pre-processing an OSG web forum
True answer: data has been developed by crawling and pre-processing an OSG web forum

Validation ID: 33554065284110859a8ea3ca7346474ab2cab100
Question: How large is the Twitter dataset?
From the context "We have downloaded 1,873 Twitter conversation threads, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted.", so we know that the Twitter dataset is 1,873 Twitter conversation threads.
Answer: 1,873 Twitter conversation threads
True answer: 1,873 Twitter conversation threads, roughly 14k tweets

Validation ID: 33554065284110859a8ea3ca7346474ab2cab100
Question: How large is the Twitter dataset?
From the context "We have downloaded 1,873 Twitter conversation threads, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted.", so we know that the Twitter dataset is 1,873 Twitter conversation threads.
Answer: 1,873 Twitter conversation threads
True answer: 1,873 Twitter conversation threads, roughly 14k tweets

Validation ID: 57f23dfc264feb62f45d9a9e24c60bd73d7fe563
Question: what is the size of the augmented dataset?
From the context "We augment DB with healthy samples from FP with varying ages (Tab. SECREF11 ), considering 50 samples for each 15 year duration starting from age 30", so we know that the augmented dataset is 50 samples.
Answer: 50 samples
True answer: Unanswerable

Validation ID: 57f23dfc264feb62f45d9a9e24c60bd73d7fe563
Question: what is the size of the augmented dataset?
From the context "We augment DB with healthy samples from FP with varying ages (Tab. SECREF11 ), considering 50 samples for each 15 year duration starting from age 30", so we know that the augmented dataset is 50 samples.
Answer: 50 samples
True answer: 609

Validation ID: 54830abe73fef4e629a36866ceeeca10214bd2c8
Question: How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?
From the context "In this paper, we presented a taxonomy of recommendation systems and applications based on LDA of the recent research, including app, travel, friend, location, scientific paper, and music recommendation. Furthermore, we applied LDA algorithm and Gibbs sampling on ISWC and WWW conference's publications from 2013", so we know that they utilized LDA and Gibbs sampling to evaluate ISWC and WWW publications.
Answer: LDA and Gibbs sampling
True answer: the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter

Validation ID: 54830abe73fef4e629a36866ceeeca10214bd2c8
Question: How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?
From the context "In this paper, we presented a taxonomy of recommendation systems and applications based on LDA of the recent research, including app, travel, friend, location, scientific paper, and music recommendation. Furthermore, we applied LDA algorithm and Gibbs sampling on ISWC and WWW conference's publications from 2013", so we know that they utilized LDA and Gibbs sampling to evaluate ISWC and WWW publications.
Answer: LDA and Gibbs sampling
True answer: discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\alpha =0.01$, $\beta = 0.01$ and using Gibbs sampling as a parameter estimation

Validation ID: 2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6
Question: What dataset do they use to evaluate their method?
From the context "We use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik.", so we know that they use the dataset collected from WordNet and Wordnik.
Answer: WordNet and Wordnik
True answer: antonym and synonym pairs, collected from WordNet BIBREF9 and Wordnik

Validation ID: 2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6
Question: What dataset do they use to evaluate their method?
From the context "We use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik.", so we know that they use the dataset collected from WordNet and Wordnik.
Answer: WordNet and Wordnik
True answer: English Wikipedia dump from June 2016

Validation ID: ef7212075e80bf35b7889dc8dd52fcbae0d1400a
Question: Why are current ELS's not sufficiently effective?
From the context "Despite its usefulness, linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12. These issues can be summarized into two parts: ambiguity and coarseness.", so we know that current ELS's have issues because of low precision rates and design challenges in training datasets.
Answer: low precision rates and design challenges in training datasets
True answer: Linked entities may be ambiguous or too common

Validation ID: ef7212075e80bf35b7889dc8dd52fcbae0d1400a
Question: Why are current ELS's not sufficiently effective?
From the context "Despite its usefulness, linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12. These issues can be summarized into two parts: ambiguity and coarseness.", so we know that current ELS's have issues because of low precision rates and design challenges in training datasets.
Answer: low precision rates and design challenges in training datasets
True answer: linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . These issues can be summarized into two parts: ambiguity and coarseness., the linked entities may also be too common to be considered an entity.

Validation ID: 567dc9bad8428ea9a2658c88203a0ed0f8da0dc3
Question: What is the best model?
From the context "We can clearly imply from tables TABREF23, TABREF24, and TABREF35 that we need more data to get better results because OurNepali dataset volume is almost ten times bigger compared to ILPRL dataset in terms of entities.", so we know that the best model is BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS.
Answer: BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS
True answer: BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS 

Validation ID: d51dc36fbf6518226b8e45d4c817e07e8f642003
Question: How many sentences does the dataset contain?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline.", so we know that the dataset contains sentences.
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the dataset contains sentences.
True answer: 3606

Validation ID: d51dc36fbf6518226b8e45d4c817e07e8f642003
Question: How many sentences does the dataset contain?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline.", so we know that the dataset contains sentences.
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the dataset contains sentences.
True answer: 6946

Validation ID: d8627ba08b7342e473b8a2b560baa8cdbae3c7fd
Question: Do the authors train a Naive Bayes classifier on their dataset?
From the context "We trained our model with shuffled training set using Adam optimizer with hyper-parameters mentioned in table TABREF30. All our models were trained on single layer of LSTM network. We found out Adam was giving better performance and faster convergence compared to Stochastic Gradient Descent (SGD). We chose those hyper-parameters after many ablation studies. The dropout of 0.5 is applied after LSTM layer.", so we know that the authors train a LSTM model on their dataset.
Answer: No
True answer: No

Validation ID: d8627ba08b7342e473b8a2b560baa8cdbae3c7fd
Question: Do the authors train a Naive Bayes classifier on their dataset?
From the context "We trained our model with shuffled training set using Adam optimizer with hyper-parameters mentioned in table TABREF30. All our models were trained on single layer of LSTM network. We found out Adam was giving better performance and faster convergence compared to Stochastic Gradient Descent (SGD). We chose those hyper-parameters after many ablation studies. The dropout of 0.5 is applied after LSTM layer.", so we know that the authors train a LSTM model on their dataset.
Answer: No
True answer: No

Validation ID: cb77d6a74065cb05318faf57e7ceca05e126a80d
Question: What is the baseline?
From the context "We also found out that stemming post-positions can help a lot in improving model performance because of inflectional characteristics of Nepali language. So when we separate out its inflections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its inflected versions.", so we know that the baseline is the model without stemming post-positions.
Answer: the model without stemming post-positions
True answer: CNN modelBIBREF0, Stanford CRF modelBIBREF21

Validation ID: cb77d6a74065cb05318faf57e7ceca05e126a80d
Question: What is the baseline?
From the context "We also found out that stemming post-positions can help a lot in improving model performance because of inflectional characteristics of Nepali language. So when we separate out its inflections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its inflected versions.", so we know that the baseline is the model without stemming post-positions.
Answer: the model without stemming post-positions
True answer: Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec

Validation ID: 8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d
Question: Which machine learning models do they explore?
From the context "We first run our experiment on BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF using the hyper-parameters mentioned in Table TABREF30. The training and evaluation was done on sentence-level.", so we know that they explore BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF.
Answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF
True answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF

Validation ID: 8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d
Question: Which machine learning models do they explore?
From the context "We first run our experiment on BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF using the hyper-parameters mentioned in Table TABREF30. The training and evaluation was done on sentence-level.", so we know that they explore BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF.
Answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF
True answer: BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21

Validation ID: a1b3e2107302c5a993baafbe177684ae88d6f505
Question: What is the size of the dataset?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the size of the dataset is the number of words in the dataset.
Answer: The number of words in the dataset
True answer: Dataset contains 3606 total sentences and 79087 total entities.

Validation ID: a1b3e2107302c5a993baafbe177684ae88d6f505
Question: What is the size of the dataset?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that the size of the dataset is the number of words in the dataset.
Answer: The number of words in the dataset
True answer: ILPRL contains 548 sentences, OurNepali contains 3606 sentences

Validation ID: bb2de20ee5937da7e3e6230e942bec7b6e8f61ee
Question: What is the source of their dataset?
From the context "The authors of this paper would like to express sincere thanks to Bal Krishna Bal, Kathmandu University Professor for providing us the POS-tagged Nepali NER data.", so we know that the source of their dataset is Bal Krishna Bal, Kathmandu University Professor.
Answer: Bal Krishna Bal, Kathmandu University Professor
True answer: daily newspaper of the year 2015-2016

Validation ID: bb2de20ee5937da7e3e6230e942bec7b6e8f61ee
Question: What is the source of their dataset?
From the context "The authors of this paper would like to express sincere thanks to Bal Krishna Bal, Kathmandu University Professor for providing us the POS-tagged Nepali NER data.", so we know that the source of their dataset is Bal Krishna Bal, Kathmandu University Professor.
Answer: Bal Krishna Bal, Kathmandu University Professor
True answer: daily newspaper of the year 2015-2016

Validation ID: 1170e4ee76fa202cabac9f621e8fbeb4a6c5f094
Question: Do they try to use byte-pair encoding representations?
From the context "We created grapheme clusters using uniseg package which is helpful in unicode text segmentations", so we know that they use grapheme clusters.
From the context "We have used fasttext embeddings in particular because of its sub-word representation ability, which is very useful in highly inflectional language as shown in Table TABREF25", so we know that they use sub-word representation.
True answer: No

Validation ID: 1170e4ee76fa202cabac9f621e8fbeb4a6c5f094
Question: Do they try to use byte-pair encoding representations?
From the context "We created grapheme clusters using uniseg package which is helpful in unicode text segmentations", so we know that they use grapheme clusters.
From the context "We have used fasttext embeddings in particular because of its sub-word representation ability, which is very useful in highly inflectional language as shown in Table TABREF25", so we know that they use sub-word representation.
True answer: No

Validation ID: 1462eb312944926469e7cee067dfc7f1267a2a8c
Question: How many different types of entities exist in the dataset?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that there are three types of entities in the dataset.
Answer: 3
True answer: OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities

Validation ID: 1462eb312944926469e7cee067dfc7f1267a2a8c
Question: How many different types of entities exist in the dataset?
From the context "The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.", so we know that there are three types of entities in the dataset.
Answer: 3
True answer: three

Validation ID: f59f1f5b528a2eec5cfb1e49c87699e0c536cc45
Question: How big is the new Nepali NER dataset?
From the context "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed. Currently, the dataset is in standard CoNLL-2003 IO formatBIBREF25.", so we know that the new Nepali NER dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG).
Answer: The new Nepali NER dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG).
True answer: 3606 sentences

Validation ID: f59f1f5b528a2eec5cfb1e49c87699e0c536cc45
Question: How big is the new Nepali NER dataset?
From the context "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed. Currently, the dataset is in standard CoNLL-2003 IO formatBIBREF25.", so we know that the new Nepali NER dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG).
Answer: The new Nepali NER dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG).
True answer: Dataset contains 3606 total sentences and 79087 total entities.

Validation ID: 9bd080bb2a089410fd7ace82e91711136116af6c
Question: What is the performance improvement of the grapheme-level representation model over the character-level model?
From the context "We also found out that stemming post-positions can help a lot in improving model performance because of inflectional characteristics of Nepali language. So when we separate out its inflections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its inflected versions.", so we know that the performance improvement of the grapheme-level representation model over the character-level model is because of the inflectional characteristics of Nepali language.
Answer: inflectional characteristics of Nepali language
True answer: On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement

Validation ID: 9bd080bb2a089410fd7ace82e91711136116af6c
Question: What is the performance improvement of the grapheme-level representation model over the character-level model?
From the context "We also found out that stemming post-positions can help a lot in improving model performance because of inflectional characteristics of Nepali language. So when we separate out its inflections or morphemes, we can minimize the variations of same word which gives its root word a stronger word vector representations compared to its inflected versions.", so we know that the performance improvement of the grapheme-level representation model over the character-level model is because of the inflectional characteristics of Nepali language.
Answer: inflectional characteristics of Nepali language
True answer: BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration

Validation ID: 6d1217b3d9cfb04be7fcd2238666fa02855ce9c5
Question: Which models are used to solve NER for Nepali?
From the context "We trained our model with shuffled training set using Adam optimizer with hyper-parameters mentioned in table TABREF30. All our models were trained on single layer of LSTM network. We found out Adam was giving better performance and faster convergence compared to Stochastic Gradient Descent (SGD). We chose those hyper-parameters after many ablation studies. The dropout of 0.5 is applied after LSTM layer.", so we know that the models used to solve NER for Nepali are BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF.
Answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF
True answer: BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21

Validation ID: 6d1217b3d9cfb04be7fcd2238666fa02855ce9c5
Question: Which models are used to solve NER for Nepali?
From the context "We trained our model with shuffled training set using Adam optimizer with hyper-parameters mentioned in table TABREF30. All our models were trained on single layer of LSTM network. We found out Adam was giving better performance and faster convergence compared to Stochastic Gradient Descent (SGD). We chose those hyper-parameters after many ablation studies. The dropout of 0.5 is applied after LSTM layer.", so we know that the models used to solve NER for Nepali are BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF.
Answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF
True answer: BiLSTM, BiLSTM+CNN, BiLSTM+CRF, BiLSTM+CNN+CRF, CNN, Stanford CRF

Validation ID: 1e775cf30784e6b1c2b573294a82e145a3f959bb
Question: What language(s) is/are represented in the dataset?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.", so we know that the languages represented in the dataset are harassment and non-harassment tweets.
Answer: harassment and non-harassment tweets
True answer: english

Validation ID: 1e775cf30784e6b1c2b573294a82e145a3f959bb
Question: What language(s) is/are represented in the dataset?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.", so we know that the languages represented in the dataset are harassment and non-harassment tweets.
Answer: harassment and non-harassment tweets
True answer: english

Validation ID: 392fb87564c4f45d0d8d491a9bb217c4fce87f03
Question: What baseline model is used?
From the context "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", so we know that the baseline model is the one with the Projected Layer.
Answer: the one with the Projected Layer
True answer:  LastStateRNN, AvgRNN, AttentionRNN

Validation ID: 392fb87564c4f45d0d8d491a9bb217c4fce87f03
Question: What baseline model is used?
From the context "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", so we know that the baseline model is the one with the Projected Layer.
Answer: the one with the Projected Layer
True answer: LastStateRNN, AvgRNN, AttentionRNN 

Validation ID: 203337c15bd1ee05763c748391d295a1f6415b9b
Question: Which variation provides the best results on this dataset?
From the context "According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance", so we know that the multi-attention method having a projected layer provides the best results on this dataset.
Answer: multi-attention method having a projected layer
True answer: the model with multi-attention mechanism and a projected layer

Validation ID: 203337c15bd1ee05763c748391d295a1f6415b9b
Question: Which variation provides the best results on this dataset?
From the context "According to the results of our experiments and considering the F1 Score, the multi-attention method having a projected layer, achieved the highest performance", so we know that the multi-attention method having a projected layer provides the best results on this dataset.
Answer: multi-attention method having a projected layer
True answer: Projected Layer

Validation ID: d004ca2e999940ac5c1576046e30efa3059832fa
Question: What are the different variations of the attention-based approach which are examined?
From the context "Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models", so we know that the different variations of the attention-based approach which are examined are multi-attention and single attention models.
Answer: multi-attention and single attention models
True answer: classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer

Validation ID: d004ca2e999940ac5c1576046e30efa3059832fa
Question: What are the different variations of the attention-based approach which are examined?
From the context "Moreover, we present a comparison between different variations of this attention-based approach like multi-attention and single attention models", so we know that the different variations of the attention-based approach which are examined are multi-attention and single attention models.
Answer: multi-attention and single attention models
True answer:  four attention mechanisms instead of one, a projection layer for the word embeddings

Validation ID: 21548433abd21346659505296fb0576e78287a74
Question: What dataset is used for this work?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media"", so we know that the dataset is "First workshop on categorizing different types of online harassment languages in social media".
Answer: First workshop on categorizing different types of online harassment languages in social media
True answer: Twitter dataset provided by the organizers

Validation ID: 21548433abd21346659505296fb0576e78287a74
Question: What dataset is used for this work?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media"", so we know that the dataset is "First workshop on categorizing different types of online harassment languages in social media".
Answer: First workshop on categorizing different types of online harassment languages in social media
True answer: The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.

Validation ID: f0b2289cb887740f9255909018f400f028b1ef26
Question: What types of online harassment are studied?
From the context "In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks. The first one is the classification of the tweets in harassment and non-harassment categories, while the second one is the classification in specific harassment categories like indirect harassment, physical and sexual harassment as well.", so we know that the types of online harassment studied are indirect harassment, physical and sexual harassment.
Answer: indirect harassment, physical and sexual harassment
True answer: indirect harassment, sexual and physical harassment

Validation ID: f0b2289cb887740f9255909018f400f028b1ef26
Question: What types of online harassment are studied?
From the context "In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. The topic of the competition is the classification of different types of harassment and it is divided in two tasks. The first one is the classification of the tweets in harassment and non-harassment categories, while the second one is the classification in specific harassment categories like indirect harassment, physical and sexual harassment as well.", so we know that the types of online harassment studied are indirect harassment, physical and sexual harassment.
Answer: indirect harassment, physical and sexual harassment
True answer: indirect, physical, sexual

Validation ID: 51b1142c1d23420dbf6d49446730b0e82b32137c
Question: What was the baseline?
From the context "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.", so we know that the baseline is the F1 Score.
Answer: F1 Score
True answer: LastStateRNN, AvgRNN, AttentionRNN

Validation ID: 58355e2a782bf145b61ee2a3e0e426119985c179
Question: What were the datasets used in this paper?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.", so we know that the datasets used in this paper are train set, validation set and test set.
Answer: train set, validation set and test set
True answer: The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. 

Validation ID: 58355e2a782bf145b61ee2a3e0e426119985c179
Question: What were the datasets used in this paper?
From the context "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the "First workshop on categorizing different types of online harassment languages in social media". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment.", so we know that the datasets used in this paper are train set, validation set and test set.
Answer: train set, validation set and test set
True answer: Twitter dataset provided by organizers containing harassment and non-harassment tweets

Validation ID: 25c1c4a91f5dedd4e06d14121af3b5921db125e9
Question: Is car-speak language collection of abstract features that classifier is later trained on?
From the context "We aim to curate a data set of car-speak in order to train a model properly. However, there are a few challenges that present themselves: What is a good source of car-speak? How can we acquire the data? How can we be sure the data set is relevant?", so we know that car-speak is language collection of abstract features that classifier is later trained on.
Answer: Yes
True answer: No

Validation ID: 25c1c4a91f5dedd4e06d14121af3b5921db125e9
Question: Is car-speak language collection of abstract features that classifier is later trained on?
From the context "We aim to curate a data set of car-speak in order to train a model properly. However, there are a few challenges that present themselves: What is a good source of car-speak? How can we acquire the data? How can we be sure the data set is relevant?", so we know that car-speak is language collection of abstract features that classifier is later trained on.
Answer: Yes
True answer: No

Validation ID: f88036174b4a0dbf4fe70ddad884d16082c5748d
Question: Is order of "words" important in car speak language?
From the context "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$.", so we know that the order of "words" is not important in car speak language.
Answer: No
True answer: Unanswerable

Validation ID: f88036174b4a0dbf4fe70ddad884d16082c5748d
Question: Is order of "words" important in car speak language?
From the context "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$.", so we know that the order of "words" is not important in car speak language.
Answer: No
True answer: No

Validation ID: a267d620af319b48e56c191aa4c433ea3870f6fb
Question: What are labels in car speak language dataset?
From the context "We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12.", so we know that the labels in car speak language dataset are car models.
Answer: car models
True answer: car 

Validation ID: a267d620af319b48e56c191aa4c433ea3870f6fb
Question: What are labels in car speak language dataset?
From the context "We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12.", so we know that the labels in car speak language dataset are the car models.
Answer: the car models
True answer: the car

Validation ID: 899ed05c460bf2aa0aa65101cad1986d4f622652
Question: How big is dataset of car-speak language?
From the context "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$.", so we know that the dataset of car-speak language is $10,867$.
Answer: $10,867$
True answer: $3,209$ reviews 

Validation ID: 899ed05c460bf2aa0aa65101cad1986d4f622652
Question: How big is dataset of car-speak language?
From the context "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$.", so we know that the dataset of car-speak language is $10,867$.
Answer: $10,867$
True answer: $3,209$ reviews about 553 different cars from 49 different car manufacturers

Validation ID: d53299fac8c94bd0179968eb868506124af407d1
Question: What is the performance of classifiers?
From the context "Finally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.", so we know that the performance of classifiers is not good.
Answer: not good
True answer: Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject

Validation ID: d53299fac8c94bd0179968eb868506124af407d1
Question: What is the performance of classifiers?
From the context "Finally, we create and test several classifiers that are trained on the data we gathered. While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject.", so we know that the performance of classifiers is not good.
Answer: not good
True answer: Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.

Validation ID: 29f2954098f055fb19d9502572f085862d75bf61
Question: What classifiers have been trained?
From the context "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", so we know that the classifiers have been trained are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP).
Answer: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)
True answer: KNN
RF
SVM
MLP

Validation ID: 29f2954098f055fb19d9502572f085862d75bf61
Question: What classifiers have been trained?
From the context "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", so we know that the classifiers have been trained are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP).
Answer: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)
True answer:  K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)

Validation ID: 6bf93968110c6e3e3640360440607744007a5228
Question: How does car speak pertains to a car's physical attributes?
From the context "Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term “fast” pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term “fast” refers to.", so we know that car-speak pertains to a car's physical attributes.
Answer: a car's physical attributes
True answer: we do not know exactly

Validation ID: 37a79be0148e1751ffb2daabe4c8ec6680036106
Question: What topic is covered in the Chinese Facebook data??
From the context "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs", so we know that the topic is anti-nuclear-power.
Answer: anti-nuclear-power
True answer: anti-nuclear-power

Validation ID: 37a79be0148e1751ffb2daabe4c8ec6680036106
Question: What topic is covered in the Chinese Facebook data??
From the context "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs", so we know that the topic is anti-nuclear-power.
Answer: anti-nuclear-power
True answer: anti-nuclear-power

Validation ID: 518dae6f936882152c162058895db4eca815e649
Question: How many layers does the UTCNN model have?
From the context "Figure FIGREF10 illustrates the UTCNN model. As more than one user may interact with a given post, we first add a maximum pooling layer after the user matrix embedding layer and user vector embedding layer to form a moderator matrix embedding INLINEFORM0 and a moderator vector embedding INLINEFORM1 for moderator INLINEFORM2 respectively, where INLINEFORM3 is used for the semantic transformation in the document composition process, as mentioned in the previous section. The term moderator here is to denote the pseudo user who provides the overall semantic/sentiment of all the engaged users for one document. The embedding INLINEFORM4 models the moderator stance preference, that is, the pattern of the revealed user stance: whether a user is willing to show his preference, whether a user likes to show impartiality with neutral statements and reasonable arguments, or just wants to show strong support for one stance. Ideally, the latent user stance is modeled by INLINEFORM5 for each user. Likewise, for topic information, a maximum pooling layer is added after the topic matrix embedding layer and topic vector embedding layer to form a joint topic matrix embedding INLINEFORM6 and a joint topic vector embedding INLINEFORM7 for topic INLINEFORM8 respectively, where INLINEFORM9 models the semantic transformation of topic INLINEFORM10 as in users and INLINEFORM11 models the topic stance tendency. The latent topic stance is also modeled by INLINEFORM12 for each topic.
As for comments, we view them as short documents with authors only but without likers nor their own comments. Therefore we apply document composition on comments although here users are commenters (users who comment). It is noticed that the word embeddings INLINEFORM0 for the same word in the posts and comments are the same, but after being transformed to INLINEFORM1 in the document composition process shown in Figure FIGREF4, they might become different because of their different engaged users. The output comment representation together with the commenter vector embedding INLINEFORM2 and topic vector embedding INLINEFORM3 are concatenated and a maximum pooling layer is added to select the most important feature for comments. Instead of requiring that the comment stance agree with the post, UTCNN simply extracts the most important features of the comment contents; they could be helpful, whether they show obvious agreement or disagreement. Therefore when combining comment information here, the maximum pooling layer
True answer: eight layers

Validation ID: e44a6bf67ce3fde0c6608b150030e44d87eb25e3
Question: What topics are included in the debate data?
From the context "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)", so we know that the topics are abortion, gay rights, Obama, and marijuana.
Answer: abortion, gay rights, Obama, and marijuana
True answer: abortion, gay rights, Obama, marijuana

Validation ID: e44a6bf67ce3fde0c6608b150030e44d87eb25e3
Question: What topics are included in the debate data?
From the context "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)", so we know that the topics included in the debate data are abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR).
Answer: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)
True answer: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)

Validation ID: 6a31db1aca57a818f36bba9002561724655372a7
Question: What is the size of the Chinese data?
From the context "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.", so we know that the size of the Chinese data is 505,412.
Answer: 505,412
True answer: 32,595 posts

Validation ID: 6a31db1aca57a818f36bba9002561724655372a7
Question: What is the size of the Chinese data?
From the context "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. There are a total of 2,496 authors, 505,137 likers, 33,686 commenters, and 505,412 unique users.", so we know that the size of the Chinese data is 505,412.
Answer: 505,412
True answer: 32,595

Validation ID: e330e162ec29722f5ec9f83853d129c9e0693d65
Question: Did they collected the two datasets?
From the context "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset.", so we know that they collected the two datasets.
Answer: Yes
True answer: No

Validation ID: e330e162ec29722f5ec9f83853d129c9e0693d65
Question: Did they collected the two datasets?
From the context "We tested the proposed UTCNN on two different datasets: FBFans and CreateDebate. FBFans is a privately-owned, single-topic, Chinese, unbalanced, social media dataset, and CreateDebate is a public, multiple-topic, English, balanced, forum dataset.", so we know that they collected the two datasets.
Answer: Yes
True answer: No

Validation ID: d3093062aebff475b4deab90815004051e802aa6
Question: What are the baselines?
From the context "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings INLINEFORM1 and INLINEFORM2 for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset; 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information. All these models were trained on the training set, and parameters as well as the SVM kernel selections (linear or RBF) were fine-tuned on the development set. Also, we adopt oversampling on SVMs, CNN and RCNN because the FBFans dataset is highly imbalanced.", so we know that the baselines are: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) CNN; 5) RCNN; 6) SVM with comment information; 7) UTCNN without user information; 8) UTCNN without the LDA model; 9) UTCNN without comments.
Answer: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) CNN; 5) RCNN; 6) SVM with comment information; 7) UTCNN without user information; 8) UTCNN without the LDA model; 9) UTCNN without comments.
True answer: SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information

Validation ID: d3093062aebff475b4deab90815004051e802aa6
Question: What are the baselines?
From the context "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0, where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings INLINEFORM1 and INLINEFORM2 for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset; 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information.", so we know that the baselines are: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN); 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information; 7) UTCNN without the LDA model; 8) UTCNN without comments.
Answer: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN); 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information; 7) UTCNN without the LDA model; 8) UTCNN without comments.
True answer: SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
From the context "We evaluate SBERT on the Argument Facet Similarity (AFS) corpus by MisraEW16. The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: gun control, gay marriage, and death penalty. The data was annotated on a scale from 0 (“different topic") to 5 (“completely equivalent"). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually descriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset BIBREF24.", so we know that the transfer learning tasks are sentiment prediction, question-type classification, and argumentative excerpts from dialogs.
Answer: sentiment prediction, question-type classification, and argumentative excerpts from dialogs
True answer: MR, CR, SUBJ, MPQA, SST, TREC, MRPC

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
From the context "We evaluate SBERT on the Argument Facet Similarity (AFS) corpus by MisraEW16. The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: gun control, gay marriage, and death penalty. The data was annotated on a scale from 0 (“different topic") to 5 (“completely equivalent"). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually descriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset BIBREF24.", so we know that the transfer learning tasks are sentiment prediction, question-type classification, and argumentative excerpts from dialogs.
Answer: sentiment prediction, question-type classification, and argumentative excerpts from dialogs
True answer: MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.

CR: Sentiment prediction of customer product reviews BIBREF26.

SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.

MPQA: Phrase level opinion polarity classification from newswire BIBREF28.

SST: Stanford Sentiment Treebank with binary labels BIBREF29.

TREC: Fine grained question-type classification from TREC BIBREF30.

MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
From the context "We evaluate SBERT on the Argument Facet Similarity (AFS) corpus by MisraEW16. The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: gun control, gay marriage, and death penalty. The data was annotated on a scale from 0 (“different topic") to 5 (“completely equivalent"). The similarity notion in the AFS corpus is fairly different to the similarity notion in the STS datasets from SemEval. STS data is usually descriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much larger. Hence, simple unsupervised methods as well as state-of-the-art STS systems perform badly on this dataset BIBREF24.", so we know that the transfer learning tasks are sentiment prediction, question-type classification, and argumentative excerpts from dialogs.
Answer: sentiment prediction, question-type classification, and argumentative excerpts from dialogs
True answer: Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification.

Validation ID: a29c071065d26e5ee3c3bcd877e7f215c59d1d33
Question: What metrics are used for the STS tasks?
From the context "We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels.", so we know that the metrics used for the STS tasks is Spearman's rank correlation.
Answer: Spearman's rank correlation
True answer:  Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels

Validation ID: a29c071065d26e5ee3c3bcd877e7f215c59d1d33
Question: What metrics are used for the STS tasks?
From the context "We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels.", so we know that the metrics used for the STS tasks is Spearman's rank correlation.
Answer: Spearman's rank correlation
True answer: Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels

Validation ID: 7f207549c75f5c4388efc15ed28822672b845663
Question: How much time takes its training?
From the context "We train SBERT on the combination of the SNLI BIBREF13 and the Multi-Genre NLI BIBREF14 dataset. The SNLI is a collection of 570,000 sentence pairs annotated with the labels contradiction, eintailment, and neutral. MultiNLI contains 430,000 sentence pairs and covers a range of genres of spoken and written text. We fine-tune SBERT with a 3-way softmax-classifier objective function for one epoch. We used a batch-size of 16, Adam optimizer with learning rate $2\mathrm {e}{-5}$, and a linear learning rate warm-up over 10% of the training data. Our default pooling strategy is MEAN.", so we know that SBERT is trained for one epoch.
Answer: one epoch
True answer: 20 minutes

Validation ID: 596aede2b311deb8cb0a82d2e7de314ef6e83e4e
Question: How many GPUs are used for the training of SBERT?
From the context "We use the training set to fine-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are trained with 10 random seeds to counter variances BIBREF23.", so we know that SBERT is trained with 10 random seeds.
From the context "Performances were measured on a server with Intel i7-5820K CPU @ 3.30GHz, Nvidia Tesla V100 GPU, CUDA 9.2 and cuDNN.", so we know that SBERT is trained on a Nvidia Tesla V100 GPU.
True answer: Unanswerable

Validation ID: 2e89ebd2e4008c67bb2413699589ee55f59c4f36
Question: How are the siamese networks trained?
From the context "We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent BIBREF4 and Universal Sentence Encoder BIBREF5. On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval BIBREF6, an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.", so we know that the siamese networks are trained on NLI data.
Answer: NLI data
True answer: Unanswerable

Validation ID: 2e89ebd2e4008c67bb2413699589ee55f59c4f36
Question: How are the siamese networks trained?
From the context "We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent BIBREF4 and Universal Sentence Encoder BIBREF5. On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval BIBREF6, an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.", so we know that the siamese networks are trained on NLI data.
Answer: NLI data
True answer: update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity., Classification Objective Function, Regression Objective Function, Triplet Objective Function

Validation ID: e2db361ae9ad9dbaa9a85736c5593eb3a471983d
Question: What other sentence embeddings methods are evaluated?
From the context "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:", so we know that the other sentence embeddings methods are:

True answer: GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent

Validation ID: e2db361ae9ad9dbaa9a85736c5593eb3a471983d
Question: What other sentence embeddings methods are evaluated?
From the context "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:", so we know that the other sentence embeddings methods are:

True answer: Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.

Validation ID: 252a645af9876241fb166e5822992ce17fec6eb6
Question: What is the average length of the title text?
From the context "We cast the problem of popularity prediction as a binary classification task. We assume our data points contain a string of characters representing article title and a popularity metric, such as number of comments or views.", so we know that the average length of the title text is a string of characters.
Answer: a string of characters
True answer: Unanswerable

Validation ID: ed67359889cf61fa11ee291d6c378cccf83d599d
Question: Which pretrained word vectors did they use?
From the context "As a text embedding in our experiments, we use publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC). Since their output dimensionality can be modified, we show the results for varying dimensionality sizes. On top of that, we evaluate two training approaches: using static word vectors and fine-tuning them during training phase.", so we know that they used GloVe word vectors pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC).
Answer: GloVe word vectors pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)
True answer:  pre-trained GloVe word vectors 

Validation ID: ed67359889cf61fa11ee291d6c378cccf83d599d
Question: Which pretrained word vectors did they use?
From the context "As a text embedding in our experiments, we use publicly available GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC). Since their output dimensionality can be modified, we show the results for varying dimensionality sizes. On top of that, we evaluate two training approaches: using static word vectors and fine-tuning them during training phase.", so we know that they used GloVe word vectors pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC).
Answer: GloVe word vectors pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)
True answer: GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)

Validation ID: 425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82
Question: What evaluation metrics are used?
From the context "We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples.", so we know that the evaluation metrics are accuracy.
Answer: accuracy
True answer: standard accuracy metric

Validation ID: 425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82
Question: What evaluation metrics are used?
From the context "We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples.", so we know that the evaluation metrics are accuracy.
Answer: accuracy
True answer: accuracy

Validation ID: 955de9f7412ba98a0c91998919fa048d339b1d48
Question: Which shallow approaches did they experiment with?
From the context "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21, combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM.", so we know that the shallow approaches they experimented with are Bag-of-Words and Support Vector Machine with linear kernel.
Answer: Bag-of-Words and Support Vector Machine with linear kernel
True answer: SVM

Validation ID: 955de9f7412ba98a0c91998919fa048d339b1d48
Question: Which shallow approaches did they experiment with?
From the context "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21, combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM.", so we know that they experiment with Bag-of-Words and Support Vector Machine with linear kernel.
Answer: Bag-of-Words and Support Vector Machine with linear kernel
True answer: SVM with linear kernel using bag-of-words features

Validation ID: 3b371ea554fa6639c76a364060258454e4b931d4
Question: Where do they obtain the news videos from?
From the context "contains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016", so we know that they obtain the news videos from NowThisNews Facebook page.
Answer: NowThisNews Facebook page
True answer: NowThisNews Facebook page

Validation ID: 3b371ea554fa6639c76a364060258454e4b931d4
Question: Where do they obtain the news videos from?
From the context "contains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016", so we know that they obtain the news videos from NowThisNews Facebook page.
Answer: NowThisNews Facebook page
True answer: NowThisNews Facebook page

Validation ID: ddb23a71113cbc092cbc158066d891cae261e2c6
Question: What is the source of the news articles?
From the context "The BreakingNews dataset BIBREF4, publicly available dataset of news articles. BIBREF4 contains a variety of news-related information such as images, captions, geo-location information and comments which could be used as a proxy for article popularity. The articles in this dataset were collected between January and December 2014.", so we know that the source of the news articles is BIBREF4.
Answer: BIBREF4
True answer: main news channels, such as Yahoo News, The Guardian or The Washington Post

Validation ID: ddb23a71113cbc092cbc158066d891cae261e2c6
Question: What is the source of the news articles?
From the context "The BreakingNews dataset BIBREF4, publicly available dataset of news articles. BIBREF4 contains a variety of news-related information such as images, captions, geo-location information and comments which could be used as a proxy for article popularity. The articles in this dataset were collected between January and December 2014.", so we know that the source of the news articles is BIBREF4.
Answer: BIBREF4
True answer: The BreakingNews dataset

Validation ID: e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38
Question: which non-english language had the best performance?
From the context "Experimental results showed that the proposed multilingual approach outperforms both the majority baseline and the lexicon-based baseline.", so we know that the proposed multilingual approach outperforms both the majority baseline and the lexicon-based baseline.
From the context "In this paper we made the sentiment analysis model specific to a single domain. For future work, we would like to investigate the effectiveness of our model on different review domains including hotel reviews and on different problems such as detecting stance.", so we know that the sentiment analysis model is specific to a single domain.
True answer: Russian

Validation ID: e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38
Question: which non-english language had the best performance?
From the context "Experimental results showed that the proposed multilingual approach outperforms both the majority baseline and the lexicon-based baseline.", so we know that the proposed multilingual approach outperforms both the majority baseline and the lexicon-based baseline.
From the context "In this paper we made the sentiment analysis model specific to a single domain. For future work, we would like to investigate the effectiveness of our model on different review domains including hotel reviews and on different problems such as detecting stance.", so we know that the sentiment analysis model is specific to a single domain.
True answer: Russsian

Validation ID: c7486d039304ca9d50d0571236429f4f6fbcfcf7
Question: which non-english language was the had the worst results?
From the context "In this paper we made the sentiment analysis model specific to a single domain. For future work, we would like to investigate the effectiveness of our model on different review domains including hotel reviews and on different problems such as detecting stance.", so we know that the sentiment analysis model was specific to a single domain.
From the context "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages.", so we know that the RNN model was robust to handle multiple languages.
True answer: Turkish

Validation ID: f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9
Question: what datasets were used in evaluation?
From the context "For evaluation of the multilingual approach, we use four languages. These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27, BIBREF28. Table TABREF7 shows the number of observations in each test corpus.", so we know that the datasets used in evaluation are part of SemEval-2016 Challenge Task 5.
Answer: SemEval-2016 Challenge Task 5
True answer: SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28

Validation ID: f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9
Question: what datasets were used in evaluation?
From the context "For evaluation of the multilingual approach, we use four languages. These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27, BIBREF28. Table TABREF7 shows the number of observations in each test corpus.", so we know that the datasets used in evaluation are part of SemEval-2016 Challenge Task 5.
Answer: SemEval-2016 Challenge Task 5
True answer:  English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)

Validation ID: a103636c8d1dbfa53341133aeb751ffec269415c
Question: what are the baselines?
From the context "In addition to the majority baseline, we also compare our results with a lexicon-based approach. We use SentiWordNet BIBREF29 to obtain a positive and a negative sentiment score for each token in a review. Then sum of positive sentiment scores and negative sentiment scores for each review is obtained by summing up the scores for each token. If the positive sum score for a given review is greater than the negative sum score, we accept that review as a positive review. If negative sum is larger than or equal to the positive sum, the review is labeled as a negative review.", so we know that the baselines are majority baseline and lexicon-based baseline.
Answer: majority baseline and lexicon-based baseline
True answer: majority baseline, lexicon-based approach

Validation ID: a103636c8d1dbfa53341133aeb751ffec269415c
Question: what are the baselines?
From the context "In addition to the majority baseline, we also compare our results with a lexicon-based approach. We use SentiWordNet BIBREF29 to obtain a positive and a negative sentiment score for each token in a review. Then sum of positive sentiment scores and negative sentiment scores for each review is obtained by summing up the scores for each token. If the positive sum score for a given review is greater than the negative sum score, we accept that review as a positive review. If negative sum is larger than or equal to the positive sum, the review is labeled as a negative review.", so we know that the baselines are majority baseline and lexicon-based baseline.
Answer: majority baseline and lexicon-based baseline
True answer: majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset, lexicon-based approach

Validation ID: 55139fcfe04ce90aad407e2e5a0067a45f31e07e
Question: how did the authors translate the reviews to other languages?
From the context "To be able to employ the trained model, test sets are first translated to English via machine translation and then inference takes place.", so we know that the authors translated the reviews to other languages via machine translation.
Answer: machine translation
True answer: Using Google translation API.

Validation ID: 55139fcfe04ce90aad407e2e5a0067a45f31e07e
Question: how did the authors translate the reviews to other languages?
From the context "To be able to employ the trained model, test sets are first translated to English via machine translation and then inference takes place.", so we know that the authors translated the reviews to other languages via machine translation.
Answer: machine translation
True answer: Google translation API

Validation ID: fbaf060004f196a286fef67593d2d76826f0304e
Question: what dataset was used for training?
From the context "In this paper we made the sentiment analysis model specific to a single domain. For future work, we would like to investigate the effectiveness of our model on different review domains including hotel reviews and on different problems such as detecting stance.", so we know that the dataset used for training is hotel reviews.
Answer: hotel reviews
True answer: Amazon reviews, Yelp restaurant reviews, restaurant reviews

Validation ID: fbaf060004f196a286fef67593d2d76826f0304e
Question: what dataset was used for training?
From the context "In this paper we made the sentiment analysis model specific to a single domain. For future work, we would like to investigate the effectiveness of our model on different review domains including hotel reviews and on different problems such as detecting stance.", so we know that the dataset used for training is hotel reviews.
Answer: hotel reviews
True answer: Amazon reviews BIBREF23 , BIBREF24, Yelp restaurant reviews dataset,  restaurant reviews dataset as part of a Kaggle competition BIBREF26

Validation ID: 7ae38f51243cb80b16a1df14872b72a1f8a2048f
Question: How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech?
From the context "In order to further reduce the dimensionality of the spatio-temporal encodings and cancel background noise effects BIBREF21, we train an unsupervised deep autoenoder (DAE) on the fused heterogeneous features produced by the combined CNN and LSTM information. The DAE forms our second level of hierarchy, with 3 encoding and 3 decoding layers, and mean squared error (MSE) as the cost function.", so we know that the DAE is used to reduce the dimensionality of the spatio-temporal encodings and cancel background noise effects.
From the context "As we found the individually-trained parallel networks (CNN and LSTM) to be useful (see Table TABREF12 ), we suspected the combination of these two networks could provide a more powerful discriminative spatial and temporal representation of the data than each independent network. As such, we concatenate the last fully-connected layer from the CNN with its counterpart in the LSTM to compose a single feature vector based on these two penultimate layers. Ultimately, this forms a joint spatio-temporal encoding of the cross-covariance matrix.", so we know that the combination of CNN and LSTM can provide a more powerful discriminative spatial and temporal representation of the data than each independent network.
True answer: we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 .

Validation ID: deb89bca0925657e0f91ab5daca78b9e548de2bd
Question: What are the five different binary classification tasks?
From the context "We took BIBREF17, BIBREF18 as the baseline works investigating the same problem and compared our performance with theirs. Our proposed method highly outperforms the existing methods across all the five binary classification tasks by a large average margin of 22.51%.", so we know that the five different binary classification tasks are:
1. presence/absence of consonants
True answer:  presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.

Validation ID: deb89bca0925657e0f91ab5daca78b9e548de2bd
Question: What are the five different binary classification tasks?
From the context "We took BIBREF17, BIBREF18 as the baseline works investigating the same problem and compared our performance with theirs. Our proposed method highly outperforms the existing methods across all the five binary classification tasks by a large average margin of 22.51%.", so we know that the five different binary classification tasks are:
1. presence/absence of consonants
True answer: presence/absence of consonants, presence/absence of phonemic nasal, presence/absence of bilabial, presence/absence of high-front vowels, and presence/absence of high-back vowels

Validation ID: 9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570
Question: How was the spatial aspect of the EEG signal computed?
From the context "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19, in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers", so we know that the spatial aspect of the EEG signal was computed by a CNN.
Answer: CNN
True answer: we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.

Validation ID: 9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570
Question: How was the spatial aspect of the EEG signal computed?
From the context "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19, in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers", so we know that the spatial aspect of the EEG signal was computed by a CNN.
Answer: CNN
True answer: They use four-layered 2D CNN and two fully connected hidden layers on the channel covariance matrix to compute the spatial aspect.

Validation ID: e6583c60b13b87fc37af75ffc975e7e316d4f4e0
Question: What data was presented to the subjects to elicit event-related responses?
From the context "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", so we know that the data presented to the subjects to elicit event-related responses is composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw).
Answer: composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)
True answer: 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)

Validation ID: e6583c60b13b87fc37af75ffc975e7e316d4f4e0
Question: What data was presented to the subjects to elicit event-related responses?
From the context "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", so we know that the data presented to the subjects to elicit event-related responses is composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw).
Answer: composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)
True answer: KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)

Validation ID: c7b6e6cb997de1660fd24d31759fe6bb21c7863f
Question: How many electrodes were used on the subject in EEG sessions?
From the context "Multichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes", so we know that the dimensionality depends on the number of electrodes.
From the context "We define channel cross-covariance (CCV) between any two electrodes INLINEFORM0 and INLINEFORM1 as: INLINEFORM2. Next, we reject the channels which have significantly lower cross-covariance than auto-covariance values (where auto-covariance implies CCV on same electrode). We found this measure to be essential as the higher cognitive processes underlying speech planning and synthesis involve frequent information exchange between different parts of the brain. Hence, such matrices often contain more discriminative features and hidden information than mere raw signals. This is essentially different than our previous work BIBREF16 where we extract per-channel 1-D covariance information and feed it to the networks. We present our sample 2-D EEG cross-covariance matrices (of two individuals) in Fig. FIGREF2.", so we know that the number of electrodes is 2.
True answer: 1913 signals

Validation ID: c7b6e6cb997de1660fd24d31759fe6bb21c7863f
Question: How many electrodes were used on the subject in EEG sessions?
From the context "Multichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes", so we know that the dimensionality depends on the number of electrodes.
From the context "We define channel cross-covariance (CCV) between any two electrodes INLINEFORM0 and INLINEFORM1 as: INLINEFORM2. Next, we reject the channels which have significantly lower cross-covariance than auto-covariance values (where auto-covariance implies CCV on same electrode). We found this measure to be essential as the higher cognitive processes underlying speech planning and synthesis involve frequent information exchange between different parts of the brain. Hence, such matrices often contain more discriminative features and hidden information than mere raw signals. This is essentially different than our previous work BIBREF16 where we extract per-channel 1-D covariance information and feed it to the networks. We present our sample 2-D EEG cross-covariance matrices (of two individuals) in Fig. FIGREF2.", so we know that the number of electrodes is 2.
True answer: Unanswerable

Validation ID: f9f59c171531c452bd2767dc332dc74cadee5120
Question: How many subjects does the EEG data come from?
From the context "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual.", so we know that the EEG data comes from 14 subjects.
Answer: 14
True answer: 14

Validation ID: f9f59c171531c452bd2767dc332dc74cadee5120
Question: How many subjects does the EEG data come from?
From the context "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17, composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual.", so we know that the EEG data comes from 14 subjects.
Answer: 14
True answer: 14 participants

Validation ID: 4ac2c3c259024d7cd8e449600b499f93332dab60
Question: Do they report results only on English data?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21.", so we know that the datasets are created by the authors.
Answer: Yes
True answer: Yes

Validation ID: 4ac2c3c259024d7cd8e449600b499f93332dab60
Question: Do they report results only on English data?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21.", so we know that the datasets are created by the authors.
Answer: Yes
True answer: Unanswerable

Validation ID: bc730e4d964b6a66656078e2da130310142ab641
Question: What type of classifiers are used?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21.", so we know that the type of classifiers used are cyber security and death of politicians.
Answer: cyber security and death of politicians
True answer: probabilistic model

Validation ID: bc730e4d964b6a66656078e2da130310142ab641
Question: What type of classifiers are used?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21.", so we know that the type of classifiers used are cyber security and death of politicians.
Answer: cyber security and death of politicians
True answer: Logistic Regression, Multilayer Perceptron

Validation ID: 3941401a182a3d6234894a5c8a75d48c6116c45c
Question: Which real-world datasets are used?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21.", so we know that the real-world datasets used are CyberAttack and PoliticianDeath.
Answer: CyberAttack and PoliticianDeath
True answer: Tweets related to CyberAttack and tweets related to PoliticianDeath

Validation ID: 3941401a182a3d6234894a5c8a75d48c6116c45c
Question: Which real-world datasets are used?
From the context "We perform our experiments with two predetermined event categories: cyber security (CyberAttack) and death of politicians (PoliticianDeath). These event categories are chosen as they are representative of important event types that are of interest to many governments and companies. The need to create our own dataset was motivated by the lack of public datasets for event detection on microposts. The few available datasets do not suit our requirements. For example, the publicly available Events-2012 Twitter dataset BIBREF20 contains generic event descriptions such as Politics, Sports, Culture etc. Our work targets more specific event categories BIBREF21.", so we know that the real-world datasets used are CyberAttack and PoliticianDeath.
Answer: CyberAttack and PoliticianDeath
True answer: cyber security (CyberAttack), death of politicians (PoliticianDeath)

Validation ID: 67e9e147b2cab5ba43572ce8a17fc863690172f0
Question: How are the interpretability merits of the approach demonstrated?
From the context "Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords", so we know that our approach iteratively leverages crowd workers for estimating keyword-specific expectations and the disagreement between the model and the crowd for discovering new informative keywords.
Answer: Crowd workers for estimating keyword-specific expectations and the disagreement between the model and the crowd for discovering new informative keywords
True answer: By involving humans for post-hoc evaluation of model's interpretability

Validation ID: 67e9e147b2cab5ba43572ce8a17fc863690172f0
Question: How are the interpretability merits of the approach demonstrated?
From the context "Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords", so we know that our approach iteratively leverages crowd workers for estimating keyword-specific expectations and the disagreement between the model and the crowd for discovering new informative keywords.
Answer: Crowd workers for estimating keyword-specific expectations and the disagreement between the model and the crowd for discovering new informative keywords
True answer: directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model

Validation ID: a74190189a6ced2a2d5b781e445e36f4e527e82a
Question: How are the accuracy merits of the approach demonstrated?
From the context "We observe that our approach results in better models for both CyberAttack and PoliticianDeath. Our manual investigation reveals that workers' annotations are of high reliability, which explains the relatively good performance of majority voting. Despite limited margin for improvement, our method of expectation inference improves the performance of majority voting by $0.4\%$ and $1.19\%$ AUC on CyberAttack and PoliticianDeath, respectively.", so we know that our method of expectation inference improves the performance of majority voting by $0.4\%$ and $1.19\%$ AUC on CyberAttack and PoliticianDeath, respectively.
Answer: $0.4\%$ and $1.19\%$
True answer: significant improvements clearly demonstrate that our approach is effective at improving model performance

Validation ID: a74190189a6ced2a2d5b781e445e36f4e527e82a
Question: How are the accuracy merits of the approach demonstrated?
From the context "We observe that our approach results in better models for both CyberAttack and PoliticianDeath. Our manual investigation reveals that workers' annotations are of high reliability, which explains the relatively good performance of majority voting. Despite limited margin for improvement, our method of expectation inference improves the performance of majority voting by $0.4\%$ and $1.19\%$ AUC on CyberAttack and PoliticianDeath, respectively.", so we know that our method of expectation inference improves the performance of majority voting by $0.4\%$ and $1.19\%$ AUC on CyberAttack and PoliticianDeath, respectively.
Answer: $0.4\%$ and $1.19\%$
True answer: By evaluating the performance of the approach using accuracy and AUC

Validation ID: 43f074bacabd0a355b4e0f91a1afd538c0a6244f
Question: How is the keyword specific expectation elicited from the crowd?
From the context "The keyword-specific expectation is elicited from the crowd by asking workers to label a set of microposts containing the keyword into two classes: event-related and non event-related", so we know that the keyword specific expectation is elicited from the crowd by asking workers to label a set of microposts containing the keyword into two classes: event-related and non event-related.
Answer: By asking workers to label a set of microposts containing the keyword into two classes: event-related and non event-related
True answer: workers are first asked to find those microposts where the model predictions are deemed correct,  asked to find the keyword that best indicates the class of the microposts

Validation ID: 58ef2442450c392bfc55c4dc35f216542f5f2dbb
Question: Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?
From the context "Researchers can benefit from Macaw for developing algorithms and techniques for conversational information seeking research, for user studies with different interfaces, for data collection from real users, and for preparing a demonstration of a CIS model.", so we know that the paper provides case studies to illustrate how one can use Macaw for CIS research.
Answer: Yes
True answer: No

Validation ID: 58ef2442450c392bfc55c4dc35f216542f5f2dbb
Question: Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?
From the context "Researchers can benefit from Macaw for developing algorithms and techniques for conversational information seeking research, for user studies with different interfaces, for data collection from real users, and for preparing a demonstration of a CIS model.", so we know that the paper provides case studies to illustrate how one can use Macaw for CIS research.
Answer: Yes
True answer: No

Validation ID: 78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d
Question: What functionality does Macaw provide?
From the context "Macaw supports multi-turn, multi-modal, and mixed-initiative interactions", so we know that Macaw provides multi-turn, multi-modal, and mixed-initiative interactions.
Answer: multi-turn, multi-modal, and mixed-initiative interactions
True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d
Question: What functionality does Macaw provide?
From the context "Macaw supports multi-turn, multi-modal, and mixed-initiative interactions", so we know that Macaw provides multi-turn, multi-modal, and mixed-initiative interactions.
Answer: multi-turn, multi-modal, and mixed-initiative interactions
True answer: conversational search, conversational question answering, conversational recommendation, conversational natural language interface to structured and semi-structured data

Validation ID: 375b281e7441547ba284068326dd834216e55c07
Question: What is a wizard of oz setup?
From the context "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", so we know that a wizard of oz setup is a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.
Answer: a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research
True answer: seeker interacts with a real conversational interface, intermediary (or the wizard) receives the seeker's message and performs different information seeking actions

Validation ID: 375b281e7441547ba284068326dd834216e55c07
Question: What is a wizard of oz setup?
From the context "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.", so we know that a wizard of oz setup is a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.
Answer: a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research
True answer: a setup where the seeker interacts with a real conversational interface and the wizard, an intermediary, performs actions related to the seeker's message

Validation ID: 05c49b9f84772e6df41f530d86c1f7a1da6aa489
Question: What interface does Macaw currently have?
From the context "We have implemented the following interfaces for Macaw: File IO, Standard IO, and Telegram", so we know that Macaw currently has File IO, Standard IO, and Telegram interfaces.
Answer: File IO, Standard IO, and Telegram
True answer: File IO, Standard IO, Telegram

Validation ID: 05c49b9f84772e6df41f530d86c1f7a1da6aa489
Question: What interface does Macaw currently have?
From the context "We have implemented the following interfaces for Macaw: File IO, Standard IO, and Telegram", so we know that Macaw currently has File IO, Standard IO, and Telegram interfaces.
Answer: File IO, Standard IO, and Telegram
True answer: The current implementation of Macaw supports a command line interface as well as mobile, desktop, and web apps.

Validation ID: 6ecb69360449bb9915ac73c0a816c8ac479cbbfc
Question: What modalities are supported by Macaw?
From the context "Macaw supports multi-turn, multi-modal, and mixed-initiative interactions", so we know that Macaw supports multi-modal interactions.
From the context "This interface allows multi-modal interactions (text, speech, click, image)", so we know that Macaw supports text, speech, click, and image.
True answer: text, speech, image, click, etc

Validation ID: 68df324e5fa697baed25c761d0be4c528f7f5cf7
Question: What are the different modules in Macaw?
From the context "Macaw has a modular design, with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup also follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and the interface produces a Message object from the current interaction of user. The interaction can be in multi-modal form, such as text, speech, image, and click. Macaw stores all interactions in an “Interaction Database”. For every interaction, Macaw looks for most recent user-system interactions (including the system's responses) to create a list of Messages, called the conversation list. It is then dispatched to multiple information seeking (and related) actions. The actions run in parallel, and each should respond within a pre-defined time interval. The output selection component selects from (or potentially combines) the outputs generated by different actions and creates a Message object as the system's response. This message is logged into the interaction database and is sent to the interface to be presented to the user. Again, the response message can be multi-modal and include text, speech, link, list of options, etc.", so we know that the different modules in Macaw are:

True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 68df324e5fa697baed25c761d0be4c528f7f5cf7
Question: What are the different modules in Macaw?
From the context "Macaw has a modular design, with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup also follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and the interface produces a Message object from the current interaction of user. The interaction can be in multi-modal form, such as text, speech, image, and click. Macaw stores all interactions in an “Interaction Database”. For every interaction, Macaw looks for most recent user-system interactions (including the system's responses) to create a list of Messages, called the conversation list. It is then dispatched to multiple information seeking (and related) actions. The actions run in parallel, and each should respond within a pre-defined time interval. The output selection component selects from (or potentially combines) the outputs generated by different actions and creates a Message object as the system's response. This message is logged into the interaction database and is sent to the interface to be presented to the user. Again, the response message can be multi-modal and include text, speech, link, list of options, etc.", so we know that the different modules in Macaw are:

True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 77c34f1033702278f7f044806c1eba0c6ecb8b04
Question: Do they report results only on English data?
From the context "We group entities based on their class INLINEFORM0 and construct section templates INLINEFORM1. For different entity classes, e.g. Person and Location, the section structure and the information represented in those section varies heavily. Therefore, the section templates are with respect to the individual classes in our experimental setup (see Figure FIGREF42 ).", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 77c34f1033702278f7f044806c1eba0c6ecb8b04
Question: Do they report results only on English data?
From the context "We group entities based on their class INLINEFORM0 and construct section templates INLINEFORM1. For different entity classes, e.g. Person and Location, the section structure and the information represented in those section varies heavily. Therefore, the section templates are with respect to the individual classes in our experimental setup (see Figure FIGREF42 ).", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 2ee715c7c6289669f11a79743a6b2b696073805d
Question: What baseline model is used?
From the context "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline.", so we know that the baseline model is AEP task.
Answer: AEP task
True answer: For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. 

For Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.

Validation ID: 2ee715c7c6289669f11a79743a6b2b696073805d
Question: What baseline model is used?
From the context "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia properties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an `Accidents' section before this year's disaster, which was the first in the history of the airline.", so we know that the baseline model is AEP task.
Answer: AEP task
True answer: B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .

, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0

Validation ID: 61a9ea36ddc37c60d1a51dabcfff9445a2225725
Question: What news article sources are used?
From the context "We evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages", so we know that the news article sources are the news external references in Wikipedia.
Answer: news external references in Wikipedia
True answer: Unanswerable

Validation ID: 61a9ea36ddc37c60d1a51dabcfff9445a2225725
Question: What news article sources are used?
From the context "We evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages", so we know that the news article sources are the news external references in Wikipedia.
Answer: news external references in Wikipedia
True answer:  the news external references in Wikipedia

Validation ID: cc850bc8245a7ae790e1f59014371d4f35cd46d7
Question: How do they determine the exact section to use the input article?
From the context "In the second step, we take into account all `relevant' pairs INLINEFORM0 and find the correct section for article INLINEFORM1 in entity INLINEFORM2, respectively its profile INLINEFORM3 (see Section SECREF30 ).", so we know that the correct section for article INLINEFORM1 in entity INLINEFORM2, respectively its profile INLINEFORM3 is determined by the second step.
Answer: the second step
True answer: They use a multi-class classifier to determine the section it should be cited

Validation ID: 984fc3e726848f8f13dfe72b89e3770d00c3a1af
Question: What features are used to represent the novelty of news articles to entity pages?
From the context "Finally, if the article we are about to suggest is already covered in the entity profile INLINEFORM14, we do not wish to suggest redundant information, hence the novelty. Therefore, the learning objective of INLINEFORM15 should fulfill the following properties. Table TABREF21 shows a summary of the computed features for INLINEFORM16.", so we know that the features used to represent the novelty of news articles to entity pages are:

True answer: KL-divergences of language models for the news article and the already added news references

Validation ID: 984fc3e726848f8f13dfe72b89e3770d00c3a1af
Question: What features are used to represent the novelty of news articles to entity pages?
From the context "Finally, if the article we are about to suggest is already covered in the entity profile INLINEFORM14, we do not wish to suggest redundant information, hence the novelty. Therefore, the learning objective of INLINEFORM15 should fulfill the following properties. Table TABREF21 shows a summary of the computed features for INLINEFORM16.", so we know that the features used to represent the novelty of news articles to entity pages are:

True answer: KL divergence between the language model of INLINEFORM5 and articles in INLINEFORM6

Validation ID: fb1227b3681c69f60eb0539e16c5a8cd784177a7
Question: What features are used to represent the salience and relative authority of entities?
From the context "We reimplemented the ones in Dunietz and Gillick BIBREF11. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", so we know that the features used to represent the salience and relative authority of entities are positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.
Answer: positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in
True answer: Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.
The relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it.

Validation ID: fb1227b3681c69f60eb0539e16c5a8cd784177a7
Question: What features are used to represent the salience and relative authority of entities?
From the context "We reimplemented the ones in Dunietz and Gillick BIBREF11. This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in BIBREF11 gives details.", so we know that the features used to represent the salience and relative authority of entities are positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.
Answer: positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in
True answer: positional features, occurrence frequency, internal POS structure of the entity and the sentence it occurs in, relative entity frequency, centrality measures like PageRank 

Validation ID: 8df35c24af9efc3348d3b8d746df116480dfe661
Question: Do they experiment with other tasks?
From the context "The corpus presented in this paper is the very first of its kind and a valuable addition to the scarce number of corpora available in the field of biomedical NLP. Its complexity, given the discontinuous nature of entities and a high number of nested and multi-label entities, poses new challenges for NLP methods applied for NER and can, hence, be a valuable source for insights into what entities “look like in the wild”. Moreover, it can serve as a playground for new modelling techniques such as the resolution of discontinuous entities as well as multi-task learning given the combination of entities and their relations.", so we know that they experiment with other tasks.
Answer: Yes
True answer: No

Validation ID: 277a7e916e65dfefd44d2d05774f95257ac946ae
Question: What baselines do they introduce?
From the context "We evaluate the corpus using Named Entity Recognition (NER), i. e., the task of finding mentions of concepts of interest in unstructured text. We focus on detecting cases, conditions, factors, findings and modifiers in case reports (cf. Section SECREF6). We approach this as a sequence labeling problem. Four systems were developed to offer comparable robust baselines.", so we know that they introduce four baselines.
Answer: four baselines
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT


Validation ID: 277a7e916e65dfefd44d2d05774f95257ac946ae
Question: What baselines do they introduce?
From the context "We evaluate the corpus using Named Entity Recognition (NER), i. e., the task of finding mentions of concepts of interest in unstructured text. We focus on detecting cases, conditions, factors, findings and modifiers in case reports (cf. Section SECREF6). We approach this as a sequence labeling problem. Four systems were developed to offer comparable robust baselines.", so we know that they introduce four baselines.
Answer: four baselines
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: 2916bbdb95ef31ab26527ba67961cf5ec94d6afe
Question: How large is the corpus?
From the context "The corpus comprises 8,275 sentences and 167,739 words in total", so we know that the corpus is 8,275 sentences and 167,739 words in total.
Answer: 8,275 sentences and 167,739 words in total
True answer: 8,275 sentences and 167,739 words in total

Validation ID: 2916bbdb95ef31ab26527ba67961cf5ec94d6afe
Question: How large is the corpus?
From the context "The corpus comprises 8,275 sentences and 167,739 words in total", so we know that the corpus is 8,275 sentences and 167,739 words in total.
Answer: 8,275 sentences and 167,739 words in total
True answer: The corpus comprises 8,275 sentences and 167,739 words in total.

Validation ID: f2e8497aa16327aa297a7f9f7d156e485fe33945
Question: How was annotation performed?
From the context "The annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation", so we know that annotation was performed using WebAnno.
Answer: WebAnno
True answer: Experienced medical doctors used a linguistic annotation tool to annotate entities.

Validation ID: f2e8497aa16327aa297a7f9f7d156e485fe33945
Question: How was annotation performed?
From the context "The annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation", so we know that annotation was performed using WebAnno.
Answer: WebAnno
True answer: WebAnno

Validation ID: 9b76f428b7c8c9fc930aa88ee585a03478bff9b3
Question: How many documents are in the new corpus?
From the context "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total", so we know that the new corpus consists of 53 documents.
Answer: 53
True answer: 53 documents

Validation ID: 9b76f428b7c8c9fc930aa88ee585a03478bff9b3
Question: How many documents are in the new corpus?
From the context "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total", so we know that the new corpus consists of 53 documents.
Answer: 53
True answer: 53 documents

Validation ID: dd6b378d89c05058e8f49e48fd48f5c458ea2ebc
Question: What baseline systems are proposed?
From the context "We evaluate the corpus using Named Entity Recognition (NER), i. e., the task of finding mentions of concepts of interest in unstructured text. We focus on detecting cases, conditions, factors, findings and modifiers in case reports (cf. Section SECREF6). We approach this as a sequence labeling problem. Four systems were developed to offer comparable robust baselines.", so we know that four baseline systems are proposed.
Answer: four baseline systems
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: dd6b378d89c05058e8f49e48fd48f5c458ea2ebc
Question: What baseline systems are proposed?
From the context "We evaluate the corpus using Named Entity Recognition (NER), i. e., the task of finding mentions of concepts of interest in unstructured text. We focus on detecting cases, conditions, factors, findings and modifiers in case reports (cf. Section SECREF6). We approach this as a sequence labeling problem. Four systems were developed to offer comparable robust baselines.", so we know that four baseline systems are proposed.
Answer: four baseline systems
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851
Question: How did they obtain the dataset?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that they obtained the dataset from two lists annotated in previous works.
Answer: two lists annotated in previous works
True answer: For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy

Validation ID: e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851
Question: How did they obtain the dataset?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that they obtained the dataset from two lists annotated in previous works.
Answer: two lists annotated in previous works
True answer: public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy

Validation ID: c00ce1e3be14610fb4e1f0614005911bb5ff0302
Question: What activation function do they use in their model?
From the context "We use hyperopt library to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout ($0.0-0.9$), activation function ($relu$, $selu$, $tanh$), optimizer ($sgd$, $adam$, $rmsprop$) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16)", so we know that they use $relu$, $selu$, $tanh$ as activation function.
Answer: $relu$, $selu$, $tanh$
True answer: relu, selu, tanh

Validation ID: c00ce1e3be14610fb4e1f0614005911bb5ff0302
Question: What activation function do they use in their model?
From the context "We use hyperopt library to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout ($0.0-0.9$), activation function ($relu$, $selu$, $tanh$), optimizer ($sgd$, $adam$, $rmsprop$) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16)", so we know that they use hyperopt library to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout ($0.0-0.9$), activation function ($relu$, $selu$, $tanh$), optimizer ($sgd$, $adam$, $rmsprop$) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16).
Answer: $relu$, $selu$, $tanh$
True answer: Activation function is hyperparameter. Possible values: relu, selu, tanh.

Validation ID: 71fe5822d9fccb1cb391c11283b223dc8aa1640c
Question: What baselines do they compare to?
From the context "We compare our approach (FacTweet) to the following set of baselines:", so we know that they compare to the following set of baselines:
[leftmargin=4mm]
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 71fe5822d9fccb1cb391c11283b223dc8aa1640c
Question: What baselines do they compare to?
From the context "We compare our approach (FacTweet) to the following set of baselines:", so we know that they compare to the following set of baselines:
[leftmargin=4mm]
True answer: Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words

Validation ID: 97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3
Question: How are chunks defined?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that chunks are defined by the number of tweets in each account.
Answer: The number of tweets in each account
True answer: Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.

Validation ID: 97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3
Question: How are chunks defined?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that chunks are defined by the number of tweets in each account.
Answer: The number of tweets in each account
True answer: sequence of $s$ tweets

Validation ID: 1062a0506c3691a93bb914171c2701d2ae9621cb
Question: What features are extracted?
From the context "We argue that different kinds of features like the sentiment of the text, morality, and other text-based features are critical to detect the nonfactual Twitter accounts by utilizing their occurrence during reporting the news in an account's timeline. We employ a rich set of features borrowed from previous works in fake news, bias, and rumors detection BIBREF0, BIBREF1, BIBREF8, BIBREF9.", so we know that the features are sentiment of the text, morality, and other text-based features.
Answer: sentiment of the text, morality, and other text-based features
True answer: Sentiment, Morality, Style, Words embeddings

Validation ID: 1062a0506c3691a93bb914171c2701d2ae9621cb
Question: What features are extracted?
From the context "We extract a set of features from each chunk and we feed them into a recurrent neural network to model the sequential flow of the chunks' tweets. We use an attention layer with dropout to attend over the most important tweets in each chunk. Finally, the representation is fed into a softmax layer to produce a probability distribution over the account types and thus predict the factuality of the accounts.", so we know that the features are extracted from each chunk.
Answer: chunk
True answer: 15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings

Validation ID: 8e12b5c459fa963b3e549deadb864c244879fe82
Question: How many layers does their model have?
From the context "We use a rich set of features borrowed from previous works in fake news, bias, and rumors detection BIBREF0, BIBREF1, BIBREF8, BIBREF9.", so we know that their model has 4 layers.
Answer: 4
True answer: Unanswerable

Validation ID: 483a699563efcb8804e1861b18809279f21c7610
Question: Was the approach used in this work to detect fake news fully supervised?
From the context "We propose an approach to detect non-factual Twitter accounts by treating post streams as a sequence of tweets' chunks. We test several semantic and dictionary-based features together with a neural sequential approach, and apply an ablation test to investigate their contribution.", so we know that the approach used in this work to detect fake news is fully supervised.
Answer: Yes
True answer: Yes

Validation ID: d3ff2986ca8cb85a9a5cec039c266df756947b43
Question: Based on this paper, what is the more predictive set of features to detect fake news?
From the context "The results indicate that most performance gains come from words embeddings, style, and morality features. Other features (emotion and sentiment) show lower importance: nevertheless, they still improve the overall system performance (on average 0.35% Macro-F$_1$ improvement). These performance figures suggest that non-factual accounts use semantic and stylistic hidden signatures mostly while tweeting news, so as to be able to mislead the readers and behave as reputable (i.e., factual) sources.", so we know that the more predictive set of features to detect fake news is words embeddings, style, and morality features.
Answer: words embeddings, style, and morality features
True answer: words embeddings, style, and morality features

Validation ID: d3ff2986ca8cb85a9a5cec039c266df756947b43
Question: Based on this paper, what is the more predictive set of features to detect fake news?
From the context "The results indicate that most performance gains come from words embeddings, style, and morality features. Other features (emotion and sentiment) show lower importance: nevertheless, they still improve the overall system performance (on average 0.35% Macro-F$_1$ improvement). These performance figures suggest that non-factual accounts use semantic and stylistic hidden signatures mostly while tweeting news, so as to be able to mislead the readers and behave as reputable (i.e., factual) sources.", so we know that the more predictive set of features to detect fake news is words embeddings, style, and morality features.
Answer: words embeddings, style, and morality features
True answer: words embeddings, style, and morality features

Validation ID: 3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4
Question: How big is the dataset used in this work?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that the dataset used in this work is 212 Twitter accounts.
Answer: 212 Twitter accounts
True answer: Total dataset size: 171 account (522967 tweets)

Validation ID: 3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4
Question: How big is the dataset used in this work?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that the dataset used in this work is 212 Twitter accounts.
Answer: 212 Twitter accounts
True answer: 212 accounts

Validation ID: 2317ca8d475b01f6632537b95895608dc40c4415
Question: How is a "chunk of posts" defined in this work?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that a "chunk of posts" is a list of tweets.
Answer: a list of tweets
True answer: chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account

Validation ID: 2317ca8d475b01f6632537b95895608dc40c4415
Question: How is a "chunk of posts" defined in this work?
From the context "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). We discard the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. We discard some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, we remove the duplicate, media-based, and link-only tweets. For each account, we collect the maximum amount of tweets allowed by Twitter API. Table TABREF13 presents statistics on our dataset.", so we know that a "chunk of posts" is a list of tweets.
Answer: a list of tweets
True answer: sequence of $s$ tweets

Validation ID: 3e88fb3d28593309a307eb97e875575644a01463
Question: What baselines were used in this work?
From the context "We compare our approach (FacTweet) to the following set of baselines:", so we know that the baselines used in this work are:
[leftmargin=4mm]
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 3e88fb3d28593309a307eb97e875575644a01463
Question: What baselines were used in this work?
From the context "We compare our approach (FacTweet) to the following set of baselines:", so we know that the baselines used in this work are:
[leftmargin=4mm]
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 0767ca8ff1424f7a811222ca108a33b6411aaa8a
Question: What is the performance of their method?
From the context "Overall we provide strong baseline for text summarization using AMR for possible future works", so we know that the performance of their method is strong.
Answer: strong
True answer: Unanswerable

Validation ID: e8f969ffd637b82d04d3be28c51f0f3ca6b3883e
Question: Which evaluation methods are used?
From the context "We use the standard ROGUE metric. For comparison with previous AMR based summarization methods, we report the Recall, Precision and INLINEFORM0 scores for ROGUE-1. Since most of the literature on summarization uses INLINEFORM1 scores for ROGUE-2 and ROGUE-L for comparison, we also report INLINEFORM2 scores for ROGUE-2 and ROGUE-L for our method.", so we know that the evaluation methods used are ROGUE-1, ROGUE-2 and ROGUE-L.
Answer: ROGUE-1, ROGUE-2 and ROGUE-L
True answer: Quantitative evaluation methods using ROUGE, Recall, Precision and F1.

Validation ID: e8f969ffd637b82d04d3be28c51f0f3ca6b3883e
Question: Which evaluation methods are used?
From the context "We use the standard ROGUE metric. For comparison with previous AMR based summarization methods, we report the Recall, Precision and INLINEFORM0 scores for ROGUE-1. Since most of the literature on summarization uses INLINEFORM1 scores for ROGUE-2 and ROGUE-L for comparison, we also report INLINEFORM2 scores for ROGUE-2 and ROGUE-L for our method.", so we know that the evaluation methods used are ROGUE-1, ROGUE-2 and ROGUE-L.
Answer: ROGUE-1, ROGUE-2 and ROGUE-L
True answer: standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L

Validation ID: 46227b4265f1d300a5ed71bf40822829de662bc2
Question: What dataset is used in this paper?
From the context "We used two datasets for the task - AMR Bank BIBREF10 and CNN-Dailymail ( BIBREF11 BIBREF12 )", so we know that the dataset used in this paper is AMR Bank and CNN-Dailymail.
Answer: AMR Bank and CNN-Dailymail
True answer: AMR Bank, CNN-Dailymail

Validation ID: 46227b4265f1d300a5ed71bf40822829de662bc2
Question: What dataset is used in this paper?
From the context "We used two datasets for the task - AMR Bank BIBREF10 and CNN-Dailymail ( BIBREF11 BIBREF12 )", so we know that the dataset used in this paper is AMR Bank and CNN-Dailymail.
Answer: AMR Bank and CNN-Dailymail
True answer: AMR Bank BIBREF10, CNN-Dailymail ( BIBREF11 BIBREF12 )

Validation ID: a6a48de63c1928238b37c2a01c924b852fe752f8
Question: Which other methods do they compare with?
From the context "We also show that ROGUE can't be used for evaluating the abstractive summaries generated by our AMR pipeline.", so we know that they compare with ROGUE.
Answer: ROGUE
True answer: Lead-3, Lead-1-AMR

Validation ID: a6a48de63c1928238b37c2a01c924b852fe752f8
Question: Which other methods do they compare with?
From the context "We also show that ROGUE can't be used for evaluating the abstractive summaries generated by our AMR pipeline", so we know that they compare with ROGUE.
Answer: ROGUE
True answer: Lead-3 model,  Lead-1-AMR, BIBREF0 

Validation ID: b65a83a24fc66728451bb063cf6ec50134c8bfb0
Question: How are sentences selected from the summary graph?
From the context "We start by finding the position of the most referred entity in the graph, then we find the closest verb to the entity. and finally select the subtree hanging from that verb as the summary AMR.", so we know that sentences are selected from the summary graph by finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR.
Answer: finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR
True answer:  finding the important sentences from the story, extracting the key information from those sentences using their AMR graphs

Validation ID: b65a83a24fc66728451bb063cf6ec50134c8bfb0
Question: How are sentences selected from the summary graph?
From the context "We start by finding the position of the most referred entity in the graph, then we find the closest verb to the entity. and finally select the subtree hanging from that verb as the summary AMR.", so we know that sentences are selected from the summary graph by finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR.
Answer: finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR
True answer:  Two methods: first is to simply pick initial few sentences,  second is to capture the relation between the two most important entities  (select the first sentence which contains both these entities).

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
From the context "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13. We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14, as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.", so we know that the models used in the experiment are linear SVM, BiLSTM, and CNN.
Answer: linear SVM, BiLSTM, and CNN
True answer: linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
From the context "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13. We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14, as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.", so we know that the models used in the experiment are linear SVM, BiLSTM, and CNN.
Answer: linear SVM, BiLSTM, and CNN
True answer: linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
From the context "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13. We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14, as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.", so we know that the models used in the experiment are linear SVM, BiLSTM, and CNN.
Answer: linear SVM, BiLSTM, and CNN
True answer: linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) 

Validation ID: 682e26262abba473412f68cbeb5f69aa3b9968d7
Question: What are the differences between this dataset and pre-existing ones?
From the context "In OLID, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10.", so we know that the differences between this dataset and pre-existing ones are:
1. This dataset is split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language.
True answer: no prior work has explored the target of the offensive language

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
From the context "The data included in OLID has been collected from Twitter. We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'", so we know that the tweets are in English.
Answer: English
True answer: English

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
From the context "The data included in OLID has been collected from Twitter. We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'", so we know that the tweets are in English.
Answer: English
True answer: English 

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
From the context "The data included in OLID has been collected from Twitter. We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'", so we know that the tweets are in English.
Answer: English
True answer: English

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
From the context "The dataset contains 14,100 tweets and is released freely to the research community", so we know that the size of the new dataset is 14,100 tweets.
Answer: 14,100 tweets
True answer: Unanswerable

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
From the context "The dataset contains 14,100 tweets and is released freely to the research community", so we know that the size of the new dataset is 14,100 tweets.
Answer: 14,100 tweets
True answer: 14,100 tweets

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
From the context "The dataset contains 14,100 tweets and is released freely to the research community", so we know that the size of the new dataset is 14,100 tweets.
Answer: 14,100 tweets
True answer: Dataset contains total of 14100 annotations.

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
From the context "Different abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language", so we know that the kinds of offensive content explored are aggression identification, bullying detection, hate speech, toxic comments, and offensive language.
Answer: aggression identification, bullying detection, hate speech, toxic comments, and offensive language
True answer: non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
From the context "Different abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language", so we know that the kinds of offensive content explored are aggression identification, bullying detection, hate speech, toxic comments, and offensive language.
Answer: aggression identification, bullying detection, hate speech, toxic comments, and offensive language
True answer: Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others , Untargeted (UNT): Posts containing non-targeted profanity and swearing.

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
From the context "Different abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language", so we know that the kinds of offensive content explored are aggression identification, bullying detection, hate speech, toxic comments, and offensive language.
Answer: aggression identification, bullying detection, hate speech, toxic comments, and offensive language
True answer: offensive (OFF) and non-offensive (NOT), targeted (TIN) and untargeted (INT) insults, targets of insults and threats as individual (IND), group (GRP), and other (OTH)

Validation ID: 55bd59076a49b19d3283af41c5e3ccb875f3eb0c
Question: What is the best performing model?
From the context "The results show that this is a challenging task. A CNN-based sentence classifier achieved the best results in all three sub-tasks.", so we know that the best performing model is CNN-based sentence classifier.
Answer: CNN-based sentence classifier
True answer: CNN 

Validation ID: 521280a87c43fcdf9f577da235e7072a23f0673e
Question: How many annotators participated?
From the context "We first acquired two annotations for each instance. In case of 100% agreement, we considered these as acceptable annotations, and in case of disagreement, we requested more annotations until the agreement was above 66%.", so we know that the number of annotators is more than 2.
Answer: more than 2
True answer: five annotators

Validation ID: 5a8cc8f80509ea77d8213ed28c5ead501c68c725
Question: What is the definition of offensive language?
From the context "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10.
Level A: Offensive language Detection
True answer:  Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .

Validation ID: 290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30
Question: What are the three layers of the annotation scheme?
From the context "In the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language.", so we know that the three layers of the annotation scheme are A, B, and C.
Answer: A, B, and C
True answer: Level A: Offensive language Detection
, Level B: Categorization of Offensive Language
, Level C: Offensive Language Target Identification


Validation ID: 1b72aa2ec3ce02131e60626639f0cf2056ec23ca
Question: How long is the dataset for each step of hierarchy?
From the context "The breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15.", so we know that the dataset for each step of hierarchy is shown in Table TABREF15.
Answer: Table TABREF15
True answer: Level A: 14100 Tweets
Level B: 4640 Tweets
Level C: 4089 Tweets

Validation ID: c49ee6ac4dc812ff84d255886fd5aff794f53c39
Question: Do the authors report results only on English data?
From the context "We study the effectiveness of Rouge and our proposed method (Sera) by analyzing the correlations with semi-manual human judgments. Very few teams participated in TAC 2014 summarization track and the official results and the review paper of TAC 2014 systems were never published. Therefore, to evaluate the effectiveness of Rouge, we applied 9 well-known summarization approaches on the TAC 2014 scientific summarization dataset.", so we know that the authors report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: c49ee6ac4dc812ff84d255886fd5aff794f53c39
Question: Do the authors report results only on English data?
From the context "We study the effectiveness of Rouge and our proposed method (Sera) by analyzing the correlations with semi-manual human judgments. Very few teams participated in TAC 2014 summarization track and the official results and the review paper of TAC 2014 systems were never published. Therefore, to evaluate the effectiveness of Rouge, we applied 9 well-known summarization approaches on the TAC 2014 scientific summarization dataset.", so we know that the authors report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 3f856097be2246bde8244add838e83a2c793bd17
Question: In the proposed metric, how is content relevance measured?
From the context "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related.", so we know that the content relevance is measured by comparing the overlaps of the retrieved results.
Answer: comparing the overlaps of the retrieved results
True answer: The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. 

Validation ID: 3f856097be2246bde8244add838e83a2c793bd17
Question: In the proposed metric, how is content relevance measured?
From the context "Our proposed metric is based on analysis of the content relevance between a system generated summary and the corresponding human written gold-standard summaries. On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results. Larger number of overlaps, suggest that the candidate summary has higher content quality with respect to the gold-standard. This method, enables us to also reward for terms that are not lexically equivalent but semantically related.", so we know that in the proposed metric, content relevance is measured by comparing the overlaps of the retrieved results.
Answer: comparing the overlaps of the retrieved results
True answer: On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval.

Validation ID: bf52c01bf82612d0c7bbf2e6a5bb2570c322936f
Question: What different correlations result when using different variants of ROUGE scores?
From the context "Table TABREF23 shows the Pearson, Spearman and Kendall correlation of Rouge and Sera, with pyramid scores. Both Rouge and Sera are calculated with stopwords removed and with stemming. Our experiments with inclusion of stopwords and without stemming showed similar results and thus, we do not include those to avoid redundancy.", so we know that Rouge and Sera are calculated with stopwords removed and with stemming.
From the context "Table TABREF23 also shows the results of other Sera variants including discounting and query reformulation methods. Some of these variants are the result of applying query reformulation in the process of document retrieval which are described in section SECREF3 As illustrated, the Noun Phrases (NP) query reformulation at cut-off point of 5 (shown as Sera-np-5) achieves the highest correlations among all the Sera variants. In the case of Keywords (KW), without using discounting, we can see that there is no positive gain in correlation.", so we know that the Noun Phrases (NP) query reformulation at cut-off point of 5 (shown as Sera-np-5) achieves the highest correlations among all the Sera variants.
True answer: we observe that many variants of Rouge scores do not have high correlations with human pyramid scores

Validation ID: bf52c01bf82612d0c7bbf2e6a5bb2570c322936f
Question: What different correlations result when using different variants of ROUGE scores?
From the context "Table TABREF23 shows the Pearson, Spearman and Kendall correlation of Rouge and Sera, with pyramid scores. Both Rouge and Sera are calculated with stopwords removed and with stemming. Our experiments with inclusion of stopwords and without stemming showed similar results and thus, we do not include those to avoid redundancy.", so we know that Rouge and Sera are calculated with stopwords removed and with stemming.
From the context "Table TABREF23 also shows the results of other Sera variants including discounting and query reformulation methods. Some of these variants are the result of applying query reformulation in the process of document retrieval which are described in section SECREF3 As illustrated, the Noun Phrases (NP) query reformulation at cut-off point of 5 (shown as Sera-np-5) achieves the highest correlations among all the Sera variants. In the case of Keywords (KW), without using discounting, we can see that there is no positive gain in correlation.", so we know that the Noun Phrases (NP) query reformulation at cut-off point of 5 (shown as Sera-np-5) achieves the highest correlations among all the Sera variants.
True answer: Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.

Validation ID: 74e866137b3452ec50fb6feaf5753c8637459e62
Question: What manual Pyramid scores are used?
From the context "We study the effectiveness of Rouge and our proposed method (Sera) by analyzing the correlations with semi-manual human judgments. Very few teams participated in TAC 2014 summarization track and the official results and the review paper of TAC 2014 systems were never published. Therefore, to evaluate the effectiveness of Rouge, we applied 9 well-known summarization approaches on the TAC 2014 scientific summarization dataset. Obtained Rouge and Sera results of each of these approaches are then correlated with semi-manual human judgments. In the following, we briefly describe each of these summarization approaches.", so we know that the manual Pyramid scores are used to evaluate the effectiveness of Rouge.
Answer: semi-manual human judgments
True answer:  higher tiers of the pyramid

Validation ID: 74e866137b3452ec50fb6feaf5753c8637459e62
Question: What manual Pyramid scores are used?
From the context "We study the effectiveness of Rouge and our proposed method (Sera) by analyzing the correlations with semi-manual human judgments. Very few teams participated in TAC 2014 summarization track and the official results and the review paper of TAC 2014 systems were never published. Therefore, to evaluate the effectiveness of Rouge, we applied 9 well-known summarization approaches on the TAC 2014 scientific summarization dataset. Obtained Rouge and Sera results of each of these approaches are then correlated with semi-manual human judgments. In the following, we briefly describe each of these summarization approaches.", so we know that the manual Pyramid scores are used to evaluate the effectiveness of Rouge.
Answer: semi-manual human judgments
True answer: following the pyramid framework, we design an annotation scheme

Validation ID: 184b0082e10ce191940c1d24785b631828a9f714
Question: What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'?
From the context "We hypothesize that using only informative concepts in the summary prevents query drift and leads to retrieval of more relevant documents. Noun phrases and keywords are two heuristics for identifying the informative concepts.
We hypothesize that using only informative concepts in the summary prevents query drift and leads to retrieval of more relevant documents. Noun phrases and keywords are two heuristics for identifying the informative concepts.", so we know that the common belief is that using only informative concepts in the summary prevents query drift and leads to retrieval of more relevant documents.
True answer: correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization

Validation ID: c59078efa7249acfb9043717237c96ae762c0a8c
Question: which existing strategies are compared?
From the context "We also implement the bias regularization method of BIBREF5 which debiases the word embedding during language model training by minimizing the projection of neutral words on the gender axis. We use hyperparameter tuning to find the best regularization coefficient and report results from the model trained with this coefficient. We later refer to this strategy as REG.
Experiments
True answer: CDA, REG

Validation ID: c59078efa7249acfb9043717237c96ae762c0a8c
Question: which existing strategies are compared?
From the context "We also implement the bias regularization method of BIBREF5 which debiases the word embedding during language model training by minimizing the projection of neutral words on the gender axis. We use hyperparameter tuning to find the best regularization coefficient and report results from the model trained with this coefficient. We later refer to this strategy as REG.
Experiments
True answer: Unanswerable

Validation ID: 73bddaaf601a4f944a3182ca0f4de85a19cdc1d2
Question: what dataset was used?
From the context "For the training data, we use Daily Mail news articles released by BIBREF9. This dataset is composed of 219,506 articles covering a diverse range of topics including business, sports, travel, etc., and is claimed to be biased and sensational BIBREF5. For manageability, we randomly subsample 5% of the text. The subsample has around 8.25 million tokens in total.", so we know that the dataset used is Daily Mail news articles.
Answer: Daily Mail news articles
True answer: Daily Mail news articles released by BIBREF9 

Validation ID: 73bddaaf601a4f944a3182ca0f4de85a19cdc1d2
Question: what dataset was used?
From the context "For the training data, we use Daily Mail news articles released by BIBREF9. This dataset is composed of 219,506 articles covering a diverse range of topics including business, sports, travel, etc., and is claimed to be biased and sensational BIBREF5. For manageability, we randomly subsample 5% of the text. The subsample has around 8.25 million tokens in total.", so we know that the dataset used is Daily Mail news articles.
Answer: Daily Mail news articles
True answer: Daily Mail news articles

Validation ID: d4e5e3f37679ff68914b55334e822ea18e60a6cf
Question: what kinds of male and female words are looked at?
From the context "We introduce a loss term INLINEFORM0, which aims to equalize the predicted probabilities of gender pairs such as woman and man. INLINEFORM1
INLINEFORM0 and INLINEFORM1 are a set of corresponding gender pairs, INLINEFORM2 is the size of the gender pairs set, and INLINEFORM3 indicates the output softmax probability. We use gender pairs provided by BIBREF7. By considering only gender pairs we ensure that only gender information is neutralized and distribution over semantic concepts is not altered. For example, it will try to equalize the probabilities of congressman with congresswoman and actor with actress but distribution of congressman, congresswoman versus actor, actress will not be affected.", so we know that the kinds of male and female words are looked at are gender pairs provided by BIBREF7.
True answer: gendered word pairs like he and she

Validation ID: 5f60defb546f35d25a094ff34781cddd4119e400
Question: how is mitigation of gender bias evaluated?
From the context "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1. This is defined as INLINEFORM2
where INLINEFORM0
True answer: Using INLINEFORM0 and INLINEFORM1

Validation ID: 5f60defb546f35d25a094ff34781cddd4119e400
Question: how is mitigation of gender bias evaluated?
From the context "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1. This is defined as INLINEFORM2
where INLINEFORM0
True answer: Unanswerable

Validation ID: 90d946ccc3abf494890e147dd85bd489b8f3f0e8
Question: what bias evaluation metrics are used?
From the context "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1. This is defined as INLINEFORM2
where INLINEFORM0
True answer: Unanswerable

Validation ID: 90d946ccc3abf494890e147dd85bd489b8f3f0e8
Question: what bias evaluation metrics are used?
From the context "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1. This is defined as INLINEFORM2
where INLINEFORM0
True answer: gender bias, normalized version of INLINEFORM0, ratio of occurrence of male and female words in the model generated text, Causal occupation bias conditioned on occupation, causal occupation bias conditioned on gender, INLINEFORM1

Validation ID: b962cc817a4baf6c56150f0d97097f18ad6cd9ed
Question: What kind of questions are present in the dataset?
From the context "We introduce a list of 8 different competencies that a reading system should master in order to process reviews and text documents in general. These 8 tasks require different competencies and a different level of understanding of the document to be well answered. For instance, detecting if an aspect is mentioned in a review will require less understanding of the review than predicting explicitly the rating of this aspect.", so we know that the dataset contains 8 different competencies.
Answer: 8
True answer: These 8 tasks require different competencies and a different level of understanding of the document to be well answered

Validation ID: fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f
Question: What baselines are presented?
From the context "We evaluate 4 models on this dataset, including a projective model of our own design that seems to be a strong baseline for this dataset", so we know that the baselines are 4 models.
Answer: 4 models
True answer: Logistic regression, LSTM, End-to-end memory networks, Deep projective reader

Validation ID: fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f
Question: What baselines are presented?
From the context "We evaluate 4 models on this dataset, including a projective model of our own design that seems to be a strong baseline for this dataset", so we know that the baselines are 4 models.
Answer: 4 models
True answer: Logistic regression, LSTM, End-to-end memory networks, Deep projective reader

Validation ID: 52f8a3e3cd5d42126b5307adc740b71510a6bdf5
Question: What tasks were evaluated?
From the context "We evaluate 4 models on this dataset, including a projective model of our own design that seems to be a strong baseline for this dataset", so we know that the tasks were evaluated are 4 models.
Answer: 4 models
True answer: ReviewQA's test set

Validation ID: 52f8a3e3cd5d42126b5307adc740b71510a6bdf5
Question: What tasks were evaluated?
From the context "We evaluate 4 models on this dataset, including a projective model of our own design that seems to be a strong baseline for this dataset", so we know that the tasks were evaluated are 4 models.
Answer: 4 models
True answer: Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review

Validation ID: 2236386729105f5cf42f73cc055ce3acdea2d452
Question: What language are the reviews in?
From the context "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11. This corpus is available at http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2. Each review comes with the name of the associated hotel, a title, an overall rating, a comment and a list of rated aspects.", so we know that the language of the reviews is English.
Answer: English
True answer: Unanswerable

Validation ID: 2236386729105f5cf42f73cc055ce3acdea2d452
Question: What language are the reviews in?
From the context "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11. This corpus is available at http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2. Each review comes with the name of the associated hotel, a title, an overall rating, a comment and a list of rated aspects.", so we know that the language of the reviews is English.
Answer: English
True answer: English

Validation ID: 18942ab8c365955da3fd8fc901dfb1a3b65c1be1
Question: Where are the hotel reviews from?
From the context "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11", so we know that the hotel reviews are from TripAdvisor.
Answer: TripAdvisor
True answer: TripAdvisor

Validation ID: 18942ab8c365955da3fd8fc901dfb1a3b65c1be1
Question: Where are the hotel reviews from?
From the context "We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11", so we know that the hotel reviews are from TripAdvisor.
Answer: TripAdvisor
True answer: TripAdvisor

Validation ID: 7b4992e2d26577246a16ac0d1efc995ab4695d24
Question: What was the baseline used?
From the context "We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) BIBREF3. Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for training an error detection system.", so we know that the baseline used was the original annotated dataset.
Answer: the original annotated dataset
True answer: error detection system by Rei2016

Validation ID: 7b4992e2d26577246a16ac0d1efc995ab4695d24
Question: What was the baseline used?
From the context "We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) BIBREF3. Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for training an error detection system.", so we know that the baseline used was the original annotated dataset.
Answer: the original annotated dataset
True answer: error detection system by Rei2016

Validation ID: ab9b0bde6113ffef8eb1c39919d21e5913a05081
Question: What are their results on both datasets?
From the context "The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection.", so we know that the results on both datasets are substantially improved by making use of artificially generated data.
Answer: substantially improved
True answer: Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. 

Validation ID: 9a9d225f9ac35ed35ea02f554f6056af3b42471d
Question: What textual patterns are extracted?
From the context "The original and corrected sentences in the corpus are aligned and used to identify short transformation patterns in the form of (incorrect phrase, correct phrase). The length of each pattern is the affected phrase, plus up to one token of context on both sides. If a word form changes between the incorrect and correct text, it is fully saved in the pattern, otherwise the POS tags are used for matching.", so we know that the textual patterns are extracted from the original and corrected sentences in the corpus.
Answer: the original and corrected sentences in the corpus
True answer: (VVD shop_VV0 II, VVD shopping_VVG II)

Validation ID: 9a9d225f9ac35ed35ea02f554f6056af3b42471d
Question: What textual patterns are extracted?
From the context "The original and corrected sentences in the corpus are aligned and used to identify short transformation patterns in the form of (incorrect phrase, correct phrase). The length of each pattern is the affected phrase, plus up to one token of context on both sides. If a word form changes between the incorrect and correct text, it is fully saved in the pattern, otherwise the POS tags are used for matching.", so we know that the textual patterns are extracted from the original and corrected sentences in the corpus.
Answer: the original and corrected sentences in the corpus
True answer: patterns for generating all types of errors

Validation ID: ea56148a8356a1918bedcf0a99ae667c27792cfe
Question: Which annotated corpus did they use?
From the context "We trained our error generation models on the public FCE training set BIBREF16 and used them to generate additional artificial training data. Grammatically correct text is needed as the starting point for inserting artificial errors, and we used two different sources: 1) the corrected version of the same FCE training set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens).", so we know that they used the corrected version of the same FCE training set on which the system is trained (450K tokens) and example sentences extracted from the English Vocabulary Profile (270K tokens).
Answer: the corrected version of the same FCE training set on which the system is trained (450K tokens) and example sentences extracted from the English Vocabulary Profile (270K tokens)
True answer:  FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) 

Validation ID: ea56148a8356a1918bedcf0a99ae667c27792cfe
Question: Which annotated corpus did they use?
From the context "We trained our error generation models on the public FCE training set BIBREF16 and used them to generate additional artificial training data. Grammatically correct text is needed as the starting point for inserting artificial errors, and we used two different sources: 1) the corrected version of the same FCE training set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens).", so we know that they used the corrected version of the same FCE training set on which the system is trained (450K tokens) and example sentences extracted from the English Vocabulary Profile (270K tokens).
Answer: the corrected version of the same FCE training set on which the system is trained (450K tokens) and example sentences extracted from the English Vocabulary Profile (270K tokens)
True answer: FCE ,  two alternative annotations of the CoNLL 2014 Shared Task dataset

Validation ID: cd32a38e0f33b137ab590e1677e8fb073724df7f
Question: Which languages are explored in this paper?
From the context "We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) BIBREF3. Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for training an error detection system.", so we know that the languages explored in this paper are FCE and CoNLL 2014.
Answer: FCE and CoNLL 2014
True answer: English 

Validation ID: cd32a38e0f33b137ab590e1677e8fb073724df7f
Question: Which languages are explored in this paper?
From the context "We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) BIBREF3. Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for training an error detection system.", so we know that the languages explored in this paper are FCE and CoNLL 2014.
Answer: FCE and CoNLL 2014
True answer: English 

Validation ID: 2c6b50877133a499502feb79a682f4023ddab63e
Question: what language does this paper focus on?
From the context "We evaluate the performance of text simplification using neural machine translation on available parallel sentences and additional simplified sentences", so we know that this paper focus on text simplification.
Answer: text simplification
True answer: English

Validation ID: 2c6b50877133a499502feb79a682f4023ddab63e
Question: what language does this paper focus on?
From the context "We evaluate the performance of text simplification using neural machine translation on available parallel sentences and additional simplified sentences", so we know that this paper focus on text simplification.
Answer: text simplification
True answer: Simple English

Validation ID: f651cd144b7749e82aa1374779700812f64c8799
Question: what evaluation metrics did they use?
From the context "We evaluate the performance of text simplification using neural machine translation on available parallel sentences and additional simplified sentences.
Dataset. We use two simplification datasets (WikiSmall and WikiLarge). WikiSmall consists of ordinary and simplified sentences from the ordinary and simple English Wikipedias, which has been used as benchmark for evaluating text simplification BIBREF17, BIBREF18, BIBREF8. The training set has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge is also from Wikipedia corpus whose training set contains 296,402 sentence pairs BIBREF19, BIBREF20. WikiLarge includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing.
True answer: BLEU , FKGL , SARI 

Validation ID: f651cd144b7749e82aa1374779700812f64c8799
Question: what evaluation metrics did they use?
From the context "We evaluate the performance of text simplification using neural machine translation on available parallel sentences and additional simplified sentences.
Dataset. We use two simplification datasets (WikiSmall and WikiLarge). WikiSmall consists of ordinary and simplified sentences from the ordinary and simple English Wikipedias, which has been used as benchmark for evaluating text simplification BIBREF17, BIBREF18, BIBREF8. The training set has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge is also from Wikipedia corpus whose training set contains 296,402 sentence pairs BIBREF19, BIBREF20. WikiLarge includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing.
True answer: BLEU, FKGL, SARI, Simplicity

Validation ID: 4625cfba3083346a96e573af5464bc26c34ec943
Question: by how much did their model improve?
From the context "In the experiments on two datasets, we achieve substantial gains in all tasks, and new SOTA results, via back-translation of simplified sentences into the ordinary sentences, and treating this synthetic data as additional training data", so we know that their model improved by substantial gains.
Answer: substantial gains
True answer: For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.
For the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.

Validation ID: 4625cfba3083346a96e573af5464bc26c34ec943
Question: by how much did their model improve?
From the context "In the experiments on two datasets, we achieve substantial gains in all tasks, and new SOTA results, via back-translation of simplified sentences into the ordinary sentences, and treating this synthetic data as additional training data", so we know that their model improved by substantial gains.
Answer: substantial gains
True answer: 6.37 BLEU

Validation ID: 326588b1de9ba0fd049ab37c907e6e5413e14acd
Question: what state of the art methods did they compare with?
From the context "We also substantially outperform Dress, who previously reported SOTA result", so we know that they compared with Dress.
Answer: Dress
True answer: OpenNMT, PBMT-R, Hybrid, SBMT-SARI, Dress

Validation ID: ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb
Question: what are the sizes of both datasets?
From the context "We use two simplification datasets (WikiSmall and WikiLarge). WikiSmall consists of ordinary and simplified sentences from the ordinary and simple English Wikipedias, which has been used as benchmark for evaluating text simplification BIBREF17, BIBREF18, BIBREF8. The training set has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge is also from Wikipedia corpus whose training set contains 296,402 sentence pairs BIBREF19, BIBREF20. WikiLarge includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing.", so we know that the sizes of both datasets are 89,042 sentence pairs and 296,402 sentence pairs.
Answer: 89,042 sentence pairs and 296,402 sentence pairs
True answer: training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing

Validation ID: ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb
Question: what are the sizes of both datasets?
From the context "We use two simplification datasets (WikiSmall and WikiLarge). WikiSmall consists of ordinary and simplified sentences from the ordinary and simple English Wikipedias, which has been used as benchmark for evaluating text simplification BIBREF17, BIBREF18, BIBREF8. The training set has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge is also from Wikipedia corpus whose training set contains 296,402 sentence pairs BIBREF19, BIBREF20. WikiLarge includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing.", so we know that the sizes of both datasets are 89,042 sentence pairs and 296,402 sentence pairs.
Answer: 89,042 sentence pairs and 296,402 sentence pairs
True answer: WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. 

Validation ID: 55507f066073b29c1736b684c09c045064053ba9
Question: What are the distinctive characteristics of how Arabic speakers use offensive language?
From the context "Figure FIGREF13 shows the distribution of topics associated with offensive tweets. As the figure shows, sports and politics are most dominant for offensive language including vulgar and hate speech. As for dialect, we looked at MSA and four major dialects, namely Egyptian (EGY), Leventine (LEV), Maghrebi (MGR), and Gulf (GLF). Figure FIGREF14 shows that 71% of vulgar tweets were written in EGY followed by GLF, which accounted for 13% of vulgar tweets. MSA was not used in any of the vulgar tweets. As for offensive tweets in general, EGY and GLF were used in 36% and 35% of the offensive tweets respectively. Unlike the case of vulgar language where MSA was non-existent, 15% of the offensive tweets were in fact written in MSA. For hate speech, GLF and EGY were again dominant and MSA consistuted 21% of the tweets. This is consistent with findings for other languages such as English and Italian where vulgar language was more frequently associated with colloquial language BIBREF24, BIBREF25. Regarding the gender, Figure FIGREF15 shows that the vast majority of offensive tweets, including vulgar and hate speech, were authored by males. Female Twitter users accounted for 14% of offensive tweets in general and 6% and 9% of vulgar and hate speech respectively. Figure FIGREF16 shows a detailed categorization of hate speech types, where the top three include insulting groups based on their political ideology, origin, and sport affiliation. Religious hate speech appeared in only 15% of all hate speech tweets.", so we know that the distinctive characteristics of how Arabic speakers use offensive language are:

True answer: Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.

Validation ID: 55507f066073b29c1736b684c09c045064053ba9
Question: What are the distinctive characteristics of how Arabic speakers use offensive language?
From the context "Figure FIGREF13 shows the distribution of topics associated with offensive tweets. As the figure shows, sports and politics are most dominant for offensive language including vulgar and hate speech. As for dialect, we looked at MSA and four major dialects, namely Egyptian (EGY), Leventine (LEV), Maghrebi (MGR), and Gulf (GLF). Figure FIGREF14 shows that 71% of vulgar tweets were written in EGY followed by GLF, which accounted for 13% of vulgar tweets. MSA was not used in any of the vulgar tweets. As for offensive tweets in general, EGY and GLF were used in 36% and 35% of the offensive tweets respectively. Unlike the case of vulgar language where MSA was non-existent, 15% of the offensive tweets were in fact written in MSA. For hate speech, GLF and EGY were again dominant and MSA consistuted 21% of the tweets. This is consistent with findings for other languages such as English and Italian where vulgar language was more frequently associated with colloquial language BIBREF24, BIBREF25. Regarding the gender, Figure FIGREF15 shows that the vast majority of offensive tweets, including vulgar and hate speech, were authored by males. Female Twitter users accounted for 14% of offensive tweets in general and 6% and 9% of vulgar and hate speech respectively. Figure FIGREF16 shows a detailed categorization of hate speech types, where the top three include insulting groups based on their political ideology, origin, and sport affiliation. Religious hate speech appeared in only 15% of all hate speech tweets.", so we know that the distinctive characteristics of how Arabic speakers use offensive language are:

True answer: Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related

Validation ID: e838275bb0673fba0d67ac00e4307944a2c17be3
Question: How did they analyze which topics, dialects and gender are most associated with tweets?
From the context "Figure FIGREF13 shows the distribution of topics associated with offensive tweets. As the figure shows, sports and politics are most dominant for offensive language including vulgar and hate speech. As for dialect, we looked at MSA and four major dialects, namely Egyptian (EGY), Leventine (LEV), Maghrebi (MGR), and Gulf (GLF). Figure FIGREF14 shows that 71% of vulgar tweets were written in EGY followed by GLF, which accounted for 13% of vulgar tweets. MSA was not used in any of the vulgar tweets. As for offensive tweets in general, EGY and GLF were used in 36% and 35% of the offensive tweets respectively. Unlike the case of vulgar language where MSA was non-existent, 15% of the offensive tweets were in fact written in MSA. For hate speech, GLF and EGY were again dominant and MSA consistuted 21% of the tweets. This is consistent with findings for other languages such as English and Italian where vulgar language was more frequently associated with colloquial language BIBREF24, BIBREF25. Regarding the gender, Figure FIGREF15 shows that the vast majority of offensive tweets, including vulgar and hate speech, were authored by males. Female Twitter users accounted for 14% of offensive tweets in general and 6% and 9% of vulgar and hate speech respectively. Figure FIGREF16 shows a detailed categorization of hate speech types, where the top three include insulting groups based on their political ideology, origin, and sport affiliation. Religious hate speech appeared in only 15% of all hate speech tweets.", so we know that the topics, dialects and gender are most associated with tweets.
Answer: topics, dialects and gender
True answer: ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language

Validation ID: 8dda1ef371933811e2a25a286529c31623cca0c6
Question: How many annotators tagged each tweet?
From the context "We calculated the Inter-Annotator Agreement between the annotators using Fleiss’s Kappa coefficient BIBREF23. The Kappa score was 0.92 indicating high quality annotation and agreement.", so we know that there are 4 annotators.
Answer: 4
True answer: One

Validation ID: 8dda1ef371933811e2a25a286529c31623cca0c6
Question: How many annotators tagged each tweet?
From the context "We calculated the Inter-Annotator Agreement between the annotators using Fleiss’s Kappa coefficient BIBREF23. The Kappa score was 0.92 indicating high quality annotation and agreement.", so we know that there are 4 annotators.
Answer: 4
True answer: One experienced annotator tagged all tweets

Validation ID: b3de9357c569fb1454be8f2ac5fcecaea295b967
Question: How many tweets are in the dataset?
From the context "We collected 660k Arabic tweets having this pattern between April 15, 2019 and May 6, 2019. To increase diversity, we sorted the word sequences between the vocative particles and took the most frequent 10,000 unique sequences. For each word sequence, we took a random tweet containing each sequence. Then we annotated those tweets, ending up with 1,915 offensive tweets which represent roughly 19% of all tweets.", so we know that there are 10,000 tweets in the dataset.
Answer: 10,000
True answer: 10,000 Arabic tweet dataset 

Validation ID: b3de9357c569fb1454be8f2ac5fcecaea295b967
Question: How many tweets are in the dataset?
From the context "We collected 660k Arabic tweets having this pattern between April 15, 2019 and May 6, 2019. To increase diversity, we sorted the word sequences between the vocative particles and took the most frequent 10,000 unique sequences. For each word sequence, we took a random tweet containing each sequence. Then we annotated those tweets, ending up with 1,915 offensive tweets which represent roughly 19% of all tweets.", so we know that there are 10,000 tweets in the dataset.
Answer: 10,000
True answer: 10,000

Validation ID: 59e58c6fc63cf5b54b632462465bfbd85b1bf3dd
Question: In what way is the offensive dataset not biased by topic, dialect or target?
From the context "Our target was to build a large Arabic offensive language dataset that is representative of their appearance on Twitter and is hopefully not biased to specific dialects, topics, or targets. One of the main challenges is that offensive tweets constitute a very small portion of overall tweets. To quantify their proportion, we took 3 random samples of tweets from different days, with each sample composed of 1,000 tweets, and we found that between 1% and 2% of them were in fact offensive (including pornographic advertisement). This percentage is consistent with previously reported percentages BIBREF19. Thus, annotating random tweets is grossly inefficient. One way to overcome this problem is to use a seed list of offensive words to filter tweets. However, doing so is problematic as it would skew the dataset to particular types of offensive language or to specific dialects. Offensiveness is often dialect and country specific.", so we know that the offensive dataset is not biased by topic, dialect or target.
Answer: The offensive dataset is not biased by topic, dialect or target.
True answer: It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets.

Validation ID: 59e58c6fc63cf5b54b632462465bfbd85b1bf3dd
Question: In what way is the offensive dataset not biased by topic, dialect or target?
From the context "Our target was to build a large Arabic offensive language dataset that is representative of their appearance on Twitter and is hopefully not biased to specific dialects, topics, or targets. One of the main challenges is that offensive tweets constitute a very small portion of overall tweets. To quantify their proportion, we took 3 random samples of tweets from different days, with each sample composed of 1,000 tweets, and we found that between 1% and 2% of them were in fact offensive (including pornographic advertisement). This percentage is consistent with previously reported percentages BIBREF19. Thus, annotating random tweets is grossly inefficient. One way to overcome this problem is to use a seed list of offensive words to filter tweets. However, doing so is problematic as it would skew the dataset to particular types of offensive language or to specific dialects. Offensiveness is often dialect and country specific.", so we know that the offensive dataset is not biased by topic, dialect or target.
Answer: The offensive dataset is not biased by topic, dialect or target.
True answer: our methodology does not use a seed list of offensive words

Validation ID: 5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83
Question: What experiments are conducted?
From the context "Experimental results demonstrate that our model outperforms other generative models and our rewards are effective. Although our model design is effective, there are still many errors and we systematically analyze them. In the future, we are interested in exploring these directions and our work may extend to other kinds of ironies which are more difficult to model.", so we know that the experiments are conducted to demonstrate that our model outperforms other generative models and our rewards are effective.
Answer: Our model outperforms other generative models and our rewards are effective.
True answer: Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences

Validation ID: 3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f
Question: What is the combination of rewards for reinforcement learning?
From the context "In order to control irony accuracy, sentiment preservation and content preservation at the same time, we also design a combination of rewards for reinforcement learning and incorporate reinforcement learning with a pre-training process", so we know that the combination of rewards for reinforcement learning is irony reward and sentiment reward.
Answer: irony reward and sentiment reward
True answer: irony accuracy, sentiment preservation

Validation ID: 3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f
Question: What is the combination of rewards for reinforcement learning?
From the context "In order to control irony accuracy, sentiment preservation and content preservation at the same time, we also design a combination of rewards for reinforcement learning and incorporate reinforcement learning with a pre-training process", so we know that the combination of rewards for reinforcement learning is irony reward and sentiment reward.
Answer: irony reward and sentiment reward
True answer:  irony accuracy and sentiment preservation

Validation ID: 14b8ae5656e7d4ee02237288372d9e682b24fdb8
Question: What are the difficulties in modelling the ironic pattern?
From the context "As ironies in the latter two categories are obscure and hard to understand, we decide to only focus on ironies in the first category in this work", so we know that ironies in the latter two categories are obscure and hard to understand.
Answer: Ironies in the latter two categories are obscure and hard to understand.
True answer: obscure and hard to understand,  lack of previous work and baselines on irony generation

Validation ID: 14b8ae5656e7d4ee02237288372d9e682b24fdb8
Question: What are the difficulties in modelling the ironic pattern?
From the context "As ironies in the latter two categories are obscure and hard to understand, we decide to only focus on ironies in the first category in this work", so we know that ironies in the latter two categories are obscure and hard to understand.
Answer: Ironies in the latter two categories are obscure and hard to understand.
True answer: ironies are often obscure and hard to understand

Validation ID: e3a2d8886f03e78ed5e138df870f48635875727e
Question: How did the authors find ironic data on twitter?
From the context "In order to build up our language model and preserve the content, we apply the auto-encoder model. To prevent the model from simply copying the input sentence, we randomly add some noises in the input sentence. Specifically, for every word in the input sentence, there is 10% chance that we delete it, 10 % chance that we duplicate it, 10% chance that we swap it with the next word, or it remains unchanged. We first encode the input sentence INLINEFORM0 or INLINEFORM1 with respective encoder INLINEFORM2 or INLINEFORM3 to obtain its latent representation INLINEFORM4 or INLINEFORM5 and reconstruct the input sentence with the latent representation and respective decoder. So we can get the reconstruction loss for auto-encoder INLINEFORM6 : DISPLAYFORM0 DISPLAYFORM1
In addition to denoising auto-encoder, we implement back-translation BIBREF19 to generate a pseudo-parallel corpus. Suppose our model takes non-ironic sentence INLINEFORM0 as input. We first encode INLINEFORM1 with INLINEFORM2 to obtain its latent representation INLINEFORM3 and decode the latent representation with INLINEFORM4 to get a transferred sentence INLINEFORM5. Then we encode INLINEFORM6 with INLINEFORM7 and decode its latent representation with INLINEFORM8 to reconstruct the original input sentence INLINEFORM9. Therefore, our reconstruction loss for back-translation INLINEFORM10 : DISPLAYFORM0
True answer: They developed a classifier to find ironic sentences in twitter data

Validation ID: e3a2d8886f03e78ed5e138df870f48635875727e
Question: How did the authors find ironic data on twitter?
From the context "In order to build up our language model and preserve the content, we apply the auto-encoder model. To prevent the model from simply copying the input sentence, we randomly add some noises in the input sentence. Specifically, for every word in the input sentence, there is 10% chance that we delete it, 10 % chance that we duplicate it, 10% chance that we swap it with the next word, or it remains unchanged. We first encode the input sentence INLINEFORM0 or INLINEFORM1 with respective encoder INLINEFORM2 or INLINEFORM3 to obtain its latent representation INLINEFORM4 or INLINEFORM5 and reconstruct the input sentence with the latent representation and respective decoder. So we can get the reconstruction loss for auto-encoder INLINEFORM6 : DISPLAYFORM0 DISPLAYFORM1
In addition to denoising auto-encoder, we implement back-translation BIBREF19 to generate a pseudo-parallel corpus. Suppose our model takes non-ironic sentence INLINEFORM0 as input. We first encode INLINEFORM1 with INLINEFORM2 to obtain its latent representation INLINEFORM3 and decode the latent representation with INLINEFORM4 to get a transferred sentence INLINEFORM5. Then we encode INLINEFORM6 with INLINEFORM7 and decode its latent representation with INLINEFORM8 to reconstruct the original input sentence INLINEFORM9. Therefore, our reconstruction loss for back-translation INLINEFORM10 : DISPLAYFORM0
True answer: by crawling

Validation ID: 62f27fe08ddb67f16857fab2a8a721926ecbb6fb
Question: Who judged the irony accuracy, sentiment preservation and content preservation?
From the context "Experimental results demonstrate that our model outperforms other generative models and our rewards are effective", so we know that the model outperforms other generative models and our rewards are effective.
Answer: The model outperforms other generative models and our rewards are effective.
True answer: Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).

Validation ID: 62f27fe08ddb67f16857fab2a8a721926ecbb6fb
Question: Who judged the irony accuracy, sentiment preservation and content preservation?
From the context "Experimental results demonstrate that our model outperforms other generative models and our rewards are effective", so we know that the model outperforms other generative models and our rewards are effective.
Answer: The model outperforms other generative models and our rewards are effective.
True answer: four annotators who are proficient in English

Validation ID: 9ca447c8959a693a3f7bdd0a2c516f4b86f95718
Question: How were the tweets annotated?
From the context "We have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2. Hence, our data set is a balanced one although it is currently limited in size.", so we know that the tweets were annotated as Favor or Against.
Answer: Favor or Against
True answer: tweets are annotated with only Favor or Against for two targets - Galatasaray and Fenerbahçe

Validation ID: 9ca447c8959a693a3f7bdd0a2c516f4b86f95718
Question: How were the tweets annotated?
From the context "We have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2. Hence, our data set is a balanced one although it is currently limited in size.", so we know that the tweets were annotated as Favor or Against.
Answer: Favor or Against
True answer: Unanswerable

Validation ID: 05887a8466e0a2f0df4d6a5ffc5815acd7d9066a
Question: Which SVM approach resulted in the best performance?
From the context "To sum up, our evaluation results are significant as reference results to be used for comparison purposes and provides evidence for the utility of unigram-based and hashtag-related features in SVM classifiers for the stance detection problem in Turkish tweets.", so we know that the SVM approach using unigram-based and hashtag-related features resulted in the best performance.
Answer: SVM approach using unigram-based and hashtag-related features
True answer: Target-1

Validation ID: c87fcc98625e82fdb494ff0f5309319620d69040
Question: What are hashtag features?
From the context "With an intention to exploit the contribution of hashtag use to stance detection, we have also used the existence of hashtags in tweets as an additional feature to unigrams", so we know that hashtag features are the existence of hashtags in tweets.
Answer: the existence of hashtags in tweets
True answer: hashtag features contain whether there is any hashtag in the tweet

Validation ID: c87fcc98625e82fdb494ff0f5309319620d69040
Question: What are hashtag features?
From the context "With an intention to exploit the contribution of hashtag use to stance detection, we have also used the existence of hashtags in tweets as an additional feature to unigrams", so we know that hashtag features are the existence of hashtags in tweets.
Answer: the existence of hashtags in tweets
True answer: Unanswerable

Validation ID: 500a8ec1c56502529d6e59ba6424331f797f31f0
Question: How many tweets did they collect?
From the context "In a previous study on the identification of public health-related tweets, two tweet data sets in Turkish (each set containing 1 million random tweets) have been compiled where these sets belong to two different periods of 20 consecutive days BIBREF11. We have decided to use one of these sets (corresponding to the period between August 18 and September 6, 2015) and firstly filtered the tweets using the possible names used to refer to the target clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against. Within the course of this study, we have not considered those tweets in which the target is not explicitly mentioned, as our initial filtering process reveals.", so we know that they collected 1 million tweets.
Answer: 1 million
True answer: 700 

Validation ID: 500a8ec1c56502529d6e59ba6424331f797f31f0
Question: How many tweets did they collect?
From the context "In a previous study on the identification of public health-related tweets, two tweet data sets in Turkish (each set containing 1 million random tweets) have been compiled where these sets belong to two different periods of 20 consecutive days BIBREF11. We have decided to use one of these sets (corresponding to the period between August 18 and September 6, 2015) and firstly filtered the tweets using the possible names used to refer to the target clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against. Within the course of this study, we have not considered those tweets in which the target is not explicitly mentioned, as our initial filtering process reveals.", so we know that they collected 1 million tweets.
Answer: 1 million
True answer: 700

Validation ID: ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc
Question: Which sports clubs are the targets?
From the context "In this study, we present a stance-annotated tweet data set in Turkish where the targets of the annotated stances are two popular sports clubs in Turkey", so we know that the targets are two popular sports clubs in Turkey.
Answer: two popular sports clubs in Turkey
True answer: Galatasaray, Fenerbahçe

Validation ID: ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc
Question: Which sports clubs are the targets?
From the context "We have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey.", so we know that Galatasaray and Fenerbahçe are the targets.
Answer: Galatasaray and Fenerbahçe
True answer: Galatasaray , Fenerbahçe 

Validation ID: f2155dc4aeab86bf31a838c8ff388c85440fce6e
Question: Does this method help in sentiment classification task improvement?
From the context "On sentiment classification, chunk features are slightly harmful on average (but variance is high); mSynC again performs similarly to ELMo-transformer", so we know that this method does not help in sentiment classification task improvement.
Answer: No
True answer: Yes

Validation ID: f2155dc4aeab86bf31a838c8ff388c85440fce6e
Question: Does this method help in sentiment classification task improvement?
From the context "On sentiment classification, chunk features are slightly harmful on average (but variance is high); mSynC again performs similarly to ELMo-transformer", so we know that this method does not help in sentiment classification task improvement.
Answer: No
True answer: No

Validation ID: ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc
Question: For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding?
From the context "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks.", so we know that the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding for 3 probe tasks.
Answer: 3
True answer: performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks

Validation ID: ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc
Question: For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding?
From the context "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks.", so we know that the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding for 3 probe tasks.
Answer: 3
True answer: 3

Validation ID: 4d706ce5bde82caf40241f5b78338ea5ee5eb01e
Question: What are the black-box probes used?
From the context "We further analyze whether awareness of shallow syntax carries over to other linguistic tasks, via probes from BIBREF1. Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.", so we know that the black-box probes used are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.
Answer: linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases
True answer: CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,
Chunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection

Validation ID: 4d706ce5bde82caf40241f5b78338ea5ee5eb01e
Question: What are the black-box probes used?
From the context "We further analyze whether awareness of shallow syntax carries over to other linguistic tasks, via probes from BIBREF1. Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.", so we know that the black-box probes used are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.
Answer: linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases
True answer: Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.

Validation ID: 86bf75245358f17e35fc133e46a92439ac86d472
Question: What are improvements for these two approaches relative to ELMo-only baselines?
From the context "We observe only modest gains on three of the four downstream tasks relative to ELMo-only baselines", so we know that the improvements for these two approaches relative to ELMo-only baselines are modest.
Answer: modest
True answer: only modest gains on three of the four downstream tasks

Validation ID: 86bf75245358f17e35fc133e46a92439ac86d472
Question: What are improvements for these two approaches relative to ELMo-only baselines?
From the context "We observe only modest gains on three of the four downstream tasks relative to ELMo-only baselines", so we know that the improvements for these two approaches relative to ELMo-only baselines are modest.
Answer: modest
True answer:  the performance differences across all tasks are small enough 

Validation ID: 9132d56e26844dc13b3355448d0f14b95bd2178a
Question: Which syntactic features are obtained automatically on downstream task data?
From the context "Our second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels.", so we know that chunk boundary information is obtained automatically on downstream task data.
Answer: chunk boundary information
True answer: token-level chunk label embeddings,  chunk boundary information is passed into the task model via BIOUL encoding of the labels

Validation ID: f3c204723da53c7c8ef4dc1018ffbee545e81056
Question: Do they report results only on English data?
From the context "We would like to thank anonymous reviewers for their valuable comments and helpful suggestions. This work was funded by the National Key Research and Development Program of China (2016YFC1306704), the National Natural Science Foundation of China (61772132), the Natural Science Foundation of Jiangsu Province of China (BK20161430).", so we know that the paper is written in English.
Answer: Yes
True answer: Unanswerable

Validation ID: f3c204723da53c7c8ef4dc1018ffbee545e81056
Question: Do they report results only on English data?
From the context "We would like to thank anonymous reviewers for their valuable comments and helpful suggestions. This work was funded by the National Key Research and Development Program of China (2016YFC1306704), the National Natural Science Foundation of China (61772132), the Natural Science Foundation of Jiangsu Province of China (BK20161430).", so we know that the paper is written in English.
Answer: Yes
True answer: Unanswerable

Validation ID: 0602a974a879e6eae223cdf048410b5a0111665e
Question: What baseline approaches does this approach out-perform?
From the context "Table TABREF35 shows the event extraction results on the three datasets. The statistics are obtained with the default parameter setting that INLINEFORM0 is set to 5, number of hidden units INLINEFORM1 is set to 200, and INLINEFORM2 contains three fully-connected layers. The event number INLINEFORM3 for three datasets are set to 25, 25 and 35, respectively. The examples of extracted events are shown in Table. TABREF36.", so we know that the baseline approaches are K-means, LEM and DPEMM.
From the context "It can be observed that K-means performs the worst over all three datasets. On the social media datasets, AEM outpoerforms both LEM and DPEMM by 6.5% and 1.7% respectively in F-measure on the FSD dataset, and 4.4% and 3.7% in F-measure on the Twitter dataset. We can also observe that apart from K-means, all the approaches perform worse on the Twitter dataset compared to FSD, possibly due to the limited size of the Twitter dataset. Moreover, on the Google dataset, the proposed AEM performs significantly better than LEM and DPEMM. It improves upon LEM by 15.5% and upon DPEMM by more than 30% in F-measure. This is because: (1) the assumption made by LEM and DPEMM that all words in a document are generated from a single event is not suitable for long text such as news articles; (2) DPEMM generates too many irrelevant events which leads to a very low precision score. Overall, we see the superior performance of AEM across all datasets, with more significant improvement on the for Google datasets (long text).", so we know that AEM out-performs K-means, LEM and DPEMM.
True answer: K-means, LEM BIBREF13, DPEMM BIBREF14

Validation ID: 0602a974a879e6eae223cdf048410b5a0111665e
Question: What baseline approaches does this approach out-perform?
From the context "Table TABREF35 shows the event extraction results on the three datasets. The statistics are obtained with the default parameter setting that INLINEFORM0 is set to 5, number of hidden units INLINEFORM1 is set to 200, and INLINEFORM2 contains three fully-connected layers. The event number INLINEFORM3 for three datasets are set to 25, 25 and 35, respectively. The examples of extracted events are shown in Table. TABREF36.", so we know that the baseline approaches are K-means, LEM and DPEMM.
From the context "It can be observed that K-means performs the worst over all three datasets. On the social media datasets, AEM outpoerforms both LEM and DPEMM by 6.5% and 1.7% respectively in F-measure on the FSD dataset, and 4.4% and 3.7% in F-measure on the Twitter dataset. We can also observe that apart from K-means, all the approaches perform worse on the Twitter dataset compared to FSD, possibly due to the limited size of the Twitter dataset. Moreover, on the Google dataset, the proposed AEM performs significantly better than LEM and DPEMM. It improves upon LEM by 15.5% and upon DPEMM by more than 30% in F-measure. This is because: (1) the assumption made by LEM and DPEMM that all words in a document are generated from a single event is not suitable for long text such as news articles; (2) DPEMM generates too many irrelevant events which leads to a very low precision score. Overall, we see the superior performance of AEM across all datasets, with more significant improvement on the for Google datasets (long text).", so we know that AEM out-performs K-means, LEM and DPEMM.
True answer: K-means, LEM, DPEMM

Validation ID: 56b034c303983b2e276ed6518d6b080f7b8abe6a
Question: What datasets are used?
From the context "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12, Twitter, and Google datasets) are employed.", so we know that three datasets are used.
Answer: FSD BIBREF12, Twitter, and Google datasets
True answer: FSD BIBREF12 , Twitter, and Google datasets

Validation ID: 56b034c303983b2e276ed6518d6b080f7b8abe6a
Question: What datasets are used?
From the context "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12, Twitter, and Google datasets) are employed.", so we know that three datasets are used.
Answer: FSD BIBREF12, Twitter, and Google datasets
True answer: FSD dataset, Twitter dataset, Google dataset

Validation ID: 15e481e668114e4afe0c78eefb716ffe1646b494
Question: What alternative to Gibbs sampling is used?
From the context "To address the limitation that LEM requires the number of events to be pre-set, Zhou et al. zhou2017event further proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1. However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document. This is time consuming and takes long time to converge.", so we know that Gibbs sampling is used in LEM and DPEMM.
Answer: Gibbs sampling
True answer: generator network to capture the event-related patterns

Validation ID: 3d7a982c718ea6bc7e770d8c5da564fbb9d11951
Question: How does this model overcome the assumption that all words in a document are generated from a single event?
From the context "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", so we know that the generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).
Answer: The generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).
True answer: flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions, supervision signal provided by the discriminator will help generator to capture the event-related patterns

Validation ID: 3d7a982c718ea6bc7e770d8c5da564fbb9d11951
Question: How does this model overcome the assumption that all words in a document are generated from a single event?
From the context "To deal with these limitations, in this paper, we propose the Adversarial-neural Event Model (AEM) based on adversarial training for open-domain event extraction. The principle idea is to use a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution). Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. Furthermore, the discriminator also provides low-dimensional discriminative features which can be used to visualize documents and events.", so we know that the generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).
Answer: The generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).
True answer: by learning a projection function between the document-event distribution and four event related word distributions 

Validation ID: 692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1
Question: How many users do they look at?
From the context "We compile our industry-annotated dataset by identifying blogger profiles located in the U.S. on the profile finder on http://www.blogger.com, and scraping only those users that had the industry profile element completed.", so we know that they look at all the users that had the industry profile element completed.
Answer: all the users that had the industry profile element completed
True answer: 22,880 users

Validation ID: 692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1
Question: How many users do they look at?
From the context "We compile our industry-annotated dataset by identifying blogger profiles located in the U.S. on the profile finder on http://www.blogger.com, and scraping only those users that had the industry profile element completed.", so we know that they look at all the users that had the industry profile element completed.
Answer: all the users that had the industry profile element completed
True answer: 20,000

Validation ID: 935d6a6187e6a0c9c0da8e53a42697f853f5c248
Question: What do they mean by a person's industry?
From the context "In this paper, we examined the task of predicting a social media user's industry", so we know that a person's industry is the field that a person works in.
Answer: the field that a person works in
True answer: the aggregate of enterprises in a particular field

Validation ID: 935d6a6187e6a0c9c0da8e53a42697f853f5c248
Question: What do they mean by a person's industry?
From the context "In this paper, we examined the task of predicting a social media user's industry", so we know that a person's industry is the field that a person works in.
Answer: the field that a person works in
True answer: the aggregate of enterprises in a particular field

Validation ID: 3b77b4defc8a139992bd0b07b5cf718382cb1a5f
Question: What model did they use for their system?
From the context "We use the contextualized word embeddings proposed by Bamman et al. Bamman14, which are based on an extension of the “skip-gram" language model BIBREF28.", so we know that they used the contextualized word embeddings proposed by Bamman et al. Bamman14.
Answer: contextualized word embeddings
True answer: AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier

Validation ID: 01a41c0a4a7365cd37d28690735114f2ff5229f2
Question: What social media platform did they look at?
From the context "We compile our industry-annotated dataset by identifying blogger profiles located in the U.S. on the profile finder on http://www.blogger.com, and scraping only those users that had the industry profile element completed.", so we know that they looked at Blogger.
Answer: Blogger
True answer:  http://www.blogger.com

Validation ID: 01a41c0a4a7365cd37d28690735114f2ff5229f2
Question: What social media platform did they look at?
From the context "We compile our industry-annotated dataset by identifying blogger profiles located in the U.S. on the profile finder on http://www.blogger.com, and scraping only those users that had the industry profile element completed.", so we know that they looked at Blogger.
Answer: Blogger
True answer: http://www.blogger.com

Validation ID: cd2878c5a52542ddf080b20bec005d9a74f2d916
Question: What are the industry classes defined in this paper?
From the context "The final set of categories is shown in Table TABREF1, along with the number of users in each category", so we know that the industry classes defined in this paper are:

True answer: technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive

Validation ID: cd2878c5a52542ddf080b20bec005d9a74f2d916
Question: What are the industry classes defined in this paper?
From the context "The final set of categories is shown in Table TABREF1, along with the number of users in each category", so we know that the industry classes defined in this paper are:

True answer: Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.

Validation ID: fd2c6c26fd0ab3c10aae4f2550c5391576a77491
Question: Do they report results only on English data?
From the context "In this paper, we have shown that using the designed RNN based network we could increase the performance of classification dramatically. We showed that keeping the sequential nature of the data can be hugely beneficial when working with textual data especially faced with the hard task of detecting more complex phenomena like emotions. We accomplished that by using a recurrent network in the process of generating our hidden representation. We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation. In future we can focus on improving these representations for example by using attention networks BIBREF37, BIBREF38 to capture a more contextual representation or using language model based methods like BERT BIBREF39 that has been shown very successful in various NLP tasks.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: fd2c6c26fd0ab3c10aae4f2550c5391576a77491
Question: Do they report results only on English data?
From the context "In this paper, we have shown that using the designed RNN based network we could increase the performance of classification dramatically. We showed that keeping the sequential nature of the data can be hugely beneficial when working with textual data especially faced with the hard task of detecting more complex phenomena like emotions. We accomplished that by using a recurrent network in the process of generating our hidden representation. We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation. In future we can focus on improving these representations for example by using attention networks BIBREF37, BIBREF38 to capture a more contextual representation or using language model based methods like BERT BIBREF39 that has been shown very successful in various NLP tasks.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 6b6d498546f856ac20958f666fc3fd55811347e2
Question: What are the hyperparameters of the bi-GRU?
From the context "The architecture of the model can be seen in Figure FIGREF6. For training each classifier, a balanced dataset was created with selecting all tweets from the target set as class 1 and a random sample of the same size from other classes as class 0. For each classifier, 80% of the data was randomly selected as the training set, and 10% for the validation set, and 10% as the test set. As mentioned before we used the two embedding models, ConceptNet Numberbatch and fastText as the two more modern pre-trained word vector spaces to see how changing the embedding layer can affect the performance. The result of comparison among different embeddings can be seen in Table TABREF10. It can be seen that the best performance was divided between the two embedding models with minor performance variations.", so we know that the hyperparameters of the bi-GRU are:
- batch size: 250
True answer: They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%.

Validation ID: de3b1145cb4111ea2d4e113f816b537d052d9814
Question: What baseline is used?
From the context "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.", so we know that the baseline is CrowdFlower dataset annotated for emotions.
Answer: CrowdFlower dataset annotated for emotions
True answer:  Wang et al. BIBREF21, paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets

Validation ID: de3b1145cb4111ea2d4e113f816b537d052d9814
Question: What baseline is used?
From the context "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.", so we know that the baseline is CrowdFlower dataset annotated for emotions.
Answer: CrowdFlower dataset annotated for emotions
True answer: Wang et al. , maximum entropy classifier with bag of words model

Validation ID: 132f752169adf6dc5ade3e4ca773c11044985da4
Question: What data is used in experiments?
From the context "We then classified emotions using the pre-trained models and emotionally fitted fastText embedding. The result can be seen in Table TABREF19. The baseline results are from BIBREF33 done using BOW model and maximum entropy classifier. We saw a huge improvement from 26 point increase in F-measure for the emotion joy (happiness) up to 57 point increase for surprise with total average increase of 38.6 points. Bostan and Klinger did not report classification results for the emotion love so we did not include it in the average. These results show that our trained models perform exceptionally on a totally new dataset with a different method of annotation.", so we know that the data used in experiments is CrowdFlower emotional tweets dataset.
Answer: CrowdFlower emotional tweets dataset
True answer: Wang et al., CrowdFlower dataset 

Validation ID: 132f752169adf6dc5ade3e4ca773c11044985da4
Question: What data is used in experiments?
From the context "We then classified emotions using the pre-trained models and emotionally fitted fastText embedding. The result can be seen in Table TABREF19. The baseline results are from BIBREF33 done using BOW model and maximum entropy classifier. We saw a huge improvement from 26 point increase in F-measure for the emotion joy (happiness) up to 57 point increase for surprise with total average increase of 38.6 points. Bostan and Klinger did not report classification results for the emotion love so we did not include it in the average. These results show that our trained models perform exceptionally on a totally new dataset with a different method of annotation.", so we know that the data used in experiments is CrowdFlower emotional tweets dataset.
Answer: CrowdFlower emotional tweets dataset
True answer:  tweet dataset created by Wang et al. , CrowdFlower dataset

Validation ID: 1d9aeeaa6efa1367c22be0718f5a5635a73844bd
Question: What meaningful information does the GRU model capture, which traditional ML models do not?
From the context "We accomplished that by using a recurrent network in the process of generating our hidden representation. We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation.", so we know that the GRU model captures the sequential nature of the data and the text as a whole.
Answer: the sequential nature of the data and the text as a whole
True answer:  the context and sequential nature of the text

Validation ID: 1d9aeeaa6efa1367c22be0718f5a5635a73844bd
Question: What meaningful information does the GRU model capture, which traditional ML models do not?
From the context "We accomplished that by using a recurrent network in the process of generating our hidden representation. We have also used a max-pooling layer to capture the most relevant features and an average pooling layer to capture the text as a whole proving that we can achieve better performance by focusing on creating a more informative hidden representation.", so we know that the GRU model captures the sequential nature of the data and the text as a whole.
Answer: the sequential nature of the data and the text as a whole
True answer: information about the context and sequential nature of the text

Validation ID: 012b8a89aea27485797373adbcda32f16f9d7b54
Question: What is the approach of previous work?
From the context "In summary, LID of short texts, informal styles and similar languages remains a difficult problem which is actively being researched. Increased confusion can in general be expected between shorter pieces of text and languages that are more closely related. Shallow methods still seem to work well compared to deeper models for LID. Other remaining research opportunities seem to be data harvesting, building standardised datasets and creating shared tasks for South Africa and Africa. Support for language codes that include more languages seems to be growing and discoverability of research is improving with more survey papers coming out. Paywalls also seem to no longer be a problem; the references used in this paper was either openly published or available as preprint papers.", so we know that the approach of previous work is data harvesting, building standardised datasets and creating shared tasks for South Africa and Africa.
Answer: data harvesting, building standardised datasets and creating shared tasks for South Africa and Africa
True answer: 'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks

Validation ID: 012b8a89aea27485797373adbcda32f16f9d7b54
Question: What is the approach of previous work?
From the context "In summary, LID of short texts, informal styles and similar languages remains a difficult problem which is actively being researched. Increased confusion can in general be expected between shorter pieces of text and languages that are more closely related. Shallow methods still seem to work well compared to deeper models for LID. Other remaining research opportunities seem to be data harvesting, building standardised datasets and creating shared tasks for South Africa and Africa. Support for language codes that include more languages seems to be growing and discoverability of research is improving with more survey papers coming out. Paywalls also seem to no longer be a problem; the references used in this paper was either openly published or available as preprint papers.", so we know that the approach of previous work is data harvesting, building standardised datasets and creating shared tasks for South Africa and Africa.
Answer: data harvesting, building standardised datasets and creating shared tasks for South Africa and Africa
True answer: BIBREF11 that uses a character level n-gram language model, 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15, BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features, The winning approach for DSL 2015 used an ensemble naive Bayes classifier, The fasttext classifier BIBREF17, hierarchical stacked classifiers (including lexicons), bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24

Validation ID: c598028815066089cc1e131b96d6966d2610467a
Question: Is the lexicon the same for all languages?
From the context "The lexicon based classifier is then used to predict the specific language within a language group. For the South African languages this is done for the Nguni and Sotho groups. If the lexicon prediction of the specific language has high confidence then its result is used as the final label else the naive Bayesian classifier's specific language prediction is used as the final result. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", so we know that the lexicon is not the same for all languages.
Answer: No
True answer: Yes

Validation ID: c598028815066089cc1e131b96d6966d2610467a
Question: Is the lexicon the same for all languages?
From the context "The lexicon based classifier is then used to predict the specific language within a language group. For the South African languages this is done for the Nguni and Sotho groups. If the lexicon prediction of the specific language has high confidence then its result is used as the final label else the naive Bayesian classifier's specific language prediction is used as the final result. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", so we know that the lexicon is not the same for all languages.
Answer: No
True answer: Yes

Validation ID: ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed
Question: How do they obtain the lexicon?
From the context "The lexicon based classifier is then used to predict the specific language within a language group. For the South African languages this is done for the Nguni and Sotho groups. If the lexicon prediction of the specific language has high confidence then its result is used as the final label else the naive Bayesian classifier's specific language prediction is used as the final result. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", so we know that the lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
Answer: The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
True answer: Unanswerable

Validation ID: ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed
Question: How do they obtain the lexicon?
From the context "The lexicon based classifier is then used to predict the specific language within a language group. For the South African languages this is done for the Nguni and Sotho groups. If the lexicon prediction of the specific language has high confidence then its result is used as the final label else the naive Bayesian classifier's specific language prediction is used as the final result. The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.", so we know that the lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
Answer: The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.
True answer: built over all the data and therefore includes the vocabulary from both the training and testing sets

Validation ID: 0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50
Question: What evaluation metric is used?
From the context "The accuracies reported are for classifying a piece of text by its specific language label. Classifying text only by language group or family is a much easier task as reported in BIBREF8.", so we know that the evaluation metric is classification accuracy.
Answer: classification accuracy
True answer: average classification accuracy

Validation ID: 0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50
Question: What evaluation metric is used?
From the context "The accuracies reported are for classifying a piece of text by its specific language label. Classifying text only by language group or family is a much easier task as reported in BIBREF8.", so we know that the evaluation metric is classification accuracy.
Answer: classification accuracy
True answer: average classification accuracy, execution performance

Validation ID: 92dfacbbfa732ecea006e251be415a6f89fb4ec6
Question: Which languages are similar to each other?
From the context "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.", so we know that the Nguni languages are similar to each other and the Sotho languages are similar to each other.
Answer: Nguni languages and Sotho languages
True answer: Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)

Validation ID: 92dfacbbfa732ecea006e251be415a6f89fb4ec6
Question: Which languages are similar to each other?
From the context "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.", so we know that the Nguni languages are similar to each other and the Sotho languages are similar to each other.
Answer: Nguni languages and Sotho languages
True answer: The Nguni languages are similar to each other, The same is true of the Sotho languages

Validation ID: c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9
Question: Which datasets are employed for South African languages LID?
From the context "The NCHLT text corpora BIBREF7 is likely a good starting point for a shared LID task dataset for the South African languages BIBREF8. The NCHLT text corpora contains enough data to have 3500 training samples and 600 testing samples of 300+ character sentences per language.", so we know that the NCHLT text corpora is employed for South African languages LID.
Answer: NCHLT text corpora
True answer: DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora

Validation ID: 307e8ab37b67202fe22aedd9a98d9d06aaa169c5
Question: Does the paper report the performance of a baseline model on South African languages LID?
From the context "The stacked classifier is tested against three public LID implementations BIBREF17, BIBREF23, BIBREF8. The LID implementation described in BIBREF17 is available on GitHub and is trained and tested according to a post on the fasttext blog. Character (5-6)-gram features with 16 dimensional vectors worked the best. The implementation discussed in BIBREF23 is available from https://github.com/tomkocmi/LanideNN. Following the instructions for an OSX pip install of an old r0.8 release of TensorFlow, the LanideNN code could be executed in Python 3.7.4. Settings were left at their defaults and a learning rate of 0.001 was used followed by a refinement with learning rate of 0.0001. Only one code modification was applied to return the results from a method that previously just printed to screen. The LID algorithm described in BIBREF8 is also available on GitHub.", so we know that the paper reports the performance of a baseline model on South African languages LID.
Answer: Yes
True answer: Yes

Validation ID: 307e8ab37b67202fe22aedd9a98d9d06aaa169c5
Question: Does the paper report the performance of a baseline model on South African languages LID?
From the context "The stacked classifier is tested against three public LID implementations BIBREF17, BIBREF23, BIBREF8. The LID implementation described in BIBREF17 is available on GitHub and is trained and tested according to a post on the fasttext blog. Character (5-6)-gram features with 16 dimensional vectors worked the best. The implementation discussed in BIBREF23 is available from https://github.com/tomkocmi/LanideNN. Following the instructions for an OSX pip install of an old r0.8 release of TensorFlow, the LanideNN code could be executed in Python 3.7.4. Settings were left at their defaults and a learning rate of 0.001 was used followed by a refinement with learning rate of 0.0001. Only one code modification was applied to return the results from a method that previously just printed to screen. The LID algorithm described in BIBREF8 is also available on GitHub.", so we know that the paper reports the performance of a baseline model on South African languages LID.
Answer: Yes
True answer: Yes

Validation ID: 6415f38a06c2f99e8627e8ba6251aa4b364ade2d
Question: What are the languages represented in the DSL datasets??
From the context "The DSL datasets, like other LID datasets, consists of text sentences labelled by language. The 2017 dataset, for example, contains 14 languages over 6 language groups with 18000 training samples and 1000 testing samples per language.", so we know that the languages represented in the DSL datasets are 14 languages over 6 language groups.
Answer: 14 languages over 6 language groups
True answer: Unanswerable

Validation ID: e5c8e9e54e77960c8c26e8e238168a603fcdfcc6
Question: Does the algorithm improve on the state-of-the-art methods?
From the context "The accuracy of the proposed algorithm seems to be dependent on the support of the lexicon. Without a good lexicon a non-stacked naive Bayesian classifier might even perform better.", so we know that the algorithm does not improve on the state-of-the-art methods.
Answer: No
True answer: Yes

Validation ID: e5c8e9e54e77960c8c26e8e238168a603fcdfcc6
Question: Does the algorithm improve on the state-of-the-art methods?
From the context "The accuracy of the proposed algorithm seems to be dependent on the support of the lexicon. Without a good lexicon a non-stacked naive Bayesian classifier might even perform better.", so we know that the algorithm does not improve on the state-of-the-art methods.
Answer: No
True answer: From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.

Validation ID: 50be4a737dc0951b35d139f51075011095d77f2a
Question: What background knowledge do they leverage?
From the context "There have been much work that incorporate prior knowledge into learning, and two related lines are surveyed here. One is to use prior knowledge to label unlabeled instances and then apply a standard learning algorithm. The other is to constrain the model directly with prior knowledge.", so we know that they leverage prior knowledge to label unlabeled instances and then apply a standard learning algorithm.
Answer: prior knowledge to label unlabeled instances and then apply a standard learning algorithm
True answer: labeled features

Validation ID: 50be4a737dc0951b35d139f51075011095d77f2a
Question: What background knowledge do they leverage?
From the context "There have been much work that incorporate prior knowledge into learning, and two related lines are surveyed here. One is to use prior knowledge to label unlabeled instances and then apply a standard learning algorithm. The other is to constrain the model directly with prior knowledge.", so we know that they leverage prior knowledge to label unlabeled instances and then apply a standard learning algorithm.
Answer: prior knowledge to label unlabeled instances and then apply a standard learning algorithm
True answer: labelled features, which are words whose presence strongly indicates a specific class or topic

Validation ID: 6becff2967fe7c5256fe0b00231765be5b9db9f1
Question: What are the three regularization terms?
From the context "We propose three regularization terms on top of generalized expectation criteria", so we know that the three regularization terms are:
(1) a regularization term associated with neutral features;
True answer: a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution

Validation ID: 6becff2967fe7c5256fe0b00231765be5b9db9f1
Question: What are the three regularization terms?
From the context "We propose three regularization terms on top of generalized expectation criteria", so we know that the three regularization terms are:
(1) a regularization term associated with neutral features;
True answer: a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution

Validation ID: 76121e359dfe3f16c2a352bd35f28005f2a40da3
Question: What NLP tasks do they consider?
From the context "There have been much work that incorporate prior knowledge into learning, and two related lines are surveyed here. One is to use prior knowledge to label unlabeled instances and then apply a standard learning algorithm. The other is to constrain the model directly with prior knowledge.", so we know that they consider two NLP tasks:
1. Use prior knowledge to label unlabeled instances and then apply a standard learning algorithm.
True answer: text classification for themes including sentiment, web-page, science, medical and healthcare

Validation ID: 02428a8fec9788f6dc3a86b5d5f3aa679935678d
Question: How do they define robustness of a model?
From the context "We propose three regularization terms on top of generalized expectation criteria. As demonstrated by the experimental results, the performance can be considerably improved when taking into account these factors. Comparative results show that our proposed methods is more effective and works more robustly against baselines.", so we know that the robustness of a model is the performance of the model.
Answer: The performance of the model
True answer: ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced

Validation ID: 02428a8fec9788f6dc3a86b5d5f3aa679935678d
Question: How do they define robustness of a model?
From the context "We propose three regularization terms on top of generalized expectation criteria. As demonstrated by the experimental results, the performance can be considerably improved when taking into account these factors. Comparative results show that our proposed methods is more effective and works more robustly against baselines.", so we know that the robustness of a model is the performance of the model.
Answer: The performance of the model
True answer: Low sensitivity to bias in prior knowledge

Validation ID: 7793805982354947ea9fc742411bec314a6998f6
Question: Are the annotations automatic or manually created?
From the context "We annotated the dataset at two levels: Part of Speech (POS) and syntax. We performed the annotation with freely available tools for the Portuguese language. For POS we added a simple POS, that is, only type of word, and a fine-grained POS, which is the type of word plus its morphological features. We used the LX Parser BIBREF14, for the simple POS and the Portuguese morphological module of Freeling BIBREF15, for detailed POS. Concerning syntactic annotations, we included constituency and dependency annotations. For constituency parsing, we used the LX Parser, and for dependency, the DepPattern toolkit BIBREF16.", so we know that the annotations are automatic.
Answer: automatic
True answer: Automatic

Validation ID: 7793805982354947ea9fc742411bec314a6998f6
Question: Are the annotations automatic or manually created?
From the context "We annotated the dataset at two levels: Part of Speech (POS) and syntax. We performed the annotation with freely available tools for the Portuguese language. For POS we added a simple POS, that is, only type of word, and a fine-grained POS, which is the type of word plus its morphological features. We used the LX Parser BIBREF14, for the simple POS and the Portuguese morphological module of Freeling BIBREF15, for detailed POS. Concerning syntactic annotations, we included constituency and dependency annotations. For constituency parsing, we used the LX Parser, and for dependency, the DepPattern toolkit BIBREF16.", so we know that the annotations are automatic.
Answer: automatic
True answer: We performed the annotation with freely available tools for the Portuguese language.

Validation ID: 007b13f05d234d37966d1aa7d85b5fd78564ff45
Question: Do the errors of the model reflect linguistic similarity between different L1s?
From the context "The languages included in this experiment were Chinese (355 texts), English (236 texts), German (214 texts), Italian (216 texts), and Spanish (271 texts). We evaluated the model using stratified 10-fold cross-validation, achieving 70% accuracy.", so we know that the model achieved 70% accuracy.
From the context "An important limitation of this experiment is that it does not account for topic bias, an important issue in NLI BIBREF22. This is due to the fact that NLI-PT is not balanced by topic and the model could be learning topic associations instead.", so we know that the model could be learning topic associations instead.
True answer: Unanswerable

Validation ID: 2ceced87af4c8fdebf2dc959aa700a5c95bd518f
Question: Is the dataset balanced between speakers of different L1s?
From the context "The three corpora contain written productions from learners of Portuguese with different proficiency levels and native languages (L1s). In the dataset we included all the data in COPLE2 and sections of PEAPL2 and Leiria corpus.", so we know that the dataset is not balanced between speakers of different L1s.
Answer: No
True answer: No

Validation ID: 2ceced87af4c8fdebf2dc959aa700a5c95bd518f
Question: Is the dataset balanced between speakers of different L1s?
From the context "The three corpora contain written productions from learners of Portuguese with different proficiency levels and native languages (L1s). In the dataset we included all the data in COPLE2 and sections of PEAPL2 and Leiria corpus.", so we know that the dataset is not balanced between speakers of different L1s.
Answer: No
True answer: No

Validation ID: 72ed5fed07ace5e3ffe9de6c313625705bc8f0c7
Question: How long are the essays on average?
From the context "The longest text has 787 tokens and the shortest has only 16 tokens. Most texts, however, range roughly from 150 to 250 tokens.", so we know that the essays on average are roughly from 150 to 250 tokens.
Answer: 150 to 250 tokens
True answer: 204 tokens

Validation ID: 72ed5fed07ace5e3ffe9de6c313625705bc8f0c7
Question: How long are the essays on average?
From the context "The longest text has 787 tokens and the shortest has only 16 tokens. Most texts, however, range roughly from 150 to 250 tokens.", so we know that the essays on average are roughly from 150 to 250 tokens.
Answer: 150 to 250 tokens
True answer: Most texts, however, range roughly from 150 to 250 tokens.

Validation ID: 2e37e681942e28b5b05639baaff4cd5129adb5fb
Question: How large are the textual descriptions of entities?
From the context "In this paper, we use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper. Specifically, we use WN18 (a subset of WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available. Table 1 lists statistics of the two datasets.", so we know that the textual descriptions of entities are large.
Answer: large
True answer: Unanswerable

Validation ID: 2e37e681942e28b5b05639baaff4cd5129adb5fb
Question: How large are the textual descriptions of entities?
From the context "In this paper, we use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper. Specifically, we use WN18 (a subset of WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available. Table 1 lists statistics of the two datasets.", so we know that the textual descriptions of entities are large.
Answer: large
True answer: Unanswerable

Validation ID: b49598b05358117ab1471b8ebd0b042d2f04b2a4
Question: What neural models are used to encode the text?
From the context "We use three encoders (NBOW, LSTM and attentive LSTM) to model the text descriptions.", so we know that NBOW, LSTM and attentive LSTM are used to encode the text.
Answer: NBOW, LSTM and attentive LSTM
True answer: NBOW, LSTM, attentive LSTM

Validation ID: b49598b05358117ab1471b8ebd0b042d2f04b2a4
Question: What neural models are used to encode the text?
From the context "We use three encoders (NBOW, LSTM and attentive LSTM) to model the text descriptions.", so we know that NBOW, LSTM and attentive LSTM are used to encode the text.
Answer: NBOW, LSTM and attentive LSTM
True answer: neural bag-of-words (NBOW) model, bidirectional long short-term memory network (LSTM), attention-based encoder

Validation ID: 932b39fd6c47c6a880621a62e6a978491d881d60
Question: What baselines are used for comparison?
From the context "We compare our model with the following baselines: TransE BIBREF2, TransH BIBREF3, TransR BIBREF4, TransD BIBREF5, PTransE BIBREF6, RTransE BIBREF7, and KG2E BIBREF8.", so we know that the baselines are TransE, TransH, TransR, TransD, PTransE, RTransE and KG2E.
Answer: TransE, TransH, TransR, TransD, PTransE, RTransE and KG2E
True answer: TransE

Validation ID: 932b39fd6c47c6a880621a62e6a978491d881d60
Question: What baselines are used for comparison?
From the context "We compare our model with the following baselines: TransE BIBREF2, TransH BIBREF3, TransR BIBREF4, TransD BIBREF5, PTransE BIBREF6, RTransE BIBREF7, and KG2E BIBREF8.", so we know that the baselines are TransE, TransH, TransR, TransD, PTransE, RTransE and KG2E.
Answer: TransE, TransH, TransR, TransD, PTransE, RTransE and KG2E
True answer: TransE

Validation ID: b36f867fcda5ad62c46d23513369337352aa01d2
Question: What datasets are used to evaluate this paper?
From the context "We use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper. Specifically, we use WN18 (a subset of WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available.", so we know that the datasets used to evaluate this paper are WN18 and FB15K.
Answer: WN18 and FB15K
True answer: WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2

Validation ID: c6a0b9b5dabcefda0233320dd1548518a0ae758e
Question: Which approach out of two proposed in the paper performed better in experiments?
From the context "Results show CJFA (contextual joint factor analysis) encoder performs significantly better in both phone classification and speaker recognition tasks compared with other two approaches. The CJFS (contextual joint factor synthesis) encoder performs close to CJFA in speaker recognition task, but poorer for phone classification. Overall a gain of up to 3% relative on phone classification accuracy is observed, relative improvements on speaker recognition show 3–6% gain.", so we know that the CJFA encoder performs better in experiments.
Answer: CJFA encoder
True answer: CJFA encoder 

Validation ID: c6a0b9b5dabcefda0233320dd1548518a0ae758e
Question: Which approach out of two proposed in the paper performed better in experiments?
From the context "Results show CJFA (contextual joint factor analysis) encoder performs significantly better in both phone classification and speaker recognition tasks compared with other two approaches. The CJFS (contextual joint factor synthesis) encoder performs close to CJFA in speaker recognition task, but poorer for phone classification. Overall a gain of up to 3% relative on phone classification accuracy is observed, relative improvements on speaker recognition show 3–6% gain.", so we know that the CJFA encoder performs better in experiments.
Answer: CJFA encoder
True answer: CJFA encoder

Validation ID: 1e185a3b8cac1da939427b55bf1ba7e768c5dae4
Question: What classification baselines are used for comparison?
From the context "Table TABREF17 shows phone classification and speaker recognition results for the three model configurations: the VAE baseline, the CJFS encoder and the CJFA encoder", so we know that the classification baselines are VAE baseline, the CJFS encoder and the CJFA encoder.
Answer: VAE baseline, the CJFS encoder and the CJFA encoder
True answer: VAE

Validation ID: 1e185a3b8cac1da939427b55bf1ba7e768c5dae4
Question: What classification baselines are used for comparison?
From the context "Table TABREF17 shows phone classification and speaker recognition results for the three model configurations: the VAE baseline, the CJFS encoder and the CJFA encoder", so we know that the classification baselines are VAE baseline, the CJFS encoder and the CJFA encoder.
Answer: VAE baseline, the CJFS encoder and the CJFA encoder
True answer: VAE based phone classification

Validation ID: 26e2d4d0e482e6963a76760323b8e1c26b6eee91
Question: What TIMIT datasets are used for testing?
From the context "The TIMIT dataset contains speech from 462 speakers in training and 168 speakers in the test set, with 8 utterances for each speaker. The TIMIT training and test set are split into 8 blocks, where each block contains 2 utterances per speaker, randomly chosen. Thus each block A,B,C,D contains data from 462 speakers with 924 utterances taken from the training sets, and each block E,F,G,H contains speech from 168 test set speakers with 336 utterances.", so we know that the TIMIT datasets used for testing are block E,F,G,H.
Answer: block E,F,G,H
True answer: Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H

Validation ID: 26e2d4d0e482e6963a76760323b8e1c26b6eee91
Question: What TIMIT datasets are used for testing?
From the context "The TIMIT dataset contains speech from 462 speakers in training and 168 speakers in the test set, with 8 utterances for each speaker. The TIMIT training and test set are split into 8 blocks, where each block contains 2 utterances per speaker, randomly chosen. Thus each block A,B,C,D contains data from 462 speakers with 924 utterances taken from the training sets, and each block E,F,G,H contains speech from 168 test set speakers with 336 utterances.", so we know that the TIMIT datasets used for testing are block E,F,G,H.
Answer: block E,F,G,H
True answer:  this paper makes use of the official training and test sets, covering in total 630 speakers with 8 utterances each

Validation ID: b80a3fbeb49a8968e149955bdcf199556478eeff
Question: How does this approach compares to the state-of-the-art results on these tasks?
From the context "Results show CJFA (contextual joint factor analysis) encoder performs significantly better in both phone classification and speaker recognition tasks compared with other two approaches. The CJFS (contextual joint factor synthesis) encoder performs close to CJFA in speaker recognition task, but poorer for phone classification. Overall a gain of up to 3% relative on phone classification accuracy is observed, relative improvements on speaker recognition show 3–6% gain.", so we know that the CJFA encoder performs significantly better in both phone classification and speaker recognition tasks compared with other two approaches.
Answer: CJFA encoder performs significantly better in both phone classification and speaker recognition tasks compared with other two approaches.
True answer: Unanswerable

Validation ID: badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9
Question: What state-of-the-art results are achieved?
From the context "We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered. We can see that our model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location, compared to baselines that do not adopt joint learning in the first block. For location on heterographic puns, our model's performance is slightly lower than the system of BIBREF25, which is a rule-based locator.", so we know that the state-of-the-art results are achieved by the model with the INLINEFORM2 tagging scheme.
Answer: the model with the INLINEFORM3 tagging scheme
True answer: F1 score of 92.19 on homographic pun detection, 80.19 on homographic pun location, 89.76 on heterographic pun detection.

Validation ID: badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9
Question: What state-of-the-art results are achieved?
From the context "We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered. We can see that our model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location, compared to baselines that do not adopt joint learning in the first block. For location on heterographic puns, our model's performance is slightly lower than the system of BIBREF25, which is a rule-based locator. Compared to CRF, we can see that our model, either with the INLINEFORM2 or the INLINEFORM3 scheme, yields significantly higher recall on both detection and location tasks, while the precisions are relatively close. This demonstrates the effectiveness of BiLSTM, which learns the contextual features of given texts – such information appears to be helpful in recalling more puns.", so we know that the state-of-the-art results are achieved by the model with the INLINEFORM0 tagging scheme.
Answer: the model with the INLINEFORM0 tagging scheme
True answer: for the homographic dataset F1 score of 92.19 and 80.19 on detection and location and for the heterographic dataset F1 score of 89.76 on detection

Validation ID: 67b66fe67a3cb2ce043070513664203e564bdcbd
Question: What baselines do they compare with?
From the context "We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered.", so we know that the baselines they compare with are conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered.
Answer: conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered.
True answer: They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.

Validation ID: f56d07f73b31a9c72ea737b40103d7004ef6a079
Question: What datasets are used in evaluation?
From the context "We evaluate our model on two benchmark datasets BIBREF9. The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.", so we know that the datasets used in evaluation are homographic dataset and heterographic dataset.
Answer: homographic dataset and heterographic dataset
True answer: The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.

Validation ID: f56d07f73b31a9c72ea737b40103d7004ef6a079
Question: What datasets are used in evaluation?
From the context "We evaluate our model on two benchmark datasets BIBREF9. The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.", so we know that the datasets used in evaluation are homographic dataset and heterographic dataset.
Answer: homographic dataset and heterographic dataset
True answer: A homographic and heterographic benchmark datasets by BIBREF9.

Validation ID: 38e4aaeabf06a63a067b272f8950116733a7895c
Question: What is the tagging scheme employed?
From the context "We first design a simple tagging scheme consisting of two tags { INLINEFORM0 }:
INLINEFORM0 tag means the current word is not a pun.
True answer: A new tagging scheme that tags the words before and after the pun as well as the pun words.

Validation ID: 38e4aaeabf06a63a067b272f8950116733a7895c
Question: What is the tagging scheme employed?
From the context "We first design a simple tagging scheme consisting of two tags { INLINEFORM0 }:
INLINEFORM0 tag means the current word is not a pun.
True answer: a new tagging scheme consisting of three tags, namely { INLINEFORM0 }

Validation ID: 1d197cbcac7b3f4015416f0152a6692e881ada6c
Question: How they extract "structured answer-relevant relation"?
From the context "We utilize an off-the-shelf toolbox of OpenIE to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to keep the extractions as informative as possible and avoid extracting too many similar relations in different granularities from one sentence. We join all arguments in the extracted n-ary relation into a sequence as our to the point context. Figure FIGREF5 shows n-ary relations extracted from OpenIE. As we can see, OpenIE extracts multiple relations for complex sentences. Here we select the most informative relation according to three criteria in the order of descending importance: (1) having the maximal number of overlapped tokens between the answer and the relation; (2) being assigned the highest confidence score by OpenIE; (3) containing maximum non-stop words. As shown in Figure FIGREF5, our criteria can select answer-relevant relations (waved in Figure FIGREF5), which is especially useful for sentences with extraneous information. In rare cases, OpenIE cannot extract any relation, we treat the sentence itself as the to the point context.", so we know that they use OpenIE to extract structured answer-relevant relation.
Answer: OpenIE
True answer: Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation.

Validation ID: 1d197cbcac7b3f4015416f0152a6692e881ada6c
Question: How they extract "structured answer-relevant relation"?
From the context "We utilize an off-the-shelf toolbox of OpenIE to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to keep the extractions as informative as possible and avoid extracting too many similar relations in different granularities from one sentence. We join all arguments in the extracted n-ary relation into a sequence as our to the point context. Figure FIGREF5 shows n-ary relations extracted from OpenIE. As we can see, OpenIE extracts multiple relations for complex sentences. Here we select the most informative relation according to three criteria in the order of descending importance: (1) having the maximal number of overlapped tokens between the answer and the relation; (2) being assigned the highest confidence score by OpenIE; (3) containing maximum non-stop words. As shown in Figure FIGREF5, our criteria can select answer-relevant relations (waved in Figure FIGREF5), which is especially useful for sentences with extraneous information. In rare cases, OpenIE cannot extract any relation, we treat the sentence itself as the to the point context.", so we know that they use OpenIE to extract structured answer-relevant relation.
Answer: OpenIE
True answer: off-the-shelf toolbox of OpenIE

Validation ID: 92294820ac0d9421f086139e816354970f066d8a
Question: How big are significant improvements?
From the context "Our proposed model which combines structured answer-relevant relations and unstructured sentences achieves significant improvements over proximity-based answer-aware models BIBREF9, BIBREF15 on both dataset splits.", so we know that the significant improvements are big.
Answer: big
True answer: Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1

Validation ID: 477d9d3376af4d938bb01280fe48d9ae7c9cf7f7
Question: What metrics do they use?
From the context "We evaluate with all commonly-used metrics in question generation BIBREF13: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19. We use the evaluation script released by Chen2015MicrosoftCC.", so we know that they use BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET) and ROUGE-L (R-L) as metrics.
Answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET) and ROUGE-L (R-L)
True answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19

Validation ID: 477d9d3376af4d938bb01280fe48d9ae7c9cf7f7
Question: What metrics do they use?
From the context "We evaluate with all commonly-used metrics in question generation BIBREF13: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19. We use the evaluation script released by Chen2015MicrosoftCC.", so we know that they use BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET) and ROUGE-L (R-L) as metrics.
Answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET) and ROUGE-L (R-L)
True answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET), ROUGE-L (R-L)

Validation ID: f225a9f923e4cdd836dd8fe097848da06ec3e0cc
Question: On what datasets are experiments performed?
From the context "We conduct experiments on the SQuAD dataset BIBREF3. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles.", so we know that experiments are performed on the SQuAD dataset.
Answer: SQuAD dataset
True answer: SQuAD

Validation ID: f225a9f923e4cdd836dd8fe097848da06ec3e0cc
Question: On what datasets are experiments performed?
From the context "We conduct experiments on the SQuAD dataset BIBREF3. It contains 536 Wikipedia articles and 100k crowd-sourced question-answer pairs. The questions are written by crowd-workers and the answers are spans of tokens in the articles.", so we know that experiments are performed on the SQuAD dataset.
Answer: SQuAD dataset
True answer: SQuAD

Validation ID: ff338921e34c15baf1eae0074938bf79ee65fdd2
Question: What was the baseline model?
From the context "We started by answering always YES (in batch 2 and 3) to get the baseline performance.", so we know that the baseline model is answering always YES.
Answer: answering always YES
True answer: by answering always YES (in batch 2 and 3) 

Validation ID: e807d347742b2799bc347c0eff19b4c270449fee
Question: What dataset did they use?
From the context "The system was finetuned on the SQuAD 2.0. For data preprocessing Context / paragraph was generated from relevant snippets provided in the test data.", so we know that they used SQuAD 2.0.
Answer: SQuAD 2.0
True answer: BioASQ  dataset

Validation ID: e807d347742b2799bc347c0eff19b4c270449fee
Question: What dataset did they use?
From the context "The system was finetuned on the SQuAD 2.0. For data preprocessing Context / paragraph was generated from relevant snippets provided in the test data.", so we know that they used SQuAD 2.0.
Answer: SQuAD 2.0
True answer: A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers.

Validation ID: 31b92c03d5b9be96abcc1d588d10651703aff716
Question: What was their highest recall score?
From the context "In the fourth test batch set, our system achieved highest recall score of ‘0.7033’", so we know that their highest recall score is 0.7033.
Answer: 0.7033
True answer: 0.7033

Validation ID: 31b92c03d5b9be96abcc1d588d10651703aff716
Question: What was their highest recall score?
From the context "In the fourth test batch set, our system achieved highest recall score of ‘0.7033’", so we know that their highest recall score is 0.7033.
Answer: 0.7033
True answer: 0.7033

Validation ID: 9ec1f88ceec84a10dc070ba70e90a792fba8ce71
Question: What was their highest MRR score?
From the context "Our system scored near the top for factoid questions for all the batches of the challenge. More specifially, in the third test batch set, our system achieved highest ‘MRR’ score for Factoid Question Answering task.", so we know that the highest MRR score for Factoid Question Answering task is 0.6103.
Answer: 0.6103
True answer: 0.5115

Validation ID: 9ec1f88ceec84a10dc070ba70e90a792fba8ce71
Question: What was their highest MRR score?
From the context "Our system scored near the top for factoid questions for all the batches of the challenge. More specifially, in the third test batch set, our system achieved highest ‘MRR’ score for Factoid Question Answering task.", so we know that the highest MRR score for Factoid Question Answering task is 0.6103.
Answer: 0.6103
True answer: 0.6103

Validation ID: 384bf1f55c34b36cb03f916f50bbefade6c86a75
Question: Does their model suffer exhibit performance drops when incorporating word importance?
From the context "In this work, we resort to a gradient-based method, integrated gradients BIBREF9 (IG), which was originally proposed to attribute the model predictions to input features. It exploits the handy model gradient information by integrating first-order derivatives. IG is implementation invariant and does not require neural models to be differentiable or smooth, thereby is suitable for complex neural networks like Transformer. In this work, we use IG to interpret NMT models and reveal several interesting findings, which can potentially help debug NMT models and design better architectures for specific language pairs.", so we know that the gradient-based method is suitable for complex neural networks like Transformer.
From the context "In this work, we resort to a gradient-based method, integrated gradients BIBREF9 (IG), which was originally proposed to attribute the model predictions to input features. It exploits the handy model gradient information by integrating first-order derivatives. IG is implementation invariant and does not require neural models to be differentiable or smooth, thereby is suitable for complex neural networks like Transformer. In this work, we use IG to interpret NMT models and reveal several interesting findings, which can potentially help debug NMT models and design better architectures for specific language pairs.", so we know that the gradient-based method can potentially help debug NMT models and design better architectures for specific language pairs.
True answer: No

Validation ID: aef607d2ac46024be17b1ddd0ed3f13378c563a6
Question: How do they measure which words are under-translated by NMT models?
From the context "In this experiment, we propose to use the estimated word importance to detect the under-translated words by NMT models. Intuitively, under-translated input words should contribute little to the NMT outputs, yielding much smaller word importance.", so we know that they measure which words are under-translated by NMT models by detecting the under-translated words.
Answer: Detecting the under-translated words
True answer: They measured the under-translated words with low word importance score as calculated by Attribution.
method

Validation ID: aef607d2ac46024be17b1ddd0ed3f13378c563a6
Question: How do they measure which words are under-translated by NMT models?
From the context "In this experiment, we propose to use the estimated word importance to detect the under-translated words by NMT models. Intuitively, under-translated input words should contribute little to the NMT outputs, yielding much smaller word importance.", so we know that they measure which words are under-translated by NMT models by detecting the under-translated words.
Answer: Detecting the under-translated words
True answer: we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair

Validation ID: 93beae291b455e5d3ecea6ac73b83632a3ae7ec7
Question: How do their models decide how much improtance to give to the output words?
From the context "In this work, we resort to a gradient-based method, integrated gradients BIBREF9 (IG), which was originally proposed to attribute the model predictions to input features. It exploits the handy model gradient information by integrating first-order derivatives. IG is implementation invariant and does not require neural models to be differentiable or smooth, thereby is suitable for complex neural networks like Transformer. In this work, we use IG to interpret NMT models and reveal several interesting findings, which can potentially help debug NMT models and design better architectures for specific language pairs.", so we know that IG is a gradient-based method, which exploits the handy model gradient information by integrating first-order derivatives.
Answer: IG is a gradient-based method, which exploits the handy model gradient information by integrating first-order derivatives.
True answer: Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. 

Validation ID: 93beae291b455e5d3ecea6ac73b83632a3ae7ec7
Question: How do their models decide how much improtance to give to the output words?
From the context "In this work, we resort to a gradient-based method, integrated gradients BIBREF9 (IG), which was originally proposed to attribute the model predictions to input features. It exploits the handy model gradient information by integrating first-order derivatives. IG is implementation invariant and does not require neural models to be differentiable or smooth, thereby is suitable for complex neural networks like Transformer. In this work, we use IG to interpret NMT models and reveal several interesting findings, which can potentially help debug NMT models and design better architectures for specific language pairs.", so we know that IG is a gradient-based method, which exploits the handy model gradient information by integrating first-order derivatives.
Answer: IG is a gradient-based method, which exploits the handy model gradient information by integrating first-order derivatives.
True answer: They compute the gradient of the output at each time step with respect to the input words to decide the importance.

Validation ID: 6c91d44d5334a4ac80100eead4e105d34e99a284
Question: Which model architectures do they test their word importance approach on?
From the context "We choose the state-of-the-art Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0 as our test bed", so we know that they test their word importance approach on Transformer and RNN-Search model.
Answer: Transformer and RNN-Search model
True answer:  Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0

Validation ID: 6c91d44d5334a4ac80100eead4e105d34e99a284
Question: Which model architectures do they test their word importance approach on?
From the context "We choose the state-of-the-art Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0 as our test bed", so we know that they test their word importance approach on Transformer and RNN-Search model.
Answer: Transformer and RNN-Search model
True answer: Transformer, RNN-Search model

Validation ID: a69a59b6c0ab27bcee1a780d6867df21e30aec08
Question: Do they compare human-level performance to model performance for their dataset?
From the context "We found that non-expert annotators, in this gamified setting and with appropriate incentives to fool the model, are remarkably creative at finding and exploiting weaknesses in models", so we know that non-expert annotators are remarkably creative at finding and exploiting weaknesses in models.
From the context "We collected three rounds, and as the rounds progressed, the models became more robust and the test sets for each round became more difficult", so we know that the models became more robust and the test sets for each round became more difficult.
True answer: No

Validation ID: a69a59b6c0ab27bcee1a780d6867df21e30aec08
Question: Do they compare human-level performance to model performance for their dataset?
From the context "We found that non-expert annotators, in this gamified setting and with appropriate incentives to fool the model, are remarkably creative at finding and exploiting weaknesses in models", so we know that non-expert annotators are remarkably creative at finding and exploiting weaknesses in models.
From the context "We collected three rounds, and as the rounds progressed, the models became more robust and the test sets for each round became more difficult", so we know that the models became more robust and the test sets for each round became more difficult.
True answer: No

Validation ID: b3d01ac226ee979e188a4141877a6d2a5482de98
Question: What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models?
From the context "We found that non-expert annotators, in this gamified setting and with appropriate incentives to fool the model, are remarkably creative at finding and exploiting weaknesses in models", so we know that non-expert annotators are remarkably creative at finding and exploiting weaknesses in models.
From the context "We collected three rounds, and as the rounds progressed, the models became more robust and the test sets for each round became more difficult", so we know that the weaknesses found by non-expert annotators of current state-of-the-art NLI models are the weaknesses of the models.
True answer: state-of-the-art models learn to exploit spurious statistical patterns in datasets, human annotators—be they seasoned NLP researchers or non-experts—might easily be able to construct examples that expose model brittleness

Validation ID: af5730d82535464cedfa707a03415ac2e7a21295
Question: What data sources do they use for creating their dataset?
From the context "For the first round, we used a BERT-Large model BIBREF10 trained on a concatenation of SNLI BIBREF1 and MNLI BIBREF22, and selected the best-performing model we could train as the starting point for our dataset collection procedure. For Round 1 contexts, we randomly sampled short multi-sentence passages from Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set BIBREF23. Contexts are either ground-truth contexts from that dataset, or they are Wikipedia passages retrieved using TF-IDF BIBREF24 based on a HotpotQA question.", so we know that they use Wikipedia, SNLI, MNLI, HotpotQA for creating their dataset.
Answer: Wikipedia, SNLI, MNLI, HotpotQA
True answer: Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), RTE5

Validation ID: af5730d82535464cedfa707a03415ac2e7a21295
Question: What data sources do they use for creating their dataset?
From the context "For the first round, we used a BERT-Large model BIBREF10 trained on a concatenation of SNLI BIBREF1 and MNLI BIBREF22, and selected the best-performing model we could train as the starting point for our dataset collection procedure. For Round 1 contexts, we randomly sampled short multi-sentence passages from Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set BIBREF23. Contexts are either ground-truth contexts from that dataset, or they are Wikipedia passages retrieved using TF-IDF BIBREF24 based on a HotpotQA question.", so we know that they use Wikipedia and HotpotQA as data sources.
Answer: Wikipedia and HotpotQA
True answer: Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, formal spoken text (excerpted from court and presidential debate transcripts in the Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), causal or procedural text, which describes sequences of events or actions, extracted from WikiHow, annotations using the longer contexts present in the GLUE RTE training data, which came from the RTE5 dataset

Validation ID: ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2
Question: Do they use active learning to create their dataset?
From the context "We used a human-and-model-in-the-loop entailment training method to collect a new benchmark for natural language understanding. The benchmark is designed to be challenging to current state of the art models. Annotators were employed to act as adversaries, and encouraged to find vulnerabilities that fool the model into predicting the wrong label, but that another person would correctly classify.", so we know that they use active learning to create their dataset.
Answer: Yes
True answer: Yes

Validation ID: ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2
Question: Do they use active learning to create their dataset?
From the context "We used a human-and-model-in-the-loop entailment training method to collect a new benchmark for natural language understanding. The benchmark is designed to be challenging to current state of the art models. Annotators were employed to act as adversaries, and encouraged to find vulnerabilities that fool the model into predicting the wrong label, but that another person would correctly classify.", so we know that they use active learning to create their dataset.
Answer: Yes
True answer: No

Validation ID: b249b60a8c94d0e40d65f1ffdfcac527dab57516
Question: Do the hashtag and SemEval datasets contain only English data?
From the context "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0, created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.", so we know that the hashtag and SemEval datasets contain only English data.
Answer: Yes
True answer: Yes

Validation ID: b249b60a8c94d0e40d65f1ffdfcac527dab57516
Question: Do the hashtag and SemEval datasets contain only English data?
From the context "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0, created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.", so we know that the hashtag and SemEval datasets contain only English data.
Answer: Yes
True answer: Yes

Validation ID: 0f567251a6566f65170a1329eeeb5105932036b2
Question: What current state of the art method was used for comparison?
From the context "We compare our pairwise neural ranker with the following baseline and state-of-the-art approaches:", so we know that the current state of the art method was used for comparison is A maximum entropy classifier (MaxEnt) trained on the STAN INLINEFORM0 training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current state-of-the-art BIBREF14.
Answer: A maximum entropy classifier (MaxEnt) trained on the STAN INLINEFORM0 training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current state-of-the-art BIBREF14.
True answer: current state-of-the-art approach BIBREF14 , BIBREF15

Validation ID: 0f567251a6566f65170a1329eeeb5105932036b2
Question: What current state of the art method was used for comparison?
From the context "We compare our pairwise neural ranker with the following baseline and state-of-the-art approaches:", so we know that the current state of the art method was used for comparison is A maximum entropy classifier (MaxEnt) trained on the STAN INLINEFORM0 training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current state-of-the-art BIBREF14.
Answer: A maximum entropy classifier (MaxEnt) trained on the STAN INLINEFORM0 training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current state-of-the-art BIBREF14.
True answer:  BIBREF14, BIBREF15 

Validation ID: 4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f
Question: What set of approaches to hashtag segmentation are proposed?
From the context "Current approaches for hashtag segmentation can be broadly divided into three categories: (a) gazeteer and rule based BIBREF11, BIBREF12, BIBREF13, (b) word boundary detection BIBREF14, BIBREF15, and (c) ranking with language model and other features BIBREF16, BIBREF10, BIBREF0, BIBREF17, BIBREF18.", so we know that the set of approaches to hashtag segmentation are gazeteer and rule based, word boundary detection, and ranking with language model and other features.
Answer: gazeteer and rule based, word boundary detection, and ranking with language model and other features
True answer: Adaptive Multi-task Learning
, Margin Ranking (MR) Loss
, Pairwise Neural Ranking Model


Validation ID: 60ce4868af45753c9e124e64e518c32376f12694
Question: How is the dataset of hashtags sourced?
From the context "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0, created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.", so we know that the dataset of hashtags is sourced from Stanford Sentiment Analysis Dataset.
Answer: Stanford Sentiment Analysis Dataset
True answer: 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset

Validation ID: 60ce4868af45753c9e124e64e518c32376f12694
Question: How is the dataset of hashtags sourced?
From the context "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0, created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset.", so we know that the dataset of hashtags is sourced from Stanford Sentiment Analysis Dataset.
Answer: Stanford Sentiment Analysis Dataset
True answer: Stanford Sentiment Analysis Dataset BIBREF36

Validation ID: 1b1a30e9e68a9ae76af467e60cefb180d135e285
Question: How big is their created dataset?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that the created dataset is simulated training data.
From the context "The templates and framework are verified for logical correctness and clinical soundness.", so we know that the created dataset is verified for logical correctness and clinical soundness.
True answer: 353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers), we build templates and expression pools using linguistic analysis

Validation ID: 2c85865a65acd429508f50b5e4db9674813d67f2
Question: Which data do they use as a starting point for the dialogue dataset?
From the context "We used recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital", so we know that they use recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital as a starting point for the dialogue dataset.
Answer: recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital
True answer: A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital

Validation ID: 2c85865a65acd429508f50b5e4db9674813d67f2
Question: Which data do they use as a starting point for the dialogue dataset?
From the context "We used recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital", so we know that they use recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital as a starting point for the dialogue dataset.
Answer: recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital
True answer: recordings of nurse-initiated telephone conversations for congestive heart failure patients

Validation ID: 73a7acf33b26f5e9475ee975ba00d14fd06f170f
Question: What labels do they create on their dataset?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that they create templates and expression pools.
Answer: templates and expression pools
True answer: (1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer

Validation ID: 73a7acf33b26f5e9475ee975ba00d14fd06f170f
Question: What labels do they create on their dataset?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that they create templates and expression pools.
Answer: templates and expression pools
True answer: the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms

Validation ID: dd53baf26dad3d74872f2d8956c9119a27269bd5
Question: How do they select instances to their hold-out test set?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that they select instances to their hold-out test set by linguistic analysis followed by manual verification.
Answer: linguistic analysis followed by manual verification
True answer: 1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues

Validation ID: dd53baf26dad3d74872f2d8956c9119a27269bd5
Question: How do they select instances to their hold-out test set?
From the context "We divide the construction of data simulation into two stages. In Section SECREF16, we build templates and expression pools using linguistic analysis followed by manual verification. In Section SECREF20, we present our proposed framework for generating simulated training data. The templates and framework are verified for logical correctness and clinical soundness.", so we know that they select instances to their hold-out test set by linguistic analysis followed by manual verification.
Answer: linguistic analysis followed by manual verification
True answer: held out from the simulated data

Validation ID: 218bc82796eb8d91611996979a4a42500131a936
Question: Which models/frameworks do they compare to?
From the context "We illustrate the significance of our approach by providing the experimental results for two different tasks namely, speech/music discrimination and emotion classification. Further, we showed that the s2s framework can also handle the low resourced data imbalance problem.", so we know that they compare to MLP, Eusboost and MWMOTE.
Answer: MLP, Eusboost and MWMOTE
True answer: MLP

Validation ID: 218bc82796eb8d91611996979a4a42500131a936
Question: Which models/frameworks do they compare to?
From the context "We illustrate the significance of our approach by providing the experimental results for two different tasks namely, speech/music discrimination and emotion classification. Further, we showed that the s2s framework can also handle the low resourced data imbalance problem.", so we know that they compare to MLP, Eusboost and MWMOTE.
Answer: MLP, Eusboost and MWMOTE
True answer: Eusboost, MWMOTE

Validation ID: b21bc09193699dc9cfad523f3d5542b0b2ff1b8e
Question: Which classification algorithm do they use for s2sL?
From the context "MLP, the most commonly used feed forward neural network, is considered as the base classifier to validate our proposed s2s framework", so we know that they use MLP as the base classifier.
Answer: MLP
True answer: MLP

Validation ID: b21bc09193699dc9cfad523f3d5542b0b2ff1b8e
Question: Which classification algorithm do they use for s2sL?
From the context "MLP, the most commonly used feed forward neural network, is considered as the base classifier to validate our proposed s2s framework", so we know that they use MLP as the base classifier.
Answer: MLP
True answer: MLP

Validation ID: 352bc6de5c5068c6c19062bad1b8f644919b1145
Question: Up to how many samples do they experiment with?
From the context "We illustrate the significance of our approach by providing the experimental results for two different tasks namely, speech/music discrimination and emotion classification. Further, we showed that the s2s framework can also handle the low resourced data imbalance problem.", so we know that they experiment with two different tasks.
Answer: two
True answer: 535

Validation ID: 352bc6de5c5068c6c19062bad1b8f644919b1145
Question: Up to how many samples do they experiment with?
From the context "We illustrate the significance of our approach by providing the experimental results for two different tasks namely, speech/music discrimination and emotion classification. Further, we showed that the s2s framework can also handle the low resourced data imbalance problem.", so we know that they experiment with two different tasks.
Answer: two
True answer: we considered 4 different proportions i.e., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 of the training data to train the classifier

Validation ID: d667731ea20605580c398a1224a0094d1155ebbb
Question: Do they use pretrained models?
From the context "We hypothesize that the s2s format is expected to help the network not only to learn the characteristics of the two classes separately, but also the difference and similarity in characteristics of the two classes.", so we know that they do not use pretrained models.
Answer: No
True answer: No

Validation ID: 8bb0011ad1d63996d5650770f3be18abdd9f7fc6
Question: Do they report results only on English datasets?
From the context "Experimental results show that KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise.", so we know that they report results only on English datasets.
Answer: Yes
True answer: Unanswerable

Validation ID: 8bb0011ad1d63996d5650770f3be18abdd9f7fc6
Question: Do they report results only on English datasets?
From the context "Experimental results show that KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise.", so we know that they report results only on English datasets.
Answer: Yes
True answer: Yes

Validation ID: b0dbe75047310fec4d4ce787be5c32935fc4e37b
Question: How do the authors examine whether a model is robust to noise or not?
From the context "To evaluate the robustness to noise of MRC models, we use two adversarial sets, namely AddSent and AddOneSent BIBREF6, which contain passages with misleading sentences. Specifically, each passage in AddSent contains several sentences that are similar to the question but not contradictory to the answer, while each passage in AddOneSent contains a human-approved random sentence that may be unrelated to the passage.", so we know that the authors examine whether a model is robust to noise or not by using two adversarial sets, namely AddSent and AddOneSent.
Answer: AddSent and AddOneSent
True answer: By evaluating their model on adversarial sets containing misleading sentences

Validation ID: b0dbe75047310fec4d4ce787be5c32935fc4e37b
Question: How do the authors examine whether a model is robust to noise or not?
From the context "To evaluate the robustness to noise of MRC models, we use two adversarial sets, namely AddSent and AddOneSent BIBREF6, which contain passages with misleading sentences. Specifically, each passage in AddSent contains several sentences that are similar to the question but not contradictory to the answer, while each passage in AddOneSent contains a human-approved random sentence that may be unrelated to the passage.", so we know that the authors examine whether a model is robust to noise or not by using two adversarial sets, namely AddSent and AddOneSent.
Answer: AddSent and AddOneSent
True answer: we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise

Validation ID: d64383e39357bd4177b49c02eb48e12ba7ffd4fb
Question: What type of model is KAR?
From the context "In this paper, we innovatively integrate the neural networks of MRC models with the general knowledge of human beings", so we know that KAR is a model that integrates the neural networks of MRC models with the general knowledge of human beings.
Answer: a model that integrates the neural networks of MRC models with the general knowledge of human beings
True answer: Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer

Validation ID: 52f9cd05d8312ae3c7a43689804bac63f7cac34b
Question: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?
From the context "The reason for these phenomena, we believe, is that MRC models can only utilize the knowledge contained in each given passage-question pair, but in addition to this, human beings can also utilize general knowledge. A typical category of general knowledge is inter-word semantic connections.", so we know that the authors hypothesize that humans' robustness to noise is due to their general knowledge.
Answer: Yes
True answer: Yes

Validation ID: 52f9cd05d8312ae3c7a43689804bac63f7cac34b
Question: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?
From the context "The reason for these phenomena, we believe, is that MRC models can only utilize the knowledge contained in each given passage-question pair, but in addition to this, human beings can also utilize general knowledge. A typical category of general knowledge is inter-word semantic connections.", so we know that the authors hypothesize that humans' robustness to noise is due to their general knowledge.
Answer: Yes
True answer: Yes

Validation ID: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596
Question: What type of system does the baseline classification use?
From the context "We experimented with four different classifiers, namely, support vector machine BIBREF18, random forest, extra tree and naive bayes classifier BIBREF19.", so we know that the baseline classification uses four different classifiers.
Answer: four different classifiers
True answer: support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19

Validation ID: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596
Question: What type of system does the baseline classification use?
From the context "We experimented with four different classifiers, namely, support vector machine BIBREF18, random forest, extra tree and naive bayes classifier BIBREF19.", so we know that the baseline classification uses four different classifiers.
Answer: four different classifiers
True answer: Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers.

Validation ID: 955cbea7e5ead36fb89cd6229a97ccb3febcf8bc
Question: What experiments were carried out on the corpus?
From the context "We experimented with four different classifiers, namely, support vector machine BIBREF18, random forest, extra tree and naive bayes classifier BIBREF19. Chi square feature selection algorithm is applied to reduces the size of our feature vector. For training our system classifier, we used Scikit-learn BIBREF19.
10-fold cross validation on 3543 code-mixed tweets was carried out by dividing the corpus into 10 equal parts with nine parts as training corpus and rest one for testing. Mean accuracy is calculated by taking the average of the accuracy obtained in each iteration of the testing process.", so we know that 10-fold cross validation was carried out on the corpus.
True answer: task of humor identification in social media texts is analyzed as a classification problem

Validation ID: 04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39
Question: How many annotators tagged each text?
From the context "The final code-mixed tweets were forwarded to a group of three annotators who were university students and fluent in both English and Hindi", so we know that three annotators tagged each text.
Answer: three
True answer: three 

Validation ID: 04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39
Question: How many annotators tagged each text?
From the context "The final code-mixed tweets were forwarded to a group of three annotators who were university students and fluent in both English and Hindi", so we know that three annotators tagged each text.
Answer: three
True answer: three annotators

Validation ID: 15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8
Question: Where did the texts in the corpus come from?
From the context "In this paper, we describe a freely available corpus of 3453 English-Hindi code-mixed tweets. The tweets are annotated with humorous(H) and non-humorous(N) tags along with the language tags at the word level.", so we know that the texts in the corpus come from tweets.
Answer: tweets
True answer: tweets from the past two years from domains like `sports', `politics', `entertainment'

Validation ID: 15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8
Question: Where did the texts in the corpus come from?
From the context "In this paper, we describe a freely available corpus of 3453 English-Hindi code-mixed tweets. The tweets are annotated with humorous(H) and non-humorous(N) tags along with the language tags at the word level.", so we know that the texts in the corpus come from tweets.
Answer: tweets
True answer: twitter

Validation ID: ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797
Question: What is the previous state-of-the-art in summarization?
From the context "We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline", so we know that the previous state-of-the-art in summarization is BIBREF26.
Answer: BIBREF26
True answer: BIBREF26 

Validation ID: ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797
Question: What is the previous state-of-the-art in summarization?
From the context "We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline", so we know that the previous state-of-the-art in summarization is BIBREF26.
Answer: BIBREF26
True answer: BIBREF26

Validation ID: 6ca938324dc7e1742a840d0a54dc13cc207394a1
Question: What dataset do they use?
From the context "We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task.", so we know that they use WMT'18 English-German (en-de) news translation task and WMT'18 English-Turkish (en-tr) news task.
Answer: WMT'18 English-German (en-de) news translation task and WMT'18 English-Turkish (en-tr) news task
True answer: German newscrawl distributed by WMT'18 , English newscrawl data, WMT'18 English-German (en-de) news translation task , WMT'18 English-Turkish (en-tr) news task

Validation ID: 6ca938324dc7e1742a840d0a54dc13cc207394a1
Question: What dataset do they use?
From the context "We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task.", so we know that they use WMT'18 English-German (en-de) news translation task and WMT'18 English-Turkish (en-tr) news task.
Answer: WMT'18 English-German (en-de) news translation task and WMT'18 English-Turkish (en-tr) news task
True answer: German newscrawl, English newscrawl, WMT'18 English-German (en-de) news, WMT'18 English-Turkish (en-tr) news task, WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task

Validation ID: 4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802
Question: What other models do they compare to?
From the context "We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline", so we know that they compare to BIBREF26.
Answer: BIBREF26
True answer: BIBREF11 , BIBREF26 

Validation ID: 4d47bef19afd70c10bbceafd1846516546641a2f
Question: What language model architectures are used?
From the context "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time BIBREF17. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains INLINEFORM1 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token at position INLINEFORM2. The model has access to the entire input surrounding the current target token. Models use the standard settings for the Big Transformer BIBREF16. The bi-directional model contains 353M parameters and the uni-directional model 190M parameters.", so we know that the language model architectures are bi-directional language model and uni-directional model.
Answer: bi-directional language model and uni-directional model
True answer: uni-directional model to augment the decoder

Validation ID: 4d47bef19afd70c10bbceafd1846516546641a2f
Question: What language model architectures are used?
From the context "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time BIBREF17. The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains INLINEFORM1 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token at position INLINEFORM2. The model has access to the entire input surrounding the current target token. Models use the standard settings for the Big Transformer BIBREF16. The bi-directional model contains 353M parameters and the uni-directional model 190M parameters.", so we know that the language model architectures are bi-directional language model and uni-directional model.
Answer: bi-directional language model and uni-directional model
True answer: bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder

Validation ID: 506d21501d54a12d0c9fd3dbbf19067802439a04
Question: What are the user-defined keywords?
From the context "The DMK loss then calculates an additional term, which corresponds to the dot-product attention of each word in the generated output with each keyword specified by the user", so we know that the user-defined keywords are subway and manhattan.
Answer: subway and manhattan
True answer: Words that a user wants them to appear in the generated output.

Validation ID: 506d21501d54a12d0c9fd3dbbf19067802439a04
Question: What are the user-defined keywords?
From the context "The DMK loss then calculates an additional term, which corresponds to the dot-product attention of each word in the generated output with each keyword specified by the user", so we know that the user-defined keywords are subway and manhattan.
Answer: subway and manhattan
True answer: terms common to hosts' descriptions of popular Airbnb properties, like 'subway', 'manhattan', or 'parking'

Validation ID: 701571680724c05ca70c11bc267fb1160ea1460a
Question: Does the method achieve sota performance on this dataset?
From the context "For future work, we may also consider experimenting with different forms of word embeddings, such as improved word vectors (IWV) which were suggested by Rezaeinia et al [10]. These IMV word embeddings are trained in a similar fashion to GloVe vectors, but also encode additional information about each word, like the word’s part of speech. For our model, we may consider encoding similar information in order for the generator to more easily learn common semantic patterns inherent to the marketing data being analyzed.", so we know that the method does not achieve sota performance on this dataset.
Answer: No
True answer: No

Validation ID: 701571680724c05ca70c11bc267fb1160ea1460a
Question: Does the method achieve sota performance on this dataset?
From the context "For future work, we may also consider experimenting with different forms of word embeddings, such as improved word vectors (IWV) which were suggested by Rezaeinia et al [10]. These IMV word embeddings are trained in a similar fashion to GloVe vectors, but also encode additional information about each word, like the word’s part of speech. For our model, we may consider encoding similar information in order for the generator to more easily learn common semantic patterns inherent to the marketing data being analyzed.", so we know that the method does not achieve sota performance on this dataset.
Answer: No
True answer: Unanswerable

Validation ID: 600b097475b30480407ce1de81c28c54a0b3b2f8
Question: What are the baselines used in the paper?
From the context "We were similarly influenced by the work of Radford et. al, who argue for the importance of layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization. These findings lead us to question whether it is possible to employ recurrent neural networks with long short-term memory gates, as defined by Mikolov et al., to categorize product descriptions into categories based on the product's popularity [6].", so we know that the baselines used in the paper are layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization.
Answer: layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization
True answer: GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling

Validation ID: 600b097475b30480407ce1de81c28c54a0b3b2f8
Question: What are the baselines used in the paper?
From the context "We were similarly influenced by the work of Radford et. al, who argue for the importance of layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization. These findings lead us to question whether it is possible to employ recurrent neural networks with long short-term memory gates, as defined by Mikolov et al., to categorize product descriptions into categories based on the product's popularity [6].", so we know that the baselines used in the paper are layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization.
Answer: layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization
True answer: Unanswerable

Validation ID: ee7e9a948ee6888aa5830b1a3d0d148ff656d864
Question: What is the size of the Airbnb?
From the context "The data provided to us contained information for roughly 40,000 Manhattan listings that were posted on Airbnb during this defined time period", so we know that the size of the Airbnb is 40,000.
Answer: 40,000
True answer: roughly 40,000 Manhattan listings

Validation ID: 5fda8539a97828e188ba26aad5cda1b9dd642bc8
Question: How better is performance compared to previous state-of-the-art models?
From the context "Our model is evaluated on standard benchmark dataset, SIGHAN Bakeoff 2005, which shows not only our model performs segmentation faster than any previous models but also gives new higher or comparable segmentation performance against previous state-of-the-art models.", so we know that our model performs segmentation faster than any previous models and gives new higher or comparable segmentation performance against previous state-of-the-art models.
Answer: Our model performs segmentation faster than any previous models and gives new higher or comparable segmentation performance against previous state-of-the-art models.
True answer: F1 score of 97.5 on MSR and 95.7 on AS

Validation ID: 5fda8539a97828e188ba26aad5cda1b9dd642bc8
Question: How better is performance compared to previous state-of-the-art models?
From the context "Our model is evaluated on standard benchmark dataset, SIGHAN Bakeoff 2005, which shows not only our model performs segmentation faster than any previous models but also gives new higher or comparable segmentation performance against previous state-of-the-art models.", so we know that our model performs segmentation faster than any previous models and gives new higher or comparable segmentation performance against previous state-of-the-art models.
Answer: Our model performs segmentation faster than any previous models and gives new higher or comparable segmentation performance against previous state-of-the-art models.
True answer: MSR: 97.7 compared to 97.5 of baseline
AS: 95.7 compared to 95.6 of baseline

Validation ID: 709feae853ec0362d4e883db8af41620da0677fe
Question: How does Gaussian-masked directional multi-head attention works?
From the context "To improve the ability of capturing the localness and directional information of self-attention based encoder, we propose a variant of self-attention called Gaussian-masked directional multi-head attention to replace the standard self-attention", so we know that Gaussian-masked directional multi-head attention is a variant of self-attention.
From the context "We also extend the Transformer encoder to capture directional features", so we know that Gaussian-masked directional multi-head attention is used to capture directional features.
True answer: pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters

Validation ID: 709feae853ec0362d4e883db8af41620da0677fe
Question: How does Gaussian-masked directional multi-head attention works?
From the context "To improve the ability of capturing the localness and directional information of self-attention based encoder, we propose a variant of self-attention called Gaussian-masked directional multi-head attention to replace the standard self-attention", so we know that Gaussian-masked directional multi-head attention is a variant of self-attention.
From the context "We also extend the Transformer encoder to capture directional features", so we know that Gaussian-masked directional multi-head attention is used in Transformer encoder.
True answer: Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters

Validation ID: 186b7978ee33b563a37139adff1da7d51a60f581
Question: What is meant by closed test setting?
From the context "To train model with pre-trained embeddings in AS and CITYU, we use OpenCC to transfer data from traditional Chinese to simplified Chinese. In this work, we will focus on the closed test setting by finding a better model design for further CWS performance improvement.", so we know that closed test setting means that all the data for learning should not be beyond the given training set.
Answer: All the data for learning should not be beyond the given training set.
True answer: closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation

Validation ID: 186b7978ee33b563a37139adff1da7d51a60f581
Question: What is meant by closed test setting?
From the context "To train model with pre-trained embeddings in AS and CITYU, we use OpenCC to transfer data from traditional Chinese to simplified Chinese. In this work, we will focus on the closed test setting by finding a better model design for further CWS performance improvement.", so we know that closed test setting means that all the data for learning should not be beyond the given training set.
Answer: All the data for learning should not be beyond the given training set.
True answer: closed test limits all the data for learning should not be beyond the given training set

Validation ID: fabcd71644bb63559d34b38d78f6ef87c256d475
Question: What are strong baselines model is compared to?
From the context "Our model is evaluated on standard benchmark dataset, SIGHAN Bakeoff 2005, which shows not only our model performs segmentation faster than any previous models but also gives new higher or comparable segmentation performance against previous state-of-the-art models.", so we know that the strong baselines model is compared to are previous state-of-the-art models.
Answer: previous state-of-the-art models
True answer: Baseline models are:
- Chen et al., 2015a
- Chen et al., 2015b
- Liu et al., 2016
- Cai and Zhao, 2016
- Cai et al., 2017
- Zhou et al., 2017
- Ma et al., 2018
- Wang et al., 2019

Validation ID: da9c0637623885afaf023a319beee87898948fe9
Question: Does the dataset feature only English language data?
From the context "In the data preprocessing steps, user IDs, URLs, and frequently used emojis are replaced as special tokens. Since hashtags tend to have a high correlation with the content of the tweet BIBREF23, we use a segmentation library BIBREF24 for hashtags to extract more information. For character-level representations, we apply the method Zhang et al. zhang2015character proposed. Tweets are transformed into one-hot encoded vectors using 70 character dimensions—26 lower-cased alphabets, 10 digits, and 34 special characters including whitespace.", so we know that the dataset feature only English language data.
Answer: Yes
True answer: Yes

Validation ID: da9c0637623885afaf023a319beee87898948fe9
Question: Does the dataset feature only English language data?
From the context "In the data preprocessing steps, user IDs, URLs, and frequently used emojis are replaced as special tokens. Since hashtags tend to have a high correlation with the content of the tweet BIBREF23, we use a segmentation library BIBREF24 for hashtags to extract more information. For character-level representations, we apply the method Zhang et al. zhang2015character proposed. Tweets are transformed into one-hot encoded vectors using 70 character dimensions—26 lower-cased alphabets, 10 digits, and 34 special characters including whitespace.", so we know that the dataset feature only English language data.
Answer: Yes
True answer: Unanswerable

Validation ID: 8a1c0ef69b6022a0642ca131a8eacb5c97016640
Question: What additional features and context are proposed?
From the context "In order to avoid data dependency, text sequences of context tweets are directly used as an additional feature of neural network models. We use the same baseline model to convert context tweets to vectors, then concatenate these vectors with outputs of their corresponding labeled tweets. More specifically, we concatenate max-pooled layers of context and labeled tweets for the CNN baseline model. As for RNN, the last hidden states of context and labeled tweets are concatenated.", so we know that the additional features and context are context tweets.
Answer: context tweets
True answer: using tweets that one has replied or quoted to as contextual information

Validation ID: 8a1c0ef69b6022a0642ca131a8eacb5c97016640
Question: What additional features and context are proposed?
From the context "In order to avoid data dependency, text sequences of context tweets are directly used as an additional feature of neural network models. We use the same baseline model to convert context tweets to vectors, then concatenate these vectors with outputs of their corresponding labeled tweets. More specifically, we concatenate max-pooled layers of context and labeled tweets for the CNN baseline model. As for RNN, the last hidden states of context and labeled tweets are concatenated.", so we know that the additional features and context are context tweets.
Answer: context tweets
True answer: text sequences of context tweets

Validation ID: 48088a842f7a433d3290eb45eb0d4c6ab1d8f13c
Question: What learning models are used on the dataset?
From the context "We implement five feature engineering based machine learning classifiers that are most often used for abusive language detection. In data preprocessing, text sequences are converted into Bag Of Words (BOW) representations, and normalized with Term Frequency-Inverse Document Frequency (TF-IDF) values. We experiment with word-level features using n-grams ranging from 1 to 3, and character-level features from 3 to 8-grams. Each classifier is implemented with the following specifications:
Naïve Bayes (NB): Multinomial NB with additive smoothing constant 1
True answer: Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)

Validation ID: 48088a842f7a433d3290eb45eb0d4c6ab1d8f13c
Question: What learning models are used on the dataset?
From the context "We implement five feature engineering based machine learning classifiers that are most often used for abusive language detection. In data preprocessing, text sequences are converted into Bag Of Words (BOW) representations, and normalized with Term Frequency-Inverse Document Frequency (TF-IDF) values. We experiment with word-level features using n-grams ranging from 1 to 3, and character-level features from 3 to 8-grams. Each classifier is implemented with the following specifications:
Naïve Bayes (NB): Multinomial NB with additive smoothing constant 1
True answer: Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN

Validation ID: 4907096cf16d506937e592c50ae63b642da49052
Question: What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give?
From the context "As humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language.", so we know that the authors give the examples of the difficulties presented by the context-dependent nature of online aggression that humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language.
Answer: humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language.
True answer: detecting abusive language extremely laborious, it is difficult to build a large and reliable dataset

Validation ID: 8748e8f64af57560d124c7b518b853bf2711c13e
Question: Do they report results only on English data?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 8748e8f64af57560d124c7b518b853bf2711c13e
Question: Do they report results only on English data?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 893ec40b678a72760b6802f6abf73b8f487ae639
Question: What evidence do the authors present that the model can capture some biases in data annotation and collection?
From the context "Furthermore, examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data.", so we know that the authors present that the model can capture some biases in data annotation and collection.
Answer: The model can capture some biases in data annotation and collection.
True answer: The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate

Validation ID: c81f215d457bdb913a5bade2b4283f19c4ee826c
Question: Which publicly available datasets are used?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.", so we know that the publicly available datasets are provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.
Answer: Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9
True answer: Waseem-dataset, Davidson-dataset,

Validation ID: c81f215d457bdb913a5bade2b4283f19c4ee826c
Question: Which publicly available datasets are used?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.", so we know that the publicly available datasets are provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.
Answer: Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9
True answer: Waseem and Hovey BIBREF5, Davidson et al. BIBREF9

Validation ID: e101e38efaa4b931f7dd75757caacdc945bb32b4
Question: What baseline is used?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9. Waseem and Hovy BIBREF5 collected $16k$ of tweets based on an initial ad-hoc approach that searched common slurs and terms related to religious, sexual, gender, and ethnic minorities. They annotated their dataset manually as racism, sexism, or neither. To extend this dataset, Waseem BIBREF23 also provided another dataset containing $6.9k$ of tweets annotated with both expert and crowdsourcing users as racism, sexism, neither, or both. Since both datasets are overlapped partially and they used the same strategy in definition of hateful content, we merged these two datasets following Waseem et al. BIBREF10 to make our imbalance data a bit larger. Davidson et al. BIBREF9 used the Twitter API to accumulate 84.4 million tweets from 33,458 twitter users containing particular terms from a pre-defined lexicon of hate speech words and phrases, called Hatebased.org. To annotate collected tweets as Hate, Offensive, or Neither, they randomly sampled $25k$ tweets and asked users of CrowdFlower crowdsourcing platform to label them. In detail, the distribution of different classes in both datasets will be provided in Subsection SECREF15.", so we know that the baseline is Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.
Answer: Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10

Validation ID: e101e38efaa4b931f7dd75757caacdc945bb32b4
Question: What baseline is used?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9. Waseem and Hovy BIBREF5 collected $16k$ of tweets based on an initial ad-hoc approach that searched common slurs and terms related to religious, sexual, gender, and ethnic minorities. They annotated their dataset manually as racism, sexism, or neither. To extend this dataset, Waseem BIBREF23 also provided another dataset containing $6.9k$ of tweets annotated with both expert and crowdsourcing users as racism, sexism, neither, or both. Since both datasets are overlapped partially and they used the same strategy in definition of hateful content, we merged these two datasets following Waseem et al. BIBREF10 to make our imbalance data a bit larger. Davidson et al. BIBREF9 used the Twitter API to accumulate 84.4 million tweets from 33,458 twitter users containing particular terms from a pre-defined lexicon of hate speech words and phrases, called Hatebased.org. To annotate collected tweets as Hate, Offensive, or Neither, they randomly sampled $25k$ tweets and asked users of CrowdFlower crowdsourcing platform to label them. In detail, the distribution of different classes in both datasets will be provided in Subsection SECREF15.", so we know that the baselines are Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9.
Answer: Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10

Validation ID: afb77b11da41cd0edcaa496d3f634d18e48d7168
Question: What new fine-tuning methods are presented?
From the context "Here, we introduce new fine-tuning strategies to examine the effect of different layers of BERT in hate speech detection task", so we know that the new fine-tuning methods are examining the effect of different layers of BERT in hate speech detection task.
Answer: examining the effect of different layers of BERT in hate speech detection task
True answer: BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer

Validation ID: afb77b11da41cd0edcaa496d3f634d18e48d7168
Question: What new fine-tuning methods are presented?
From the context "Here, we introduce new fine-tuning strategies to examine the effect of different layers of BERT in hate speech detection task", so we know that the new fine-tuning methods are examining the effect of different layers of BERT in hate speech detection task.
Answer: examining the effect of different layers of BERT in hate speech detection task
True answer: BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer

Validation ID: 41b2355766a4260f41b477419d44c3fd37f3547d
Question: What are the existing biases?
From the context "Furthermore, examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data.", so we know that the existing biases are:
1. Bias in the process of collecting or annotating datasets
True answer: systematic and substantial racial biases, biases from data collection, rules of annotation

Validation ID: 41b2355766a4260f41b477419d44c3fd37f3547d
Question: What are the existing biases?
From the context "Furthermore, examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data.", so we know that the existing biases are:
1. Bias in the process of collecting or annotating datasets
True answer: sampling tweets from specific keywords create systematic and substancial racial biases in datasets

Validation ID: 96a4091f681872e6d98d0efee777d9e820cb8dae
Question: What biases does their model capture?
From the context "Furthermore, examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data.", so we know that their model can detect some biases in the process of collecting or annotating datasets.
Answer: biases in the process of collecting or annotating datasets
True answer: Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters

Validation ID: 81a35b9572c9d574a30cc2164f47750716157fc8
Question: What existing approaches do they compare to?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9. Waseem and Hovy BIBREF5 collected $16k$ of tweets based on an initial ad-hoc approach that searched common slurs and terms related to religious, sexual, gender, and ethnic minorities. They annotated their dataset manually as racism, sexism, or neither. To extend this dataset, Waseem BIBREF23 also provided another dataset containing $6.9k$ of tweets annotated with both expert and crowdsourcing users as racism, sexism, neither, or both. Since both datasets are overlapped partially and they used the same strategy in definition of hateful content, we merged these two datasets following Waseem et al. BIBREF10 to make our imbalance data a bit larger. Davidson et al. BIBREF9 used the Twitter API to accumulate 84.4 million tweets from 33,458 twitter users containing particular terms from a pre-defined lexicon of hate speech words and phrases, called Hatebased.org. To annotate collected tweets as Hate, Offensive, or Neither, they randomly sampled $25k$ tweets and asked users of CrowdFlower crowdsourcing platform to label them. In detail, the distribution of different classes in both datasets will be provided in Subsection SECREF15.", so we know that they compare their method to Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.
Answer: Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10

Validation ID: 81a35b9572c9d574a30cc2164f47750716157fc8
Question: What existing approaches do they compare to?
From the context "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9. Waseem and Hovy BIBREF5 collected $16k$ of tweets based on an initial ad-hoc approach that searched common slurs and terms related to religious, sexual, gender, and ethnic minorities. They annotated their dataset manually as racism, sexism, or neither. To extend this dataset, Waseem BIBREF23 also provided another dataset containing $6.9k$ of tweets annotated with both expert and crowdsourcing users as racism, sexism, neither, or both. Since both datasets are overlapped partially and they used the same strategy in definition of hateful content, we merged these two datasets following Waseem et al. BIBREF10 to make our imbalance data a bit larger. Davidson et al. BIBREF9 used the Twitter API to accumulate 84.4 million tweets from 33,458 twitter users containing particular terms from a pre-defined lexicon of hate speech words and phrases, called Hatebased.org. To annotate collected tweets as Hate, Offensive, or Neither, they randomly sampled $25k$ tweets and asked users of CrowdFlower crowdsourcing platform to label them. In detail, the distribution of different classes in both datasets will be provided in Subsection SECREF15.", so we know that they compare their method to Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9.
Answer: Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10

Validation ID: f4496316ddd35ee2f0ccc6475d73a66abf87b611
Question: What is the benchmark dataset?
From the context "We use the 20 Newsgroups dataset (20NG) BIBREF17 which is commonly used for benchmarking text classification algorithms", so we know that the benchmark dataset is 20 Newsgroups dataset.
Answer: 20 Newsgroups dataset
True answer: a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data

Validation ID: f4496316ddd35ee2f0ccc6475d73a66abf87b611
Question: What is the benchmark dataset?
From the context "We use the 20 Newsgroups dataset (20NG) BIBREF17 which is commonly used for benchmarking text classification algorithms", so we know that the benchmark dataset is 20 Newsgroups dataset.
Answer: 20 Newsgroups dataset
True answer: dataset created by ceccarelli2013learning from the CoNLL 2003 data

Validation ID: e8a32460fba149003566969f92ab5dd94a8754a4
Question: What are the two neural embedding models?
From the context "In this paper we proposed two models for learning concept embeddings based on the skip-gram model", so we know that the two neural embedding models are Concept Raw Context model (CRC) and Concept-Concept Context model (3C).
Answer: Concept Raw Context model (CRC) and Concept-Concept Context model (3C)
True answer: Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only)

Validation ID: e8a32460fba149003566969f92ab5dd94a8754a4
Question: What are the two neural embedding models?
From the context "In this paper we proposed two models for learning concept embeddings based on the skip-gram model", so we know that the two neural embedding models are Concept Raw Context model (CRC) and Concept-Concept Context model (3C).
Answer: Concept Raw Context model (CRC) and Concept-Concept Context model (3C)
True answer: Concept Raw Context model, Concept-Concept Context model

Validation ID: 2a6003a74d051d0ebbe62e8883533a5f5e55078b
Question: which neural embedding model works better?
From the context "We also proposed an efficient and effective mechanism for BOC densification which outperformed the prior proposed densification schemes on dataless classification", so we know that the efficient and effective mechanism for BOC densification is better than the prior proposed densification schemes.
From the context "In addition, we demonstrated through the results how this efficient mechanism allows generating high quality dense BOC vectors from few concepts alleviating the need of obtaining hundreds of concepts when generating the concept vector", so we know that the efficient mechanism allows generating high quality dense BOC vectors from few concepts.
True answer: the CRX model

Validation ID: 2a6003a74d051d0ebbe62e8883533a5f5e55078b
Question: which neural embedding model works better?
From the context "We also proposed an efficient and effective mechanism for BOC densification which outperformed the prior proposed densification schemes on dataless classification", so we know that the efficient and effective mechanism for BOC densification is better than the prior proposed densification schemes.
From the context "In addition, we demonstrated through the results how this efficient mechanism allows generating high quality dense BOC vectors from few concepts alleviating the need of obtaining hundreds of concepts when generating the concept vector", so we know that the efficient mechanism allows generating high quality dense BOC vectors from few concepts.
True answer: 3C model

Validation ID: 1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b
Question: What is the degree of dimension reduction of the efficient aggregation method?
From the context "In addition, we demonstrated through the results how this efficient mechanism allows generating high quality dense BOC vectors from few concepts alleviating the need of obtaining hundreds of concepts when generating the concept vector", so we know that the degree of dimension reduction of the efficient aggregation method is hundreds of concepts.
Answer: hundreds of concepts
True answer: The number of dimensions can be reduced by up to 212 times.

Validation ID: 9c44df7503720709eac933a15569e5761b378046
Question: For which languages do they build word embeddings for?
From the context "We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations BIBREF7, BIBREF8, BIBREF9, BIBREF10, but the method proposed here should transfer to GloVe unchanged.", so we know that they build word embeddings for PPMI-SVD and GloVe.
Answer: PPMI-SVD and GloVe
True answer: Unanswerable

Validation ID: 9c44df7503720709eac933a15569e5761b378046
Question: For which languages do they build word embeddings for?
From the context "We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations BIBREF7, BIBREF8, BIBREF9, BIBREF10, but the method proposed here should transfer to GloVe unchanged.", so we know that they build word embeddings for PPMI-SVD and GloVe.
Answer: PPMI-SVD and GloVe
True answer: English

Validation ID: b7381927764536bd97b099b6a172708125364954
Question: How do they evaluate their resulting word embeddings?
From the context "In this paper, we incorporated subword information (simple n-grams and unsupervised morphemes) into the LexVec word embedding model and evaluated its impact on the resulting IV and OOV word vectors", so we know that they evaluate their resulting word embeddings by comparing the impact on the resulting IV and OOV word vectors.
Answer: by comparing the impact on the resulting IV and OOV word vectors
True answer: We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.

Validation ID: df95b3cb6aa0187655fd4856ae2b1f503d533583
Question: What types of subwords do they incorporate in their model?
From the context "We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams.", so we know that they incorporate n-gram subwords and unsupervised morphemes in their model.
Answer: n-gram subwords and unsupervised morphemes
True answer: n-gram subwords, unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords 

Validation ID: df95b3cb6aa0187655fd4856ae2b1f503d533583
Question: What types of subwords do they incorporate in their model?
From the context "We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams.", so we know that they incorporate n-gram subwords and unsupervised morphemes in their model.
Answer: n-gram subwords and unsupervised morphemes
True answer: simple n-grams (like fastText) and unsupervised morphemes

Validation ID: f7ed3b9ed469ed34f46acde86b8a066c52ecf430
Question: Which matrix factorization methods do they use?
From the context "The LexVec objective is modified such that a word's vector is the sum of all its subword vectors. We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams.", so we know that they use n-gram subwords and unsupervised morphemes.
Answer: n-gram subwords and unsupervised morphemes
True answer: weighted factorization of a word-context co-occurrence matrix 

Validation ID: f7ed3b9ed469ed34f46acde86b8a066c52ecf430
Question: Which matrix factorization methods do they use?
From the context "The LexVec objective is modified such that a word's vector is the sum of all its subword vectors. We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams.", so we know that they use n-gram subwords and unsupervised morphemes.
Answer: n-gram subwords and unsupervised morphemes
True answer: The LexVec BIBREF7

Validation ID: c7eb71683f53ab7acffd691a36cad6edc7f5522e
Question: Do they report results only on English data?
From the context "The proposed method and interpretability measurements are both based on utilizing concepts represented by word-groups. Therefore it is expected that there will be higher interpretability scores for some of the dimensions for which the imparted concepts are also contained in SEMCAT", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: c7eb71683f53ab7acffd691a36cad6edc7f5522e
Question: Do they report results only on English data?
From the context "The proposed method and interpretability measurements are both based on utilizing concepts represented by word-groups. Therefore it is expected that there will be higher interpretability scores for some of the dimensions for which the imparted concepts are also contained in SEMCAT", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 17a1eff7993c47c54eddc7344e7454fbe64191cd
Question: What experiments do they use to quantify the extent of interpretability?
From the context "Fig. FIGREF18 presents the measured average interpretability scores across dimensions for original GloVe embeddings, for the proposed method and for the other four methods we compare, along with a randomly generated embedding. Results are calculated for the parameters INLINEFORM0 and INLINEFORM1. Our proposed method significantly improves the interpretability for all INLINEFORM2 compared to the original GloVe approach. Our proposed method is second to only SPINE in increasing interpretability. However, as we will experimentally demonstrate in the next subsection, in doing this, SPINE almost entirely destroys the underlying semantic structure of the word embeddings, which is the primary function of a word embedding.", so we know that they use the average interpretability scores across dimensions for original GloVe embeddings, for the proposed method and for the other four methods they compare, along with a randomly generated embedding. Results are calculated for the parameters INLINEFORM0 and INLINEFORM1.
Answer: average interpretability scores across dimensions for original GloVe embeddings, for the proposed method and for the other four methods they compare, along with a randomly generated embedding. Results are calculated for the parameters INLINEFORM0 and INLINEFORM1.
True answer: Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT).

Validation ID: 17a1eff7993c47c54eddc7344e7454fbe64191cd
Question: What experiments do they use to quantify the extent of interpretability?
From the context "Fig. FIGREF18 presents the measured average interpretability scores across dimensions for original GloVe embeddings, for the proposed method and for the other four methods we compare, along with a randomly generated embedding. Results are calculated for the parameters INLINEFORM0 and INLINEFORM1. Our proposed method significantly improves the interpretability for all INLINEFORM2 compared to the original GloVe approach. Our proposed method is second to only SPINE in increasing interpretability. However, as we will experimentally demonstrate in the next subsection, in doing this, SPINE almost entirely destroys the underlying semantic structure of the word embeddings, which is the primary function of a word embedding.", so we know that they use the average interpretability scores across dimensions for original GloVe embeddings, for the proposed method and for the other four methods they compare, along with a randomly generated embedding.
Answer: average interpretability scores across dimensions
True answer: semantic category-based approach

Validation ID: a5e5cda1f6195ab1336855f1e39a609d61326d62
Question: Along which dimension do the semantically related words take larger values?
From the context "Fig. FIGREF18 presents the measured average interpretability scores across dimensions for original GloVe embeddings, for the proposed method and for the other four methods we compare, along with a randomly generated embedding. Results are calculated for the parameters INLINEFORM0 and INLINEFORM1. Our proposed method significantly improves the interpretability for all INLINEFORM2 compared to the original GloVe approach.", so we know that the proposed method significantly improves the interpretability for all dimensions compared to the original GloVe approach.
Answer: all dimensions
True answer: dimension corresponding to the concept that the particular word belongs to

Validation ID: 32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9
Question: What is the additive modification to the objective function?
From the context "In ( SECREF4 ), INLINEFORM2 denotes the indices for the elements of the INLINEFORM3 th concept word-group which we wish to assign in the vector dimension INLINEFORM4. The objective ( SECREF4 ) is designed as a mixture of two individual cost terms: the original GloVe cost term along with a second term that encourages embedding vectors of a given concept word-group to achieve deliberately increased values along an associated dimension INLINEFORM5. The relative weight of the second term is controlled by the parameter INLINEFORM6. The simultaneous minimization of both objectives ensures that words that are similar to, but not included in, one of these concept word-groups are also “nudged” towards the associated dimension INLINEFORM7. The trained word vectors are thus encouraged to form a distribution where the individual vector dimensions align with certain semantic concepts represented by a collection of concept word-groups, one assigned to each vector dimension. To facilitate this behaviour, ( SECREF4 ) introduces a monotone decreasing function INLINEFORM8 defined as INLINEFORM9
which serves to increase the total cost incurred if the value of the INLINEFORM0 th dimension for the two vector representations INLINEFORM1 and INLINEFORM2 for a concept word INLINEFORM3 with INLINEFORM4 fails to be large enough. INLINEFORM5 is also shown in Fig. FIGREF7.", so we know that the additive modification to the objective function is a second term that encourages embedding vectors of a given concept word-group to achieve deliberately increased values along an associated dimension.
True answer: The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,

Validation ID: 32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9
Question: What is the additive modification to the objective function?
From the context "In ( SECREF4 ), INLINEFORM2 denotes the indices for the elements of the INLINEFORM3 th concept word-group which we wish to assign in the vector dimension INLINEFORM4. The objective ( SECREF4 ) is designed as a mixture of two individual cost terms: the original GloVe cost term along with a second term that encourages embedding vectors of a given concept word-group to achieve deliberately increased values along an associated dimension INLINEFORM5. The relative weight of the second term is controlled by the parameter INLINEFORM6. The simultaneous minimization of both objectives ensures that words that are similar to, but not included in, one of these concept word-groups are also “nudged” towards the associated dimension INLINEFORM7. The trained word vectors are thus encouraged to form a distribution where the individual vector dimensions align with certain semantic concepts represented by a collection of concept word-groups, one assigned to each vector dimension. To facilitate this behaviour, ( SECREF4 ) introduces a monotone decreasing function INLINEFORM8 defined as INLINEFORM9
which serves to increase the total cost incurred if the value of the INLINEFORM0 th dimension for the two vector representations INLINEFORM1 and INLINEFORM2 for a concept word INLINEFORM3 with INLINEFORM4 fails to be large enough. INLINEFORM5 is also shown in Fig. FIGREF7.", so we know that the additive modification to the objective function is a second term that encourages embedding vectors of a given concept word-group to achieve deliberately increased values along an associated dimension.
True answer: An additive term added to the cost function for any one of the words of concept word-groups

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
From the context "In this work we study the problem of learning unsupervised tweet representations. We believe our survey of the existing works based on the objective function can give vital perspectives to researchers and aid their understanding of the field. We also believe the future research directions studied in this work can help in breaking the barriers in building high quality, general purpose tweet representation models.", so we know that they use tweet dataset.
Answer: tweet dataset
True answer: Unanswerable

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
From the context "In this work we study the problem of learning unsupervised tweet representations. We believe our survey of the existing works based on the objective function can give vital perspectives to researchers and aid their understanding of the field. We also believe the future research directions studied in this work can help in breaking the barriers in building high quality, general purpose tweet representation models.", so we know that they use tweet dataset.
Answer: tweet dataset
True answer:  Paraphrase Database (PPDB) ,  book corpus

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
From the context "In this work we study the problem of learning unsupervised tweet representations. We believe our survey of the existing works based on the objective function can give vital perspectives to researchers and aid their understanding of the field. We also believe the future research directions studied in this work can help in breaking the barriers in building high quality, general purpose tweet representation models.", so we know that they use tweet dataset.
Answer: tweet dataset
True answer: Unanswerable

Validation ID: 2c7494d47b2a69f182e83455fe4c75ae3b2893e9
Question: Do they evaluate their learned representations on downstream tasks?
From the context "In this work we study the problem of learning unsupervised tweet representations. We believe our survey of the existing works based on the objective function can give vital perspectives to researchers and aid their understanding of the field. We also believe the future research directions studied in this work can help in breaking the barriers in building high quality, general purpose tweet representation models.", so we know that they evaluate their learned representations on downstream tasks.
Answer: Yes
True answer: No

Validation ID: 2c7494d47b2a69f182e83455fe4c75ae3b2893e9
Question: Do they evaluate their learned representations on downstream tasks?
From the context "In this work we study the problem of learning unsupervised tweet representations. We believe our survey of the existing works based on the objective function can give vital perspectives to researchers and aid their understanding of the field. We also believe the future research directions studied in this work can help in breaking the barriers in building high quality, general purpose tweet representation models.", so we know that they evaluate their learned representations on downstream tasks.
Answer: Yes
True answer: No

Validation ID: 4d7ff4e5d06902de85b0e9a364dc455196d06a7d
Question: Which representation learning architecture do they adopt?
From the context "Models: CHARAGRAM BIBREF20 embeds textual sequences by learning a character-based compositional model that involves addition of the vectors of its character n-grams followed by an elementwise nonlinearity. This simpler architecture trained on PPDB is able to beat models with complex architectures like CNN, LSTM on SemEval 2015 Twitter textual similarity task by a large margin. This result emphasizes the importance of character-level models that address differences due to spelling variation and word choice.", so we know that they adopt character-level models.
Answer: character-level models
True answer: Unanswerable

Validation ID: ecc63972b2783ee39b3e522653cfb6dc5917d522
Question: How do they encourage understanding of literature as part of their objective function?
From the context "We believe our survey of the existing works based on the objective function can give vital perspectives to researchers and aid their understanding of the field", so we know that they encourage understanding of literature as part of their objective function.
Answer: They encourage understanding of literature as part of their objective function
True answer: They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.

Validation ID: 8d074aabf4f51c8455618c5bf7689d3f62c4da1d
Question: What are the limitations of existing Vietnamese word segmentation systems?
From the context "The final challenge relates to building a system, which is able to incrementally learn new corpora and interactively process feedback. In particular, it is feasible to build an advance NLP system for Vietnamese based on Hadoop platform to improve system performance and to address existing limitations.", so we know that the limitations of existing Vietnamese word segmentation systems are:

True answer:  ambiguous words, unknown words

Validation ID: 8d074aabf4f51c8455618c5bf7689d3f62c4da1d
Question: What are the limitations of existing Vietnamese word segmentation systems?
From the context "The final challenge relates to building a system, which is able to incrementally learn new corpora and interactively process feedback. In particular, it is feasible to build an advance NLP system for Vietnamese based on Hadoop platform to improve system performance and to address existing limitations.", so we know that the limitations of existing Vietnamese word segmentation systems are:

True answer: lacks of complete review approaches, datasets and toolkits 

Validation ID: fe2666ace293b4bfac3182db6d0c6f03ea799277
Question: Why challenges does word segmentation in Vietnamese pose?
From the context "There are several challenges on supervised learning approaches in future work. The first challenge is to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy. In addition, applying linguistics knowledge on word context to extract useful features also enhances prediction performance.", so we know that challenges does word segmentation in Vietnamese pose is to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy. In addition, applying linguistics knowledge on word context to extract useful features also enhances prediction performance.
Answer: to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy. In addition, applying linguistics knowledge on word context to extract useful features also enhances prediction performance.
True answer: Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback.

Validation ID: fe2666ace293b4bfac3182db6d0c6f03ea799277
Question: Why challenges does word segmentation in Vietnamese pose?
From the context "There are several challenges on supervised learning approaches in future work. The first challenge is to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy. In addition, applying linguistics knowledge on word context to extract useful features also enhances prediction performance.", so we know that challenges does word segmentation in Vietnamese pose is to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy. In addition, applying linguistics knowledge on word context to extract useful features also enhances prediction performance.
Answer: to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy. In addition, applying linguistics knowledge on word context to extract useful features also enhances prediction performance.
True answer: to acquire very large Vietnamese corpus and to use them in building a classifier,  design and development of big data warehouse and analytic framework for Vietnamese documents, to building a system, which is able to incrementally learn new corpora and interactively process feedback

Validation ID: 70a1b0f9f26f1b82c14783f1b76dfb5400444aa4
Question: How successful are the approaches used to solve word segmentation in Vietnamese?
From the context "This study reviewed state-of-the-art approaches and systems of Vietnamese word segmentation. The review pointed out common features and methods used in Vietnamese word segmentation studies. This study also had an evaluation of the existing Vietnamese word segmentation toolkits based on a same corpus to show advantages and disadvantages as to shed some lights on system enhancement.", so we know that the approaches used to solve word segmentation in Vietnamese are successful.
Answer: successful
True answer: Their accuracy in word segmentation is about 94%-97%.

Validation ID: d3ca5f1814860a88ff30761fec3d860d35e39167
Question: Which approaches have been applied to solve word segmentation in Vietnamese?
From the context "There are several studies for Vietnamese Word Segmentation during last decade. For instance, Dinh et al. started the word segmentation task for Vietnamese with Neural Network and Weighted Finite State Transducer (WFST) BIBREF9. Nguyen et al. continued with machine learning approaches, Conditional Random Fields and Support Vector Machine BIBREF7. Most of statistical approaches are based on the architecture as shown in Figure 2. According to the architecture, recent studies and systems focus on either improving or modifying difference learning models to get the highest accuracy. Features used in word segmentation systems are syllable, dictionary, and entity name. The detail of all widely used techniques applied are collected and described in following subsections.", so we know that the approaches have been applied to solve word segmentation in Vietnamese are Maximum Matching, Hidden Markov Model (HMM), Maximum Entropy (ME), Conditional Random Fields and Support Vector Machines.
Answer: Maximum Matching, Hidden Markov Model (HMM), Maximum Entropy (ME), Conditional Random Fields and Support Vector Machines
True answer: Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)

Validation ID: d3ca5f1814860a88ff30761fec3d860d35e39167
Question: Which approaches have been applied to solve word segmentation in Vietnamese?
From the context "There are several studies for Vietnamese Word Segmentation during last decade. For instance, Dinh et al. started the word segmentation task for Vietnamese with Neural Network and Weighted Finite State Transducer (WFST) BIBREF9. Nguyen et al. continued with machine learning approaches, Conditional Random Fields and Support Vector Machine BIBREF7. Most of statistical approaches are based on the architecture as shown in Figure 2. According to the architecture, recent studies and systems focus on either improving or modifying difference learning models to get the highest accuracy. Features used in word segmentation systems are syllable, dictionary, and entity name. The detail of all widely used techniques applied are collected and described in following subsections.", so we know that the approaches have been applied to solve word segmentation in Vietnamese are Maximum Matching, Hidden Markov Model (HMM), Maximum Entropy (ME), Conditional Random Fields and Support Vector Machines.
Answer: Maximum Matching, Hidden Markov Model (HMM), Maximum Entropy (ME), Conditional Random Fields and Support Vector Machines
True answer: Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines

Validation ID: dd20d93166c14f1e57644cd7fa7b5e5738025cd0
Question: Which two news domains are country-independent?
From the context "We also highlighted the most discriminative features across different layers in both countries; the results suggest that differences between the two news domains might be country-independent but rather due only to the typology of content shared, and that disinformation news shape broader and deeper cascades.", so we know that the two news domains are disinformation news and mainstream news.
Answer: disinformation news and mainstream news
True answer: mainstream news and disinformation

Validation ID: dd20d93166c14f1e57644cd7fa7b5e5738025cd0
Question: Which two news domains are country-independent?
From the context "We also highlighted the most discriminative features across different layers in both countries; the results suggest that differences between the two news domains might be country-independent but rather due only to the typology of content shared, and that disinformation news shape broader and deeper cascades.", so we know that the two news domains are disinformation news and mainstream news.
Answer: disinformation news and mainstream news
True answer: mainstream and disinformation news

Validation ID: dc2a2c177cd5df6da5d03e6e74262bf424850ec9
Question: How is the political bias of different sources included in the model?
From the context "As it is reported that conservatives and liberals exhibit different behaviors on online social platforms BIBREF19BIBREF20BIBREF21, we further assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2. In order to assess the robustness of our method, we performed classification experiments by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources, as well as excluding particular sources that outweigh the others in terms of samples to avoid over-fitting.", so we know that the political bias of different sources is included in the model.
Answer: The political bias of different sources is included in the model.
True answer: By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains

Validation ID: dc2a2c177cd5df6da5d03e6e74262bf424850ec9
Question: How is the political bias of different sources included in the model?
From the context "As it is reported that conservatives and liberals exhibit different behaviors on online social platforms BIBREF19BIBREF20BIBREF21, we further assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2. In order to assess the robustness of our method, we performed classification experiments by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources, as well as excluding particular sources that outweigh the others in terms of samples to avoid over-fitting.", so we know that the political bias of different sources is included in the model.
Answer: The political bias of different sources is included in the model.
True answer: we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries.

Validation ID: ae90c5567746fe25af2fcea0cc5f355751e05c71
Question: What are the two large-scale datasets used?
From the context "We performed classification experiments using a basic off-the-shelf classifier, namely Logistic Regression (LR) with L2 penalty; this also allows us to compare results with our baseline. We applied a standardization of the features and we used the default configuration for parameters as described in scikit-learn package BIBREF33. We also tested other classifiers (such as K-Nearest Neighbors, Support Vector Machines and Random Forest) but we omit results as they give comparable performances. We remark that our goal is to show that a very simple machine learning framework, with no parameter tuning and optimization, allows for accurate results with our network-based approach.", so we know that the two large-scale datasets are US dataset and Italian dataset.
Answer: US dataset and Italian dataset
True answer: US dataset, Italian dataset

Validation ID: ae90c5567746fe25af2fcea0cc5f355751e05c71
Question: What are the two large-scale datasets used?
From the context "We performed classification experiments using a basic off-the-shelf classifier, namely Logistic Regression (LR) with L2 penalty; this also allows us to compare results with our baseline. We applied a standardization of the features and we used the default configuration for parameters as described in scikit-learn package BIBREF33. We also tested other classifiers (such as K-Nearest Neighbors, Support Vector Machines and Random Forest) but we omit results as they give comparable performances. We remark that our goal is to show that a very simple machine learning framework, with no parameter tuning and optimization, allows for accurate results with our network-based approach.", so we know that the two large-scale datasets are US dataset and Italian dataset.
Answer: US dataset and Italian dataset
True answer: US dataset, Italian dataset

Validation ID: d7644c674887ca9708eb12107acd964ae53b216d
Question: What are the global network features which quantify different aspects of the sharing process?
From the context "We used a set of global network indicators which allow us to encode each network layer by a tuple of features. Then we simply concatenated tuples as to represent each multi-layer network with a single feature vector. We used the following global network properties:", so we know that the global network features which quantify different aspects of the sharing process are:
Number of Strongly Connected Components (SCC)
True answer: Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)

Validation ID: a3bb9a936f61bafb509fa12ac0a61f91abcc5106
Question: Which datasets are used for evaluation?
From the context "We compare the performance of BERT-QC with recently reported performance on this dataset in Table TABREF11. BERT-QC achieves state-of-the-art performance on fine-grained classification (TREC-50) for a learned model at 92.0% accuracy, and near state-of-the-art performance on coarse classification (TREC-6) at 96.2% accuracy.", so we know that BERT-QC is evaluated on TREC-50 and TREC-6.
Answer: TREC-50 and TREC-6
True answer: ARC , TREC, GARD , MLBioMedLAT 

Validation ID: a3bb9a936f61bafb509fa12ac0a61f91abcc5106
Question: Which datasets are used for evaluation?
From the context "We compare the performance of BERT-QC with recently reported performance on this dataset in Table TABREF11. BERT-QC achieves state-of-the-art performance on fine-grained classification (TREC-50) for a learned model at 92.0% accuracy, and near state-of-the-art performance on coarse classification (TREC-6) at 96.2% accuracy.", so we know that BERT-QC is evaluated on TREC-50 and TREC-6.
Answer: TREC-50 and TREC-6
True answer: ARC, TREC, GARD, MLBioMedLAT

Validation ID: df6d327e176740da9edcc111a06374c54c8e809c
Question: What previous methods is their model compared to?
From the context "We compare the performance of BERT-QC with recently reported performance on this dataset in Table TABREF11. BERT-QC achieves state-of-the-art performance on fine-grained classification (TREC-50) for a learned model at 92.0% accuracy, and near state-of-the-art performance on coarse classification (TREC-6) at 96.2% accuracy.", so we know that the model is compared to the state-of-the-art performance on TREC-50 and TREC-6.
Answer: TREC-50 and TREC-6
True answer: bag-of-words model, CNN

Validation ID: 49764eee7fb523a6a28375cc699f5e0220b81766
Question: Did they use a crowdsourcing platform?
From the context "Annotation proceeded in a two-stage process, where in stage 1 annotators completed their annotation independently, and in stage 2 each of the questions where the annotators did not have complete agreement were manually resolved by the annotators, resulting in high-quality", so we know that they used a crowdsourcing platform.
Answer: Yes
True answer: No

Validation ID: 49764eee7fb523a6a28375cc699f5e0220b81766
Question: Did they use a crowdsourcing platform?
From the context "Annotation proceeded in a two-stage process, where in stage 1 annotators completed their annotation independently, and in stage 2 each of the questions where the annotators did not have complete agreement were manually resolved by the annotators, resulting in high-quality", so we know that they used a crowdsourcing platform.
Answer: Yes
True answer: No

Validation ID: 3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1
Question: How was the dataset collected?
From the context "We make use of the 7,787 science exam questions of the Aristo Reasoning Challenge (ARC) corpus BIBREF31, which contains standardized 3rd to 9th grade science questions from 12 US states from the past decade", so we know that the dataset was collected from standardized 3rd to 9th grade science questions from 12 US states from the past decade.
Answer: standardized 3rd to 9th grade science questions from 12 US states from the past decade
True answer: from 3rd to 9th grade science questions collected from 12 US states

Validation ID: 3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1
Question: How was the dataset collected?
From the context "We make use of the 7,787 science exam questions of the Aristo Reasoning Challenge (ARC) corpus BIBREF31, which contains standardized 3rd to 9th grade science questions from 12 US states from the past decade", so we know that the dataset was collected from standardized 3rd to 9th grade science questions from 12 US states from the past decade.
Answer: standardized 3rd to 9th grade science questions from 12 US states from the past decade
True answer: Used from  science exam questions of the Aristo Reasoning Challenge (ARC) corpus.

Validation ID: bb3267c3f0a12d8014d51105de5d81686afe5f1b
Question: Which datasets do they use?
From the context "We evaluate the trained model on five different benchmarks to verify the linking precision as well as the generalization ability. Furthermore, we investigate the effectiveness of key modules in NCEL and give qualitative results for comprehensive analysis.", so we know that they use five different benchmarks.
Answer: five different benchmarks
True answer: CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW

Validation ID: bb3267c3f0a12d8014d51105de5d81686afe5f1b
Question: Which datasets do they use?
From the context "We evaluate the trained model on five different benchmarks to verify the linking precision as well as the generalization ability. Furthermore, we investigate the effectiveness of key modules in NCEL and give qualitative results for comprehensive analysis.", so we know that they use five different benchmarks.
Answer: five different benchmarks
True answer: CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW

Validation ID: 114934e1a1e818630ff33ac5c4cd4be6c6f75bb2
Question: How effective is their NCEL approach overall?
From the context "In experiments, we first verify the efficiency of NCEL via theoretically comparing its time complexity with other collective alternatives. Afterwards, we train our neural model using collected Wikipedia hyperlinks instead of dataset-specific annotations, and perform evaluations on five public available benchmarks. The results show that NCEL consistently outperforms various baselines with a favorable generalization ability. Finally, we further present the performance on a challenging dataset WW BIBREF19 as well as qualitative results, investigating the effectiveness of each key module.", so we know that NCEL is effective overall.
Answer: effective
True answer: NCEL consistently outperforms various baselines with a favorable generalization ability

Validation ID: 2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72
Question: How do they verify generalization ability?
From the context "In experiments, we first verify the efficiency of NCEL via theoretically comparing its time complexity with other collective alternatives. Afterwards, we train our neural model using collected Wikipedia hyperlinks instead of dataset-specific annotations, and perform evaluations on five public available benchmarks. The results show that NCEL consistently outperforms various baselines with a favorable generalization ability.", so we know that they verify generalization ability by comparing the performance of NCEL with other collective alternatives on five public available benchmarks.
Answer: by comparing the performance of NCEL with other collective alternatives on five public available benchmarks
True answer: By calculating Macro F1 metric at the document level.

Validation ID: 2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72
Question: How do they verify generalization ability?
From the context "In experiments, we first verify the efficiency of NCEL via theoretically comparing its time complexity with other collective alternatives. Afterwards, we train our neural model using collected Wikipedia hyperlinks instead of dataset-specific annotations, and perform evaluations on five public available benchmarks. The results show that NCEL consistently outperforms various baselines with a favorable generalization ability.", so we know that they verify generalization ability by comparing the performance of NCEL with other collective alternatives on five public available benchmarks.
Answer: by comparing the performance of NCEL with other collective alternatives on five public available benchmarks
True answer: by evaluating their model on five different benchmarks

Validation ID: b8d0e4e0e820753ffc107c1847fe1dfd48883989
Question: Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?
From the context "Suppose neighbor mentions are topical coherent, a candidate entity shall also be compatible with neighbor mentions if it has a high compatibility score with the current mention, otherwise not. That is, we extract the vectorial global features by computing the similarities between INLINEFORM0 and all neighbor mentions: INLINEFORM1, where INLINEFORM2 is the mention embedding by averaging the global vectors of words in its surface form: INLINEFORM3, where INLINEFORM4 are tokenized words of mention INLINEFORM5.", so we know that they only use adjacent entity mentions.
Answer: adjacent entity mentions
True answer: NCEL considers only adjacent mentions.

Validation ID: b8d0e4e0e820753ffc107c1847fe1dfd48883989
Question: Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?
From the context "Suppose INLINEFORM0 is the hidden states of the neighbor INLINEFORM1, we stack them to expand the current hidden states of INLINEFORM2 as INLINEFORM3, such that each row corresponds to that in the subgraph adjacent matrix INLINEFORM4.", so we know that they only use adjacent entity mentions.
Answer: adjacent entity mentions
True answer: More than that in some cases (next to adjacent) 

Validation ID: 5aa12b4063d6182a71870c98e4e1815ff3dc8a72
Question: Do the authors mention any downside of lemmatizing input before training ELMo?
From the context "The take-home message here is twofold: first, text pre-processing still matters for contemporary deep learning algorithms. Their impressive learning abilities do not always allow them to infer normalisation rules themselves, from simply optimising the language modelling task. Second, the nature of language at hand matters as well, and differences in this nature can result in different decisions being optimal or sub-optimal at the stage of deep learning models training. The simple truth `English is not representative of all languages on Earth' still holds here.", so we know that the authors mention that lemmatizing input before training ELMo can result in different decisions being optimal or sub-optimal at the stage of deep learning models training.
Answer: Yes
True answer: Yes

Validation ID: 5aa12b4063d6182a71870c98e4e1815ff3dc8a72
Question: Do the authors mention any downside of lemmatizing input before training ELMo?
From the context "The take-home message here is twofold: first, text pre-processing still matters for contemporary deep learning algorithms. Their impressive learning abilities do not always allow them to infer normalisation rules themselves, from simply optimising the language modelling task. Second, the nature of language at hand matters as well, and differences in this nature can result in different decisions being optimal or sub-optimal at the stage of deep learning models training. The simple truth `English is not representative of all languages on Earth' still holds here.", so we know that the authors mention that lemmatizing input before training ELMo can result in different decisions being optimal or sub-optimal at the stage of deep learning models training.
Answer: Yes
True answer: Yes

Validation ID: 22815878083ebd2f9e08bc33a5e733063dac7a0f
Question: What other examples of morphologically-rich languages do the authors give?
From the context "But we only scratched the surface of the linguistic diversity. To verify this claim, it is necessary to analyse more strongly inflected languages like Russian as well as more weakly inflected (analytical) languages similar to English", so we know that the authors give Russian and English as examples of morphologically-rich languages.
Answer: Russian and English
True answer: Russian

Validation ID: 22815878083ebd2f9e08bc33a5e733063dac7a0f
Question: What other examples of morphologically-rich languages do the authors give?
From the context "But we only scratched the surface of the linguistic diversity. To verify this claim, it is necessary to analyse more strongly inflected languages like Russian as well as more weakly inflected (analytical) languages similar to English", so we know that the authors give Russian and English as examples of morphologically-rich languages.
Answer: Russian and English
True answer: Russian

Validation ID: 220d11a03897d85af91ec88a9b502815c7d2b6f3
Question: Why is lemmatization not necessary in English?
From the context "In the majority of research papers on deep learning approaches to NLP, it is assumed that lemmatization is not necessary, especially when using powerful contextualised embeddings. Our experiments show that this is indeed true for languages with simple morphology (like English)", so we know that lemmatization is not necessary in English.
Answer: Because English has simple morphology
True answer: Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations.

Validation ID: d509081673f5667060400eb325a8050fa5db7cc8
Question: How big was the corpora they trained ELMo on?
From the context "For English, the training corpus consisted of the English Wikipedia dump from February 2017. For Russian, it was a concatenation of the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC). The RNC texts were added to the Russian Wikipedia dump so as to make the Russian training corpus more comparable in size to the English one (Wikipedia texts would comprise only half of the size). As Table TABREF3 shows, the English Wikipedia is still two times larger, but at least the order is the same.", so we know that the corpora they trained ELMo on was the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC).
Answer: the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC)
True answer: 2174000000, 989000000

Validation ID: d509081673f5667060400eb325a8050fa5db7cc8
Question: How big was the corpora they trained ELMo on?
From the context "For English, the training corpus consisted of the English Wikipedia dump from February 2017. For Russian, it was a concatenation of the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC). The RNC texts were added to the Russian Wikipedia dump so as to make the Russian training corpus more comparable in size to the English one (Wikipedia texts would comprise only half of the size). As Table TABREF3 shows, the English Wikipedia is still two times larger, but at least the order is the same.", so we know that the corpora they trained ELMo on was the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC).
Answer: the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC)
True answer: 2174 million tokens for English and 989 million tokens for Russian

Validation ID: c2e475adeddcdc4d637ef0d4f5065b6a9b299827
Question: What metrics are used for evaluation?
From the context "Following lebret2016neural, we used BLEU-4, NIST-4 and ROUGE-4 as the evaluation metrics", so we know that BLEU-4, NIST-4 and ROUGE-4 are used for evaluation.
Answer: BLEU-4, NIST-4 and ROUGE-4
True answer: BLEU-4, NIST-4, ROUGE-4

Validation ID: c2e475adeddcdc4d637ef0d4f5065b6a9b299827
Question: What metrics are used for evaluation?
From the context "Following lebret2016neural, we used BLEU-4, NIST-4 and ROUGE-4 as the evaluation metrics", so we know that BLEU-4, NIST-4 and ROUGE-4 are used for evaluation.
Answer: BLEU-4, NIST-4 and ROUGE-4
True answer: BLEU-4, NIST-4, ROUGE-4

Validation ID: cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5
Question: Do they use pretrained embeddings?
From the context "We also introduce two new biography datasets, one in French and one in German. These datasets were created and pre-processed using the same procedure as outlined in lebret2016neural. Specifically, we extracted the infoboxes and the first sentence from the corresponding Wikipedia article. As with the English dataset, we split the French and German datasets randomly into train (80%), test (10%) and valid (10%). The French and German datasets extracted by us has been made publicly available. The number of examples was 170K and 50K and the vocabulary size was 297K and 143K for French and German respectively.", so we know that they use pretrained embeddings.
Answer: Yes
True answer: Yes

Validation ID: cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5
Question: Do they use pretrained embeddings?
From the context "We also introduce two new biography datasets, one in French and one in German. These datasets were created and pre-processed using the same procedure as outlined in lebret2016neural. Specifically, we extracted the infoboxes and the first sentence from the corresponding Wikipedia article. As with the English dataset, we split the French and German datasets randomly into train (80%), test (10%) and valid (10%). The French and German datasets extracted by us has been made publicly available. The number of examples was 170K and 50K and the vocabulary size was 297K and 143K for French and German respectively.", so we know that they use pretrained embeddings.
Answer: Yes
True answer: Yes

Validation ID: 6cd25c637c6b772ce29e8ee81571e8694549c5ab
Question: What dataset is used?
From the context "We use the WikiBio dataset introduced by lebret2016neural. It consists of INLINEFORM0 biography articles from English Wikipedia", so we know that the dataset is WikiBio.
Answer: WikiBio
True answer: English WIKIBIO, French WIKIBIO , German WIKIBIO 

Validation ID: 6cd25c637c6b772ce29e8ee81571e8694549c5ab
Question: What dataset is used?
From the context "We use the WikiBio dataset introduced by lebret2016neural. It consists of INLINEFORM0 biography articles from English Wikipedia.", so we know that the dataset is WikiBio.
Answer: WikiBio
True answer: WikiBio dataset,  introduce two new biography datasets, one in French and one in German

Validation ID: 1088255980541382a2aa2c0319427702172bbf84
Question: What is a bifocal attention mechanism?
From the context "To capture this behavior, we use a bifocal attention mechanism as described below.", so we know that bifocal attention mechanism is used to capture this behavior.
From the context "Macro Attention: Consider the INLINEFORM0 -th field INLINEFORM1 which has values INLINEFORM2. Let INLINEFORM3 be the representation of this field in the infobox. This representation can either be (i) the word embedding of the field name or (ii) some function INLINEFORM4 of the values in the field or (iii) a concatenation of (i) and (ii). The function INLINEFORM5 could simply be the sum or average of the embeddings of the values in the field. Alternately, this function could be a GRU (or LSTM) which treats these values within a field as a sequence and computes the field representation as the final representation of this sequence (i.e., the representation of the last time-step). We found that bidirectional GRU is a better choice for INLINEFORM6 and concatenating the embedding of the field name with this GRU representation works best. Further, using a bidirectional GRU cell to take contextual information from neighboring fields also helps (these are the orange colored cells in the top-left block in Figure FIGREF3 with macro attention). Given these representations INLINEFORM7 for all the INLINEFORM8 fields we compute an attention over the fields (macro level).", so we know that bifocal attention mechanism is used to compute an attention over the fields (macro level).
True answer: At the macro level, it is important to decide which is the appropriate field to attend to next, micro level (i.e., within a field) it is important to know which values to attend to next, fuse the attention weights at the two levels

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
From the context "We can quantify this notion for a word recognition system $W$ as the expected number of unique outputs it assigns to a set of adversarial perturbations. Given a sentence $s$ from the set of sentences $\mathcal {S}$, let $A(s) = {s_1}^{\prime }, {s_2}^{\prime }, \dots, {s_n}^{\prime }$ denote the set of $n$ perturbations to it under attack type $A$, and let $V$ be the function that maps strings to an input representation for the downstream classifier. For a word level model, $V$ would transform sentences to a sequence of word ids, mapping OOV words to the same UNK ID. Whereas, for a char (or word+char, word-piece) model, $V$ would map inputs to a sequence of character IDs. Formally, sensitivity is defined as", so we know that the "sensitivity" quantity denotes the expected number of unique outputs it assigns to a set of adversarial perturbations.
Answer: the expected number of unique outputs it assigns to a set of adversarial perturbations
True answer: the number of distinct word recognition outputs that an attacker can induce

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
From the context "We can quantify this notion for a word recognition system $W$ as the expected number of unique outputs it assigns to a set of adversarial perturbations. Given a sentence $s$ from the set of sentences $\mathcal {S}$, let $A(s) = {s_1}^{\prime }, {s_2}^{\prime }, \dots, {s_n}^{\prime }$ denote the set of $n$ perturbations to it under attack type $A$, and let $V$ be the function that maps strings to an input representation for the downstream classifier. For a word level model, $V$ would transform sentences to a sequence of word ids, mapping OOV words to the same UNK ID. Whereas, for a char (or word+char, word-piece) model, $V$ would map inputs to a sequence of character IDs. Formally, sensitivity is defined as", so we know that the "sensitivity" quantity denotes the expected number of unique outputs it assigns to a set of adversarial perturbations.
Answer: the expected number of unique outputs it assigns to a set of adversarial perturbations
True answer: The expected number of unique outputs a word recognition system assigns to a set of adversarial perturbations 

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
From the context "We can quantify this notion for a word recognition system $W$ as the expected number of unique outputs it assigns to a set of adversarial perturbations. Given a sentence $s$ from the set of sentences $\mathcal {S}$, let $A(s) = {s_1}^{\prime }, {s_2}^{\prime }, \dots, {s_n}^{\prime }$ denote the set of $n$ perturbations to it under attack type $A$, and let $V$ be the function that maps strings to an input representation for the downstream classifier. For a word level model, $V$ would transform sentences to a sequence of word ids, mapping OOV words to the same UNK ID. Whereas, for a char (or word+char, word-piece) model, $V$ would map inputs to a sequence of character IDs. Formally, sensitivity is defined as", so we know that the "sensitivity" quantity denotes the expected number of unique outputs it assigns to a set of adversarial perturbations.
Answer: the expected number of unique outputs it assigns to a set of adversarial perturbations
True answer: the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is “fooled”

Validation ID: 8910ee2236a497c92324bbbc77c596dba39efe46
Question: What end tasks do they evaluate on?
From the context "We use sentiment analysis and paraphrase detection as downstream tasks, as for these two tasks, 1-2 character edits do not change the output labels.", so we know that they evaluate on sentiment analysis and paraphrase detection.
Answer: sentiment analysis and paraphrase detection
True answer: Sentiment analysis and paraphrase detection under adversarial attacks

Validation ID: 2c59528b6bc5b5dc28a7b69b33594b274908cca6
Question: What is a semicharacter architecture?
From the context "We now describe semi-character RNNs for word recognition, explain their limitations, and suggest techniques to improve them.", so we know that semi-character RNNs is a word recognition model.
From the context "Inspired by the psycholinguistic studies BIBREF5, BIBREF4, BIBREF7 proposed a semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step.", so we know that ScRNN is a semi-character based RNN.
True answer: A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters

Validation ID: 2c59528b6bc5b5dc28a7b69b33594b274908cca6
Question: What is a semicharacter architecture?
From the context "We now describe semi-character RNNs for word recognition, explain their limitations, and suggest techniques to improve them.", so we know that semi-character RNNs is a word recognition model.
From the context "Inspired by the psycholinguistic studies BIBREF5, BIBREF4, BIBREF7 proposed a semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step.", so we know that ScRNN is a semi-character based RNN.
True answer: processes a sentence of words with misspelled characters, predicting the correct words at each step

Validation ID: 6b367775a081f4d2423dc756c9b65b6eef350345
Question: Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?
From the context "We recommend word recognition as a safeguard against this and build upon RNN-based semi-character word recognizers. We discover that when used as a defense mechanism, the most accurate word recognition models are not always the most robust against adversarial attacks. Additionally, we highlight the need to control the sensitivity of these models to achieve high robustness.", so we know that the answer is yes.
Answer: Yes
True answer: No

Validation ID: bc01853512eb3c11528e33003ceb233d7c1d7038
Question: Why is the adversarial setting appropriate for misspelling recognition?
From the context "In this paper, we focus on adversarially-chosen spelling mistakes in the context of text classification, addressing the following attack types: dropping, adding, and swapping internal characters within words", so we know that the adversarial setting is appropriate for misspelling recognition.
Answer: dropping, adding, and swapping internal characters within words
True answer: Adversarial misspellings are a real-world problem

Validation ID: 67ec8ef85844e01746c13627090dc2706bb2a4f3
Question: Why do they experiment with RNNs instead of transformers for this task?
From the context "We build upon RNN-based semi-character word recognizers", so we know that they experiment with RNNs instead of transformers for this task.
Answer: They experiment with RNNs instead of transformers for this task.
True answer: Unanswerable

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
From the context "We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially.", so we know that the backoff strategies are falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK.
Answer: falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK
True answer: In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
From the context "We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially.", so we know that the backoff strategies are falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK.
Answer: falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK
True answer: Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
From the context "We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially.", so we know that the backoff strategies are falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK.
Answer: falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK
True answer: Backoff to "a" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK

Validation ID: 6bf5620f295b5243230bc97b340fae6e92304595
Question: What baseline model is used?
From the context "We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.", so we know that the baseline model is the model that assigns a semantic role to a constituent based on its syntactic function.
Answer: the model that assigns a semantic role to a constituent based on its syntactic function
True answer: same baseline as used by lang2011unsupervised

Validation ID: 6bf5620f295b5243230bc97b340fae6e92304595
Question: What baseline model is used?
From the context "We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head. If there is a total of INLINEFORM0 clusters, INLINEFORM1 most frequent syntactic functions get a cluster each, and the rest are assigned to the INLINEFORM2 th cluster.", so we know that the baseline model is assigning a semantic role to a constituent based on its syntactic function.
Answer: assigning a semantic role to a constituent based on its syntactic function
True answer: We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.

Validation ID: 4986f420884f917d1f60d3cea04dc8e64d3b5bf1
Question: Which additional latent variables are used in the model?
From the context "The multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns.", so we know that the additional latent variables are crosslingual latent variables (CLVs).
Answer: crosslingual latent variables (CLVs)
True answer: CLV as a parent of the two corresponding role variables

Validation ID: 4986f420884f917d1f60d3cea04dc8e64d3b5bf1
Question: Which additional latent variables are used in the model?
From the context "The multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns.", so we know that the additional latent variables are crosslingual latent variables (CLVs).
Answer: crosslingual latent variables (CLVs)
True answer: crosslingual latent variables

Validation ID: 747b847d687f703cc20a87877c5b138f26ff137d
Question: Which parallel corpora are used?
From the context "Following titovcrosslingual, we run our experiments on the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, and EN-DE section of the Europarl corpus BIBREF14", so we know that the parallel corpora are CoNLL 2009 corpus and Europarl corpus.
Answer: CoNLL 2009 corpus and Europarl corpus
True answer: English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14

Validation ID: 747b847d687f703cc20a87877c5b138f26ff137d
Question: Which parallel corpora are used?
From the context "Following titovcrosslingual, we run our experiments on the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, and EN-DE section of the Europarl corpus BIBREF14", so we know that the parallel corpora are CoNLL 2009 corpus and Europarl corpus.
Answer: CoNLL 2009 corpus and Europarl corpus
True answer: the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 , and EN-DE section of the Europarl corpus BIBREF14 

Validation ID: 111afb77cfbf4c98e0458606378fa63a0e965e36
Question: Overall, does having parallel data improve semantic role induction across multiple languages?
From the context "We evaluate monolingual SRI with a large unlabeled dataset, bilingual SRI with a parallel corpus, bilingual SRI with annotations available for the source language, and monolingual SRI with a small labeled dataset. Increasing the amount of monolingual unlabeled data significantly improves SRI in German but not in English. Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora.", so we know that having parallel data does not improve semantic role induction across multiple languages.
Answer: No
True answer: No

Validation ID: 111afb77cfbf4c98e0458606378fa63a0e965e36
Question: Overall, does having parallel data improve semantic role induction across multiple languages?
From the context "We evaluate monolingual SRI with a large unlabeled dataset, bilingual SRI with a parallel corpus, bilingual SRI with annotations available for the source language, and monolingual SRI with a small labeled dataset. Increasing the amount of monolingual unlabeled data significantly improves SRI in German but not in English. Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora.", so we know that having parallel data does not improve semantic role induction across multiple languages.
Answer: No
True answer: No

Validation ID: 6568a31241167f618ef5ede939053feaa2fb0d7e
Question: Do they add one latent variable for each language pair in their Bayesian model?
From the context "The multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns.", so we know that they add one latent variable for each language pair in their Bayesian model.
Answer: Yes
True answer: Yes

Validation ID: 50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed
Question: What does an individual model consist of?
From the context "The multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns.", so we know that an individual model consists of the monolingual model and additional crosslingual latent variables.
Answer: the monolingual model and additional crosslingual latent variables
True answer: Bayesian model of garg2012unsupervised as our base monolingual model

Validation ID: 0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c
Question: Do they improve on state-of-the-art semantic role induction?
From the context "We evaluate monolingual SRI with a large unlabeled dataset, bilingual SRI with a parallel corpus, bilingual SRI with annotations available for the source language, and monolingual SRI with a small labeled dataset. Increasing the amount of monolingual unlabeled data significantly improves SRI in German but not in English. Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora.", so we know that they improve on state-of-the-art semantic role induction.
Answer: Yes
True answer: Unanswerable

Validation ID: 0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c
Question: Do they improve on state-of-the-art semantic role induction?
From the context "We evaluate monolingual SRI with a large unlabeled dataset, bilingual SRI with a parallel corpus, bilingual SRI with annotations available for the source language, and monolingual SRI with a small labeled dataset. Increasing the amount of monolingual unlabeled data significantly improves SRI in German but not in English. Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora.", so we know that they improve on state-of-the-art semantic role induction.
Answer: Yes
True answer: Yes

Validation ID: 4dc268e3d482e504ca80d2ab514e68fd9b1c3af1
Question: how many tags do they look at?
From the context "We evaluate our approaches for the 2,896 e-books, for whom we got review keywords.", so we know that they evaluate their approaches for the 2,896 e-books.
Answer: 2,896
True answer: Unanswerable

Validation ID: 4dc268e3d482e504ca80d2ab514e68fd9b1c3af1
Question: how many tags do they look at?
From the context "We evaluate our approaches for the 2,896 e-books, for whom we got review keywords.", so we know that they evaluate their approaches for the 2,896 e-books.
Answer: 2,896
True answer: 48,705

Validation ID: ab54cd2dc83141bad3cb3628b3f0feee9169a556
Question: which algorithm was the highest performer?
From the context "We find that using popular tags from e-books of a specific author leads to the best accuracy of the popularity-based approaches. This suggests that editors and readers do seem to reuse tags for e-books of same authors. If we use both editor tags and Amazon search terms, we can further increase accuracy, especially for higher values of INLINEFORM1 like in the case of INLINEFORM2. This is, however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low. The reason for this is the limited amount of e-books from within the Amazon search query logs that have BISAC information (i.e., only INLINEFORM5 ).
Overall, this confirms our initial intuition that combining the best performing popularity-based approach with the best similarity-based approach should result in the highest accuracy (i.e., INLINEFORM7 for INLINEFORM8 ). Moreover, our goal, namely to exploit editor tags in combination with search terms used by readers to increase the metadata quality of e-books, is shown to be best supported by applying hybrid approaches as they provide the best prediction results.", so we know that the highest performer is INLINEFORM7.
True answer: A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach

Validation ID: 249c805ee6f2ebe4dbc972126b3d82fb09fa3556
Question: how is diversity measured?
From the context "Figure FIGREF16 shows the diversity of the tag recommendation approaches. We achieve the highest diversity with the similarity-based approaches, which extract Amazon search terms. Their accuracy is, however, very low. Thus, the combination of the two vocabularies can provide a good trade-off between recommendation accuracy and diversity.", so we know that diversity is measured by the combination of the two vocabularies.
Answer: the combination of the two vocabularies
True answer: average dissimilarity of all pairs of tags in the list of recommended tags

Validation ID: 249c805ee6f2ebe4dbc972126b3d82fb09fa3556
Question: how is diversity measured?
From the context "Figure FIGREF16 shows the diversity of the tag recommendation approaches. We achieve the highest diversity with the similarity-based approaches, which extract Amazon search terms. Their accuracy is, however, very low. Thus, the combination of the two vocabularies can provide a good trade-off between recommendation accuracy and diversity.", so we know that the diversity is measured by the combination of the two vocabularies.
Answer: the combination of the two vocabularies
True answer:  the average dissimilarity of all pairs of tags in the list of recommended tags

Validation ID: b4f881331b975e6e4cab1868267211ed729d782d
Question: how large is the vocabulary?
From the context "We then train a Doc2Vec BIBREF7 model on the content", so we know that the vocabulary is the content.
From the context "We then train a Doc2Vec BIBREF7 model on the content. Then, we use the model to infer the latent representation for both the complete list of recommended tags as well as the list of expected tags from the test set. Finally, we use the cosine similarity measure to calculate how semantically similar these two lists are.", so we know that the latent representation is the complete list of recommended tags and the list of expected tags from the test set.
True answer: 33,663

Validation ID: b4f881331b975e6e4cab1868267211ed729d782d
Question: how large is the vocabulary?
From the context "We then train a Doc2Vec BIBREF7 model on the content", so we know that the vocabulary is the content.
From the context "We then train a Doc2Vec BIBREF7 model on the content. Then, we use the model to infer the latent representation for both the complete list of recommended tags as well as the list of expected tags from the test set. Finally, we use the cosine similarity measure to calculate how semantically similar these two lists are.", so we know that the latent representation is the complete list of recommended tags and the list of expected tags from the test set.
True answer: 33,663 distinct review keywords 

Validation ID: 79413ff5d98957c31866f22179283902650b5bb6
Question: what dataset was used?
From the context "We evaluate our approaches for the 2,896 e-books, for whom we got review keywords.", so we know that the dataset used is 2,896 e-books.
Answer: 2,896 e-books
True answer: 48,705 e-books from 13 publishers, search query logs of 21,243 e-books for 12 months

Validation ID: 79413ff5d98957c31866f22179283902650b5bb6
Question: what dataset was used?
From the context "We evaluate our approaches for the 2,896 e-books, for whom we got review keywords.", so we know that the dataset used is 2,896 e-books.
Answer: 2,896 e-books
True answer:  E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.

Validation ID: 29c014baf99fb9f40b5171aab3e2c7f12a748f79
Question: what algorithms did they use?
From the context "We implement three types of tag recommendation approaches, i.e., (i) popularity-based, (ii) similarity-based (i.e., using content information), and (iii) hybrid approaches. Due to the lack of personalized tags (i.e., we do not know which user has assigned a tag), we do not implement other types of algorithms such as collaborative filtering BIBREF8. In total, we evaluate 19 different algorithms to recommend tags for annotating e-books.", so we know that they use 19 different algorithms to recommend tags for annotating e-books.
Answer: 19 different algorithms
True answer: popularity-based, similarity-based, hybrid

Validation ID: 09c86ef78e567033b725fc56b85c5d2602c1a7c3
Question: How does their ensemble method work?
From the context "The ensembles were formed by simply averaging the predictions from the constituent single models. These single models were selected using the following algorithm.", so we know that the ensemble method is to average the predictions from the constituent single models.
Answer: average the predictions from the constituent single models
True answer: simply averaging the predictions from the constituent single models

Validation ID: d67c01d9b689c052045f3de1b0918bab18c3f174
Question: How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?
From the context "If we take the best psr ensemble trained on CBT as a baseline, improving the model architecture as in BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, continuing to use the original CBT training data, lead to improvements of INLINEFORM0 and INLINEFORM1 absolute on named entities and common nouns respectively. By contrast, inflating the training dataset provided a boost of INLINEFORM2 while using the same model. The ensemble of our models even exceeded the human baseline provided by Facebook BIBREF2 on the Common Noun dataset.", so we know that the improvements of the Attention-Sum Reader model when using the BookTest dataset is INLINEFORM3.
Answer: INLINEFORM3
True answer: INLINEFORM2 

Validation ID: d67c01d9b689c052045f3de1b0918bab18c3f174
Question: How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?
From the context "If we take the best psr ensemble trained on CBT as a baseline, improving the model architecture as in BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, continuing to use the original CBT training data, lead to improvements of INLINEFORM0 and INLINEFORM1 absolute on named entities and common nouns respectively. By contrast, inflating the training dataset provided a boost of INLINEFORM2 while using the same model. The ensemble of our models even exceeded the human baseline provided by Facebook BIBREF2 on the Common Noun dataset.", so we know that the improvements of the Attention-Sum Reader model when using the BookTest dataset is INLINEFORM3.
Answer: INLINEFORM3
True answer: Answer with content missing: (Table 2) Accuracy of best AS reader results including ensembles are 78.4 and 83.7 when trained on BookTest compared to 71.0 and 68.9 when trained on CBT for Named endity and Common noun respectively.

Validation ID: e5bc73974c79d96eee2b688e578a9de1d0eb38fd
Question: How do they show there is space for further improvement?
From the context "We decided to explore the remaining space for improvement on the CBT by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly. These questions were answered by 10 non-native English speakers from our research laboratory, each on a disjoint subset of questions.. Participants had unlimited time to answer the questions and were told that these questions were not correctly answered by a machine, providing additional motivation to prove they are better than computers. The results of the human study are summarized in Table TABREF28. They show that a majority of questions that our system could not answer so far are in fact answerable. This suggests that 1) the original human baselines might have been underestimated, however, it might also be the case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement.", so we know that there is still space for improvement.
Answer: There is still space for improvement.
True answer:  by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly

Validation ID: e5bc73974c79d96eee2b688e578a9de1d0eb38fd
Question: How do they show there is space for further improvement?
From the context "We decided to explore the remaining space for improvement on the CBT by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly. These questions were answered by 10 non-native English speakers from our research laboratory, each on a disjoint subset of questions.. Participants had unlimited time to answer the questions and were told that these questions were not correctly answered by a machine, providing additional motivation to prove they are better than computers. The results of the human study are summarized in Table TABREF28. They show that a majority of questions that our system could not answer so far are in fact answerable. This suggests that 1) the original human baselines might have been underestimated, however, it might also be the case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement.", so we know that there is still space for improvement.
Answer: There is still space for improvement.
True answer: majority of questions that our system could not answer so far are in fact answerable

Validation ID: 2cd37743bcc7ea3bd405ce6d91e79e5339d7642e
Question: Do they report results only on English data?
From the context "We first present related works with a focus on annotations and then review experiments with classifying argument components, schemes, or relations.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: 2cd37743bcc7ea3bd405ce6d91e79e5339d7642e
Question: Do they report results only on English data?
From the context "We first present related works with a focus on annotations and then review experiments with classifying argument components, schemes, or relations.", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: eac9dae3492e17bc49c842fb566f464ff18c049b
Question: What argument components do the ML methods aim to identify?
From the context "We experiment on the annotated data using various machine learning methods in order to extract argument structure from documents. We propose several novel feature sets and identify configurations that run best in in-domain and cross-domain scenarios.", so we know that the ML methods aim to identify argument components.
Answer: argument components
True answer: claim, premise, backing, rebuttal, and refutation

Validation ID: eac9dae3492e17bc49c842fb566f464ff18c049b
Question: What argument components do the ML methods aim to identify?
From the context "We experiment on the annotated data using various machine learning methods in order to extract argument structure from documents. We propose several novel feature sets and identify configurations that run best in in-domain and cross-domain scenarios.", so we know that the ML methods aim to identify argument components.
Answer: argument components
True answer: claim, premise, backing, rebuttal, refutation

Validation ID: 7697baf8d8d582c1f664a614f6332121061f87db
Question: Which machine learning methods are used in experiments?
From the context "We experiment on the annotated data using various machine learning methods in order to extract argument structure from documents. We propose several novel feature sets and identify configurations that run best in in-domain and cross-domain scenarios.", so we know that the machine learning methods used in experiments are SVM, CRF, and Maximum Entropy.
Answer: SVM, CRF, and Maximum Entropy
True answer: Structural Support Vector Machine

Validation ID: 7697baf8d8d582c1f664a614f6332121061f87db
Question: Which machine learning methods are used in experiments?
From the context "We experiment on the annotated data using various machine learning methods in order to extract argument structure from documents. We propose several novel feature sets and identify configurations that run best in in-domain and cross-domain scenarios.", so we know that the machine learning methods used in experiments are:

True answer: SVMhmm 

Validation ID: 1cb100182508cf55b3509283c0e2bbcd527d625e
Question: How is the data in the new corpus come sourced?
From the context "We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date. We choose several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming. A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments to articles, discussion forum posts, blog posts, as well as professional newswire articles.", so we know that the data in the new corpus come sourced from several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming.
Answer: several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming
True answer: user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles

Validation ID: 1cb100182508cf55b3509283c0e2bbcd527d625e
Question: How is the data in the new corpus come sourced?
From the context "We create a new corpus which is, to the best of our knowledge, the largest corpus that has been annotated within the argumentation mining field to date. We choose several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming. A novel aspect of the corpus is its coverage of different registers of user-generated Web content, such as comments to articles, discussion forum posts, blog posts, as well as professional newswire articles.", so we know that the data in the new corpus come sourced from several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming.
Answer: several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming
True answer: refer to each article, blog post, comment, or forum posts as a document

Validation ID: 206739417251064b910ae9e5ff096e867ee10fb8
Question: What argumentation phenomena encounter in actual data are now accounted for by this work?
From the context "We provide a hand-analysis of all the phenomena typical to argumentation that are prevalent in our data. These findings may also serve as empirical evidence to issues that are on the spot of current argumentation research.", so we know that the argumentation phenomena encounter in actual data are now accounted for by this work.
Answer: The argumentation phenomena encounter in actual data are now accounted for by this work.
True answer: Unanswerable

Validation ID: d6401cece55a14d2a35ba797a0878dfe2deabedc
Question: What challenges do different registers and domains pose to this task?
From the context "We tackle the above-mentioned research questions as well as the previously discussed challenges and issues. First, we target user-generated Web discourse from several domains across various registers, to examine how argumentation is communicated in different contexts.", so we know that different registers and domains pose challenges to this task.
Answer: different registers and domains
True answer: linguistic variability

Validation ID: ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff
Question: who transcribed the corpus?
From the context "The DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes)", so we know that the DNN based acoustic model is a modified network from one of the best performing LibriSpeech recipes.
From the context "The acoustic model is trained for 10 epochs using lattice-free maximum mutual information (LF-MMI) with cross-entropy regularization BIBREF14", so we know that the acoustic model is trained for 10 epochs using lattice-free maximum mutual information (LF-MMI) with cross-entropy regularization.
True answer: Unanswerable

Validation ID: ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff
Question: who transcribed the corpus?
From the context "The DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes)", so we know that the DNN based acoustic model is a modified network from one of the best performing LibriSpeech recipes.
From the context "The acoustic model is trained for 10 epochs using lattice-free maximum mutual information (LF-MMI) with cross-entropy regularization BIBREF14", so we know that the acoustic model is trained for 10 epochs using lattice-free maximum mutual information (LF-MMI) with cross-entropy regularization.
True answer: Unanswerable

Validation ID: fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c
Question: how was the speech collected?
From the context "The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions", so we know that the speech was collected using an Android application.
Answer: Android application
True answer: The speech was collected from respondents using an android application.

Validation ID: fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c
Question: how was the speech collected?
From the context "The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions", so we know that the speech was collected using an Android application.
Answer: Android application
True answer: Android application

Validation ID: b1a068c1050e2bed12d5c9550c73e59cd5b1f78d
Question: what accents are present in the corpus?
From the context "The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions", so we know that the corpus contains different accents.
Answer: different accents
True answer: Unanswerable

Validation ID: b1a068c1050e2bed12d5c9550c73e59cd5b1f78d
Question: what accents are present in the corpus?
From the context "The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions", so we know that the corpus contains different accents.
Answer: different accents
True answer: Unanswerable

Validation ID: f9edd8f9c13b54d8b1253ed30e7decc1999602da
Question: what evaluation protocols are provided?
From the context "We provided several evaluation protocols for each part of the database. The protocols allow researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration and phrase text on the performance.", so we know that the evaluation protocols are provided for each part of the database.
Answer: each part of the database
True answer: three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set

Validation ID: d93c0e78a3fe890cd534a11276e934be68583f4b
Question: what age range is in the data?
From the context "The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions", so we know that the data is collected from people who have Android phones.
From the context "The project finished at the end of 2018 and the cleaned-up and final version of the database was released at the beginning of 2019", so we know that the data is collected from people who have Android phones in 2018.
True answer: Unanswerable

Validation ID: d93c0e78a3fe890cd534a11276e934be68583f4b
Question: what age range is in the data?
From the context "The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions", so we know that the data is collected from people who have Android phones.
From the context "The project finished at the end of 2018 and the cleaned-up and final version of the database was released at the beginning of 2019", so we know that the data is collected from people who have Android phones in 2018.
True answer: Unanswerable

Validation ID: 30af1926559079f59b0df055da76a3a34df8336f
Question: what is the source of the data?
From the context "The data collection project was mainly supported by Sharif DeepMine company", so we know that the source of the data is Sharif DeepMine company.
Answer: Sharif DeepMine company
True answer: Android application

Validation ID: ceb767e33fde4b927e730f893db5ece947ffb0d8
Question: what topics did they label?
From the context "We developed a classifier to label topics in the history of present illness (HPI) notes, including demographics, diagnosis history, and symptoms/signs, among others.", so we know that they labeled demographics, diagnosis history, and symptoms/signs.
Answer: demographics, diagnosis history, and symptoms/signs
True answer: Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.

Validation ID: ceb767e33fde4b927e730f893db5ece947ffb0d8
Question: what topics did they label?
From the context "We developed a classifier to label topics in the history of present illness (HPI) notes, including demographics, diagnosis history, and symptoms/signs, among others.", so we know that they labeled demographics, diagnosis history, and symptoms/signs.
Answer: demographics, diagnosis history, and symptoms/signs
True answer: Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others

Validation ID: c2cb6c4500d9e02fc9a1bdffd22c3df69655189f
Question: did they compare with other extractive summarization methods?
From the context "We demonstrate that our model can achieve excellent performance on a small dataset with known heterogeneity among annotators. This model can be applied to the 55,000 discharge summaries in MIMIC to create a dataset for evaluation of extractive summarization methods.", so we know that they did not compare with other extractive summarization methods.
Answer: No
True answer: No

Validation ID: c571deefe93f0a41b60f9886db119947648e967c
Question: what datasets were used?
From the context "We evaluated our model on the 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set.", so we know that the datasets used are 515 annotated history of present illness notes.
Answer: 515 annotated history of present illness notes
True answer: MIMIC-III

Validation ID: c571deefe93f0a41b60f9886db119947648e967c
Question: what datasets were used?
From the context "We evaluated our model on the 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set.", so we know that the datasets used are 515 annotated history of present illness notes.
Answer: 515 annotated history of present illness notes
True answer: MIMIC-III

Validation ID: 06eb9f2320451df83e27362c22eb02f4a426a018
Question: what levels of document preprocessing are looked at?
From the context "In this study, we re-assessed the performance of several keyphrase extraction models and showed that performance variation across models is partly a function of the effectiveness of the document preprocessing. Our results also suggest that supervised keyphrase extraction models are more robust to noisy input.", so we know that the levels of document preprocessing are looked at are four levels of preprocessing.
Answer: four levels of preprocessing
True answer: raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document

Validation ID: 06eb9f2320451df83e27362c22eb02f4a426a018
Question: what levels of document preprocessing are looked at?
From the context "In this study, we re-assessed the performance of several keyphrase extraction models and showed that performance variation across models is partly a function of the effectiveness of the document preprocessing. Our results also suggest that supervised keyphrase extraction models are more robust to noisy input.", so we know that the levels of document preprocessing are looked at are the effectiveness of the document preprocessing.
Answer: the effectiveness of the document preprocessing
True answer: Level 1, Level 2 and Level 3.

Validation ID: e54257585cc75564341eb02bdc63ff8111992f82
Question: what keyphrase extraction models were reassessed?
From the context "We re-implemented five keyphrase extraction models : the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task BIBREF0", so we know that the keyphrase extraction models were reassessed are five keyphrase extraction models.
Answer: five keyphrase extraction models
True answer: Answer with content missing: (LVL1, LVL2, LVL3) 
- Stanford CoreNLP
- Optical Character Recognition (OCR) system, ParsCIT 
- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.

Validation ID: 2a3e36c220e7b47c1b652511a4fdd7238a74a68f
Question: how many articles are in the dataset?
From the context "The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers)", so we know that the dataset is composed of 244 scientific articles.
Answer: 244
True answer: 244

Validation ID: 2a3e36c220e7b47c1b652511a4fdd7238a74a68f
Question: how many articles are in the dataset?
From the context "The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers)", so we know that the dataset is composed of 244 scientific articles.
Answer: 244
True answer: 244 

Validation ID: 9658b5ffb5c56e5a48a3fea0342ad8fc99741908
Question: Is this dataset publicly available for commercial use?
From the context "We hope that the release of this dataset will accelerate the development of algorithms for patient phenotyping, which in turn would significantly help medical research progress faster.", so we know that this dataset is publicly available for commercial use.
Answer: Yes
True answer: No

Validation ID: 9658b5ffb5c56e5a48a3fea0342ad8fc99741908
Question: Is this dataset publicly available for commercial use?
From the context "We hope that the release of this dataset will accelerate the development of algorithms for patient phenotyping, which in turn would significantly help medical research progress faster.", so we know that this dataset is publicly available for commercial use.
Answer: Yes
True answer: Yes

Validation ID: 46c9e5f335b2927db995a55a18b7c7621fd3d051
Question: How many different phenotypes are present in the dataset?
From the context "Each patient note was annotated by at least one clinical researcher and one resident physician for 13 high-context patient phenotypes", so we know that there are 13 different phenotypes in the dataset.
Answer: 13
True answer: 15 clinical patient phenotypes

Validation ID: 46c9e5f335b2927db995a55a18b7c7621fd3d051
Question: How many different phenotypes are present in the dataset?
From the context "Each patient note was annotated by at least one clinical researcher and one resident physician for 13 high-context patient phenotypes", so we know that there are 13 different phenotypes in the dataset.
Answer: 13
True answer: Thirteen different phenotypes are present in the dataset.

Validation ID: ce0e2a8675055a5468c4c54dbb099cfd743df8a7
Question: What are 10 other phenotypes that are annotated?
From the context "Each patient note was annotated by at least one clinical researcher and one resident physician for 13 high-context patient phenotypes", so we know that there are 13 phenotypes that are annotated.
From the context "Table defines each of the considered clinical patient phenotypes", so we know that there are 13 phenotypes that are annotated.
True answer: Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse

Validation ID: 3a6e843c6c81244c14730295cfb8b865cd7ede46
Question: What are the state of the art models?
From the context "In our generalizability test, when the pre-trained features are used with baseline features, we get 4.19% F1-score improvement over the baseline features. On the other hand, when they are not used with the baseline features, together they produce 64.25% F1-score", so we know that the state of the art models are the pre-trained features and the baseline features.
Answer: pre-trained features and the baseline features
True answer: BIBREF9 , BIBREF8 

Validation ID: 3a6e843c6c81244c14730295cfb8b865cd7ede46
Question: What are the state of the art models?
From the context "In our generalizability test, when the pre-trained features are used with baseline features, we get 4.19% F1-score improvement over the baseline features. On the other hand, when they are not used with the baseline features, together they produce 64.25% F1-score", so we know that the state of the art models are the pre-trained features and the baseline features.
Answer: pre-trained features and the baseline features
True answer: BIBREF9 , BIBREF8

Validation ID: fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf
Question: Which benchmark datasets are used?
From the context "We use the publicly available word2vec vectors, which were trained on 100 billion words from Google News. The vectors are of dimensionality 300, trained using the continuous bag-of-words architecture BIBREF33. Words not present in the set of pre-trained words are initialized randomly. However, while training the neural network, we use non-static representations. These include the word vectors, taken as input, into the list of parameters to be learned during training.", so we know that the publicly available word2vec vectors are used.
Answer: the publicly available word2vec vectors
True answer: Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset ,  dataset was created by BIBREF8,  English dataset from BIBREF8,  dataset from The Sarcasm Detector

Validation ID: fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf
Question: Which benchmark datasets are used?
From the context "We use the publicly available word2vec vectors, which were trained on 100 billion words from Google News. The vectors are of dimensionality 300, trained using the continuous bag-of-words architecture BIBREF33. Words not present in the set of pre-trained words are initialized randomly. However, while training the neural network, we use non-static representations. These include the word vectors, taken as input, into the list of parameters to be learned during training.", so we know that the publicly available word2vec vectors are used.
Answer: the publicly available word2vec vectors
True answer: This dataset was created by BIBREF8, another English dataset from BIBREF8 ,  dataset from The Sarcasm Detector

Validation ID: 1beb4a590fa6127a138f4ed1dd13d5d51cc96809
Question: What are the network's baseline features?
From the context "Our experimental results show that the baseline features outperform the pre-trained features for sarcasm detection", so we know that the baseline features are the features that outperform the pre-trained features for sarcasm detection.
Answer: the features that outperform the pre-trained features for sarcasm detection
True answer:  The features extracted from CNN.

Validation ID: 5c5aeee83ea3b34f5936404f5855ccb9869356c1
Question: What tasks are used for evaluation?
From the context "We apply our adaptively sparse Transformers on four machine translation tasks", so we know that the tasks are machine translation tasks.
Answer: machine translation tasks
True answer: four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German

Validation ID: 5c5aeee83ea3b34f5936404f5855ccb9869356c1
Question: What tasks are used for evaluation?
From the context "We apply our adaptively sparse Transformers on four machine translation tasks", so we know that the tasks are machine translation tasks.
Answer: machine translation tasks
True answer:  four machine translation tasks, IWSLT 2017 German $\rightarrow $ English BIBREF27, KFTT Japanese $\rightarrow $ English BIBREF28, WMT 2016 Romanian $\rightarrow $ English BIBREF29, WMT 2014 English $\rightarrow $ German BIBREF30

Validation ID: f8c1b17d265a61502347c9a937269b38fc3fcab1
Question: HOw does the method perform compared with baselines?
From the context "We can see that replacing softmax by entmax does not hurt performance in any of the datasets; indeed, sparse attention Transformers tend to have slightly higher BLEU, but their sparsity leads to a better potential for analysis", so we know that the method performs better than baselines.
Answer: better
True answer: On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The α-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.

Validation ID: 5913930ce597513299e4b630df5e5153f3618038
Question: How does their model improve interpretability compared to softmax transformers?
From the context "We contribute a novel strategy for adaptively sparse attention, and, in particular, for adaptively sparse Transformers. We present the first empirical analysis of Transformers with sparse attention mappings (i.e., entmax), showing potential in both translation accuracy as well as in model interpretability.", so we know that their model improves interpretability compared to softmax transformers.
Answer: Their model improves interpretability compared to softmax transformers.
True answer: the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence

Validation ID: 5913930ce597513299e4b630df5e5153f3618038
Question: How does their model improve interpretability compared to softmax transformers?
From the context "We contribute a novel strategy for adaptively sparse attention, and, in particular, for adaptively sparse Transformers. We present the first empirical analysis of Transformers with sparse attention mappings (i.e., entmax), showing potential in both translation accuracy as well as in model interpretability.", so we know that their model improves interpretability compared to softmax transformers.
Answer: Their model improves interpretability compared to softmax transformers.
True answer: We introduce sparse attention into the Transformer architecture

Validation ID: 81d193672090295e687bc4f4ac1b7a9c76ea35df
Question: What baseline method is used?
From the context "We also utilised two other datasets in English to test the cross-linguality of our approaches. One of them is a movie corpus collected from the web. There are 5,331 positive reviews and 5,331 negative reviews in this corpus. The other is a Twitter dataset, which has nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels. We have selected 7,020 positive tweets and 7,020 negative tweets randomly to generate a balanced dataset.", so we know that the baseline method is the distant supervised method.
Answer: distant supervised method
True answer: using word2vec to create features that are used as input to the SVM

Validation ID: 81d193672090295e687bc4f4ac1b7a9c76ea35df
Question: What baseline method is used?
From the context "We also utilised two other datasets in English to test the cross-linguality of our approaches. One of them is a movie corpus collected from the web. There are 5,331 positive reviews and 5,331 negative reviews in this corpus. The other is a Twitter dataset, which has nearly 1.6 million tweets annotated through a distant supervised method BIBREF14. These tweets have positive, neutral, and negative labels. We have selected 7,020 positive tweets and 7,020 negative tweets randomly to generate a balanced dataset.", so we know that the baseline method is the distant supervised method.
Answer: distant supervised method
True answer: use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach

Validation ID: cf171fad0bea5ab985c53d11e48e7883c23cdc44
Question: What details are given about the Twitter dataset?
From the context "The second Turkish dataset is the Twitter corpus which is formed of tweets about Turkish mobile network operators. Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive. These tweets are manually annotated by two humans, where the labels are either positive or negative. We measured the Cohen's Kappa inter-annotator agreement score to be 0.82. If there was a disagreement on the polarity of a tweet, we removed it.", so we know that the Twitter dataset is formed of tweets about Turkish mobile network operators, there are 1,716 tweets, 973 of them are negative and 743 of them are positive, these tweets are manually annotated by two humans, where the labels are either positive or negative, the Cohen's Kappa inter-annotator agreement score is 0.82.
Answer: The Twitter dataset is formed of tweets about Turkish mobile network operators, there are 1,716 tweets, 973 of them are negative and 743 of them are positive, these tweets are manually annotated by two humans, where the labels are either positive or negative, the Cohen's Kappa inter-annotator agreement score is 0.82.
True answer: Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive.

Validation ID: cf171fad0bea5ab985c53d11e48e7883c23cdc44
Question: What details are given about the Twitter dataset?
From the context "The second Turkish dataset is the Twitter corpus which is formed of tweets about Turkish mobile network operators. Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive. These tweets are manually annotated by two humans, where the labels are either positive or negative. We measured the Cohen's Kappa inter-annotator agreement score to be 0.82. If there was a disagreement on the polarity of a tweet, we removed it.", so we know that the Twitter dataset is formed of tweets about Turkish mobile network operators, there are 1,716 tweets, 973 of them are negative and 743 of them are positive, these tweets are manually annotated by two humans, where the labels are either positive or negative, the Cohen's Kappa inter-annotator agreement score is 0.82.
Answer: The Twitter dataset is formed of tweets about Turkish mobile network operators, there are 1,716 tweets, 973 of them are negative and 743 of them are positive, these tweets are manually annotated by two humans, where the labels are either positive or negative, the Cohen's Kappa inter-annotator agreement score is 0.82.
True answer: one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels

Validation ID: 2a564b092916f2fabbfe893cf13de169945ef2e1
Question: What details are given about the movie domain dataset?
From the context "For Turkish, as the first dataset, we utilised the movie reviews which are collected from a popular website. The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5. We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, if it is equal to or higher than 4, it is assumed to be positive. We have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.", so we know that the movie domain dataset has 20,244 reviews, the average number of words in reviews is 39, the polarity scores are between the values 0.5 and 5, at intervals of 0.5, and we have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.
Answer: 20,244 reviews, the average number of words in reviews is 39, the polarity scores are between the values 0.5 and 5, at intervals of 0.5, and we have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.
True answer: there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score

Validation ID: 2a564b092916f2fabbfe893cf13de169945ef2e1
Question: What details are given about the movie domain dataset?
From the context "For Turkish, as the first dataset, we utilised the movie reviews which are collected from a popular website. The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5. We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, if it is equal to or higher than 4, it is assumed to be positive. We have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.", so we know that the movie domain dataset has 20,244 reviews, the average number of words in reviews is 39, the polarity scores are between the values 0.5 and 5, at intervals of 0.5, and we have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.
Answer: 20,244 reviews, the average number of words in reviews is 39, the polarity scores are between the values 0.5 and 5, at intervals of 0.5, and we have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.
True answer: The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment.

Validation ID: 0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d
Question: Which hand-crafted features are combined with word2vec?
From the context "In addition to it, we extract three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores, from each review. These polarity scores of words are computed as in (DISPLAY_FORM4). For example, if a review consists of five words, it would have five polarity scores and we utilise only three of these sentiment scores as mentioned. Lastly, we concatenate these three scores to the averaged word vector per review.", so we know that the hand-crafted features are minimum, mean, and maximum polarity scores.
Answer: minimum, mean, and maximum polarity scores
True answer: three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores

Validation ID: 0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d
Question: Which hand-crafted features are combined with word2vec?
From the context "In addition to it, we extract three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores, from each review. These polarity scores of words are computed as in (DISPLAY_FORM4). For example, if a review consists of five words, it would have five polarity scores and we utilise only three of these sentiment scores as mentioned. Lastly, we concatenate these three scores to the averaged word vector per review.", so we know that the hand-crafted features are minimum, mean, and maximum polarity scores.
Answer: minimum, mean, and maximum polarity scores
True answer: polarity scores, which are minimum, mean, and maximum polarity scores, from each review

Validation ID: 73e83c54251f6a07744413ac8b8bed6480b2294f
Question: What word-based and dictionary-based feature are used?
From the context "We first construct a matrix whose row entries are corpus words and column entries are the words in their dictionary definitions. We followed the boolean approach. For instance, for the word cat, the column words occurring in its dictionary definition are given a score of 1. Those column words not appearing in the definition of cat are assigned a score of 0 for that corresponding row entry.", so we know that the word-based feature is the words in their dictionary definitions.
From the context "We first construct a matrix whose row entries are corpus words and column entries are the words in their dictionary definitions. We followed the boolean approach. For instance, for the word cat, the column words occurring in its dictionary definition are given a score of 1. Those column words not appearing in the definition of cat are assigned a score of 0 for that corresponding row entry.", so we know that the dictionary-based feature is the words in their dictionary definitions.
True answer: generate word embeddings specific to a domain, TDK (Türk Dil Kurumu - “Turkish Language Institution”) dictionary to obtain word polarities

Validation ID: 3355918bbdccac644afe441f085d0ffbbad565d7
Question: How are the supervised scores of the words calculated?
From the context "We extract these scores as follows. For a target word in the corpus, we scan through all of its contexts. In addition to the target word's polarity score (the self score), out of all the polarity scores of words occurring in the same contexts as the target word, minimum, maximum, and average scores are taken into consideration. The word polarity scores are computed using (DISPLAY_FORM4)", so we know that the supervised scores of the words are calculated using (DISPLAY_FORM4).
Answer: (DISPLAY_FORM4)
True answer: (+1 or -1), words of opposite polarities (e.g. “happy" and “unhappy") get far away from each other

Validation ID: e48e750743aef36529fbea4328b8253dbe928b4d
Question: what dataset was used?
From the context "The dev data set BIBREF19 in the competition was small hence, the train and dev sets were merged to perform 10-fold cross validation. On each fold, a model was trained and the predictions were collected on the remaining dataset. The predictions are averaged across all the folds to generalize the solution and prevent over-fitting.", so we know that the dataset was dev data set.
Answer: dev data set
True answer: WASSA-2017 Shared Task on Emotion Intensity

Validation ID: c08aab979dcdc8f4fe8ec1337c3c8290ab13414e
Question: how many total combined features were there?
From the context "The final feature vector is the concatenation of all the individual features. For example, we concatenate average word vectors, sum of NRC Affect Intensities, number of positive and negative Bing Liu lexicons, number of negation words and so on to get final feature vector.", so we know that the total combined features were there.
Answer: 14
True answer: Fourteen 

Validation ID: c08aab979dcdc8f4fe8ec1337c3c8290ab13414e
Question: how many total combined features were there?
From the context "The final feature vector is the concatenation of all the individual features. For example, we concatenate average word vectors, sum of NRC Affect Intensities, number of positive and negative Bing Liu lexicons, number of negation words and so on to get final feature vector.", so we know that the total combined features were there.
Answer: 14
True answer: Unanswerable

Validation ID: 8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f
Question: what pretrained word embeddings were used?
From the context "Few problems explained in the analysis section can be resolved with the help of sentence embeddings which take the context information into consideration. The features used in the system are generic enough to use them in other affective computing tasks on social media text, not just tweet data. Another interesting feature of lexicon-based systems is their good run-time performance during prediction, future work to benchmark the performance of the system can prove vital for deploying in a real-world setting.", so we know that the pretrained word embeddings were used.
Answer: pretrained word embeddings
True answer: Pretrained word embeddings  were not used

Validation ID: 8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f
Question: what pretrained word embeddings were used?
From the context "Few problems explained in the analysis section can be resolved with the help of sentence embeddings which take the context information into consideration. The features used in the system are generic enough to use them in other affective computing tasks on social media text, not just tweet data. Another interesting feature of lexicon-based systems is their good run-time performance during prediction, future work to benchmark the performance of the system can prove vital for deploying in a real-world setting.", so we know that the pretrained word embeddings were used.
Answer: pretrained word embeddings
True answer: GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16

Validation ID: cc608df2884e1e82679f663ed9d9d67a4b6c03f3
Question: What evaluation metrics did look at?
From the context "The evaluation metrics used in this paper are precision, recall, and F1-score", so we know that the evaluation metrics used in this paper are precision, recall, and F1-score.
Answer: precision, recall, and F1-score
True answer: precision, recall, F1 and accuracy

Validation ID: cc608df2884e1e82679f663ed9d9d67a4b6c03f3
Question: What evaluation metrics did look at?
From the context "The evaluation metrics used in this paper are precision, recall, and F1-score", so we know that the evaluation metrics used in this paper are precision, recall, and F1-score.
Answer: precision, recall, and F1-score
True answer: Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.

Validation ID: 3e432d71512ffbd790a482c716e7079ee78ce732
Question: What datasets are used?
From the context "Given that these approaches are data-oriented, a major issue is to collect a large corpora of annotated task-specific dialogs", so we know that the datasets used are large corpora of annotated task-specific dialogs.
Answer: large corpora of annotated task-specific dialogs
True answer: Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.

Validation ID: 3e432d71512ffbd790a482c716e7079ee78ce732
Question: What datasets are used?
From the context "Given that these approaches are data-oriented, a major issue is to collect a large corpora of annotated task-specific dialogs", so we know that the datasets used are large corpora of annotated task-specific dialogs.
Answer: large corpora of annotated task-specific dialogs
True answer: a self-collected financial intents dataset in Portuguese

Validation ID: dd76130ec5fac477123fe8880472d03fbafddef6
Question: What is the state of the art described in the paper?
From the context "In this section we discuss the state of the art on conversational systems in three perspectives: types of interactions, types of architecture, and types of context reasoning", so we know that the state of the art described in the paper is conversational systems in three perspectives: types of interactions, types of architecture, and types of context reasoning.
Answer: conversational systems in three perspectives: types of interactions, types of architecture, and types of context reasoning
True answer: ELIZA,  PARRY, A.L.I.C.E., Cleverbot

Validation ID: 43eecc576348411b0634611c81589f618cd4fddf
Question: What GAN models were used as baselines to compare against?
From the context "We compared our model with MLE, RL and GAN baselines. Since COCO and EMNLP2017 WMT don't have input while WeiboDial regards posts as input, we chose the following baselines respectively:", so we know that the GAN models used as baselines to compare against are SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, and DPGAN.
Answer: SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, and DPGAN
True answer: MLE, SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN

Validation ID: 43eecc576348411b0634611c81589f618cd4fddf
Question: What GAN models were used as baselines to compare against?
From the context "We compared our model with MLE, RL and GAN baselines. Since COCO and EMNLP2017 WMT don't have input while WeiboDial regards posts as input, we chose the following baselines respectively:", so we know that the GAN models used as baselines to compare against are SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, and DPGAN.
Answer: SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, and DPGAN
True answer: SeqGAN, LeakGAN, MaliGAN, DialogGAN, DPGAN

Validation ID: 79f9468e011670993fd162543d1a4b3dd811ac5d
Question: How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?
From the context "Experiments show that our model performs better than several state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks.", so we know that Adversarial Reward Augmented Maximum Likelihood (ARAML) gains better performance on three text generation tasks.
Answer: Better performance on three text generation tasks
True answer: ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.

Validation ID: 79f9468e011670993fd162543d1a4b3dd811ac5d
Question: How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?
From the context "Experiments show that our model performs better than several state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks.", so we know that Adversarial Reward Augmented Maximum Likelihood (ARAML) gains better performance on three text generation tasks.
Answer: Better performance on three text generation tasks
True answer: Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.

Validation ID: c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf
Question: Is the discriminator's reward made available at each step to the generator?
From the context "We devise the reward function INLINEFORM0 according to the discriminator's output INLINEFORM1 and the stationary distribution INLINEFORM2 : DISPLAYFORM0", so we know that the discriminator's reward is made available at each step to the generator.
Answer: Yes
True answer: Unanswerable

Validation ID: 902b3123aec0f3a39319ffa9d05ab8e08a2eb567
Question: What is the algorithm used to create word embeddings?
From the context "Participants mostly rely on the models compared in BIBREF0 and apply modifications to improve them. In particular, most teams make use of skip-gram with negative sampling (SGNS) based on BIBREF13 to learn the semantic spaces of the two time periods and orthogonal procrustes (OP) to align these vector spaces, similar to the approach by BIBREF14.", so we know that the algorithm used to create word embeddings is skip-gram with negative sampling (SGNS).
Answer: skip-gram with negative sampling (SGNS)
True answer: Unanswerable

Validation ID: 1038542243efe5ab3e65c89385e53c4831cd9981
Question: What is the corpus used for the task?
From the context "The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. The corpus version used in the task has the following format: "year [tab] lemma1 lemma2 lemma3...".", so we know that the corpus used for the task is DTA corpus.
Answer: DTA corpus
True answer: DTA18, DTA19

Validation ID: 1038542243efe5ab3e65c89385e53c4831cd9981
Question: What is the corpus used for the task?
From the context "The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. The corpus version used in the task has the following format: "year [tab] lemma1 lemma2 lemma3...".", so we know that the corpus used for the task is DTA corpus.
Answer: DTA corpus
True answer: Diachronic Usage Relatedness (DURel) gold standard data set

Validation ID: e2b0cd30cf56a4b13f96426489367024310c3a05
Question: How is evaluation performed?
From the context "The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used. The higher Spearman's rank-order correlation the better the system's performance.", so we know that evaluation is performed by comparing the model's output with the gold ranking of the DURel data set.
Answer: comparing the model's output with the gold ranking of the DURel data set
True answer: As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used

Validation ID: e2b0cd30cf56a4b13f96426489367024310c3a05
Question: How is evaluation performed?
From the context "The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used. The higher Spearman's rank-order correlation the better the system's performance.", so we know that evaluation is performed by comparing the model's output with the gold ranking of the DURel data set.
Answer: comparing the model's output with the gold ranking of the DURel data set
True answer: Spearman's rank-order correlation

Validation ID: e831041d50f3922265330fcbee5a980d0e2586dd
Question: What is a normal reading paradigm?
From the context "In the first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions", so we know that a normal reading paradigm is a task in which participants were instructed to read the sentences naturally, without any specific task other than comprehension.
Answer: a task in which participants were instructed to read the sentences naturally, without any specific task other than comprehension
True answer: read the sentences normally without any special instructions

Validation ID: e831041d50f3922265330fcbee5a980d0e2586dd
Question: What is a normal reading paradigm?
From the context "In the first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions", so we know that a normal reading paradigm is a task that participants were instructed to read the sentences naturally, without any specific task other than comprehension.
Answer: a task that participants were instructed to read the sentences naturally, without any specific task other than comprehension
True answer: participants were instructed to read the sentences naturally, without any specific task other than comprehension

Validation ID: 7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93
Question: Did they experiment with this new dataset?
From the context "We presented a new, freely available corpus of eye movement and electrical brain activity recordings during natural reading as well as during annotation. This is the first dataset that allows for the comparison between these two reading paradigms. We described the materials and experiment design in detail and conducted an extensive validation to ensure the quality of the recorded data.", so we know that they did experiment with this new dataset.
Answer: Yes
True answer: No

Validation ID: ac7f6497be4bcca64e75f28934b207c9e8097576
Question: What kind of sentences were read?
From the context "During the recording session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating", so we know that the sentences were selected from the Wikipedia corpus.
Answer: Wikipedia corpus
True answer: sentences that were selected from the Wikipedia corpus provided by culotta2006integrating

Validation ID: ac7f6497be4bcca64e75f28934b207c9e8097576
Question: What kind of sentences were read?
From the context "During the recording session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating", so we know that the sentences were selected from the Wikipedia corpus.
Answer: Wikipedia corpus
True answer: seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer

Validation ID: 87bb3105e03ed6ac5abfde0a7ca9b8de8985663c
Question: why are their techniques cheaper to implement?
From the context "We have analyzed various ways to integrate monolingual data in an NMT framework, focusing on their impact on quality and domain adaptation. While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT. When no high quality BT is available, using GANs to make the pseudo-source sentences closer to natural source sentences is an efficient solution for domain adaptation.", so we know that the techniques are cheaper to implement.
Answer: the techniques are cheaper to implement
True answer: They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper

Validation ID: 87bb3105e03ed6ac5abfde0a7ca9b8de8985663c
Question: why are their techniques cheaper to implement?
From the context "We have analyzed various ways to integrate monolingual data in an NMT framework, focusing on their impact on quality and domain adaptation. While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT. When no high quality BT is available, using GANs to make the pseudo-source sentences closer to natural source sentences is an efficient solution for domain adaptation.", so we know that the techniques are cheaper to implement.
Answer: the techniques are cheaper to implement
True answer: They do not require the availability of a backward translation engine.

Validation ID: d9980676a83295dda37c20cfd5d58e574d0a4859
Question: what data simulation techniques were introduced?
From the context "We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine. We use the following cheap ways to generate pseudo-source texts: copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary. copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. In this setup, BIBREF25 ensure that both vocabularies never overlap by marking the target word copies with a special language identifier. Therefore the English word resume cannot be confused with the homographic French word, which is marked @fr@resume. copy-dummies: instead of using actual copies, we replace each word with “dummy” tokens. We use this unrealistic setup to observe the training over noisy and hardly informative source sentences.", so we know that the data simulation techniques are copy, copy-marked and copy-dummies.
Answer: copy, copy-marked and copy-dummies
True answer: copy, copy-marked, copy-dummies

Validation ID: d9980676a83295dda37c20cfd5d58e574d0a4859
Question: what data simulation techniques were introduced?
From the context "We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine. We use the following cheap ways to generate pseudo-source texts: copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary. copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. In this setup, BIBREF25 ensure that both vocabularies never overlap by marking the target word copies with a special language identifier. Therefore the English word resume cannot be confused with the homographic French word, which is marked @fr@resume. copy-dummies: instead of using actual copies, we replace each word with “dummy” tokens. We use this unrealistic setup to observe the training over noisy and hardly informative source sentences.", so we know that the data simulation techniques are copy, copy-marked and copy-dummies.
Answer: copy, copy-marked and copy-dummies
True answer: copy, copy-marked, copy-dummies

Validation ID: 9225b651e0fed28d4b6261a9f6b443b52597e401
Question: what is their explanation for the effectiveness of back-translation?
From the context "We have analyzed various ways to integrate monolingual data in an NMT framework, focusing on their impact on quality and domain adaptation. While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT. When no high quality BT is available, using GANs to make the pseudo-source sentences closer to natural source sentences is an efficient solution for domain adaptation.", so we know that the effectiveness of back-translation is that it is an efficient solution for domain adaptation.
Answer: an efficient solution for domain adaptation
True answer: when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources

Validation ID: 565189b672efee01d22f4fc6b73cd5287b2ee72c
Question: what dataset is used?
From the context "We have followed BIBREF29 and reimplemented their deep-fusion technique. It requires to first independently learn a RNN-LM on the in-domain target data with a cross-entropy objective; then to train the optimal combination of the translation and the language models by adding the hidden state of the RNN-LM as an additional input to the softmax layer of the decoder.", so we know that the dataset is in-domain target data.
Answer: in-domain target data
True answer: Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)

Validation ID: 565189b672efee01d22f4fc6b73cd5287b2ee72c
Question: what dataset is used?
From the context "We have followed BIBREF29 and reimplemented their deep-fusion technique. It requires to first independently learn a RNN-LM on the in-domain target data with a cross-entropy objective; then to train the optimal combination of the translation and the language models by adding the hidden state of the RNN-LM as an additional input to the softmax layer of the decoder.", so we know that the dataset is in-domain target data.
Answer: in-domain target data
True answer: Europarl tests from 2006, 2007, 2008; WMT newstest 2014.

Validation ID: b6f7fadaa1bb828530c2d6780289f12740229d84
Question: what language pairs are explored?
From the context "We have followed BIBREF29 and reimplemented their deep-fusion technique. It requires to first independently learn a RNN-LM on the in-domain target data with a cross-entropy objective; then to train the optimal combination of the translation and the language models by adding the hidden state of the RNN-LM as an additional input to the softmax layer of the decoder.", so we know that the language pairs are English-French and English-German.
Answer: English-French and English-German
True answer: English-German, English-French.

Validation ID: b6f7fadaa1bb828530c2d6780289f12740229d84
Question: what language pairs are explored?
From the context "We have followed BIBREF29 and reimplemented their deep-fusion technique. It requires to first independently learn a RNN-LM on the in-domain target data with a cross-entropy objective; then to train the optimal combination of the translation and the language models by adding the hidden state of the RNN-LM as an additional input to the softmax layer of the decoder.", so we know that the language pairs are English-French and English-German.
Answer: English-French and English-German
True answer: English-German, English-French

Validation ID: 7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613
Question: what language is the data in?
From the context "We have followed BIBREF29 and reimplemented their deep-fusion technique. It requires to first independently learn a RNN-LM on the in-domain target data with a cross-entropy objective; then to train the optimal combination of the translation and the language models by adding the hidden state of the RNN-LM as an additional input to the softmax layer of the decoder.", so we know that the data is in English.
Answer: English
True answer: English , German, French

Validation ID: 4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f
Question: Does the experiments focus on a specific domain?
From the context "We obtained our Quora dataset BIBREF7 through web-based crawls between June 2014 to August 2014. This crawling exercise has resulted in the accumulation of a massive Q&A dataset spanning over a period of over four years starting from January 2010 to May 2014. We initiated crawling with 100 questions randomly selected from different topics so that different genre of questions can be covered. The crawling of the questions follow a BFS pattern through the related question links. We obtained 822,040 unique questions across 80,253 different topics with a total of 1,833,125 answers to these questions. For each question, we separately crawl their revision logs that contain different types of edit information for the question and the activity log of the question asker.", so we know that the experiments focus on a specific domain.
Answer: Yes
True answer: No

Validation ID: 4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f
Question: Does the experiments focus on a specific domain?
From the context "We obtained our Quora dataset BIBREF7 through web-based crawls between June 2014 to August 2014. This crawling exercise has resulted in the accumulation of a massive Q&A dataset spanning over a period of over four years starting from January 2010 to May 2014. We initiated crawling with 100 questions randomly selected from different topics so that different genre of questions can be covered. The crawling of the questions follow a BFS pattern through the related question links. We obtained 822,040 unique questions across 80,253 different topics with a total of 1,833,125 answers to these questions. For each question, we separately crawl their revision logs that contain different types of edit information for the question and the activity log of the question asker.", so we know that the experiments focus on a specific domain.
Answer: Yes
True answer: No

Validation ID: 6c96e910bd98c9fd58ba2050f99b9c9bac69840a
Question: how many training samples do you have for training?
From the context "In this section, we describe the prediction framework in detail. Our goal is to predict whether a given question after a time period $t$ will be answered or not.", so we know that the prediction framework is used to predict whether a given question after a time period $t$ will be answered or not.
Answer: whether a given question after a time period $t$ will be answered or not
True answer: Unanswerable

Validation ID: 6c96e910bd98c9fd58ba2050f99b9c9bac69840a
Question: how many training samples do you have for training?
From the context "In this section, we describe the prediction framework in detail. Our goal is to predict whether a given question after a time period $t$ will be answered or not.", so we know that the prediction framework is used to predict whether a given question after a time period $t$ will be answered or not.
Answer: whether a given question after a time period $t$ will be answered or not
True answer: Unanswerable

Validation ID: 9af3142630b350c93875441e1e1767312df76d17
Question: Do the answered questions measure for the usefulness of the answer?
From the context "The content and way of posing a question is important to attract answers. We have observed in the previous section that these linguistic as well as psycholinguistic aspects of the question asker are discriminatory factors. For the prediction, we use the following features:", so we know that the answered questions measure for the usefulness of the answer.
Answer: usefulness of the answer
True answer: No

Validation ID: e374169ee10f835f660ab8403a5701114586f167
Question: What profile metadata is used for this analysis?
From the context "We consider 5 major profile attributes for further analysis. These attributes are username, display name, profile image, location and description respectively.", so we know that the profile metadata used for this analysis are username, display name, profile image, location and description.
Answer: username, display name, profile image, location and description
True answer: username, display name, profile image, location, description

Validation ID: e374169ee10f835f660ab8403a5701114586f167
Question: What profile metadata is used for this analysis?
From the context "We consider 5 major profile attributes for further analysis. These attributes are username, display name, profile image, location and description respectively.", so we know that the profile metadata used for this analysis are username, display name, profile image, location and description.
Answer: username, display name, profile image, location and description
True answer: username, display name, profile image, location and description

Validation ID: 82595ca5d11e541ed0c3353b41e8698af40a479b
Question: What are the organic and inorganic ways to show political affiliation through profile changes?
From the context "We argue that we can predict the political inclination of a user using just the profile attribute of the users. We further show that the presence of party name in the profile attribute can be considered as an organic behavior and signals support to a party. However, we argue that the addition of election campaign related keywords", so we know that the organic ways to show political affiliation through profile changes are the presence of party name in the profile attribute.
From the context "We further show that the addition of election campaign related keywords to the profile attributes is an inorganic behavior and signals support to a party or a leader", so we know that the inorganic ways to show political affiliation through profile changes are the addition of election campaign related keywords to the profile attributes.
True answer: Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.
Inorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile.

Validation ID: 82595ca5d11e541ed0c3353b41e8698af40a479b
Question: What are the organic and inorganic ways to show political affiliation through profile changes?
From the context "We argue that we can predict the political inclination of a user using just the profile attribute of the users. We further show that the presence of party name in the profile attribute can be considered as an organic behavior and signals support to a party. However, we argue that the addition of election campaign related keywords", so we know that the organic ways to show political affiliation through profile changes are the presence of party name in the profile attribute.
From the context "We further show that the addition of election campaign related keywords to the profile attributes is an inorganic behavior and signals support to a party or a leader", so we know that the inorganic ways to show political affiliation through profile changes are the addition of election campaign related keywords to the profile attributes.
True answer: Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way.

Validation ID: d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f
Question: How do profile changes vary for influential leads and their followers over the social movement?
From the context "We further show that the presence of party name in the profile attribute can be considered as an organic behavior and signals support to a party. However, we argue that the addition of election campaign related keywords", so we know that the profile changes vary for influential leads and their followers over the social movement.
Answer: The profile changes vary for influential leads and their followers over the social movement.
True answer: Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.

Validation ID: 53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e
Question: What evaluation metrics do they use?
From the context "We test the performance of all baseline systems using the three generative metrics mentioned in Section SECREF22. As shown in Table TABREF40, there is a large performance gap between human performance and all baseline methods, including BERT, which has achieved superhuman performance on SQuAD. This confirms than TweetQA is more challenging than formal-test RC tasks.", so we know that they use BLEU-1, Meteor and Rouge-L.
Answer: BLEU-1, Meteor and Rouge-L
True answer: BLEU-1, Meteor , Rouge-L 

Validation ID: 53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e
Question: What evaluation metrics do they use?
From the context "We test the performance of all baseline systems using the three generative metrics mentioned in Section SECREF22. As shown in Table TABREF40, there is a large performance gap between human performance and all baseline methods, including BERT, which has achieved superhuman performance on SQuAD. This confirms than TweetQA is more challenging than formal-test RC tasks.", so we know that they use BLEU-1, Meteor and Rouge-L.
Answer: BLEU-1, Meteor and Rouge-L
True answer: BLEU-1, Meteor ,  Rouge-L 

Validation ID: 869feb7f47606105005efdb6bea1c549824baea0
Question: What is the size of this dataset?
From the context "We then use Amazon Mechanical Turk to collect question-answer pairs for the filtered tweets. For each Human Intelligence Task (HIT), we ask the worker to read three tweets and write two question-answer pairs for each tweet.", so we know that the size of this dataset is two question-answer pairs for each tweet.
Answer: two question-answer pairs for each tweet
True answer: 13,757

Validation ID: 869feb7f47606105005efdb6bea1c549824baea0
Question: What is the size of this dataset?
From the context "We then use Amazon Mechanical Turk to collect question-answer pairs for the filtered tweets. For each Human Intelligence Task (HIT), we ask the worker to read three tweets and write two question-answer pairs for each tweet.", so we know that the size of this dataset is two question-answer pairs for each tweet.
Answer: two question-answer pairs for each tweet
True answer: 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs

Validation ID: c497e8701060583d91bb64b9f9202d40047effc4
Question: How do they determine if tweets have been used by journalists?
From the context "Rather than naively obtaining tweets from Twitter using the Twitter API which can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implying that such tweets contain useful and relevant information", so we know that they determine if tweets have been used by journalists by checking if the tweets are used in news articles.
Answer: by checking if the tweets are used in news articles
True answer:  we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles

Validation ID: 8060a773f6a136944f7b59758d08cc6f2a59693b
Question: how small of a dataset did they train on?
From the context "As a result, its CER decrease relatively 14%, compared with the 2-layers regular trained model.", so we know that the size of training dataset is 14% of the size of training dataset.
From the context "If a great model has well trained from generic scenario, only 14% of the size of training dataset is needed to train a more accuracy acoustic model for specific scenario.", so we know that the size of training dataset is 14% of the size of training dataset.
True answer: 1000 hours data

Validation ID: 8060a773f6a136944f7b59758d08cc6f2a59693b
Question: how small of a dataset did they train on?
From the context "As a result, its CER decrease relatively 14%, compared with the 2-layers regular trained model.", so we know that the size of training dataset is 14% of the size of training dataset.
From the context "If a great model has well trained from generic scenario, only 14% of the size of training dataset is needed to train a more accuracy acoustic model for specific scenario.", so we know that the size of training dataset is 14% of the size of training dataset.
True answer: 23085 hours of data

Validation ID: 1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306
Question: what was their character error rate?
From the context "As a result, its CER decrease relatively 14%, compared with the 2-layers regular trained model", so we know that the character error rate of the 2-layers regular trained model is 14%.
Answer: 14%
True answer: 2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.

Validation ID: 1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306
Question: what was their character error rate?
From the context "As a result, its CER decrease relatively 14%, compared with the 2-layers regular trained model", so we know that the character error rate of the 2-layers regular trained model is 14%.
Answer: 14%
True answer: Their best model achieved a 2.49% Character Error Rate.

Validation ID: c0af8b7bf52dc15e0b33704822c4a34077e09cd1
Question: which lstm models did they compare with?
From the context "The recognition performance improves when the network goes deep. Distillation makes it possible that deep LSTM model transfer its knowledge to shallow model with little loss. The model could be distilled to 2-layers model with very low RTF, so that it can display the immediate recognition results. As a result, its CER decrease relatively 14%, compared with the 2-layers regular trained model.", so we know that they compared with 2-layers regular trained model.
Answer: 2-layers regular trained model
True answer: Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.

Validation ID: 9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e
Question: Do they use datasets with transcribed text or do they determine text from the audio?
From the context "We assume that speech transcripts can be extracted from audio signals with high accuracy, given the advancement of ASR technologies BIBREF7. We attempt to use the processed textual information as another modality in predicting the emotion class of a given signal.", so we know that they determine text from the audio.
Answer: They determine text from the audio.
True answer: They use text transcription.

Validation ID: 9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e
Question: Do they use datasets with transcribed text or do they determine text from the audio?
From the context "We assume that speech transcripts can be extracted from audio signals with high accuracy, given the advancement of ASR technologies BIBREF7. We attempt to use the processed textual information as another modality in predicting the emotion class of a given signal.", so we know that they determine text from the audio.
Answer: They determine text from the audio.
True answer: both

Validation ID: e0122fc7b0143d5cbcda2120be87a012fb987627
Question: By how much does their model outperform the state of the art results?
From the context "Extensive experiments show that our proposed model outperforms other state-of-the-art methods in classifying the four emotion categories, and accuracies ranging from 68.8% to 71.8% are obtained when the model is applied to the IEMOCAP dataset.", so we know that their model outperform the state of the art results by 68.8% to 71.8%.
Answer: 68.8% to 71.8%
True answer: the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)

Validation ID: 5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4
Question: How do they combine audio and text sequences in their RNN?
From the context "The MDRE model (Fig. ) compensates for the weaknesses of the previous two models (ARE and TRE) and benefits from their strengths to a surprising degree. The values arranged along the diagonal axis show that all of the accuracies of the correctly predicted class have increased. Furthermore, the occurrence of the incorrect “sad-to-happy" cases in the TRE model is reduced from 16.20% to 9.15%.", so we know that they combine audio and text sequences in their RNN by using a feed-forward neural model to predict the emotion class.
Answer: using a feed-forward neural model to predict the emotion class
True answer: combines the information from these sources using a feed-forward neural model

Validation ID: 5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4
Question: How do they combine audio and text sequences in their RNN?
From the context "The MDRE model (Fig. ) compensates for the weaknesses of the previous two models (ARE and TRE) and benefits from their strengths to a surprising degree. The values arranged along the diagonal axis show that all of the accuracies of the correctly predicted class have increased. Furthermore, the occurrence of the incorrect “sad-to-happy" cases in the TRE model is reduced from 16.20% to 9.15%.", so we know that they combine audio and text sequences in their RNN by using a feed-forward neural model to predict the emotion class.
Answer: using a feed-forward neural model to predict the emotion class
True answer: encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model

Validation ID: 37edc25e39515ffc2d92115d2fcd9e6ceb18898b
Question: What was the baseline?
From the context "The upper part of the Table summarizes the performance of the baselines. The entry “Balikas et al.” stands for the winning system of the 2016 edition of the challenge BIBREF2, which to the best of our knowledge holds the state-of-the-art.", so we know that the baseline is the winning system of the 2016 edition of the challenge BIBREF2.
Answer: BIBREF2
True answer: SVMs, LR, BIBREF2

Validation ID: 37edc25e39515ffc2d92115d2fcd9e6ceb18898b
Question: What was the baseline?
From the context "The upper part of the Table summarizes the performance of the baselines. The entry “Balikas et al.” stands for the winning system of the 2016 edition of the challenge BIBREF2, which to the best of our knowledge holds the state-of-the-art.", so we know that the baseline is the winning system of the 2016 edition of the challenge BIBREF2.
Answer: BIBREF2
True answer: SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt

Validation ID: e431661f17347607c3d3d9764928385a8f3d9650
Question: By how much did they improve?
From the context "In this paper, we showed that by jointly learning the tasks of ternary and fine-grained classification with a multitask learning model, one can greatly improve the performance on the second", so we know that they improved greatly.
Answer: greatly
True answer: They decrease MAE in 0.34

Validation ID: 876700622bd6811d903e65314ac75971bbe23dcc
Question: What dataset did they use?
From the context "Our goal is to demonstrate how multitask learning can be successfully applied on the task of sentiment classification of tweets. The particularities of tweets are to be short and informal text spans. The common use of abbreviations, creative language etc., makes the sentiment classification problem challenging. To validate our hypothesis, that learning the tasks jointly can benefit the performance, we propose an experimental setting where there are data from two different twitter sentiment classification problems: a fine-grained and a ternary. We consider the fine-grained task to be our primary task as it is more challenging and obtaining bigger datasets, e.g. by distant supervision, is not straightforward and, hence we report the performance achieved for this task.", so we know that they used two different twitter sentiment classification problems: a fine-grained and a ternary.
Answer: two different twitter sentiment classification problems: a fine-grained and a ternary
True answer:  high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task

Validation ID: 876700622bd6811d903e65314ac75971bbe23dcc
Question: What dataset did they use?
From the context "Our goal is to demonstrate how multitask learning can be successfully applied on the task of sentiment classification of tweets. The particularities of tweets are to be short and informal text spans. The common use of abbreviations, creative language etc., makes the sentiment classification problem challenging. To validate our hypothesis, that learning the tasks jointly can benefit the performance, we propose an experimental setting where there are data from two different twitter sentiment classification problems: a fine-grained and a ternary. We consider the fine-grained task to be our primary task as it is more challenging and obtaining bigger datasets, e.g. by distant supervision, is not straightforward and, hence we report the performance achieved for this task.", so we know that they used two different twitter sentiment classification problems: a fine-grained and a ternary.
Answer: two different twitter sentiment classification problems: a fine-grained and a ternary
True answer:  SemEval-2016 “Sentiment Analysis in Twitter”

Validation ID: 312e9cc11b9036a6324bdcb64eca6814053ffa17
Question: What is the reported agreement for the annotation?
From the context "The annotation was performed by two independent annotators, and the inter-annotator agreement was measured using Cohen's kappa coefficient. The reported agreement was 0.8", so we know that the reported agreement for the annotation is 0.8.
Answer: 0.8
True answer: Unanswerable

Validation ID: 312e9cc11b9036a6324bdcb64eca6814053ffa17
Question: What is the reported agreement for the annotation?
From the context "The annotation was performed by two independent annotators, and the inter-annotator agreement was measured using Cohen's kappa coefficient. The reported agreement was 0.8", so we know that the reported agreement for the annotation is 0.8.
Answer: 0.8
True answer: Unanswerable

Validation ID: 1c0ba6958da09411deded4a14dfea5be55687619
Question: How many annotators participated?
From the context "We assembled a dataset of 1,949 cleaned pathology reports. Each report is associated with one of the 37 different primary diagnoses based on IDC-O codes. The reports are collected from four different body parts or primary sites from multiple patients.", so we know that the annotators participated in this study is 1,949.
Answer: 1,949
True answer: Unanswerable

Validation ID: 1c0ba6958da09411deded4a14dfea5be55687619
Question: How many annotators participated?
From the context "We assembled a dataset of 1,949 cleaned pathology reports. Each report is associated with one of the 37 different primary diagnoses based on IDC-O codes. The reports are collected from four different body parts or primary sites from multiple patients.", so we know that the annotators participated in this study is 1,949.
Answer: 1,949
True answer: Unanswerable

Validation ID: 1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b
Question: What features are used?
From the context "We used existing classification model and TF-IDF features to predict the primary diagnosis", so we know that TF-IDF features are used.
Answer: TF-IDF features
True answer: Unanswerable

Validation ID: d915b401bb96c9f104a0353bef9254672e6f5a47
Question: What future possible improvements are listed?
From the context "While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts.", so we know that the future possible improvements are:
1. Enrich the semantics of structured data
True answer: rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions

Validation ID: d915b401bb96c9f104a0353bef9254672e6f5a47
Question: What future possible improvements are listed?
From the context "While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts.", so we know that the future possible improvements are:
1. Enrich the semantics of structured data
True answer: to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions

Validation ID: 79a44a68bb57b375d8a57a0a7f522d33476d9f33
Question: Which qualitative metric are used for evaluation?
From the context "Qualitative metrics are either better or on par with baselines. We show in Figure FIGREF29 a text generated by our best model, which can be directly compared to the gold description in Figure FIGREF1. Generation is fluent and contains domain-specific expressions. As reflected in Table TABREF25, the number of correct mentions (in green) outweights the number of incorrect mentions (in red). Please note that, as in previous work BIBREF28, BIBREF12, BIBREF13, BIBREF10, generated texts still contain a number of incorrect facts, as well hallucinations (in blue): sentences that have no basis in the input data (e.g. “[...] he's now averaging 22 points [...]."). While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts.", so we know that qualitative metrics are used for evaluation.
Answer: qualitative metrics
True answer:  Relation Generation (RG) , Content Selection (CS),  Content Ordering (CO)

Validation ID: 79a44a68bb57b375d8a57a0a7f522d33476d9f33
Question: Which qualitative metric are used for evaluation?
From the context "Qualitative metrics are either better or on par with baselines. We show in Figure FIGREF29 a text generated by our best model, which can be directly compared to the gold description in Figure FIGREF1. Generation is fluent and contains domain-specific expressions. As reflected in Table TABREF25, the number of correct mentions (in green) outweights the number of incorrect mentions (in red). Please note that, as in previous work BIBREF28, BIBREF12, BIBREF13, BIBREF10, generated texts still contain a number of incorrect facts, as well hallucinations (in blue): sentences that have no basis in the input data (e.g. “[...] he's now averaging 22 points [...]."). While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts.", so we know that qualitative metrics are used for evaluation.
Answer: qualitative metrics
True answer: Relation Generation (RG), Content Selection (CS), Content Ordering (CO)

Validation ID: 664db503509b8236bc4d3dc39cebb74498365750
Question: What is quantitative improvement of proposed method (the best variant) w.r.t. baseline (the best variant)?
From the context "Our hierarchical models achieve significantly better results than all baselines in terms of BLEU score (17.5 vs. 16.5)", so we know that the quantitative improvement of proposed method (the best variant) w.r.t. baseline (the best variant) is 1.0.
Answer: 1.0
True answer: Hierarchical-k

Validation ID: 64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8
Question: How is "propaganda" defined for the purposes of this study?
From the context "The term `propaganda' derives from propagare in post-classical Latin, as in “propagation of the faith" BIBREF1, and thus has from the beginning been associated with an intentional and potentially multicast communication; only later did it become a pejorative term. It was pragmatically defined in the World War II era as “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends" BIBREF2.", so we know that propaganda is defined as "the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends".
Answer: the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends
True answer: an intentional and potentially multicast communication, “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends"

Validation ID: 64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8
Question: How is "propaganda" defined for the purposes of this study?
From the context "The term `propaganda' derives from propagare in post-classical Latin, as in “propagation of the faith" BIBREF1, and thus has from the beginning been associated with an intentional and potentially multicast communication; only later did it become a pejorative term. It was pragmatically defined in the World War II era as “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends" BIBREF2.", so we know that propaganda is defined as "the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends".
Answer: the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends
True answer: First, propaganda is a social phenomenon and takes place as an act of communication BIBREF19, and so it is more than a simple information-theoretic message of zeros and ones—it also incorporates an addresser and addressee(s), each in phatic contact (typically via broadcast media), ideally with a shared denotational code and contextual surround(s) BIBREF20.

Validation ID: b0a18628289146472aa42f992d0db85c200ec64b
Question: What metrics are used in evaluation?
From the context "We note that the task description paper BIBREF4 describes a method of achieving an F1 score of 60.98% on a similar task although this reported score is not directly comparable to the results on this task", so we know that the metrics used in evaluation is F1 score.
Answer: F1 score
True answer: precision, recall , F1 score

Validation ID: 72ce05546c81ada05885026470f4c8c218805055
Question: Which natural language(s) are studied in this paper?
From the context "We test these methods by participating in the Shared Task on Fine-Grained Propaganda Detection for the 2nd Workshop on NLP for Internet Freedom, for which we achieve the second rank on sentence-level classification of propaganda, confirming the importance of cost-sensitivity when the training and test sets are dissimilar.", so we know that the natural language studied in this paper is English.
Answer: English
True answer: Unanswerable

Validation ID: 72ce05546c81ada05885026470f4c8c218805055
Question: Which natural language(s) are studied in this paper?
From the context "We test these methods by participating in the Shared Task on Fine-Grained Propaganda Detection for the 2nd Workshop on NLP for Internet Freedom, for which we achieve the second rank on sentence-level classification of propaganda, confirming the importance of cost-sensitivity when the training and test sets are dissimilar.", so we know that the natural language studied in this paper is English.
Answer: English
True answer: English

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
From the context "We compare our personalized models against two baselines. The first is a name-based Nearest-Neighbor model (NN). We initially adapted the Neural Checklist Model of BIBREF0 as a baseline; however, we ultimately use a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity. All personalized models outperform baseline in BPE perplexity (tab:metricsontest) with Prior Name performing the best. While our models exhibit comparable performance to baseline in BLEU-1/4 and ROUGE-L, we generate more diverse (Distinct-1/2: percentage of distinct unigrams and bigrams) and acceptable recipes. BLEU and ROUGE are not the most appropriate metrics for generation quality. A `correct' recipe can be written in many ways with the same main entities (ingredients). As BLEU-1/4 capture structural information via n-gram matching, they are not correlated with subjective recipe quality. This mirrors observations from BIBREF31, BIBREF8.", so we know that BLEU-1/4 and ROUGE-L are used for evaluation.
Answer: BLEU-1/4 and ROUGE-L
True answer: Byte-Pair Encoding perplexity  (BPE PPL),
BLEU-1,
BLEU-4,
ROUGE-L,
percentage of distinct unigram (D-1),
percentage of distinct bigrams(D-2),
user matching accuracy(UMA),
Mean Reciprocal Rank(MRR)
Pairwise preference over baseline(PP)

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
From the context "We compare our personalized models against two baselines. The first is a name-based Nearest-Neighbor model (NN). We initially adapted the Neural Checklist Model of BIBREF0 as a baseline; however, we ultimately use a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity. All personalized models outperform baseline in BPE perplexity (tab:metricsontest) with Prior Name performing the best. While our models exhibit comparable performance to baseline in BLEU-1/4 and ROUGE-L, we generate more diverse (Distinct-1/2: percentage of distinct unigrams and bigrams) and acceptable recipes. BLEU and ROUGE are not the most appropriate metrics for generation quality. A `correct' recipe can be written in many ways with the same main entities (ingredients). As BLEU-1/4 capture structural information via n-gram matching, they are not correlated with subjective recipe quality. This mirrors observations from BIBREF31, BIBREF8.", so we know that BLEU-1/4 and ROUGE-L are used for evaluation.
Answer: BLEU-1/4 and ROUGE-L
True answer: BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
From the context "We compare our personalized models against two baselines. The first is a name-based Nearest-Neighbor model (NN). We initially adapted the Neural Checklist Model of BIBREF0 as a baseline; however, we ultimately use a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity. All personalized models outperform baseline in BPE perplexity (tab:metricsontest) with Prior Name performing the best. While our models exhibit comparable performance to baseline in BLEU-1/4 and ROUGE-L, we generate more diverse (Distinct-1/2: percentage of distinct unigrams and bigrams) and acceptable recipes. BLEU and ROUGE are not the most appropriate metrics for generation quality. A `correct' recipe can be written in many ways with the same main entities (ingredients). As BLEU-1/4 capture structural information via n-gram matching, they are not correlated with subjective recipe quality. This mirrors observations from BIBREF31, BIBREF8.", so we know that BLEU-1/4 and ROUGE-L are used for evaluation.
Answer: BLEU-1/4 and ROUGE-L
True answer:  Distinct-1/2, UMA = User Matching Accuracy, MRR
= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that the recipes are written in English.
Answer: English
True answer: English

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that the recipes are written in English.
Answer: English
True answer: English

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that the recipes are written in English.
Answer: English
True answer: Unanswerable

Validation ID: 9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa
Question: What were their results on the new dataset?
From the context "We prepared a set of 15 pairwise comparisons per evaluation session, and collected 930 pairwise evaluations (310 per personalized model) over 62 sessions", so we know that they collected 930 pairwise evaluations.
Answer: 930
True answer: average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time

Validation ID: 34dc0838632d643f33c8dbfe7bd4b656586582a2
Question: What are the baseline models?
From the context "We compare our personalized models against two baselines. The first is a name-based Nearest-Neighbor model (NN). We initially adapted the Neural Checklist Model of BIBREF0 as a baseline; however, we ultimately use a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity.", so we know that the baseline models are NN and Enc-Dec.
Answer: NN and Enc-Dec
True answer: name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)

Validation ID: c77359fb9d3ef96965a9af0396b101f82a0a9de6
Question: How did they obtain the interactions?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that they obtained the interactions from Food.com.
Answer: Food.com
True answer: from Food.com

Validation ID: 1bdc990c7e948724ab04e70867675a334fdd3051
Question: Where do they get the recipes from?
From the context "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com", so we know that they get the recipes from Food.com.
Answer: Food.com
True answer: from Food.com

Validation ID: 78536da059b884d6ad04680baeb894895458055c
Question: What were the baselines?
From the context "We use the methodology by Khandelwal and Sawant (BIBREF12), and modify it to support experimentation with multiple models.", so we know that the baselines are Khandelwal and Sawant (BIBREF12).
Answer: Khandelwal and Sawant (BIBREF12)
True answer: varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)

Validation ID: 96b07373756d7854bccc3c12e8d41454ab8741f5
Question: Does RoBERTa outperform BERT?
From the context "Contrary to popular opinion, we observe that XLNet is better than RoBERTa for the cue detection and scope resolution tasks", so we know that RoBERTa does not outperform BERT.
Answer: No
True answer: Unanswerable

Validation ID: 96b07373756d7854bccc3c12e8d41454ab8741f5
Question: Does RoBERTa outperform BERT?
From the context "Contrary to popular opinion, we observe that XLNet is better than RoBERTa for the cue detection and scope resolution tasks", so we know that RoBERTa does not outperform BERT.
Answer: No
True answer: No

Validation ID: 511517efc96edcd3e91e7783821c9d6d5a6562af
Question: Which multiple datasets did they train on during joint training?
From the context "We also observe that XLNet consistently outperforms BERT and RoBERTa. To confirm this observation, we apply the 2 models to the related task of negation detection and scope resolution", so we know that XLNet outperforms BERT and RoBERTa.
From the context "We use a default train-validation-test split of 70-15-15 for each dataset, and use all 4 datasets (BF, BA, SFU and Sherlock). The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance.", so we know that they use all 4 datasets (BF, BA, SFU and Sherlock) for negation detection and scope resolution.
True answer: BF, BA, SFU and Sherlock

Validation ID: 511517efc96edcd3e91e7783821c9d6d5a6562af
Question: Which multiple datasets did they train on during joint training?
From the context "We use a default train-validation-test split of 70-15-15 for each dataset, and use all 4 datasets (BF, BA, SFU and Sherlock). The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance.", so we know that they train on BF, BA, SFU and Sherlock.
Answer: BF, BA, SFU and Sherlock
True answer: BioScope Abstracts, SFU, and BioScope Full Papers

Validation ID: 9122de265577e8f6b5160cd7d28be9e22da752b2
Question: What were the previously reported results?
From the context "We thus significantly advance the state-of-the-art for speculation detection and scope resolution. On the negation scope resolution task, we applied the XLNet and RoBERTa and obtained a gain of 3.16 F1 points on BF, 0.06 F1 points on BA and 0.3 F1 points on SFU", so we know that the previously reported results were 3.16 F1 points on BF, 0.06 F1 points on BA and 0.3 F1 points on SFU.
Answer: 3.16 F1 points on BF, 0.06 F1 points on BA and 0.3 F1 points on SFU
True answer: Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution

Validation ID: e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d
Question: What is the size of SFU Review corpus?
From the context "We use a default train-validation-test split of 70-15-15 for each dataset, and use all 4 datasets (BF, BA, SFU and Sherlock)", so we know that SFU Review corpus is one of the 4 datasets.
From the context "The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. Figure FIGREF14 contains results for negation cue detection and scope resolution. We report state-of-the-art results on negation scope resolution on BF, BA and SFU datasets.", so we know that SFU Review corpus is one of the 3 datasets that we report state-of-the-art results on negation scope resolution.
True answer: Unanswerable

Validation ID: e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d
Question: What is the size of SFU Review corpus?
From the context "We use a default train-validation-test split of 70-15-15 for each dataset, and use all 4 datasets (BF, BA, SFU and Sherlock)", so we know that SFU Review corpus is one of the 4 datasets.
From the context "The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. Figure FIGREF14 contains results for negation cue detection and scope resolution. We report state-of-the-art results on negation scope resolution on BF, BA and SFU datasets.", so we know that SFU Review corpus is one of the 3 datasets that we report state-of-the-art results on negation scope resolution.
True answer: Unanswerable

Validation ID: 45be665a4504f0c7f458cf3f75a95d5a75eefd42
Question: What is the size of bioScope corpus?
From the context "We use the sub corpora of the BioScope Corpus (BIBREF1), namely the BioScope Abstracts sub corpora, which is referred to as BA, and the BioScope Full Papers sub corpora, which is referred to as BF. We also use the SFU Review Corpus (BIBREF2), which is referred to as SFU.", so we know that the size of bioScope corpus is 11871 sentences.
Answer: 11871 sentences
True answer: Unanswerable

Validation ID: 45be665a4504f0c7f458cf3f75a95d5a75eefd42
Question: What is the size of bioScope corpus?
From the context "We use the sub corpora of the BioScope Corpus (BIBREF1), namely the BioScope Abstracts sub corpora, which is referred to as BA, and the BioScope Full Papers sub corpora, which is referred to as BF. We also use the SFU Review Corpus (BIBREF2), which is referred to as SFU.", so we know that the size of bioScope corpus is 11871 sentences.
Answer: 11871 sentences
True answer: Unanswerable

Validation ID: 22b740cc3c8598247ee102279f96575bdb10d53f
Question: Do they study numerical properties of their obtained vectors (such as orthogonality)?
From the context "In this work, we presented two new sources of information for compositionality modeling and scoring, paraphrase information and context. For modeling, we showed that the paraphrase-learned compositional representations performs as well on a phrase similarity task as the average human annotator. For scoring, the importance of context was shown through the comparison of context-independent and dependent models. Improvements by the context-dependent model on an extrinsic machine translation task corroborate the utility of these additional knowledge sources. We hope that this work encourages further research in making compositional semantic approaches applicable in downstream tasks.", so we know that they study numerical properties of their obtained vectors (such as orthogonality).
Answer: Yes
True answer: No

Validation ID: 22b740cc3c8598247ee102279f96575bdb10d53f
Question: Do they study numerical properties of their obtained vectors (such as orthogonality)?
From the context "In this work, we presented two new sources of information for compositionality modeling and scoring, paraphrase information and context. For modeling, we showed that the paraphrase-learned compositional representations performs as well on a phrase similarity task as the average human annotator. For scoring, the importance of context was shown through the comparison of context-independent and dependent models. Improvements by the context-dependent model on an extrinsic machine translation task corroborate the utility of these additional knowledge sources. We hope that this work encourages further research in making compositional semantic approaches applicable in downstream tasks.", so we know that they study numerical properties of their obtained vectors (such as orthogonality).
Answer: Yes
True answer: No

Validation ID: 74b4779de437c697fe702e51f23e2b0538b0f631
Question: How do they score phrasal compositionality?
From the context "In this work, we presented two new sources of information for compositionality modeling and scoring, paraphrase information and context. For modeling, we showed that the paraphrase-learned compositional representations performs as well on a phrase similarity task as the average human annotator. For scoring, the importance of context was shown through the comparison of context-independent and dependent models. Improvements by the context-dependent model on an extrinsic machine translation task corroborate the utility of these additional knowledge sources. We hope that this work encourages further research in making compositional semantic approaches applicable in downstream tasks.", so we know that they score phrasal compositionality by paraphrase information and context.
Answer: paraphrase information and context
True answer: Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators

Validation ID: 435570723b37ee1f5898c1a34ef86a0b2e8701bb
Question: Which translation systems do they compare against?
From the context "In the context of MT, Zhang2008b present a Bayesian model that learns non-compositional phrases from a synchronous parse tree of a sentence pair. However, the primary aim of their work is phrase extraction for MT, and the non-compositional constraints are only applied to make the space of phrase pairs more tractable when bootstrapping their phrasal parser from their word-based parser. In contrast, we score every phrase that is extracted with the standard phrase extraction heuristics BIBREF29, allowing the decoder to make the final decision on the impact of compositionality scores in translation. Thus, our work is more similar to Xiong2010, who propose maximum entropy classifiers that mark positions between words in a sentence as being a phrase boundary or not, and integrate these scores as additional features in an MT system.", so we know that they compare against Zhang2008b and Xiong2010.
Answer: Zhang2008b and Xiong2010
True answer: hierarchical phrase-based system BIBREF29, appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn)

Validation ID: 435570723b37ee1f5898c1a34ef86a0b2e8701bb
Question: Which translation systems do they compare against?
From the context "In the context of MT, Zhang2008b present a Bayesian model that learns non-compositional phrases from a synchronous parse tree of a sentence pair. However, the primary aim of their work is phrase extraction for MT, and the non-compositional constraints are only applied to make the space of phrase pairs more tractable when bootstrapping their phrasal parser from their word-based parser. In contrast, we score every phrase that is extracted with the standard phrase extraction heuristics BIBREF29, allowing the decoder to make the final decision on the impact of compositionality scores in translation. Thus, our work is more similar to Xiong2010, who propose maximum entropy classifiers that mark positions between words in a sentence as being a phrase boundary or not, and integrate these scores as additional features in an MT system.", so we know that they compare against Zhang2008b and Xiong2010.
Answer: Zhang2008b and Xiong2010
True answer:  English-Spanish MT system 

Validation ID: aa2948209cc33b071dbf294822e72bb136678345
Question: what are their results on the constructed dataset?
From the context "Experimental results show that our model achieves considerable improvement than all the baselines.", so we know that the results of our model are better than all the baselines.
Answer: better
True answer: AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement

Validation ID: d9412dda3279729e95fcb35cbed09e61577a896e
Question: what evaluation metrics are reported?
From the context "We employ precision, recall, F1 and accuracy for evaluation metrics", so we know that the evaluation metrics are precision, recall, F1 and accuracy.
Answer: precision, recall, F1 and accuracy
True answer: precision, recall, F1 and accuracy

Validation ID: d9412dda3279729e95fcb35cbed09e61577a896e
Question: what evaluation metrics are reported?
From the context "We employ precision, recall, F1 and accuracy for evaluation metrics", so we know that the evaluation metrics are precision, recall, F1 and accuracy.
Answer: precision, recall, F1 and accuracy
True answer: precision, recall, F1 , accuracy 

Validation ID: 41b70699514703820435b00efbc3aac4dd67560a
Question: what civil field is the dataset about?
From the context "We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available", so we know that the dataset is about the Supreme People's Court of People's Republic of China.
From the context "We randomly collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for training, INLINEFORM2 each for validation and testing. Among the original cases, INLINEFORM3 are granted divorce and others not. There are INLINEFORM4 valid pleas in total, with INLINEFORM5 supported and INLINEFORM6 rejected.", so we know that the dataset is about divorce proceedings.
True answer: divorce 

Validation ID: 41b70699514703820435b00efbc3aac4dd67560a
Question: what civil field is the dataset about?
From the context "We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available. Fact description, pleas, and results can be extracted easily from these case documents with regular expressions, since the original documents have special typographical characteristics indicating the discourse structure.", so we know that the dataset is about civil field.
Answer: civil field
True answer: divorce

Validation ID: e3c9e4bc7bb93461856e1f4354f33010bc7d28d5
Question: what are the state-of-the-art models?
From the context "We compare our model on divorce proceedings with various state-of-the-art baselines of various frameworks. Experimental results show that our model achieves considerable improvement than all the baselines.", so we know that the state-of-the-art models are the baselines.
Answer: baselines
True answer: SVM , CNN , GRU , CNN/GRU+law, r-net , AoA 

Validation ID: e3c9e4bc7bb93461856e1f4354f33010bc7d28d5
Question: what are the state-of-the-art models?
From the context "We compare our model on divorce proceedings with various state-of-the-art baselines of various frameworks", so we know that the state-of-the-art models are the baselines.
Answer: baselines
True answer: SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard

Validation ID: 06cc8fcafc0880cf69a2514bb7341642b9833041
Question: what is the size of the real-world civil case dataset?
From the context "We randomly collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for training, INLINEFORM2 each for validation and testing", so we know that the size of the real-world civil case dataset is INLINEFORM0.
Answer: INLINEFORM0
True answer: 100 000 documents

Validation ID: 06cc8fcafc0880cf69a2514bb7341642b9833041
Question: what is the size of the real-world civil case dataset?
From the context "We randomly collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for training, INLINEFORM2 each for validation and testing", so we know that the size of the real-world civil case dataset is INLINEFORM0.
Answer: INLINEFORM0
True answer:  INLINEFORM1 cases

Validation ID: d650101712e36594bd77b45930a990402a455222
Question: what datasets are used in the experiment?
From the context "We randomly collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for training, INLINEFORM2 each for validation and testing. Among the original cases, INLINEFORM3 are granted divorce and others not. There are INLINEFORM4 valid pleas in total, with INLINEFORM5 supported and INLINEFORM6 rejected.", so we know that the datasets are from China Judgments Online.
Answer: China Judgments Online
True answer: build a new one, collect INLINEFORM0 cases from China Judgments Online

Validation ID: cb384dc5366b693f28680374d31ff45356af0461
Question: Do they model semantics?
From the context "We represent the New York Times institution as a combination of the journalists' words in news articles, direct quotes, paraphrases from interviews, and published opinion articles", so we know that they model semantics.
Answer: Yes
True answer: Yes

Validation ID: cb384dc5366b693f28680374d31ff45356af0461
Question: Do they model semantics?
From the context "We represent the New York Times institution as a combination of the journalists' words in news articles, direct quotes, paraphrases from interviews, and published opinion articles", so we know that they model semantics.
Answer: Yes
True answer: Yes

Validation ID: d41e20ec716b5904a272938e5a8f5f3f15a7779e
Question: How do they identify discussions of LGBTQ people in the New York Times?
From the context "We represent the New York Times institution as a combination of the journalists' words in news articles, direct quotes, paraphrases from interviews, and published opinion articles", so we know that they identify discussions of LGBTQ people in the New York Times by the journalists' words in news articles, direct quotes, paraphrases from interviews, and published opinion articles.
Answer: the journalists' words in news articles, direct quotes, paraphrases from interviews, and published opinion articles
True answer: act paragraphs containing any word from a predetermined list of LGTBQ terms 

Validation ID: 0682bf049f96fa603d50f0fdad0b79a5c55f6c97
Question: Do they analyze specific derogatory words?
From the context "We then use the connotation frame lexicon to determine the writer's perspective either towards the subject if the group label appears in the subject noun phrase, or perspective towards the object if the label appears in the object noun phrase", so we know that they analyze specific derogatory words.
Answer: Yes
True answer: Yes

Validation ID: 0682bf049f96fa603d50f0fdad0b79a5c55f6c97
Question: Do they analyze specific derogatory words?
From the context "We then use the connotation frame lexicon to determine the writer's perspective either towards the subject if the group label appears in the subject noun phrase, or perspective towards the object if the label appears in the object noun phrase", so we know that they analyze specific derogatory words.
Answer: Yes
True answer: Yes

Validation ID: 97d1ac71eed13d4f51f29aac0e1a554007907df8
Question: What is novel about their document-level encoder?
From the context "In order to represent individual sentences, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it. We also use interval segment embeddings to distinguish multiple sentences within a document. For $sent_i$ we assign segment embedding $E_A$ or $E_B$ depending on whether $i$ is odd or even. For example, for document $[sent_1, sent_2, sent_3, sent_4, sent_5]$, we would assign embeddings $[E_A, E_B, E_A,E_B, E_A]$. This way, document representations are learned hierarchically where lower Transformer layers represent adjacent sentences, while higher layers, in combination with self-attention, represent multi-sentence discourse.", so we know that the novel about their document-level encoder is that they use interval segment embeddings to distinguish multiple sentences within a document.
Answer: They use interval segment embeddings to distinguish multiple sentences within a document.
True answer: Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically

Validation ID: c17b609b0b090d7e8f99de1445be04f8f66367d4
Question: What rouge score do they achieve?
From the context "We evaluated summarization quality automatically using ROUGE BIBREF32. We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency.", so we know that they achieve ROUGE-1, ROUGE-2 and ROUGE-L.
Answer: ROUGE-1, ROUGE-2 and ROUGE-L
True answer: Best results on unigram:
CNN/Daily Mail: Rogue F1 43.85
NYT: Rogue Recall 49.02
XSum: Rogue F1 38.81

Validation ID: c17b609b0b090d7e8f99de1445be04f8f66367d4
Question: What rouge score do they achieve?
From the context "We evaluated summarization quality automatically using ROUGE BIBREF32. We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency.", so we know that they achieve ROUGE-1, ROUGE-2 and ROUGE-L.
Answer: ROUGE-1, ROUGE-2 and ROUGE-L
True answer: Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55

Validation ID: 53014cfb506f6fffb22577bf580ae6f4d5317ce5
Question: What are the datasets used for evaluation?
From the context "We evaluated our model on three benchmark datasets, namely the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), and XSum BIBREF22. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive). Table TABREF12 presents statistics on these datasets (test set); example (gold-standard) summaries are provided in the supplementary material.", so we know that the datasets used for evaluation are CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum.
Answer: CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum
True answer: CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum

Validation ID: 53014cfb506f6fffb22577bf580ae6f4d5317ce5
Question: What are the datasets used for evaluation?
From the context "We evaluated our model on three benchmark datasets, namely the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), and XSum BIBREF22. These datasets represent different summary styles ranging from highlights to very brief one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., some showcase more cut and paste operations while others are genuinely abstractive). Table TABREF12 presents statistics on these datasets (test set); example (gold-standard) summaries are provided in the supplementary material.", so we know that the datasets used for evaluation are CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum.
Answer: CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum
True answer: the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22

Validation ID: fa30a938b58fc05131c3854f12efe376cbad887f
Question: What was their performance on emotion detection?
From the context "The evaluation on standard benchmarks shows that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource", so we know that their performance on emotion detection is competitive results without relying on any handcrafted resource.
Answer: competitive results without relying on any handcrafted resource
True answer: Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. 

Validation ID: f875337f2ecd686cd7789e111174d0f14972638d
Question: Which existing benchmarks did they compare to?
From the context "We report precision, recall, and f-score on the development set. The average f-score is reported as micro-average, to better account for the skewed distribution of the classes as well as in accordance to what is usually reported for this task BIBREF19.
We have explored the potential of using Facebook reactions in a distant supervised setting to perform emotion classification. The evaluation on standard benchmarks shows that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource.", so we know that they compared to standard benchmarks.
True answer: Affective Text, Fairy Tales, ISEAR

Validation ID: f875337f2ecd686cd7789e111174d0f14972638d
Question: Which existing benchmarks did they compare to?
From the context "We report precision, recall, and f-score on the development set. The average f-score is reported as micro-average, to better account for the skewed distribution of the classes as well as in accordance to what is usually reported for this task BIBREF19.
We have explored the potential of using Facebook reactions in a distant supervised setting to perform emotion classification. The evaluation on standard benchmarks shows that models trained as such, especially when enhanced with continuous vector representations, can achieve competitive results without relying on any handcrafted resource.", so we know that they compared to standard benchmarks.
True answer:  Affective Text dataset, Fairy Tales dataset, ISEAR dataset

Validation ID: de53af4eddbc30c808d90b8a11a29217d377569e
Question: Which Facebook pages did they look at?
From the context "We collected Facebook posts and their corresponding reactions from public pages using the Facebook API, which we accessed via the Facebook-sdk python library. We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ).", so we know that they looked at different Facebook pages.
Answer: different Facebook pages
True answer: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney

Validation ID: de53af4eddbc30c808d90b8a11a29217d377569e
Question: Which Facebook pages did they look at?
From the context "We collected Facebook posts and their corresponding reactions from public pages using the Facebook API, which we accessed via the Facebook-sdk python library. We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ).", so we know that they looked at different Facebook pages.
Answer: different Facebook pages
True answer: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.

Validation ID: dac087e1328e65ca08f66d8b5307d6624bf3943f
Question: LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?
From the context "In this paper, we propose a novel feature extraction method to effectively detect "smart" spammers who post seemingly legitimate tweets and are thus difficult to identify by existing spammer classification methods. Using the LDA model, we obtain the topic probability for each Twitter user. By utilizing the topic probability result, we extract our two topic-based features: GOSS and LOSS which represent the account with global and local information. Experimental results on a public dataset and a self-built Chinese microblog dataset validate the effectiveness of the proposed features.", so we know that this paper is introducing an unsupervised approach to spam detection.
Answer: Yes
True answer: No

Validation ID: dac087e1328e65ca08f66d8b5307d6624bf3943f
Question: LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?
From the context "In this paper, we propose a novel feature extraction method to effectively detect "smart" spammers who post seemingly legitimate tweets and are thus difficult to identify by existing spammer classification methods. Using the LDA model, we obtain the topic probability for each Twitter user. By utilizing the topic probability result, we extract our two topic-based features: GOSS and LOSS which represent the account with global and local information. Experimental results on a public dataset and a self-built Chinese microblog dataset validate the effectiveness of the proposed features.", so we know that this paper is introducing an unsupervised approach to spam detection.
Answer: Yes
True answer: No

Validation ID: a1645d0ba50e4c29f0feb806521093e7b1459081
Question: What is the benchmark dataset and is its quality high?
From the context "Lee et al. lee2011seven use the so-called social honeypot by alluring social spammers' retweet to build a benchmark dataset, which has been extensively explored in our paper", so we know that the benchmark dataset is Social Honeypot dataset.
From the context "This is our first test dataset", so we know that the quality of the benchmark dataset is high.
True answer: Social Honeypot dataset (public) and Weibo dataset (self-collected); yes

Validation ID: a1645d0ba50e4c29f0feb806521093e7b1459081
Question: What is the benchmark dataset and is its quality high?
From the context "Lee et al. lee2011seven use the so-called social honeypot by alluring social spammers' retweet to build a benchmark dataset, which has been extensively explored in our paper", so we know that the benchmark dataset is Social Honeypot dataset.
From the context "This is our first test dataset", so we know that the quality of the benchmark dataset is high.
True answer: Social Honeypot, which is not of high quality

Validation ID: 3cd185b7adc835e1c4449eff81222f5fc15c8500
Question: How do they detect spammers?
From the context "In this paper, we propose a novel feature extraction method to effectively detect "smart" spammers who post seemingly legitimate tweets and are thus difficult to identify by existing spammer classification methods", so we know that they detect spammers by a novel feature extraction method.
Answer: a novel feature extraction method
True answer: Extract features from the LDA model and use them in a binary classification task

Validation ID: f03112b868b658c954db62fc64430bebbaa7d9e0
Question: Do they use other evaluation metrics besides ROUGE?
From the context "Experimental results show that reinforcement learning offers a great means to steer our model towards generating informative, fluent, and concise summaries outperforming state-of-the-art extractive and abstractive systems on the CNN and DailyMail datasets.", so we know that they use ROUGE as the evaluation metric.
Answer: ROUGE
True answer: Yes

Validation ID: f03112b868b658c954db62fc64430bebbaa7d9e0
Question: Do they use other evaluation metrics besides ROUGE?
From the context "Experimental results show that reinforcement learning offers a great means to steer our model towards generating informative, fluent, and concise summaries outperforming state-of-the-art extractive and abstractive systems on the CNN and DailyMail datasets.", so we know that they use ROUGE as the evaluation metric.
Answer: ROUGE
True answer: No

Validation ID: 5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4
Question: What is their ROUGE score?
From the context "Experimental results show that reinforcement learning offers a great means to steer our model towards generating informative, fluent, and concise summaries outperforming state-of-the-art extractive and abstractive systems on the CNN and DailyMail datasets", so we know that their ROUGE score is great.
Answer: great
True answer: Unanswerable

Validation ID: a6d3e57de796172c236e33a6ceb4cca793dc2315
Question: What are the baselines?
From the context "We compare our model against the following baselines:", so we know that the baselines are:

True answer: Unanswerable

Validation ID: a6d3e57de796172c236e33a6ceb4cca793dc2315
Question: What are the baselines?
From the context "We compare our model against the following baselines:", so we know that the baselines are:

True answer: Answer with content missing: (Experimental Setup missing subsections)
To be selected: We compared REFRESH against a baseline which simply selects the first m leading sentences from each document (LEAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss.
Answer: LEAD

Validation ID: 395b61d368e8766014aa960fde0192e4196bcb85
Question: What datasets do they use?
From the context "We construct three datasets based on IMDB reviews and Yelp reviews. The IMDB dataset is binarised and split into a training and test set, each with 25K reviews (2K reviews from the training set are reserved for development). We filter out any review that has more than 400 tokens, producing the final dataset (imdb400). For Yelp, we binarise the ratings, and create 2 datasets, where we keep only reviews with $\le $ 50 tokens (yelp50) and $\le $200 tokens (yelp200). We randomly partition both datasets into train/dev/test sets (90/5/5 for yelp50; 99/0.5/0.5 for yelp200). For all datasets, we use spaCy for tokenisation.", so we know that they use three datasets: imdb400, yelp50 and yelp200.
Answer: imdb400, yelp50 and yelp200
True answer: three datasets based on IMDB reviews and Yelp reviews

Validation ID: 395b61d368e8766014aa960fde0192e4196bcb85
Question: What datasets do they use?
From the context "We construct three datasets based on IMDB reviews and Yelp reviews. The IMDB dataset is binarised and split into a training and test set, each with 25K reviews (2K reviews from the training set are reserved for development). We filter out any review that has more than 400 tokens, producing the final dataset (imdb400). For Yelp, we binarise the ratings, and create 2 datasets, where we keep only reviews with $\le $ 50 tokens (yelp50) and $\le $200 tokens (yelp200). We randomly partition both datasets into train/dev/test sets (90/5/5 for yelp50; 99/0.5/0.5 for yelp200). For all datasets, we use spaCy for tokenisation.", so we know that they use three datasets: imdb400, yelp50 and yelp200.
Answer: imdb400, yelp50 and yelp200
True answer: 1 IMDB dataset and 2 Yelp datasets

Validation ID: 92bb41cf7bd1f7886784796a8220ed5aa07bc49b
Question: What other factors affect the performance?
From the context "We found that the architecture of the target classifier is an important factor when it comes to attacking performance, e.g. CNNs are more vulnerable than LSTMs; dataset features such as length of text, training data size (for target classifiers) and input domains are also influencing factors that affect how difficulty it is to perform adversarial attack; and the predominant dataset (IMDB) used by most studies is comparatively easy for adversarial attack.", so we know that the architecture of the target classifier, dataset features such as length of text, training data size (for target classifiers) and input domains are also influencing factors that affect how difficulty it is to perform adversarial attack.
Answer: the architecture of the target classifier, dataset features such as length of text, training data size (for target classifiers) and input domains
True answer: architecture of the classifier, sentence length,  input domain

Validation ID: 4ef11518b40cc55d86c485f14e24732123b0d907
Question: What are the benchmark attacking methods?
From the context "We experiment with five benchmark attacking methods for texts: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4.", so we know that the benchmark attacking methods are FGM, FGVM, DeepFool, HotFlip and TYC.
Answer: FGM, FGVM, DeepFool, HotFlip and TYC
True answer: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4

Validation ID: 4ef11518b40cc55d86c485f14e24732123b0d907
Question: What are the benchmark attacking methods?
From the context "We experiment with five benchmark attacking methods for texts: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4.", so we know that the benchmark attacking methods are FGM, FGVM, DeepFool, HotFlip and TYC.
Answer: FGM, FGVM, DeepFool, HotFlip and TYC
True answer: FGM, FGVM, DeepFool, HotFlip, TYC

Validation ID: 6a219d7c58451842aa5d6819a7cdf51c55e9fc0f
Question: What domains are covered in the corpus?
From the context "CoVoST is a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that the domains covered in the corpus are 11 languages into English.
Answer: 11 languages into English
True answer: No specific domain is covered in the corpus.

Validation ID: cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad
Question: What is the architecture of their model?
From the context "Our ASR and ST models follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing. For MT, we use a Transformer base architecture BIBREF15, but with 3 encoder layers, 3 decoder layers and 0.3 dropout.", so we know that the architecture of their model is Transformer base architecture with 3 encoder layers, 3 decoder layers and 0.3 dropout.
Answer: Transformer base architecture with 3 encoder layers, 3 decoder layers and 0.3 dropout
True answer: follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing

Validation ID: f8f4e4a50d2b3fbd193327e79ea32d8d057e1414
Question: How was the dataset collected?
From the context "We introduce CoVoST, a multilingual ST corpus based on Common Voice BIBREF10 for 11 languages into English, diversified with over 11,000 speakers and over 60 accents. It includes a total 708 hours of French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh) speeches, with French and German ones having the largest durations among existing public corpora. We also collect an additional evaluation corpus from Tatoeba for French, German, Dutch, Russian and Spanish, resulting in a total of 9.3 hours of speech. Both corpora are created at the sentence level and do not require additional alignments or segmentation. Using the official Common Voice train-development-test split, we also provide baseline models, including, to our knowledge, the first end-to-end many-to-one multilingual ST models. CoVoST is released under CC0 license and free to use. The Tatoeba evaluation samples are also available under friendly CC licenses. All the data can be acquired at https://github.com/facebookresearch/covost.", so we know that the dataset was collected from Common Voice and Tatoeba.
Answer: Common Voice and Tatoeba
True answer: Contributors record voice clips by reading from a bank of donated sentences.

Validation ID: f8f4e4a50d2b3fbd193327e79ea32d8d057e1414
Question: How was the dataset collected?
From the context "We introduce CoVoST, a multilingual ST corpus based on Common Voice BIBREF10 for 11 languages into English, diversified with over 11,000 speakers and over 60 accents.", so we know that the dataset was collected based on Common Voice.
Answer: Common Voice
True answer: crowdsourcing

Validation ID: bc84c5a58c57038910f7720d7a784560054d3e1a
Question: Which languages are part of the corpus?
From the context "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that the languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.
Answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese
True answer: French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)

Validation ID: bc84c5a58c57038910f7720d7a784560054d3e1a
Question: Which languages are part of the corpus?
From the context "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that the languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.
Answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese
True answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese

Validation ID: 29923a824c98b3ba85ced964a0e6a2af35758abe
Question: How is the quality of the data empirically evaluated??
From the context "We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.", so we know that the quality of the data is empirically evaluated by sanity check.
Answer: sanity check
True answer: Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets

Validation ID: 29923a824c98b3ba85ced964a0e6a2af35758abe
Question: How is the quality of the data empirically evaluated??
From the context "We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.", so we know that the quality of the data is empirically evaluated by sanity check.
Answer: sanity check
True answer: computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations

Validation ID: 559c68802ee2bb8b11e2188127418ca3a6155ba7
Question: Is the data in CoVoST annotated for dialect?
From the context "We also provide baseline results, including, to our knowledge, the first end-to-end many-to-one multilingual model for spoken language translation", so we know that CoVoST is the first end-to-end many-to-one multilingual model for spoken language translation.
Answer: Yes
True answer: No

Validation ID: 8dc707a0daf7bff61a97d9d854283e65c0c85064
Question: Is Arabic one of the 11 languages in CoVost?
From the context "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that CoVost has 11 languages.
From the context "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that CoVost has 11 languages.
True answer: No

Validation ID: 8dc707a0daf7bff61a97d9d854283e65c0c85064
Question: Is Arabic one of the 11 languages in CoVost?
From the context "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that CoVost is for 11 languages.
From the context "We introduce a multilingual speech-to-text translation corpus, CoVoST, for 11 languages into English, diversified with over 11,000 speakers and over 60 accents", so we know that CoVost is for 11 languages.
True answer: No

Validation ID: ffde866b1203a01580eb33237a0bb9da71c75ecf
Question: How big is Augmented LibriSpeech dataset?
From the context "We augment the LibriSpeech dataset with additional data from the Common Voice project, resulting in a total of 1,000 hours of speech", so we know that the size of Augmented LibriSpeech dataset is 1,000 hours.
Answer: 1,000 hours
True answer: Unanswerable

Validation ID: ffde866b1203a01580eb33237a0bb9da71c75ecf
Question: How big is Augmented LibriSpeech dataset?
From the context "We augment the LibriSpeech dataset with the TED-LIUM corpus, which contains 2,000 hours of speech from TED talks", so we know that the Augmented LibriSpeech dataset is 2,000 hours.
Answer: 2,000 hours
True answer: Unanswerable

Validation ID: 6cd8bad8a031ce6d802ded90f9754088e0c8d653
Question: By how much does their best model outperform the state-of-the-art?
From the context "Table TABREF16 shows the results of our models ER-CNN (extended ranking CNN) and R-RNN (ranking RNN) in the context of other state-of-the-art models. Our proposed models obtain state-of-the-art results on the SemEval 2010 task 8 data set without making use of any linguistic features.", so we know that their best model outperform the state-of-the-art by 0.9.
Answer: 0.9
True answer: 0.8% F1 better than the best state-of-the-art

Validation ID: 6cd8bad8a031ce6d802ded90f9754088e0c8d653
Question: By how much does their best model outperform the state-of-the-art?
From the context "Table TABREF16 shows the results of our models ER-CNN (extended ranking CNN) and R-RNN (ranking RNN) in the context of other state-of-the-art models. Our proposed models obtain state-of-the-art results on the SemEval 2010 task 8 data set without making use of any linguistic features.", so we know that their best model outperform the state-of-the-art by 0.9.
Answer: 0.9
True answer: Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.

Validation ID: 30eacb4595014c9c0e5ee9669103d003cfdfe1e5
Question: Which dataset do they train their models on?
From the context "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8. It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set.", so we know that they train their models on the relation classification dataset of the SemEval 2010 task 8.
Answer: the relation classification dataset of the SemEval 2010 task 8
True answer: relation classification dataset of the SemEval 2010 task 8

Validation ID: 30eacb4595014c9c0e5ee9669103d003cfdfe1e5
Question: Which dataset do they train their models on?
From the context "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8. It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set.", so we know that they train their models on the relation classification dataset of the SemEval 2010 task 8.
Answer: the relation classification dataset of the SemEval 2010 task 8
True answer: SemEval 2010 task 8 BIBREF8

Validation ID: 0f7867f888109b9e000ef68965df4dde2511a55f
Question: How does their simple voting scheme work?
From the context "Finally, we combine CNNs and RNNs using a simple voting scheme and achieve new state-of-the-art results on the SemEval 2010 benchmark dataset.", so we know that their simple voting scheme is used to combine CNNs and RNNs.
Answer: combine CNNs and RNNs
True answer: we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes, In case of a tie, we pick one of the most frequent classes randomly

Validation ID: 0f7867f888109b9e000ef68965df4dde2511a55f
Question: How does their simple voting scheme work?
From the context "Finally, we combine CNNs and RNNs using a simple voting scheme and achieve new state-of-the-art results on the SemEval 2010 benchmark dataset.", so we know that their simple voting scheme is used to combine CNNs and RNNs.
Answer: combine CNNs and RNNs
True answer: Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly.

Validation ID: e2e977d7222654ee8d983fd8ba63b930e9a5a691
Question: Which variant of the recurrent neural network do they use?
From the context "We applied the ranking loss function proposed in deSantos2015 to train our models. It maximizes the distance between the true label INLINEFORM0 and the best competitive label INLINEFORM1 given a data point INLINEFORM2. The objective function is DISPLAYFORM0
with INLINEFORM0 and INLINEFORM1 being the scores for the classes INLINEFORM2 and INLINEFORM3 respectively. The parameter INLINEFORM4 controls the penalization of the prediction errors and INLINEFORM5 and INLINEFORM6 are margins for the correct and incorrect classes. Following deSantos2015, we set INLINEFORM7. We do not learn a pattern for the class Other but increase its difference to the best competitive label by using only the second summand in Equation EQREF10 during training.", so we know that they use ranking loss function.
True answer: uni-directional RNN

Validation ID: 0cfe0e33fbb100751fc0916001a5a19498ae8cb5
Question: How do they obtain the new context represetation?
From the context "One of our contributions is a new input representation especially designed for relation classification. The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, we want to focus on it but not ignore the other regions completely. Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, we force the network to pay special attention to it. The two contexts are processed by two independent convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation. Figure FIGREF3 depicts this procedure. It shows an examplary sentence: “He had chest pain and <e1>headaches</e1> from <e2>mold</e2> in the bedroom.” If we only considered the middle context “from”, the network might be tempted to predict a relation like Entity-Origin(e1,e2). However, by also taking the left and right context into account, the model can detect the relation Cause-Effect(e2,e1). While this could also be achieved by integrating the whole context into the model, using the whole context can have disadvantages for longer sentences: The max pooling step can easily choose a value from a part of the sentence which is far away from the mention of the relation. With splitting the context into two parts, we reduce this danger. Repeating the middle context increases the chance for the max pooling step to pick a value from the middle context.", so we know that they obtain the new context represetation by splitting the context into two parts.
Answer: splitting the context into two parts
True answer: They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task.
Baseline Experiments ::: Majority Class Baselines
True answer: Yes

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task.
Baseline Experiments ::: Majority Class Baselines
True answer: Unanswerable

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task.
Baseline Experiments ::: Majority Class Baselines
True answer: Yes

Validation ID: 71ba1b09bb03f5977d790d91702481cc406b3767
Question: What is the performance of the baseline?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task. The first pair of baselines uses the most frequent class in the training set for prediction. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question. The latter can only be applied to the supervised test set.", so we know that the performance of the baseline is the accuracy of the global majority class baseline and the target-wise majority class baseline.
Answer: The performance of the baseline is the accuracy of the global majority class baseline and the target-wise majority class baseline.
True answer: M-Bert had 76.6 F1 macro score.

Validation ID: 71ba1b09bb03f5977d790d91702481cc406b3767
Question: What is the performance of the baseline?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task. The first pair of baselines uses the most frequent class in the training set for prediction. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question. The latter can only be applied to the supervised test set.", so we know that the performance of the baseline is the accuracy of the global majority class baseline and the target-wise majority class baseline.
Answer: The performance of the baseline is the accuracy of the global majority class baseline and the target-wise majority class baseline.
True answer: 75.1% and 75.6% accuracy

Validation ID: 612c3675b6c55b60ae6d24265ed8e20f62cb117e
Question: Did they pefrorm any cross-lingual vs single language evaluation?
From the context "We evaluate two types of baselines to obtain an impression of the difficulty of the task.
Baseline Experiments ::: Majority Class Baselines
True answer: Yes

Validation ID: bd40f33452da7711b65faaa248aca359b27fddb6
Question: What was the performance of multilingual BERT?
From the context "We fine-tune multilingual Bert (M-Bert) on the task BIBREF5 which has been pretrained jointly in 104 languages and has established itself as a state of the art for various multilingual tasks BIBREF18, BIBREF19. Within the field of stance detection, Bert can outperform both feature-based and other neural approaches in a monolingual English setting BIBREF10.", so we know that the performance of multilingual BERT is state of the art.
Answer: state of the art
True answer: BERT had 76.6 F1 macro score on x-stance dataset.

Validation ID: 787c4d4628eac00dbceb1c96020bff0090edca46
Question: What annotations are present in dataset?
From the context "The input provided by x-stance is two-fold: (A) a natural language question concerning a political issue; (B) a natural language commentary on a specific stance towards the question", so we know that the annotations are (A) a natural language question concerning a political issue and (B) a natural language commentary on a specific stance towards the question.
Answer: (A) a natural language question concerning a political issue and (B) a natural language commentary on a specific stance towards the question
True answer: answer each question with either `yes', `rather yes', `rather no', or `no'., can supplement each answer with a comment of at most 500 characters

Validation ID: 3c3807f226ba72fc41f59f0338f12a49a0c35605
Question: Are the experts comparable to real-world users?
From the context "We recruit seven experts with legal training to construct answers to Turker questions", so we know that the experts are recruited with legal training.
Answer: Yes
True answer: No

Validation ID: c70bafc35e27be9d1efae60596bc0dd390c124c0
Question: Are the answers double (and not triple) annotated?
From the context "In addition, in order to estimate annotation reliability and provide for better evaluation, every question in the test set is answered by at least two additional experts", so we know that the answers are double annotated.
Answer: Yes
True answer: Yes

Validation ID: 81d607fc206198162faa54a796717c2805282d9b
Question: Who were the experts used for annotation?
From the context "To identify legally sound answers, we recruit seven experts with legal training to construct answers to Turker questions", so we know that the experts used for annotation are seven experts with legal training.
Answer: seven experts with legal training
True answer: Individuals with legal training

Validation ID: 81d607fc206198162faa54a796717c2805282d9b
Question: Who were the experts used for annotation?
From the context "To identify legally sound answers, we recruit seven experts with legal training to construct answers to Turker questions", so we know that the experts used for annotation are seven experts with legal training.
Answer: seven experts with legal training
True answer: Yes

Validation ID: 51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad
Question: What type of neural model was used?
From the context "We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each query-policy sentence pair as a binary classification task to identify if the sentence is evidence for the question or not (Bert). We also experiment with a two-stage classifier, where we separately train the model on questions only to predict answerability. At inference time, if the answerable classifier predicts the question is answerable, the evidence identification classifier produces a set of candidate sentences (Bert + Unanswerable).", so we know that the neural model used is BERT.
Answer: BERT
True answer: Bert + Unanswerable

Validation ID: 51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad
Question: What type of neural model was used?
From the context "We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each query-policy sentence pair as a binary classification task to identify if the sentence is evidence for the question or not (Bert). We also experiment with a two-stage classifier, where we separately train the model on questions only to predict answerability. At inference time, if the answerable classifier predicts the question is answerable, the evidence identification classifier produces a set of candidate sentences (Bert + Unanswerable).", so we know that the neural model used is BERT.
Answer: BERT
True answer: CNN, BERT

Validation ID: f0848e7a339da0828278f6803ed7990366c975f0
Question: Were other baselines tested to compare with the neural baseline?
From the context "We describe baselines on this task, including a human performance baseline.", so we know that other baselines were tested to compare with the neural baseline.
Answer: Yes
True answer: SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance

Validation ID: f0848e7a339da0828278f6803ed7990366c975f0
Question: Were other baselines tested to compare with the neural baseline?
From the context "We describe baselines on this task, including a human performance baseline.", so we know that other baselines were tested to compare with the neural baseline.
Answer: Yes
True answer: No-Answer Baseline (NA), Word Count Baseline, Human Performance

Validation ID: b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8
Question: Does the paper clearly establish that the challenges listed here exist in this dataset and task?
From the context "Our dataset consists of a large collection of global posts from Twitter between the dates of June 1, 2013 to June 5, 2013. Only English language posts (as detected by the lang field in Twitter API) and posts with at least one hashtag are retained. We removed infrequent hashtags ( $<500$ posts) since they do not have enough data for good generalization. We also removed very frequent tags ( $>19K$ posts) which were almost always from automatically generated posts (ex: #androidgame) which are trivial to predict. The final dataset contains 2 million tweets for training, 10K for validation and 50K for testing, with a total of 2039 distinct hashtags. We use simple regex to preprocess the post text and remove hashtags (since these are to be predicted) and HTML tags, and replace user-names and URLs with special tokens. We also removed retweets and convert the text to lower-case.", so we know that the challenges listed here exist in this dataset and task.
Answer: Yes
True answer: Yes

Validation ID: 792f6d76d2befba2af07198584aac1b189583ae4
Question: Is this hashtag prediction task an established task, or something new?
From the context "Hashtag prediction for social media has been addressed earlier, for example in BIBREF15, BIBREF16. BIBREF15 also use a neural architecture, but compose text embeddings from a lookup table of words. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations.", so we know that hashtag prediction is an established task.
Answer: Established task
True answer: established task

Validation ID: 792f6d76d2befba2af07198584aac1b189583ae4
Question: Is this hashtag prediction task an established task, or something new?
From the context "Hashtag prediction for social media has been addressed earlier, for example in BIBREF15, BIBREF16. BIBREF15 also use a neural architecture, but compose text embeddings from a lookup table of words. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations.", so we know that hashtag prediction is an established task.
Answer: Established task
True answer: Hashtag prediction for social media has been addressed earlier

Validation ID: 127d5ddfabec5c58832e5865cbd8ed0978c25a13
Question: What is the word-level baseline?
From the context "Since our objective is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets", so we know that the word-level baseline is a simple word-level encoder for tweets.
Answer: a simple word-level encoder for tweets
True answer: a simple word-level encoder, The encoder is essentially the same as tweet2vec, with the input as words instead of characters.

Validation ID: 127d5ddfabec5c58832e5865cbd8ed0978c25a13
Question: What is the word-level baseline?
From the context "Since our objective is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets", so we know that the word-level baseline is a simple word-level encoder for tweets.
Answer: a simple word-level encoder for tweets
True answer: The encoder is essentially the same as tweet2vec, with the input as words instead of characters.

Validation ID: b91671715ad4fad56c67c28ce6f29e180fe08595
Question: What other tasks do they test their method on?
From the context "We also study the effect of model size on the performance of these models. For the word model we set vocabulary size $V$ to 8K, 15K and 20K respectively. For tweet2vec we set the GRU hidden state size to 300, 400 and 500 respectively. Figure 2 shows precision 1 of the two models as the number of parameters is increased, for each test set described above. There is not much variation in the performance, and moreover tweet2vec always outperforms the word based model for the same number of parameters.", so we know that they test their method on precision 1.
Answer: precision 1
True answer: None

Validation ID: a6d37b5975050da0b1959232ae756fc09e5f87e8
Question: what is the word level baseline they compare to?
From the context "Since our objective is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets", so we know that the word level baseline is a simple word-level encoder for tweets.
Answer: a simple word-level encoder for tweets
True answer: a simple word-level encoder, with the input as words instead of characters

Validation ID: a6d37b5975050da0b1959232ae756fc09e5f87e8
Question: what is the word level baseline they compare to?
From the context "Since our objective is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets", so we know that the word level baseline is a simple word-level encoder for tweets.
Answer: a simple word-level encoder for tweets
True answer: The encoder is essentially the same as tweet2vec, with the input as words instead of characters

Validation ID: e82fa03f1638a8c59ceb62bb9a6b41b498950e1f
Question: What is the state of the art system mentioned?
From the context "We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task", so we know that the state of the art system mentioned is BERT.
Answer: BERT
True answer: Two knowledge-based systems,
two traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.

Validation ID: 7ab9c0b4ceca1c142ff068f85015a249b14282d0
Question: Do they incoprorate WordNet into the model?
From the context "We propose a new solution to WSD by constructing context-gloss pairs and then converting WSD to a sentence-pair classification task. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.", so we know that they do not incorporate WordNet into the model.
Answer: No
True answer: construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem

Validation ID: 7ab9c0b4ceca1c142ff068f85015a249b14282d0
Question: Do they incoprorate WordNet into the model?
From the context "We propose a new solution to WSD by constructing context-gloss pairs and then converting WSD to a sentence-pair classification task. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.", so we know that they do not incorporate WordNet into the model.
Answer: No
True answer: construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word

Validation ID: 00050f7365e317dc0487e282a4c33804b58b1fb3
Question: Is SemCor3.0 reflective of English language data in general?
From the context "Following previous work BIBREF13, BIBREF12, BIBREF10, BIBREF17, BIBREF9, BIBREF7, we choose SemCor3.0 as training corpus, which is the largest corpus manually annotated with WordNet sense for WSD.", so we know that SemCor3.0 is the largest corpus manually annotated with WordNet sense for WSD.
Answer: No
True answer: Yes

Validation ID: 00050f7365e317dc0487e282a4c33804b58b1fb3
Question: Is SemCor3.0 reflective of English language data in general?
From the context "Following previous work BIBREF13, BIBREF12, BIBREF10, BIBREF17, BIBREF9, BIBREF7, we choose SemCor3.0 as training corpus, which is the largest corpus manually annotated with WordNet sense for WSD.", so we know that SemCor3.0 is the largest corpus manually annotated with WordNet sense for WSD.
Answer: No
True answer: Unanswerable

Validation ID: c5b0ed5db65051eebd858beaf303809aa815e8e5
Question: Do they use large or small BERT?
From the context "We use the pre-trained uncased BERT$_\mathrm {BASE}$ model for fine-tuning, because we find that BERT$_\mathrm {LARGE}$ model performs slightly worse than BERT$_\mathrm {BASE}$ in this task", so we know that they use small BERT.
Answer: small BERT
True answer: small BERT

Validation ID: c5b0ed5db65051eebd858beaf303809aa815e8e5
Question: Do they use large or small BERT?
From the context "We use the pre-trained uncased BERT$_\mathrm {BASE}$ model for fine-tuning, because we find that BERT$_\mathrm {LARGE}$ model performs slightly worse than BERT$_\mathrm {BASE}$ in this task", so we know that they use small BERT.
Answer: small BERT
True answer: small BERT

Validation ID: 10fb7dc031075946153baf0a0599e126de29e3a4
Question: How does the neural network architecture accomodate an unknown amount of senses per word?
From the context "We construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word, thus treating WSD task as a sentence-pair classification problem.", so we know that the neural network architecture accomodates an unknown amount of senses per word by constructing context-gloss pairs from glosses of all possible senses (in WordNet) of the target word.
Answer: by constructing context-gloss pairs from glosses of all possible senses (in WordNet) of the target word
True answer: converts WSD to a sequence learning task,  leverage gloss knowledge, by extending gloss knowledge

Validation ID: e438445cf823893c841b2bc26cdce32ccc3f5cbe
Question: Which fonts are the best indicators of high quality?
From the context "We can see that Inception identifies the two most important regions (one at the top corresponding to the table of contents, and the other at the bottom, capturing both document length and references) that contribute to the FA class prediction, and a region in the upper half of the image that contributes to the GA class prediction (capturing the length of the article body). From Figure 3 and Figure 3, we can see that the most important regions in terms of B and C class prediction capture images (down the left and right of the page, in the case of B and C), and document length/references. From Figure 3 and Figure 3, we can see that Inception finds that images in the top right corner are the strongest predictor of Start class prediction, and (the lack of) images/the link bar down the left side of the document are the most important for Stub class prediction.", so we know that the fonts in the top right corner are the best indicators of high quality.
Answer: the fonts in the top right corner
True answer: Unanswerable

Validation ID: 12f7fac818f0006cf33269c9eafd41bbb8979a48
Question: What kind of model do they use?
From the context "We adopt a bi-directional LSTM model to generate textual embeddings for document quality assessment, following the method of BIBREF12 (“biLSTM” hereafter). The input to biLSTM is a textual document, and the output is a textual embedding, which will later integrate with the visual embedding.", so we know that they use biLSTM model.
Answer: biLSTM
True answer: visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models

Validation ID: 12f7fac818f0006cf33269c9eafd41bbb8979a48
Question: What kind of model do they use?
From the context "We adopt a bi-directional LSTM model to generate textual embeddings for document quality assessment, following the method of BIBREF12 (“biLSTM” hereafter). The input to biLSTM is a textual document, and the output is a textual embedding, which will later integrate with the visual embedding.", so we know that they use biLSTM model.
Answer: biLSTM
True answer: Inception V3, biLSTM

Validation ID: d5a8fd8bb48dd1f75927e874bdea582b4732a0cd
Question: Did they release their data set of academic papers?
From the context "All code and data associated with this research will be released on publication.", so we know that they released their data set of academic papers.
Answer: Yes
True answer: No

Validation ID: d5a8fd8bb48dd1f75927e874bdea582b4732a0cd
Question: Did they release their data set of academic papers?
From the context "All code and data associated with this research will be released on publication.", so we know that they released their data set of academic papers.
Answer: Yes
True answer: Unanswerable

Validation ID: 1097768b89f8bd28d6ef6443c94feb04c1a1318e
Question: Do the methods that work best on academic papers also work best on Wikipedia?
From the context "Experimental results show that we achieve a 2.9% higher accuracy than state-of-the-art approaches based on textual features over Wikipedia, and performance competitive with or surpassing state-of-the-art approaches over arXiv", so we know that the methods that work best on academic papers also work best on Wikipedia.
Answer: Yes
True answer: Yes

Validation ID: 1097768b89f8bd28d6ef6443c94feb04c1a1318e
Question: Do the methods that work best on academic papers also work best on Wikipedia?
From the context "Experimental results show that we achieve a 2.9% higher accuracy than state-of-the-art approaches based on textual features over Wikipedia, and performance competitive with or surpassing state-of-the-art approaches over arXiv", so we know that the methods that work best on academic papers also work best on Wikipedia.
Answer: Yes
True answer: No

Validation ID: fc1679c714eab822431bbe96f0e9cf4079cd8b8d
Question: What is their system's absolute accuracy?
From the context "Experimental results show that we achieve a 2.9% higher accuracy than state-of-the-art approaches based on textual features over Wikipedia, and performance competitive with or surpassing state-of-the-art approaches over arXiv", so we know that their system's absolute accuracy is 2.9%.
Answer: 2.9%
True answer: 59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers

Validation ID: 23e2971c962bb6486bc0a66ff04242170dd22a1d
Question: Which is more useful, visual or textual features?
From the context "Experimental results show that we achieve a 2.9% higher accuracy than state-of-the-art approaches based on textual features over Wikipedia, and performance competitive with or surpassing state-of-the-art approaches over arXiv. We further proposed a joint model, combining textual and visual representations, to predict the quality of a document. Experimental results show that our joint model outperforms the visual-only model in all cases, and the text-only model on Wikipedia and two subsets of arXiv. These results underline the feasibility of assessing document quality via visual features, and the complementarity of visual and textual document representations for quality assessment.", so we know that visual features are more useful than textual features.
Answer: visual features
True answer: It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. 

Validation ID: c9bc6f53b941863e801280343afa14248521ce43
Question: Which languages do they use?
From the context "We use the pre-trained GloVe BIBREF39 50-dimensional word embeddings to represent words. For words not in GloVe, word embeddings are randomly initialized based on sampling from a uniform distribution $U(-1, 1)$", so we know that they use GloVe and uniform distribution $U(-1, 1)$.
Answer: GloVe and uniform distribution $U(-1, 1)$
True answer: English

Validation ID: c9bc6f53b941863e801280343afa14248521ce43
Question: Which languages do they use?
From the context "We use the pre-trained GloVe BIBREF39 50-dimensional word embeddings to represent words. For words not in GloVe, word embeddings are randomly initialized based on sampling from a uniform distribution $U(-1, 1)$", so we know that they use GloVe and uniform distribution $U(-1, 1)$.
Answer: GloVe and uniform distribution $U(-1, 1)$
True answer: English

Validation ID: 07b70b2b799b9efa630e8737df8b1dd1284f032c
Question: How large is their data set?
From the context "We randomly sampled 5,000 articles from each quality class and removed all redirect pages, resulting in a dataset of 29,794 articles", so we know that the data set is 29,794 articles.
Answer: 29,794 articles
True answer: a sample of  29,794 wikipedia articles and 2,794 arXiv papers 

Validation ID: 71a0c4f19be4ce1b1bae58a6e8f2a586e125d074
Question: Where do they get their ground truth quality judgments?
From the context "The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus", so we know that the ground truth quality judgments are assigned by Wikipedia reviewers or any registered user.
Answer: Wikipedia reviewers or any registered user
True answer: Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”)., The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus., The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). 

Validation ID: 71a0c4f19be4ce1b1bae58a6e8f2a586e125d074
Question: Where do they get their ground truth quality judgments?
From the context "The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus", so we know that the ground truth quality judgments are assigned by Wikipedia reviewers or any registered user.
Answer: Wikipedia reviewers or any registered user
True answer: quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI

Validation ID: c2eb743c9d0baf1781c3c0df9533fab588250af3
Question: Which models did they experiment with?
From the context "We evaluated the proposed method on various benchmark tasks: natural language inference, paraphrase identification, and sentiment classification", so we know that they experimented with natural language inference, paraphrase identification, and sentiment classification.
Answer: natural language inference, paraphrase identification, and sentiment classification
True answer: Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers

Validation ID: c35806cf68220b2b9bb082b62f493393b9bdff86
Question: What were their best results on the benchmark datasets?
From the context "Our models achieve the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and obtain comparable results on MultiNLI and SST datasets", so we know that their best results on the benchmark datasets are:
SNLI: 87.0%
True answer: In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5

Validation ID: c35806cf68220b2b9bb082b62f493393b9bdff86
Question: What were their best results on the benchmark datasets?
From the context "Our models achieve the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets and obtain comparable results on MultiNLI and SST datasets", so we know that their best results on the benchmark datasets are:
SNLI: 87.0%
True answer: accuracy of 87.0%

Validation ID: f7d0fa52017a642a9f70091a252857fccca31f12
Question: What were the baselines?
From the context "We evaluate our method on natural language inference (NLI), paraphrase identification (PI), and sentiment classification. We also conduct analysis on gate values and experiments on model variants. For detailed experimental settings, we refer readers to the supplemental material.", so we know that the baselines are NLI, PI, sentiment classification, gate values, and model variants.
Answer: NLI, PI, sentiment classification, gate values, and model variants
True answer: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections

Validation ID: 01209a3bead7c87bcdc628be2a5a26b41abde9d1
Question: Which datasets were used?
From the context "For the evaluation of performance of the proposed method on the NLI task, SNLI BIBREF22 and MultiNLI BIBREF23 datasets are used.", so we know that SNLI and MultiNLI datasets were used.
Answer: SNLI and MultiNLI datasets
True answer: SNLI BIBREF22 and MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24,  Stanford Sentiment Treebank (SST) BIBREF25

Validation ID: 01209a3bead7c87bcdc628be2a5a26b41abde9d1
Question: Which datasets were used?
From the context "For the NLI and PI tasks, there exists recent work specializing in sentence pair classification. However in this work we confine our model to the architecture that encodes each sentence using a shared encoder without any inter-sentence interaction, in order to focus on the effectiveness of the models in extracting semantics. But note that the applicability of CAS-LSTM is not limited to sentence encoding based approaches.", so we know that the datasets used are SNLI and MultiNLI.
Answer: SNLI and MultiNLI
True answer: SNLI BIBREF22 and MultiNLI BIBREF23 datasets, Quora Question Pairs dataset BIBREF24, Stanford Sentiment Treebank (SST) BIBREF25

Validation ID: 2740e3d7d33173664c1c5ab292c7ec75ff6e0802
Question: what datasets were used?
From the context "For MSA, we acquired the diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31. The corpus contains 9.7M tokens with approximately 194K unique surface forms (excluding numbers and punctuation marks). The corpus covers multiple genres such as politics and sports and is a mix of MSA and CA. This corpus is considerably larger than the Arabic Treebank BIBREF35 and is more consistent in its diacritization. For testing, we used the freely available WikiNews test set BIBREF31, which is composed of 70 MSA WikiNews articles (18,300 tokens) and evenly covers a variety of genres including politics, economics, health, science and technology, sports, arts and culture. For CA, we obtained a large collection of fully diacritized classical texts (2.7M tokens) from a book publisher, and we held-out a small subset of 5,000 sentences (approximately 400k words) for testing. Then, we used the remaining sentences to train the CA models.", so we know that the datasets used are:

True answer: diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31, WikiNews test set BIBREF31,  large collection of fully diacritized classical texts (2.7M tokens) from a book publisher

Validation ID: 2740e3d7d33173664c1c5ab292c7ec75ff6e0802
Question: what datasets were used?
From the context "For MSA, we acquired the diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31. The corpus contains 9.7M tokens with approximately 194K unique surface forms (excluding numbers and punctuation marks). The corpus covers multiple genres such as politics and sports and is a mix of MSA and CA. This corpus is considerably larger than the Arabic Treebank BIBREF35 and is more consistent in its diacritization. For testing, we used the freely available WikiNews test set BIBREF31, which is composed of 70 MSA WikiNews articles (18,300 tokens) and evenly covers a variety of genres including politics, economics, health, science and technology, sports, arts and culture.
For CA, we obtained a large collection of fully diacritized classical texts (2.7M tokens) from a book publisher, and we held-out a small subset of 5,000 sentences (approximately 400k words) for testing. Then, we used the remaining sentences to train the CA models.", so we know that the datasets used are:
True answer: the diacritized corpus that was used to train the RDI BIBREF7 diacritizer , WikiNews , a large collection of fully diacritized classical texts

Validation ID: db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21
Question: what are the previous state of the art?
From the context "We compare our models to multiple systems on the same test set. We achieve results that reduce diacritization error rates by more than half compared to the best SOTA systems", so we know that the previous state of the art are multiple systems.
Answer: multiple systems
True answer: Farasa, RDI

Validation ID: db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21
Question: what are the previous state of the art?
From the context "We compare our models to multiple systems on the same test set. We achieve results that reduce diacritization error rates by more than half compared to the best SOTA systems", so we know that the previous state of the art are multiple systems.
Answer: multiple systems
True answer: Farasa BIBREF31, MADAMIRA BIBREF29, RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), Microsoft ATKS BIBREF28

Validation ID: 48bd71477d5f89333fa7ce5c4556e4d950fb16ed
Question: what surface-level features are used?
From the context "Surface-level features resulted in the most gain, followed by POS tags, and lastly stem templates", so we know that surface-level features are used.
Answer: surface-level features
True answer: affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities

Validation ID: 76ed74788e3eb3321e646c48ae8bf6cdfe46dca1
!!! Has error during processing !!!
Question: what linguistics features are used?
Answer: From the context "We used 4 feature types, namely: CHAR: the characters. SEG: the position of the character in a word segment. For example, given the word “wAlktAb” (والكتاب> and the book/writers), which is composed of 3 segments “w+Al+ktAb” (و+ال+كتاب>). Letters were marked as “B” if they begin a segment, “M” if they are in the middle of a segment, “E” if they end a segment, and “S” if they are single letter segments. So for “w+Al+ktAb”, the corresponding character positions are “S+BE+BMME”. We used Farasa to perform segmentation, which has a reported segmentation accuracy of 99% on the WikiNews dataset BIBREF36. PRIOR: diacritics seen in the training set per segment. Since we used a character level model, this feature informed the model with word level information. For example, the word “ktAb” (كتاب>) was observed to have two diacritized forms in the training set, namely “kitaAb” (كِتَاب> – book) and “kut$\sim $aAb” (كُتَّاب> – writers). The first letter in the word (“k”) accepted the diacritics “i” and “u”. Thus given a binary vector representing whether a character is allowed to assume any of the eight primitive Arabic diacritic marks (a, i, u, o, K, N, F, and $\sim $ in order), the first letter would be given the following vector “01100000”. If a word segment was never observed during training, the vector for all letters therein would be set to 11111111. This feature borrows information from HMM models, which have been fairly successful in diacritizing word cores. CASE: whether the letter expects a core word diacritic or a case ending. Case endings are placed on only one letter in a word, which may or may not be the last letter in the word. This is a binary
True answer: POS, gender/number and stem POS

Validation ID: ad1be65c4f0655ac5c902d17f05454c0d4c4a15d
Question: what dataset statistics are provided?
From the context "For all experiments, we admitted only experienced MTurk workers who are based in the US. One HIT consisted of writing one text for the text collection, formulating 15 questions for the question collection, or finding 15 pairs of answers for the answer collection. We paid $0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection.", so we know that the dataset statistics are provided.
Answer: dataset statistics
True answer: More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).

Validation ID: ad1be65c4f0655ac5c902d17f05454c0d4c4a15d
Question: what dataset statistics are provided?
From the context "For all experiments, we admitted only experienced MTurk workers who are based in the US. One HIT consisted of writing one text for the text collection, formulating 15 questions for the question collection, or finding 15 pairs of answers for the answer collection. We paid $0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection.", so we know that the dataset statistics are provided.
Answer: dataset statistics
True answer: Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text

Validation ID: 2eb9280d72cde9de3aabbed993009a98a5fe0990
Question: what is the size of their dataset?
From the context "We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts)", so we know that the size of their dataset is 9,731 + 1,411 + 2,797 = 13,939.
Answer: 13,939
True answer: 13,939

Validation ID: 154a721ccc1d425688942e22e75af711b423e086
Question: what crowdsourcing platform was used?
From the context "We collected 20 texts for each scenario. Each participant was allowed to write only one story per scenario, but work on as many scenarios as they liked. For each of the 10 scenarios from InScript, we randomly selected 20 existing texts from that resource.", so we know that the crowdsourcing platform is Amazon Mechanical Turk.
Answer: Amazon Mechanical Turk
True answer: Amazon Mechanical Turk

Validation ID: 154a721ccc1d425688942e22e75af711b423e086
Question: what crowdsourcing platform was used?
From the context "We collected 20 texts for each scenario. Each participant was allowed to write only one story per scenario, but work on as many scenarios as they liked. For each of the 10 scenarios from InScript, we randomly selected 20 existing texts from that resource.", so we know that the crowdsourcing platform is Amazon Mechanical Turk.
Answer: Amazon Mechanical Turk
True answer: Amazon Mechanical Turk

Validation ID: 84bad9a821917cb96584cf5383c6d2a035358d7c
Question: how was the data collected?
From the context "We collected 20 texts for each scenario. Each participant was allowed to write only one story per scenario, but work on as many scenarios as they liked. For each of the 10 scenarios from InScript, we randomly selected 20 existing texts from that resource.", so we know that the data was collected by crowdsourcing.
Answer: crowdsourcing
True answer: The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.

Validation ID: c9305e5794b65b33399c22ac8e4e024f6b757a30
Question: What is best performing model among author's submissions, what performance it had?
From the context "Our system (Team: MIC-CIS) explores different neural architectures (CNN, BERT and LSTM-CRF) with linguistic, layout and topical features to address the tasks of fine-grained propaganda detection. We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures. Compared to the other participating systems, our submissions are ranked 3rd and 4th in FLC and SLC tasks, respectively.", so we know that the best performing model among author's submissions is LSTM-CRF+Multi-task that performs propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification).
From the context "Finally, we use the ensemble+ of (II and IV) from each of the folds 1-3, i.e., $|{\mathcal {M}}|=6$ models to obtain predictions on test. Table TABREF9 (FLC) shows that our submission is ranked at 3rd position.", so we know that the performance of the best performing model is 3rd position.
True answer: For SLC task, the "ltuorp" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the "newspeak" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).

Validation ID: 56b7319be68197727baa7d498fa38af0a8440fe4
Question: What extracted features were most influencial on performance?
From the context "We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures", so we know that the extracted features were most influencial on performance.
Answer: extracted features
True answer: Linguistic

Validation ID: 56b7319be68197727baa7d498fa38af0a8440fe4
Question: What extracted features were most influencial on performance?
From the context "We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures", so we know that the extracted features were most influencial on performance.
Answer: extracted features
True answer: BERT

Validation ID: 2268c9044e868ba0a16e92d2063ada87f68b5d03
Question: Did ensemble schemes help in boosting peformance, by how much?
From the context "We explore two ensemble strategies (Table TABREF1): majority-voting and relax-voting to boost precision and recall, respectively.", so we know that ensemble schemes help in boosting peformance.
From the context "Table TABREF10 shows the scores on dev (internal and external) for SLC task. Observe that the pre-trained embeddings (FastText or BERT) outperform TF-IDF vector representation. In row r2, we apply logistic regression classifier with BERTSentEmb that leads to improved scores over FastTextSentEmb. Subsequently, we augment the sentence vector with additional features that improves F1 on dev (external), however not dev (internal). Next, we initialize CNN by FastTextWordEmb or BERTWordEmb and augment the last hidden layer (before classification) with BERTSentEmb and feature vectors, leading to gains in F1 for both the dev sets. Further, we fine-tune BERT and apply different thresholds in relaxing the decision boundary, where $\tau \ge 0.35$ is found optimal.", so we know that ensemble schemes help in boosting peformance by 0.673.
True answer: The best ensemble topped the best single model by 0.029 in F1 score on dev (external).

Validation ID: 2268c9044e868ba0a16e92d2063ada87f68b5d03
Question: Did ensemble schemes help in boosting peformance, by how much?
From the context "We explore two ensemble strategies (Table TABREF1): majority-voting and relax-voting to boost precision and recall, respectively.", so we know that ensemble schemes help in boosting peformance.
From the context "Table TABREF10 shows the scores on dev (internal and external) for SLC task. Observe that the pre-trained embeddings (FastText or BERT) outperform TF-IDF vector representation. In row r2, we apply logistic regression classifier with BERTSentEmb that leads to improved scores over FastTextSentEmb. Subsequently, we augment the sentence vector with additional features that improves F1 on dev (external), however not dev (internal). Next, we initialize CNN by FastTextWordEmb or BERTWordEmb and augment the last hidden layer (before classification) with BERTSentEmb and feature vectors, leading to gains in F1 for both the dev sets. Further, we fine-tune BERT and apply different thresholds in relaxing the decision boundary, where $\tau \ge 0.35$ is found optimal.", so we know that ensemble schemes help in boosting peformance by 0.673.
True answer: They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification

Validation ID: 6b7354d7d715bad83183296ce2f3ddf2357cb449
Question: Which basic neural architecture perform best by itself?
From the context "Our system (Team: MIC-CIS) explores different neural architectures (CNN, BERT and LSTM-CRF) with linguistic, layout and topical features to address the tasks of fine-grained propaganda detection. We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures. Compared to the other participating systems, our submissions are ranked 3rd and 4th in FLC and SLC tasks, respectively.", so we know that the basic neural architecture that perform best by itself is LSTM-CRF.
Answer: LSTM-CRF
True answer: BERT

Validation ID: e949b28f6d1f20e18e82742e04d68158415dc61e
Question: What participating systems had better results than ones authors submitted?
From the context "Compared to the other participating systems, our submissions are ranked 3rd and 4th in FLC and SLC tasks, respectively.", so we know that the participating systems that had better results than ones authors submitted are ranked 1st and 2nd in FLC and SLC tasks, respectively.
Answer: ranked 1st and 2nd in FLC and SLC tasks, respectively
True answer: For SLC task : Ituorp, ProperGander and YMJA  teams had better results.
For FLC task: newspeak and Antiganda teams had better results.

Validation ID: a1ac2a152710335519c9a907eec60d9f468b19db
Question: What is specific to multi-granularity and multi-tasking neural arhiteture design?
From the context "We design sequence taggers BIBREF9, BIBREF10 in three modes: (1) LSTM-CRF BIBREF11 with word embeddings ($w\_e$) and character embeddings $c\_e$, token-level features ($t\_f$) such as polarity, POS, NER, etc. (2) LSTM-CRF+Multi-grain that jointly performs FLC and SLC with FastTextWordEmb and BERTSentEmb, respectively. Here, we add binary sentence classification loss to sequence tagging weighted by a factor of $\alpha $. (3) LSTM-CRF+Multi-task that performs propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification).", so we know that the specific to multi-granularity and multi-tasking neural arhiteture design is that it performs propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification).
Answer: propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification)
True answer: An output layer for each task

Validation ID: a1ac2a152710335519c9a907eec60d9f468b19db
Question: What is specific to multi-granularity and multi-tasking neural arhiteture design?
From the context "We design sequence taggers BIBREF9, BIBREF10 in three modes: (1) LSTM-CRF BIBREF11 with word embeddings ($w\_e$) and character embeddings $c\_e$, token-level features ($t\_f$) such as polarity, POS, NER, etc. (2) LSTM-CRF+Multi-grain that jointly performs FLC and SLC with FastTextWordEmb and BERTSentEmb, respectively. Here, we add binary sentence classification loss to sequence tagging weighted by a factor of $\alpha $. (3) LSTM-CRF+Multi-task that performs propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification).", so we know that the specific to multi-granularity and multi-tasking neural arhiteture design is that it performs propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification).
Answer: propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification)
True answer: Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT.

Validation ID: ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25
Question: Do they report results only on English data?
From the context "We evaluate our proposed models over a corpus of 12 MOOC iterations (offerings) on Coursera.org In partnership with Coursera and in line with its Terms of Service, we obtained the data for use in our academic research.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25
Question: Do they report results only on English data?
From the context "We evaluate our proposed models over a corpus of 12 MOOC iterations (offerings) on Coursera.org In partnership with Coursera and in line with its Terms of Service, we obtained the data for use in our academic research.", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 49eb52b3ec0647e165a5e41488088c80a20cc78f
Question: What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?
From the context "We propose three neural attention BIBREF11 variants based on how an instructor might attend and reply to a context in a thread: the ultimate, penultimate and any post attention models. We review each of these in turn.", so we know that the aspects of discussion are relevant to instructor intervention are the ultimate, penultimate and any post attention models.
Answer: the ultimate, penultimate and any post attention models
True answer: context inference

Validation ID: 9bb7ae50bff91571a945c1af025ed2e67714a788
Question: What was the previous state of the art for this task?
From the context "We compare our models against a neural baseline models, hierarchical LSTM (hLSTM), with the attention ablated but with access to the complete context, and a strong, open-sourced feature-rich baseline BIBREF7. We choose BIBREF7 over other prior works such as BIBREF0 since we do not have access to the dataset or the system used in their papers for replication. BIBREF7 is a logistic regression classifier with features inclusive of bag-of-words representation of the unigrams and thread length, normalised counts of agreements to previous posts, counts of non-lexical reference items such as URLs, and the Coursera forum type in which a thread appeared.", so we know that the previous state of the art for this task is BIBREF7.
Answer: BIBREF7
True answer: hLSTM

Validation ID: 9bb7ae50bff91571a945c1af025ed2e67714a788
Question: What was the previous state of the art for this task?
From the context "We compare our models against a neural baseline models, hierarchical LSTM (hLSTM), with the attention ablated but with access to the complete context, and a strong, open-sourced feature-rich baseline BIBREF7. We choose BIBREF7 over other prior works such as BIBREF0 since we do not have access to the dataset or the system used in their papers for replication. BIBREF7 is a logistic regression classifier with features inclusive of bag-of-words representation of the unigrams and thread length, normalised counts of agreements to previous posts, counts of non-lexical reference items such as URLs, and the Coursera forum type in which a thread appeared.", so we know that the previous state of the art for this task is BIBREF7.
Answer: BIBREF7
True answer: hLSTM

Validation ID: 81dbe9a9ddaa5d02b02e01a306d898015a56ffb6
Question: What type of latent context is used to predict instructor intervention?
From the context "We propose three neural attention BIBREF11 variants based on how an instructor might attend and reply to a context in a thread: the ultimate, penultimate and any post attention models. We review each of these in turn.", so we know that the type of latent context is ultimate, penultimate and any post attention models.
Answer: ultimate, penultimate and any post attention models
True answer: the series of posts that trigger an intervention

Validation ID: 348886b4762db063711ef8b7a10952375fbdcb57
Question: Do they report results only on English dataset?
From the context "We have proposed a novel approach to multimodal machine translation which makes better use of context, both textual and visual. Our results show that further exploring textual context through deliberation networks already leads to better results than the previous state of the art. Adding visual information, and in particular structural representations of this information, proved beneficial when input text contains noise and the language pair requires substantial restructuring from source to target. Our findings suggest that the combination of a deliberation approach and information from additional modalities is a promising direction for machine translation that is robust to noisy input.", so we know that they report results on English dataset.
Answer: English dataset
True answer: No

Validation ID: 348886b4762db063711ef8b7a10952375fbdcb57
Question: Do they report results only on English dataset?
From the context "We have proposed a novel approach to multimodal machine translation which makes better use of context, both textual and visual. Our results show that further exploring textual context through deliberation networks already leads to better results than the previous state of the art. Adding visual information, and in particular structural representations of this information, proved beneficial when input text contains noise and the language pair requires substantial restructuring from source to target. Our findings suggest that the combination of a deliberation approach and information from additional modalities is a promising direction for machine translation that is robust to noisy input.", so we know that they report results on English dataset.
Answer: English dataset
True answer: No

Validation ID: 1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b
Question: What dataset does this approach achieve state of the art results on?
From the context "We have proposed a novel approach to multimodal machine translation which makes better use of context, both textual and visual. Our results show that further exploring textual context through deliberation networks already leads to better results than the previous state of the art. Adding visual information, and in particular structural representations of this information, proved beneficial when input text contains noise and the language pair requires substantial restructuring from source to target. Our findings suggest that the combination of a deliberation approach and information from additional modalities is a promising direction for machine translation that is robust to noisy input. Our code and pre-processing scripts are available at https://github.com/ImperialNLP/MMT-Delib.", so we know that this approach achieves state of the art results on multimodal machine translation.
Answer: multimodal machine translation
True answer: the English-German dataset

Validation ID: f9aa055bf73185ba939dfb03454384810eb17ad1
Question: How much training data from the non-English language is used by the system?
From the context "We use WikiExtractor to extract extract raw sentences from Wikipedias as monolingual data for fine-tuning target embeddings and bilingual LMs (SECREF15)", so we know that the system uses monolingual data from Wikipedias.
Answer: Monolingual data from Wikipedias
True answer: No data. Pretrained model is used.

Validation ID: d571e0b0f402a3d36fb30d70cdcd2911df883bc7
Question: Is the system tested on low-resource languages?
From the context "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi). These languages belong to four different language families. French, Russian, and Hindi are Indo-European languages, similar to English. Arabic, Chinese, and Vietnamese belong to Afro-Asiatic, Sino-Tibetan, and Austro-Asiatic family respectively. The choice of the six languages also reflects different training conditions depending on the amount of monolingual data. French and Russian, and Arabic can be regarded as high resource languages whereas Hindi has far less data and can be considered as low resource.", so we know that the system is tested on low-resource languages.
Answer: Yes
True answer: Yes

Validation ID: d571e0b0f402a3d36fb30d70cdcd2911df883bc7
Question: Is the system tested on low-resource languages?
From the context "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi). These languages belong to four different language families. French, Russian, and Hindi are Indo-European languages, similar to English. Arabic, Chinese, and Vietnamese belong to Afro-Asiatic, Sino-Tibetan, and Austro-Asiatic family respectively. The choice of the six languages also reflects different training conditions depending on the amount of monolingual data. French and Russian, and Arabic can be regarded as high resource languages whereas Hindi has far less data and can be considered as low resource.", so we know that the system is tested on low-resource languages.
Answer: Yes
True answer: Yes

Validation ID: ce2b921e4442a21555d65d8ce4ef7e3bde931dfc
Question: What languages are the model transferred to?
From the context "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi). These languages belong to four different language families.", so we know that the model is transferred to French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi).
Answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)
True answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)

Validation ID: ce2b921e4442a21555d65d8ce4ef7e3bde931dfc
Question: What languages are the model transferred to?
From the context "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi). These languages belong to four different language families.", so we know that the model is transferred to French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi).
Answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)
True answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)

Validation ID: 2275b0e195cd9cb25f50c5c570da97a4cce5dca8
Question: How is the model transferred to other languages?
From the context "We have successfully adapted autoencoding pre-trained LMs from English to other languages", so we know that the model is transferred to other languages by adapting autoencoding pre-trained LMs.
Answer: adapting autoencoding pre-trained LMs
True answer: Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM.

Validation ID: 37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6
Question: What metrics are used for evaluation?
From the context "We evaluate our models on two cross-lingual zero-shot tasks: (1) Cross-lingual Natural Language Inference (XNLI) and (2) dependency parsing.", so we know that the metrics used for evaluation are XNLI and dependency parsing.
Answer: XNLI and dependency parsing
True answer: translation probabilities, Labeled Attachment Scores (LAS)

Validation ID: 37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6
Question: What metrics are used for evaluation?
From the context "We evaluate our models on two cross-lingual zero-shot tasks: (1) Cross-lingual Natural Language Inference (XNLI) and (2) dependency parsing.", so we know that the metrics used for evaluation are XNLI and dependency parsing.
Answer: XNLI and dependency parsing
True answer: accuracy, Labeled Attachment Scores (LAS)

Validation ID: d01c51155e4719bf587d114bcd403b273c77246f
Question: What datasets are used for evaluation?
From the context "We use XNLI dataset BIBREF9 for classification task and Universal Dependencies v2.4 BIBREF21 for parsing task.", so we know that XNLI dataset and Universal Dependencies v2.4 are used for evaluation.
Answer: XNLI dataset and Universal Dependencies v2.4
True answer: United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018

Validation ID: 9b4dc790e4ff49562992aae4fad3a38621fadd8b
Question: what are the existing approaches?
From the context "Many recent studies have highlighted that Flickr tags capture valuable ecological information, which can be used as a complementary source to more traditional sources. To date, however, ecologists have mostly used social media to conduct manual evaluations of image content with little automated exploitation of the associated tags BIBREF4, BIBREF5, BIBREF6. One recent exception is BIBREF7, where bag-of-words representations derived from Flickr tags were found to give promising result for predicting a range of different environemental phenomena.", so we know that the existing approaches are manual evaluations of image content and bag-of-words representations.
Answer: manual evaluations of image content and bag-of-words representations
True answer: BOW-Tags, BOW-KL(Tags), BOW-All, GloVe

Validation ID: a1dac888f63c9efaf159d9bdfde7c938636f07b1
Question: what dataset is used in this paper?
From the context "We will use the method from BIBREF7 as our main baseline. This will allow us to directly evaluate the effectiveness of embeddings for the considered problem, since we have used the same structured datasets and same tag weighting scheme.", so we know that the dataset used in this paper is the same as BIBREF7.
Answer: the same as BIBREF7
True answer:  the same datasets as BIBREF7

Validation ID: a1dac888f63c9efaf159d9bdfde7c938636f07b1
Question: what dataset is used in this paper?
From the context "We will use the method from BIBREF7 as our main baseline. This will allow us to directly evaluate the effectiveness of embeddings for the considered problem, since we have used the same structured datasets and same tag weighting scheme.", so we know that the dataset used in this paper is the same as BIBREF7.
Answer: the same as BIBREF7
True answer: same datasets as BIBREF7

Validation ID: 1e4dbfc556cf237accb8b370de2f164fa723687b
Question: How is keyphrase diversity measured?
From the context "We use two metrics, one quantitative and one qualitative, to measure diversity of generation.", so we know that keyphrase diversity is measured by two metrics.
Answer: two metrics
True answer: average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set

Validation ID: fff5c24dca92bc7d5435a2600e6764f039551787
Question: How was the StackExchange dataset collected?
From the context "Inspired by the StackLite tag recommendation task on Kaggle, we build a new benchmark based on the public StackExchange data.", so we know that the StackExchange dataset was collected from the public StackExchange data.
Answer: from the public StackExchange data
True answer: they obtained computer science related topics by looking at titles and user-assigned tags

Validation ID: fff5c24dca92bc7d5435a2600e6764f039551787
Question: How was the StackExchange dataset collected?
From the context "Inspired by the StackLite tag recommendation task on Kaggle, we build a new benchmark based on the public StackExchange data.", so we know that the StackExchange dataset was collected from the public StackExchange data.
Answer: from the public StackExchange data
True answer: Unanswerable

Validation ID: b2ecfd5480a2a4be98730e2d646dfb84daedab17
Question: What does the TextWorld ACG dataset contain?
From the context "The TextWorld ACG dataset contains a collection of text-based adventure games, each of which is a set of text-based instructions that can be used to play a game", so we know that the TextWorld ACG dataset contains a collection of text-based adventure games.
Answer: a collection of text-based adventure games
True answer: Unanswerable

Validation ID: a3efe43a72b76b8f5e5111b54393d00e6a5c97ab
Question: What is the size of the StackExchange dataset?
From the context "We use questions with titles as source, and user-assigned tags as target keyphrases.", so we know that the size of the StackExchange dataset is the number of questions with titles.
From the context "We report our models' performance on StackEx in Table TABREF37.", so we know that the size of the StackExchange dataset is 5,000.
True answer: Unanswerable

Validation ID: a3efe43a72b76b8f5e5111b54393d00e6a5c97ab
Question: What is the size of the StackExchange dataset?
From the context "We use questions with titles as source, and user-assigned tags as target keyphrases.", so we know that the size of the StackExchange dataset is the number of questions with titles.
From the context "We report our models' performance on StackEx in Table TABREF37.", so we know that the size of the StackExchange dataset is 5,000.
True answer: around 332k questions

Validation ID: f1e90a553a4185a4b0299bd179f4f156df798bce
Question: What were the baselines?
From the context "We include four non-neural extractive models and CopyRNN BIBREF0 as baselines. We use CopyRNN to denote the model reported by BIBREF0, CopyRNN* to denote our implementation of CopyRNN based on their open sourced code.", so we know that the baselines are four non-neural extractive models and CopyRNN.
Answer: four non-neural extractive models and CopyRNN
True answer: CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)

Validation ID: f1e90a553a4185a4b0299bd179f4f156df798bce
Question: What were the baselines?
From the context "We include four non-neural extractive models and CopyRNN BIBREF0 as baselines. We use CopyRNN to denote the model reported by BIBREF0, CopyRNN* to denote our implementation of CopyRNN based on their open sourced code.", so we know that the baselines are four non-neural extractive models and CopyRNN.
Answer: four non-neural extractive models and CopyRNN
True answer: CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*

Validation ID: 19b7312cfdddb02c3d4eaa40301a67143a72a35a
Question: What two metrics are proposed?
From the context "We propose new metrics to evaluate keyphrase generation", so we know that two metrics are proposed.
Answer: two metrics
True answer: average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )

Validation ID: 22744c3bc68f120669fc69490f8e539b09e34b94
Question: Can the findings of this paper be generalized to a general-purpose task?
From the context "The task presented here was easy and simple to analyze, however, future work may be done on natural language tasks. If these properties hold it might indicate that a new evaluation paradigm for NLP should be pushed; one that emphasizes performance on uncharacteristic (but structurally sound) inputs in addition to the data typically seen in training.", so we know that the findings of this paper can be generalized to a general-purpose task.
Answer: Yes
True answer: Yes

Validation ID: 22744c3bc68f120669fc69490f8e539b09e34b94
Question: Can the findings of this paper be generalized to a general-purpose task?
From the context "The task presented here was easy and simple to analyze, however, future work may be done on natural language tasks. If these properties hold it might indicate that a new evaluation paradigm for NLP should be pushed; one that emphasizes performance on uncharacteristic (but structurally sound) inputs in addition to the data typically seen in training.", so we know that the findings of this paper can be generalized to a general-purpose task.
Answer: Yes
True answer: Yes

Validation ID: dcea88698949da4a1bd00277c06df06c33f6a5ff
Question: Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks?
From the context "The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.", so we know that the proposed task is a good proxy for the general-purpose sequence to sequence tasks.
Answer: The proposed task is a good proxy for the general-purpose sequence to sequence tasks
True answer: The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.

Validation ID: d7b60abb0091246e29d1a9c28467de598e090c20
Question: What was the baseline?
From the context "We find that on average 8.4% of all political comments are offensive compared to 7.8% of all apolitical comments", so we know that the baseline is 7.8%.
Answer: 7.8%
True answer: stochastic gradient descent, naive bayes, decision tree

Validation ID: d7b60abb0091246e29d1a9c28467de598e090c20
Question: What was the baseline?
From the context "We find that on average 8.4% of all political comments are offensive compared to 7.8% of all apolitical comments", so we know that the baseline is 7.8%.
Answer: 7.8%
True answer: Unanswerable

Validation ID: bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e
Question: What was their system's performance?
From the context "We find that 93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaways and 1.3% are trolls. Complete results are illustrated in fig:offensive-authors-cdf.", so we know that the system's performance is 93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaways and 1.3% are trolls.
Answer: 93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaways and 1.3% are trolls.
True answer: accuracy and F1-score of 89.6% and 89.2%, respectively

Validation ID: bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e
Question: What was their system's performance?
From the context "We find that 93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaways and 1.3% are trolls. Complete results are illustrated in fig:offensive-authors-cdf.", so we know that the system's performance is 93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaways and 1.3% are trolls.
Answer: 93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaways and 1.3% are trolls.
True answer: accuracy and F1-score of 89.6% and 89.2%, respectively

Validation ID: 5a6926de13a8cc25ce687c22741ba97a6e63d4ee
Question: What other political events are included in the database?
From the context "We will also examine whether there is a more general long-term trend toward offensive online political speech, which would be consistent with what scholars have found when studying political elites", so we know that the database includes other political events.
Answer: Other political events
True answer: US presidential primaries, Democratic and Republican National Conventions

Validation ID: dcc1115aeaf87118736e86f3e3eb85bf5541281c
Question: What classifier did they use?
From the context "We use these scalars as input to a random forest classifier to perform classification into Offensive and Not Offensive classes", so we know that they used a random forest classifier.
Answer: Random forest classifier
True answer: Random Forest

Validation ID: c74185bced810449c5f438f11ed6a578d1e359b4
Question: What labels for antisocial events are available in datasets?
From the context "Our work is motivated by the goal of assisting human moderators of online communities by preemptively signaling at-risk conversations that might deserve their attention", so we know that the labels for antisocial events are available in datasets.
Answer: available
True answer: The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: "Don't be rude or hostile to others users."

Validation ID: 88e5d37617e14d6976cc602a168332fc23644f19
Question: What are two datasets model is applied to?
From the context "We consider two datasets, representing related but slightly different forecasting tasks. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure the conversations are civil up to the moment of a personal attack. This is a useful property for the purposes of model analysis, and hence we focus on this as our primary dataset. However, we are conscious of the possibility that these strict labels may not fully capture the kind of behavior that moderators care about in practice. We therefore introduce a secondary dataset, constructed from the subreddit ChangeMyView (CMV) that does not use post-hoc annotations. Instead, the prediction task is to forecast whether the conversation will be subject to moderator action in the future.", so we know that the two datasets are Wikipedia conversations dataset and ChangeMyView (CMV).
Answer: Wikipedia conversations dataset and ChangeMyView (CMV)
True answer:  `Conversations Gone Awry' dataset, subreddit ChangeMyView

Validation ID: 88e5d37617e14d6976cc602a168332fc23644f19
Question: What are two datasets model is applied to?
From the context "We consider two datasets, representing related but slightly different forecasting tasks. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure the conversations are civil up to the moment of a personal attack. This is a useful property for the purposes of model analysis, and hence we focus on this as our primary dataset. However, we are conscious of the possibility that these strict labels may not fully capture the kind of behavior that moderators care about in practice. We therefore introduce a secondary dataset, constructed from the subreddit ChangeMyView (CMV) that does not use post-hoc annotations. Instead, the prediction task is to forecast whether the conversation will be subject to moderator action in the future.", so we know that the two datasets are Wikipedia conversations dataset and ChangeMyView (CMV).
Answer: Wikipedia conversations dataset and ChangeMyView (CMV)
True answer: An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. 

Validation ID: 45f7c03a686b68179cadb1413c5f3c1d373328bd
Question: What is the CORD-19 dataset?
From the context "We used the COVID-19 Open Research Dataset (CORD-19) BIBREF2 for our study. In response to the COVID-19 pandemic, the White House and a coalition of research groups prepared the CORD-19 dataset. It contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. These articles are contributed by hospitals and medical institutes all over the world.", so we know that the CORD-19 dataset is a dataset containing over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.
Answer: a dataset containing over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses
True answer: which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses

Validation ID: 45f7c03a686b68179cadb1413c5f3c1d373328bd
Question: What is the CORD-19 dataset?
From the context "We used the COVID-19 Open Research Dataset (CORD-19) BIBREF2 for our study. In response to the COVID-19 pandemic, the White House and a coalition of research groups prepared the CORD-19 dataset. It contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. These articles are contributed by hospitals and medical institutes all over the world.", so we know that the CORD-19 dataset is a dataset containing over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.
Answer: a dataset containing over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses
True answer: contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses

Validation ID: a2015f02dfb376bf9b218d1c897018f4e70424d7
Question: How large is the collection of COVID-19 literature?
From the context "We used the COVID-19 Open Research Dataset (CORD-19) BIBREF2 for our study. In response to the COVID-19 pandemic, the White House and a coalition of research groups prepared the CORD-19 dataset. It contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.", so we know that the collection of COVID-19 literature contains over 45,000 scholarly articles.
Answer: over 45,000 scholarly articles
True answer: 45,000 scholarly articles, including over 33,000 with full text

Validation ID: f697d00a82750b14376fe20a5a2b249e98bebe9b
Question: Which deep learning architecture do they use for sentence segmentation?
From the context "Our model architecture is based on Bi-LSTM-CRF, as shown in Fig.. The model is divided into three modules. The first, low-level module, consists of two separate structures: local and distant structures. The second, high-level module, contains a sequence of stacked bidirectional LSTM and self-attention layers. The final module, the prediction module, is responsible for predicting the tags INLINEFORM0 for the tokens in a word sequence INLINEFORM1 where INLINEFORM2 is the sequence size and INLINEFORM3, INLINEFORM4 denote the token and its tag at timestep INLINEFORM5, respectively. Each token INLINEFORM6 consists of a word, its POS and its type. There are five defined word types: English, Thai, punctuation, digits, and spaces.", so we know that they use Bi-LSTM-CRF for sentence segmentation.
Answer: Bi-LSTM-CRF
True answer: Bi-LSTM-CRF

Validation ID: f697d00a82750b14376fe20a5a2b249e98bebe9b
Question: Which deep learning architecture do they use for sentence segmentation?
From the context "Our model architecture is based on Bi-LSTM-CRF, as shown in Fig.. The model is divided into three modules. The first, low-level module, consists of two separate structures: local and distant structures. The second, high-level module, contains a sequence of stacked bidirectional LSTM and self-attention layers. The final module, the prediction module, is responsible for predicting the tags INLINEFORM0 for the tokens in a word sequence INLINEFORM1 where INLINEFORM2 is the sequence size and INLINEFORM3, INLINEFORM4 denote the token and its tag at timestep INLINEFORM5, respectively. Each token INLINEFORM6 consists of a word, its POS and its type. There are five defined word types: English, Thai, punctuation, digits, and spaces.", so we know that they use Bi-LSTM-CRF for sentence segmentation.
Answer: Bi-LSTM-CRF
True answer: Bi-LSTM-CRF

Validation ID: e0e379e546f1da9da874a2e90c79b41c60feb817
Question: How do they utilize unlabeled data to improve model representations?
From the context "The first, low-level module, consists of two separate structures: local and distant structures. The second, high-level module, contains a sequence of stacked bidirectional LSTM and self-attention layers. The final module, the prediction module, is responsible for predicting the tags INLINEFORM0 for the tokens in a word sequence INLINEFORM1 where INLINEFORM2 is the sequence size and INLINEFORM3, INLINEFORM4 denote the token and its tag at timestep INLINEFORM5, respectively. Each token INLINEFORM6 consists of a word, its POS and its type. There are five defined word types: English, Thai, punctuation, digits, and spaces.", so we know that the model is composed of three modules: low-level module, high-level module, and prediction module.
From the context "The first, low-level module, consists of two separate structures: local and distant structures. The second, high-level module, contains a sequence of stacked bidirectional LSTM and self-attention layers. The final module, the prediction module, is responsible for predicting the tags INLINEFORM0 for the tokens in a word sequence INLINEFORM1 where INLINEFORM2 is the sequence size and INLINEFORM3, INLINEFORM4 denote the token and its tag at timestep INLINEFORM5, respectively. Each token INLINEFORM6 consists of a word, its POS and its type. There are five defined word types: English, Thai, punctuation, digits, and spaces.", so we know that the low-level module consists of two structures: local and distant structures.
True answer: During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data.

Validation ID: 70148c8d0f345ea36200d5ba19d021924d98e759
Question: What is the McGurk effect?
From the context "The McGurk effect is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see BIBREF0. A prototypical example is that the audio of the phoneme “baa,” accompanied by a video of someone mouthing “vaa”, can be perceived as “vaa” or “gaa” (Figure 1 )", so we know that the McGurk effect is the phenomenon by which the perception of what we hear can be influenced by what we see.
Answer: The phenomenon by which the perception of what we hear can be influenced by what we see
True answer: a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard

Validation ID: 70148c8d0f345ea36200d5ba19d021924d98e759
Question: What is the McGurk effect?
From the context "The McGurk effect is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see BIBREF0. A prototypical example is that the audio of the phoneme “baa,” accompanied by a video of someone mouthing “vaa”, can be perceived as “vaa” or “gaa” (Figure 1 )", so we know that the McGurk effect is the phenomenon by which the perception of what we hear can be influenced by what we see.
Answer: The phenomenon by which the perception of what we hear can be influenced by what we see
True answer: When the perception of what we hear is influenced by what we see.

Validation ID: 27cf16bc9ef71761b9df6217f00f39f21130ce15
Question: Are humans and machine learning systems fooled by the same kinds of illusions?
From the context "This work is an initial step towards exploring the density of illusionable phenomena for humans. There are many natural directions for future work. In the vein of further understanding McGurk-style illusions, it seems worth building more accurate predictive models for sentence-level effects, and further investigating the security risks posed by McGurk illusions. For example, one concrete next step in understanding McGurk-style illusions would be to actually implement a system which takes an audio input, and outputs a video dub resulting in significant misunderstanding. Such a system would need to combine a high-quality speech-to-video-synthesis system BIBREF25, BIBREF26, with a fleshed-out language model and McGurk prediction model. There is also the question of how to guard against “attacks” on human perception. For example, in the case of the McGurk effect, how can one rephrase a passage of text in such a way that the meaning is unchanged, but the rephrased text is significantly more robust to McGurk style manipulations? The central question in this direction is what fraction of natural language can be made robust without significantly changing the semantics.", so we know that humans and machine learning systems are fooled by the same kinds of illusions.
Answer: Yes
True answer: Unanswerable

Validation ID: 627b8d7b5b985394428c974aca5ba0c1bbbba377
Question: how many humans evaluated the results?
From the context "We conduct a human evaluation on random 700 examples from our general test set. We picked only examples where a DocRepair translation is not a full copy of the baseline one.", so we know that 700 humans evaluated the results.
Answer: 700
True answer: Unanswerable

Validation ID: 627b8d7b5b985394428c974aca5ba0c1bbbba377
Question: how many humans evaluated the results?
From the context "We conduct a human evaluation on random 700 examples from our general test set. We picked only examples where a DocRepair translation is not a full copy of the baseline one.", so we know that 700 humans evaluated the results.
Answer: 700
True answer: Unanswerable

Validation ID: 126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0
Question: what was the baseline?
From the context "We use the publicly available OpenSubtitles2018 corpus BIBREF12 for English and Russian. For a fair comparison with previous work, we train the baseline MT system on the data released by BIBREF11. Namely, our MT system is trained on 6m instances. These are sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least $0.9$.", so we know that the baseline is the data released by BIBREF11.
Answer: the data released by BIBREF11
True answer:  MT system on the data released by BIBREF11

Validation ID: 126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0
Question: what was the baseline?
From the context "We use the publicly available OpenSubtitles2018 corpus BIBREF12 for English and Russian. For a fair comparison with previous work, we train the baseline MT system on the data released by BIBREF11. Namely, our MT system is trained on 6m instances. These are sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least $0.9$.", so we know that the baseline is the data released by BIBREF11.
Answer: the data released by BIBREF11
True answer: Transformer base, two-pass CADec model

Validation ID: 7e4ef0a4debc048b244b61b4f7dc2518b5b466c0
Question: what phenomena do they mention is hard to capture?
From the context "While in the current work we used text fragments of 4 sentences, in future work we would like to consider longer contexts.", so we know that they mention that it is hard to capture long contexts.
Answer: long contexts
True answer: Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection.

Validation ID: b68f72aed961d5ba152e9dc50345e1e832196a76
Question: by how much did the BLEU score improve?
From the context "The BLEU scores are provided in Table TABREF24 (we evaluate translations of 4-sentence fragments). To see which part of the improvement is due to fixing agreement between sentences rather than simply sentence-level post-editing, we train the same repair model at the sentence level. Each sentence in a group is now corrected separately, then they are put back together in a group. One can see that most of the improvement comes from accounting for extra-sentential dependencies. DocRepair outperforms the baseline and CADec by 0.7 BLEU, and its sentence-level repair version by 0.5 BLEU.", so we know that the BLEU score improved by 0.7 BLEU.
Answer: 0.7 BLEU
True answer: On average 0.64 

Validation ID: cf874cd9023d901e10aa8664b813d32501e7e4d2
Question: What is NER?
From the context "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc.", so we know that NER is Named Entity Recognition.
Answer: Named Entity Recognition
True answer: Named Entity Recognition

Validation ID: cf874cd9023d901e10aa8664b813d32501e7e4d2
Question: What is NER?
From the context "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc.", so we know that NER is Named Entity Recognition.
Answer: Named Entity Recognition
True answer: Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain

Validation ID: 42084c41343e5a6ae58a22e5bfc5ce987b5173de
Question: Does the paper explore extraction from electronic health records?
From the context "BioIE systems aim to extract information from a wide spectrum of articles including medical literature, biological literature, electronic health records, etc. that can be used by clinicians and researchers in the field", so we know that the paper explores extraction from electronic health records.
Answer: Yes
True answer: Yes

Validation ID: b637d6393ef3af7462917b81861531022b291933
Question: Does jiant involve datasets for the 50 NLU tasks?
From the context "jiant provides a configuration-driven interface for defining transfer learning and representation learning experiments using a bank of over 50 NLU tasks, cutting-edge sentence encoder models, and multi-task and multi-stage training procedures", so we know that jiant involves datasets for the 50 NLU tasks.
Answer: Yes
True answer: Yes

Validation ID: 8b9c12df9f89040f1485b3847a29f11b5c9262e0
Question: Is jiant compatible with models in any programming language?
From the context "jiant provides support for cutting-edge sentence encoder models, including support for Huggingface's Transformers. Supported models include: BERT BIBREF17, RoBERTa BIBREF27, XLNet BIBREF28, XLM BIBREF29, GPT BIBREF30, GPT-2 BIBREF31, ALBERT BIBREF32 and ELMo BIBREF33. jiant also supports the from-scratch training of (bidirectional) LSTMs BIBREF34 and deep bag of words models BIBREF35, as well as syntax-aware models such as PRPN BIBREF36 and ON-LSTM BIBREF37. jiant also supports word embeddings such as GloVe BIBREF38.", so we know that jiant is compatible with models in Python.
Answer: Python
True answer: Yes

Validation ID: 8b9c12df9f89040f1485b3847a29f11b5c9262e0
Question: Is jiant compatible with models in any programming language?
From the context "jiant provides support for cutting-edge sentence encoder models, including support for Huggingface's Transformers. Supported models include: BERT BIBREF17, RoBERTa BIBREF27, XLNet BIBREF28, XLM BIBREF29, GPT BIBREF30, GPT-2 BIBREF31, ALBERT BIBREF32 and ELMo BIBREF33. jiant also supports the from-scratch training of (bidirectional) LSTMs BIBREF34 and deep bag of words models BIBREF35, as well as syntax-aware models such as PRPN BIBREF36 and ON-LSTM BIBREF37. jiant also supports word embeddings such as GloVe BIBREF38.", so we know that jiant is compatible with models in Python.
Answer: Python
True answer: Unanswerable

Validation ID: 72e4e26d0dd79c590c28b10938952a9f9497ff1e
Question: What models are used for painting embedding and what for language style transfer?
From the context "For generating a poem from images we use an existing actor-critic architecture BIBREF1. This involves 3 parallel CNNs: an object CNN, sentiment CNN, and scene CNN, for feature extraction. These features are combined with a skip-thought model which provides poetic clues, which are then fed into a sequence-to-sequence model trained by policy gradient with 2 discriminator networks for rewards. This as a whole forms a pipeline that takes in an image and outputs a poem as shown on the top left of Figure FIGREF4.", so we know that the models used for painting embedding are object CNN, sentiment CNN, and scene CNN.
From the context "For Shakespearizing modern English texts, we experimented with various types of sequence to sequence models. Since the size of the parallel translation data available is small, we leverage a dictionary providing a mapping between Shakespearean words and modern English words to retrofit pre-trained word embeddings. Incorporating this extra information improves the translation task. The large number of shared word types between the source and target sentences indicates that sharing the representation between them is beneficial.", so we know that the models used for language style transfer are seq2seq with global attention and seq2seq with pointer networks.
True answer: generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models

Validation ID: 63b92dcc701ec77fdb3355ede5d37d2fbf057bcc
Question: What applicability of their approach is demonstrated by the authors?
From the context "In conclusion, combining two pipelines with an intermediate representation works well in practice. We observe that a CNN-RNN based image-to-poem net combined with a seq2seq model with parallel text corpus for text style transfer synthesizes Shakespeare-style prose for a given painting.", so we know that the applicability of their approach is demonstrated by the authors is that a CNN-RNN based image-to-poem net combined with a seq2seq model with parallel text corpus for text style transfer synthesizes Shakespeare-style prose for a given painting.
Answer: a CNN-RNN based image-to-poem net combined with a seq2seq model with parallel text corpus for text style transfer synthesizes Shakespeare-style prose for a given painting.
True answer: Unanswerable

Validation ID: 58ee0cbf1d8e3711c617b1cd3d7aca8620e26187
Question: What limitations do the authors demnostrate of their model?
From the context "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", so we know that the limitations of the model are that the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.
Answer: the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.
True answer: Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer

Validation ID: 58ee0cbf1d8e3711c617b1cd3d7aca8620e26187
Question: What limitations do the authors demnostrate of their model?
From the context "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.", so we know that the limitations of the model are that the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.
Answer: the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.
True answer: we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score

Validation ID: f71b52e00e0be80c926f153b9fe0a06dd93af11e
Question: How does final model rate on Likert scale?
From the context "The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare.", so we know that the final model rate on Likert scale is 3.9.
Answer: 3.9
True answer: average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9 

Validation ID: f71b52e00e0be80c926f153b9fe0a06dd93af11e
Question: How does final model rate on Likert scale?
From the context "The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare.", so we know that the final model rate on Likert scale is 3.9.
Answer: 3.9
True answer: average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9

Validation ID: 8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe
Question: How big is English poem description of the painting dataset?
From the context "We train a model for generating poems from images based on two datasets BIBREF1. The first dataset consists of image and poem pairs, namely a multi-modal poem dataset (MultiM-Poem), and the second dataset is a large poem corpus, namely a uni-modal poem dataset (UniM-Poem). The image and poem pairs are extended by adding the nearest three neighbor poems from the poem corpus without redundancy, and an extended image and poem pair dataset is constructed and denoted as MultiM-Poem(Ex)BIBREF1.", so we know that the size of the English poem description of the painting dataset is large.
Answer: large
True answer: Unanswerable

Validation ID: 54e945ea4b014e11ed4e1e61abc2aa9e68fea310
Question: What is best BLEU score of language style transfer authors got?
From the context "We also perform a quantitative analysis of style transfer by generating BLEU scores for the model output using the style transfer dataset. The variation of the BLEU scores with the source sentence lengths is shown in Figure FIGREF11. As expected, the BLEU scores decrease with increase in source sentence lengths.", so we know that the best BLEU score is 29.65.
Answer: 29.65
True answer: seq2seq model with global attention gives the best results with an average target BLEU score of 29.65

Validation ID: 54e945ea4b014e11ed4e1e61abc2aa9e68fea310
Question: What is best BLEU score of language style transfer authors got?
From the context "We also perform a quantitative analysis of style transfer by generating BLEU scores for the model output using the style transfer dataset. The variation of the BLEU scores with the source sentence lengths is shown in Figure FIGREF11. As expected, the BLEU scores decrease with increase in source sentence lengths.", so we know that the best BLEU score is 29.65.
Answer: 29.65
True answer: average target BLEU score of 29.65

Validation ID: df0257ab04686ddf1c6c4d9b0529a7632330b98e
Question: How better does new approach behave than existing solutions?
From the context "In this work, we opted for a simple Seq2Seq model for our policy, since our goal is to show the effectiveness of our proposed exploration methods. However, a more complex Hierarchical-Seq2Seq model BIBREF37 or a better encoder representation based on knowledge graphs BIBREF38, BIBREF39 would likely improve the of performance this approach.", so we know that a more complex Hierarchical-Seq2Seq model or a better encoder representation based on knowledge graphs would likely improve the of performance this approach.
Answer: a more complex Hierarchical-Seq2Seq model or a better encoder representation based on knowledge graphs
True answer:  On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model

Validation ID: df0257ab04686ddf1c6c4d9b0529a7632330b98e
Question: How better does new approach behave than existing solutions?
From the context "In this work, we opted for a simple Seq2Seq model for our policy, since our goal is to show the effectiveness of our proposed exploration methods. However, a more complex Hierarchical-Seq2Seq model BIBREF37 or a better encoder representation based on knowledge graphs BIBREF38, BIBREF39 would likely improve the of performance this approach.", so we know that a more complex Hierarchical-Seq2Seq model or a better encoder representation based on knowledge graphs would likely improve the of performance this approach.
Answer: a more complex Hierarchical-Seq2Seq model or a better encoder representation based on knowledge graphs
True answer: On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.
On Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.

Validation ID: 568fb7989a133564d84911e7cb58e4d8748243ef
Question: How is trajectory with how rewards extracted?
From the context "In phase 1, Go-Explore builds an archive of cells, where a cell is defined as a set of observations that are mapped to the same, discrete representation by some mapping function $f(x)$. Each cell is associated with meta-data including the trajectory towards that cell, the length of that trajectory, and the cumulative reward of that trajectory.", so we know that the trajectory with how rewards is extracted by Go-Explore.
Answer: Go-Explore
True answer: explores the state space through keeping track of previously visited states by maintaining an archive

Validation ID: 2c947447d81252397839d58c75ebcc71b34379b5
Question: On what Text-Based Games are experiments performed?
From the context "In this paper, we embrace more challenging environments where multiple words are needed at each step to solve the games and the reward is particularly sparse. Hence, we have selected the following environments:", so we know that experiments are performed on CoinCollector and CookingWorld.
Answer: CoinCollector and CookingWorld
True answer: CoinCollector , CookingWorld 

Validation ID: 2c947447d81252397839d58c75ebcc71b34379b5
Question: On what Text-Based Games are experiments performed?
From the context "In this paper, we embrace more challenging environments where multiple words are needed at each step to solve the games and the reward is particularly sparse. Hence, we have selected the following environments:", so we know that experiments are performed on CoinCollector and CookingWorld.
Answer: CoinCollector and CookingWorld
True answer: CoinCollector, CookingWorld

Validation ID: c01784b995f6594fdb23d7b62f20a35ae73eaa77
Question: How do the authors show that their learned policy generalize better than existing solutions to unseen games?
From the context "In the former experiment we show that Go-Explore finds winning trajectories faster than existing solutions, and in the latter, we show that training a Seq2Seq model on the trajectories found by Go-Explore results in stronger generalization, as suggested by the stronger performance on unseen games, compared to existing competitive baselines BIBREF10, BIBREF7.", so we know that the authors show that their learned policy generalize better than existing solutions to unseen games by comparing the performance on unseen games.
Answer: By comparing the performance on unseen games
True answer: promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game

Validation ID: 3415762847ed13acc3c90de60e3ef42612bc49af
Question: How much is classification performance improved in experiments for low data regime and class-imbalance problems?
From the context "We show that, even with the large-scale pretraining, data manipulation can still be very helpful to boost the model performance on downstream tasks. Since our approach uses validation sets for manipulation parameter learning, to avoid possible overfitting we restrict the training to small number (e.g., 5 or 10) of epochs.", so we know that the classification performance is improved in experiments for low data regime and class-imbalance problems.
Answer: improved
True answer: Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline
Imbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000

Validation ID: 223dc2b9ea34addc0f502003c2e1c1141f6b36a7
Question: What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?
From the context "To learn the parameters $\mathbf {\phi }$ in the manipulation reward $R_\phi (\mathbf {x}, y | \mathcal {D})$, we could in principle adopt any off-the-shelf reward learning algorithm in the literature. In this work, we draw inspiration from the above gradient-based reward learning (section SECREF3) due to its simplicity and efficiency.", so we know that the off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is gradient-based reward learning.
Answer: gradient-based reward learning
True answer: BIBREF7

Validation ID: 223dc2b9ea34addc0f502003c2e1c1141f6b36a7
Question: What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?
From the context "To learn the parameters $\mathbf {\phi }$ in the manipulation reward $R_\phi (\mathbf {x}, y | \mathcal {D})$, we could in principle adopt any off-the-shelf reward learning algorithm in the literature. In this work, we draw inspiration from the above gradient-based reward learning (section SECREF3) due to its simplicity and efficiency.", so we know that the off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is gradient-based reward learning.
Answer: gradient-based reward learning
True answer:  reward learning algorithm BIBREF7

Validation ID: e1ab11885f72b4658263a60751d956ba661c1d61
Question: What subtasks did they participate in?
From the context "We participated in four Spanish subtasks and our submissions ranked second, second, fourth and fifth place", so we know that they participated in four Spanish subtasks.
Answer: four Spanish subtasks
True answer: Answer with content missing: (Subscript 1: "We did not participate in subtask 5 (E-c)") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks.

Validation ID: c85b6f9bafc4c64fc538108ab40a0590a2f5768e
Question: What were the scores of their system?
From the context "Our official scores (column Ens Test in Table TABREF19 ) have placed us second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard", so we know that the scores of their system are second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc).
Answer: second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc)
True answer: column Ens Test in Table TABREF19

Validation ID: 8e52637026bee9061f9558178eaec08279bf7ac6
Question: How was the training data translated?
From the context "The training set provided by BIBREF0 is not very large, so it was interesting to find a way to augment the training set. A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish. This new set of “Spanish” data was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.", so we know that the training data was translated by machine translation platform Apertium.
Answer: machine translation platform Apertium
True answer: using the machine translation platform Apertium 

Validation ID: 8e52637026bee9061f9558178eaec08279bf7ac6
Question: How was the training data translated?
From the context "The training set provided by BIBREF0 is not very large, so it was interesting to find a way to augment the training set. A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish. This new set of “Spanish” data was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.", so we know that the training data was translated by machine translation platform Apertium.
Answer: machine translation platform Apertium
True answer: machine translation platform Apertium BIBREF5

Validation ID: 0f6216b9e4e59252b0c1adfd1a848635437dfcdc
Question: What dataset did they use?
From the context "For each task, the training data that was made available by the organizers is used, which is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment BIBREF1", so we know that the dataset they used is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment.
Answer: a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment
True answer:  Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish.

Validation ID: 0f6216b9e4e59252b0c1adfd1a848635437dfcdc
Question: What dataset did they use?
From the context "For each task, the training data that was made available by the organizers is used, which is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment BIBREF1", so we know that the dataset they used is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment.
Answer: a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment
True answer: Spanish tweets were scraped between November 8, 2017 and January 12, 2018, Affect in Tweets Distant Supervision Corpus (DISC)

Validation ID: 22ccee453e37536ddb0c1c1d17b0dbac04c6c607
Question: What other languages did they translate the data from?
From the context "The training set provided by BIBREF0 is not very large, so it was interesting to find a way to augment the training set. A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish.", so we know that they translated the data from English.
Answer: English
True answer: English 

Validation ID: 22ccee453e37536ddb0c1c1d17b0dbac04c6c607
Question: What other languages did they translate the data from?
From the context "The training set provided by BIBREF0 is not very large, so it was interesting to find a way to augment the training set. A possible method is to simply translate the datasets into other languages, leaving the labels intact. Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish.", so we know that they translated the data from English.
Answer: English
True answer: English

Validation ID: d00bbeda2a45495e6261548710afa6b21ea32870
Question: What semi-supervised learning is applied?
From the context "Our semi-supervised approach is quite straightforward: first a model is trained on the training set and then this model is used to predict the labels of the silver data. This silver data is then simply added to our training set, after which the model is retrained. However, an extra step is applied to ensure that the silver data is of reasonable quality. Instead of training a single model initially, ten different models were trained which predict the labels of the silver instances. If the highest and lowest prediction do not differ more than a certain threshold the silver instance is maintained, otherwise it is discarded.", so we know that the semi-supervised learning is applied.
Answer: applied
True answer: first a model is trained on the training set and then this model is used to predict the labels of the silver data, This silver data is then simply added to our training set, after which the model is retrained

Validation ID: 71b1af123fe292fd9950b8439db834212f0b0e32
Question: How were the datasets annotated?
From the context "The work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in $SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are", so we know that the datasets were annotated by human raters.
Answer: human raters
True answer: 1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process

Validation ID: a616a3f0d244368ec588f04dfbc37d77fda01b4c
Question: What are the 12 languages covered?
From the context "Multi-SimLex comprises eleven languages in addition to English", so we know that the 12 languages covered are English and eleven other languages.
Answer: English and eleven other languages
True answer: Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese

Validation ID: a616a3f0d244368ec588f04dfbc37d77fda01b4c
Question: What are the 12 languages covered?
From the context "Multi-SimLex comprises eleven languages in addition to English", so we know that the 12 languages covered are English and eleven other languages.
Answer: English and eleven other languages
True answer: Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
From the context "As a starting point, we used the DIP corpus BIBREF37, a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs) with a short description of each topic. It was created from a large web crawl using state-of-the-art information retrieval.", so we know that the corpus contains only English documents.
Answer: Yes
True answer: Yes

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
From the context "As a starting point, we used the DIP corpus BIBREF37, a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs) with a short description of each topic. It was created from a large web crawl using state-of-the-art information retrieval.", so we know that the corpus contains only English documents.
Answer: Yes
True answer: Unanswerable

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
From the context "As a starting point, we used the DIP corpus BIBREF37, a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs) with a short description of each topic. It was created from a large web crawl using state-of-the-art information retrieval.", so we know that the corpus contains only English documents.
Answer: Yes
True answer: Unanswerable

Validation ID: 1522ccedbb1f668958f24cca070f640274bc2549
Question: What type of evaluation is proposed for this task?
From the context "In addition, we provide implementations of a baseline method and evaluation scripts and hope that our efforts facilitate future research on this variant of summarization.", so we know that the evaluation is a baseline method.
Answer: baseline method
True answer: Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2

Validation ID: 97466a37525536086ed5d6e5ed143df085682318
Question: What baseline system is proposed?
From the context "In this section, we briefly describe a baseline and evaluation scripts that we release, with a detailed documentation, along with the corpus", so we know that the baseline system is described in this section.
Answer: In this section
True answer: Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction.

Validation ID: e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a
Question: How were crowd workers instructed to identify important elements in large document collections?
From the context "To overcome these issues, we introduce a new task design, low-context importance annotation, to determine summary-worthy parts of documents. Compared to Lloret et al.'s approach, it is more in line with crowdsourcing best practices, as the tasks are simple, intuitive and small BIBREF30 and workers receive reasonable payment BIBREF31. Most importantly, it is also much more efficient and scalable, as it does not require workers to read all documents in a cluster.", so we know that crowd workers were instructed to identify important elements in large document collections by low-context importance annotation.
Answer: low-context importance annotation
True answer: provide only a description of the document cluster's topic along with the propositions

Validation ID: e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a
Question: How were crowd workers instructed to identify important elements in large document collections?
From the context "To overcome these issues, we introduce a new task design, low-context importance annotation, to determine summary-worthy parts of documents. Compared to Lloret et al.'s approach, it is more in line with crowdsourcing best practices, as the tasks are simple, intuitive and small BIBREF30 and workers receive reasonable payment BIBREF31. Most importantly, it is also much more efficient and scalable, as it does not require workers to read all documents in a cluster.", so we know that crowd workers were instructed to identify important elements in large document collections by low-context importance annotation.
Answer: low-context importance annotation
True answer: They break down the task of importance annotation to the level of single propositions and obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary.

Validation ID: d6191c4643201262a770947fc95a613f57bedb6b
Question: Which collections of web documents are included in the corpus?
From the context "As a starting point, we used the DIP corpus BIBREF37, a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs) with a short description of each topic. It was created from a large web crawl using state-of-the-art information retrieval. We selected 30 of the topics for which we created the necessary concept map annotations.", so we know that the collections of web documents included in the corpus are 49 clusters of 100 web pages on educational topics.
Answer: 49 clusters of 100 web pages on educational topics
True answer: DIP corpus BIBREF37

Validation ID: ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8
Question: How do the authors define a concept map?
From the context "We define a concept map as a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the documents, making the summarization task extractive. A concept can be an entity, abstract idea, event or activity, designated by its unique label. Good maps should be propositionally coherent, meaning that every relation together with the two connected concepts form a meaningful proposition.", so we know that a concept map is a labeled graph showing concepts as nodes and relationships between them as edges.
Answer: A labeled graph showing concepts as nodes and relationships between them as edges
True answer: concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges

Validation ID: fc4ae12576ea3a85ea6d150b46938890d63a7d18
Question: Is the LSTM baseline a sub-word model?
From the context "We can directly compare the perplexity scores with the previous best BIBREF19 as both are unidirectional models, Transformer-XL model has outperformed the latter by 27%.", so we know that the LSTM baseline is a unidirectional model.
From the context "We can directly compare the perplexity scores with the previous best BIBREF19 as both are unidirectional models, Transformer-XL model has outperformed the latter by 27%.", so we know that the Transformer-XL model is a unidirectional model.
True answer: Yes

Validation ID: fc4ae12576ea3a85ea6d150b46938890d63a7d18
Question: Is the LSTM baseline a sub-word model?
From the context "We can directly compare the perplexity scores with the previous best BIBREF19 as both are unidirectional models, Transformer-XL model has outperformed the latter by 27%.", so we know that the LSTM baseline is a unidirectional model.
From the context "We can directly compare the perplexity scores with the previous best BIBREF19 as both are unidirectional models, Transformer-XL model has outperformed the latter by 27%.", so we know that the Transformer-XL model is a unidirectional model.
True answer: Yes

Validation ID: 19cf7884c0c509c189b1e74fe92c149ff59e444b
Question: How is pseudo-perplexity defined?
From the context "We therefore, define a pseudo-perplexity score from the above approximated joint probability.", so we know that pseudo-perplexity is defined from the approximated joint probability.
Answer: from the approximated joint probability
True answer: Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated.

Validation ID: ecd5770cf8cb12cb34285e26ab834301c17c53e1
Question: What is the model architecture used?
From the context "We next adapt a VQA deep learning architecture BIBREF24 to learn the predictive combination of visual and textual features. The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25. The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.", so we know that the model architecture used is LSTM-CNN.
Answer: LSTM-CNN
True answer: LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.

Validation ID: ecd5770cf8cb12cb34285e26ab834301c17c53e1
Question: What is the model architecture used?
From the context "We next adapt a VQA deep learning architecture BIBREF24 to learn the predictive combination of visual and textual features. The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25. The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.", so we know that the model architecture used is LSTM-CNN.
Answer: LSTM-CNN
True answer: random forest, The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.

Validation ID: 4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729
Question: How is the data used for training annotated?
From the context "We capitalize on today's largest visual question answering dataset BIBREF2 to evaluate our prediction system, which includes 369,861 visual questions about real images. Of these, 248,349 visual questions (i.e., Training questions 2015 v1.0) are kept for for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) are employed for testing our classification system.", so we know that the data used for training is annotated by keeping 248,349 visual questions (i.e., Training questions 2015 v1.0) for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) for testing.
Answer: The data used for training is annotated by keeping 248,349 visual questions (i.e., Training questions 2015 v1.0) for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) for testing.
True answer: The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions.

Validation ID: 5529f26f72ce47440c2a64248063a6d5892b9fde
Question: what quantitative analysis is done?
From the context "We show the loss function analysis of transformer and our model. Figure FIGREF23 shows the validation performance of transformer against global training steps. Figure FIGREF21 show the validation performance of our model for the same number of global steps. Figure FIGREF22 shows that NMT loss is going down with the number of steps, while Figure FIGREF20 shows the degradation of the loss of our proposed RAT task.", so we know that the quantitative analysis is done is loss function analysis.
Answer: loss function analysis
True answer: Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP).

Validation ID: f85ca6135b101736f5c16c5b5d40895280016023
Question: what are the baselines?
From the context "We show the loss function analysis of transformer and our model. Figure FIGREF23 shows the validation performance of transformer against global training steps. Figure FIGREF21 show the validation performance of our model for the same number of global steps. Figure FIGREF22 shows that NMT loss is going down with the number of steps, while Figure FIGREF20 shows the degradation of the loss of our proposed RAT task.", so we know that the baselines are transformer and our model.
Answer: transformer and our model
True answer: the baseline transformer BIBREF8

Validation ID: f85ca6135b101736f5c16c5b5d40895280016023
Question: what are the baselines?
From the context "We show the loss function analysis of transformer and our model. Figure FIGREF23 shows the validation performance of transformer against global training steps. Figure FIGREF21 show the validation performance of our model for the same number of global steps. Figure FIGREF22 shows that NMT loss is going down with the number of steps, while Figure FIGREF20 shows the degradation of the loss of our proposed RAT task.", so we know that the baselines are transformer and our model.
Answer: transformer and our model
True answer: baseline transformer BIBREF8

Validation ID: 5fa36dc8f7c4e65acb962fc484989d20b8fdaeec
Question: Do they report results only on English data?
From the context "We extracted free text from 277 information pages on neurological and stroke-related diseases from this resource (1,104 QA pairs).", so we know that they report results only on English data.
Answer: Yes
True answer: Unanswerable

Validation ID: 5fa36dc8f7c4e65acb962fc484989d20b8fdaeec
Question: Do they report results only on English data?
From the context "We extracted free text from 277 information pages on neurological and stroke-related diseases from this resource (1,104 QA pairs).", so we know that they report results only on English data.
Answer: Yes
True answer: Yes

Validation ID: d98847340e46ffe381992f1a594e75d3fb8d385e
Question: What machine learning and deep learning methods are used for RQE?
From the context "We first define the RQE task, then present the two approaches, and evaluate their performance on five different datasets.", so we know that the machine learning and deep learning methods are used for RQE.
Answer: RQE
True answer: Logistic Regression, neural networks

Validation ID: 7006c66a15477b917656f435d66f63760d33a304
Question: by how much did nus outperform abus?
From the context "The NUS excelled on both evaluation tasks", so we know that the NUS outperformed the ABUS.
From the context "The NUS was compared to the Agenda-Based User Simulator by evaluating policies trained with these user simulators. The trained policies were compared both by testing them with simulated users and also with real users.", so we know that the NUS outperformed the ABUS on both evaluation tasks.
True answer: Average success rate is higher by 2.6 percent points.

Validation ID: a15bc19674d48cd9919ad1cf152bf49c88f4417d
Question: what corpus is used to learn behavior?
From the context "The neural sequence-to-sequence model is trained to maximize the log probability that it assigns to the user utterances of the training data set", so we know that the corpus is the user utterances of the training data set.
Answer: user utterances of the training data set
True answer: DSTC2

Validation ID: a15bc19674d48cd9919ad1cf152bf49c88f4417d
Question: what corpus is used to learn behavior?
From the context "The neural sequence-to-sequence model is trained to maximize the log probability that it assigns to the user utterances of the training data set", so we know that the corpus is the user utterances of the training data set.
Answer: user utterances of the training data set
True answer: The manual transcriptions of the DSTC2 training set 

Validation ID: 440faf8d0af8291d324977ad0f68c8d661fe365e
Question: Which dataset has been used in this work?
From the context "In this section we describe the experiments performed to demonstrate the validity of our proposed method and its extension. We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578", so we know that the dataset used in this work is Reuters-8 dataset.
Answer: Reuters-8 dataset
True answer: Reuters-8 dataset without stop words

Validation ID: 440faf8d0af8291d324977ad0f68c8d661fe365e
Question: Which dataset has been used in this work?
From the context "In this section we describe the experiments performed to demonstrate the validity of our proposed method and its extension. We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578", so we know that the dataset used in this work is Reuters-8 dataset.
Answer: Reuters-8 dataset
True answer: The Reuters-8 dataset (with stop words removed)

Validation ID: 0ec56e15005a627d0b478a67fd627a9d85c3920e
Question: What can word subspace represent?
From the context "Subspace representations are very efficient in compactly represent data that is close to a normal distribution. This characteristic is due to the application of the PCA, that is optimal to find the direction with the highest variation within the data.", so we know that word subspace can represent data that is close to a normal distribution.
Answer: data that is close to a normal distribution
True answer: Word vectors, usually in the context of others within the same class

Validation ID: a712718e6596ba946f29a99838d82f95b9ebb1ce
Question: How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?
From the context "We now select two representative terms AC, IM and plot their receiver operating characteristic(ROC) curves. The term has a relatively large number of classes and the second one has extremely unbalanced training samples. We show the details in Table TABREF16. We have 8 classes in term AC. Figure FIGREF15(a) illustrates the results of our best performed model and Figure FIGREF15(b) shows the results of the base model. The accuracy and F1 score have an improvement from 0.3898 and 0.2830 to 0.4915 and 0.4059 respectively. Regarding the rare senses (for example, class 0, 1, 4 and 6), we can observe an increase in the ROC areas. Class 6 has an obvious improvement from 0.75 to 1.00. Such improvements in the rare senses make a huge difference in the reported average accuracy and F1 score, since we have a nearly equal number of samples for each class in the testing data. Similarly, we show the plots for IM term in Figure FIGREF15(c) and FIGREF15(d). IM has only two classes, but they are very unbalanced in training set, as shown in Table TABREF16. The accuracy and F1 scores improved from 0.6667 and 0.6250 to 0.8667 and 0.8667 respectively. We observe improvements in the ROC areas for both classes. This observation further shows that our model is more sensitive to all the class samples compared to the base model, even for the terms that have only a few samples in the training set. Again, by plotting the ROC curves and comparing AUC areas, we show that our model, which applies ELMo and topic-attention, has a better representation ability under the setting of few-shot learning.", so we know that the improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information are big.
Answer: big
True answer: 7.36% on accuracy and 9.69% on F1 score

Validation ID: a712718e6596ba946f29a99838d82f95b9ebb1ce
Question: How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?
From the context "We now select two representative terms AC, IM and plot their receiver operating characteristic(ROC) curves. The term has a relatively large number of classes and the second one has extremely unbalanced training samples. We show the details in Table TABREF16. We have 8 classes in term AC. Figure FIGREF15(a) illustrates the results of our best performed model and Figure FIGREF15(b) shows the results of the base model. The accuracy and F1 score have an improvement from 0.3898 and 0.2830 to 0.4915 and 0.4059 respectively. Regarding the rare senses (for example, class 0, 1, 4 and 6), we can observe an increase in the ROC areas. Class 6 has an obvious improvement from 0.75 to 1.00. Such improvements in the rare senses make a huge difference in the reported average accuracy and F1 score, since we have a nearly equal number of samples for each class in the testing data. Similarly, we show the plots for IM term in Figure FIGREF15(c) and FIGREF15(d). IM has only two classes, but they are very unbalanced in training set, as shown in Table TABREF16. The accuracy and F1 scores improved from 0.6667 and 0.6250 to 0.8667 and 0.8667 respectively. We observe improvements in the ROC areas for both classes. This observation further shows that our model is more sensitive to all the class samples compared to the base model, even for the terms that have only a few samples in the training set. Again, by plotting the ROC curves and comparing AUC areas, we show that our model, which applies ELMo and topic-attention, has a better representation ability under the setting of few-shot learning.", so we know that the improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information are big.
Answer: big
True answer: it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too

Validation ID: 3116453e35352a3a90ee5b12246dc7f2e60cfc59
Question: To what baseline models is proposed model compared?
From the context "We conducted a comprehensive comparison with the baseline models, and some of them were never investigated for the abbreviation disambiguation task. We applied traditional features by simply taking the TF-IDF features as the inputs into the classic classifiers. Deep features are also considered: a Doc2vec model BIBREF19 was pre-trained using Gensim and these word embeddings were applied to initialize deep models and fine-tuned.", so we know that the proposed model is compared to TF-IDF features, Doc2vec model.
Answer: TF-IDF features, Doc2vec model
True answer: support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self

Validation ID: 3116453e35352a3a90ee5b12246dc7f2e60cfc59
Question: To what baseline models is proposed model compared?
From the context "We conducted a comprehensive comparison with the baseline models, and some of them were never investigated for the abbreviation disambiguation task. We applied traditional features by simply taking the TF-IDF features as the inputs into the classic classifiers. Deep features are also considered: a Doc2vec model BIBREF19 was pre-trained using Gensim and these word embeddings were applied to initialize deep models and fine-tuned.", so we know that the proposed model is compared to TF-IDF features, Doc2vec model.
Answer: TF-IDF features, Doc2vec model
True answer: support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self

Validation ID: dfca00be3284cc555a6a4eac4831471fb1f5875b
Question: How big is dataset for testing?
From the context "Eventually, we have a balanced testing dataset, where each term-sense pair has at least 11 and up to 15 samples for training (on average, each pair has 14.56 samples and the median sample number is 15)", so we know that the dataset for testing is balanced.
Answer: balanced
True answer: 30 terms, each term-sanse pair has around 15 samples for testing

Validation ID: a9a532399237b514c1227f2d6be8601474e669be
Question: What existing dataset is re-examined and corrected for training?
From the context "Training Dataset UM Inventory BIBREF5 is a public dataset created by researchers from the University of Minnesota, containing about 37,500 training samples with 75 abbreviation terms. Existing work reports abbreviation disambiguation results on 50 abbreviation terms BIBREF6, BIBREF5, BIBREF17. However, after carefully reviewing this dataset, we found that it contains many samples where medical professionals disagree: wrong samples and uncategorized samples. Due to these mistakes and flaws of this dataset, we removed the erroneous samples and eventually selected 30 abbreviation terms as our training dataset that can be made public.", so we know that the existing dataset is UM Inventory.
Answer: UM Inventory
True answer:  UM Inventory 

Validation ID: 26126068d72408555bcb52977cd669faf660bdf7
Question: What are the qualitative experiments performed on benchmark datasets?
From the context "Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.", so we know that the qualitative experiments performed on benchmark datasets are:
Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.
True answer: Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.
Evaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.

Validation ID: 26126068d72408555bcb52977cd669faf660bdf7
Question: What are the qualitative experiments performed on benchmark datasets?
From the context "Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.", so we know that the qualitative experiments performed on benchmark datasets are:
1. Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.
True answer: Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed

Validation ID: 660284b0a21fe3801e64dc9e0e51da5400223fe3
Question: How does this approach compare to other WSD approaches employing word embeddings?
From the context "We quantitatively compare the performance of the GM$\_$KL, w2g, and w2gm approaches on the SCWS dataset BIBREF6. The dataset consists of 2003 word pairs of polysemous and homonymous words with labels obtained by an average of 10 human scores. The Spearman correlation between the human scores and the model scores are computed. To obtain the model score, the following metrics are used:
MaxCos: Maximum cosine similarity among all component pairs of words $w$ and $v$:
True answer: GM$\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.

Validation ID: c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd
Question: What tasks did they use to evaluate performance for male and female speakers?
From the context "When investigating WER scores according to gender, speaker's role and speech type, huge variations are observed. We conclude that gender is clearly a factor of variation in ASR performance, with a WER increase of 24% for women compared to men, exhibiting a clear gender bias.", so we know that they used WER scores to evaluate performance for male and female speakers.
Answer: WER scores
True answer: ASR

Validation ID: c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd
Question: What tasks did they use to evaluate performance for male and female speakers?
From the context "When investigating WER scores according to gender, speaker's role and speech type, huge variations are observed. We conclude that gender is clearly a factor of variation in ASR performance, with a WER increase of 24% for women compared to men, exhibiting a clear gender bias.", so we know that they used WER scores to evaluate performance for male and female speakers.
Answer: WER scores
True answer: Unanswerable

Validation ID: f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3
Question: What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?
From the context "We find a clear disparity in terms of women presence and speech quantity in French media. Our data being recorded between 1998 and 2013, we can expect this disparity to be smaller on more recent broadcast recordings, especially since the French government displays efforts toward parity in media representation. One can also argue that even if our analysis was conducted on a large amount of data it does not reach the exhaustiveness of large-scale studies such as the one of BIBREF2. Nonetheless it does not affect the relevance of our findings, because if real-world gender representation might be more balanced today, these corpora are still used as training data for AI systems.", so we know that the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to find out the impact of gender balance on ASR performance trained on broadcast recordings.
Answer: To find out the impact of gender balance on ASR performance trained on broadcast recordings
True answer: create fair systems

Validation ID: f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3
Question: What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?
From the context "We find a clear disparity in terms of women presence and speech quantity in French media. Our data being recorded between 1998 and 2013, we can expect this disparity to be smaller on more recent broadcast recordings, especially since the French government displays efforts toward parity in media representation. One can also argue that even if our analysis was conducted on a large amount of data it does not reach the exhaustiveness of large-scale studies such as the one of BIBREF2. Nonetheless it does not affect the relevance of our findings, because if real-world gender representation might be more balanced today, these corpora are still used as training data for AI systems.", so we know that the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to find out the impact of gender balance on ASR performance trained on broadcast recordings.
Answer: To find out the impact of gender balance on ASR performance trained on broadcast recordings
True answer:  broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools

Validation ID: a253749e3b4c4f340778235f640ce694642a4555
Question: Which corpora does this paper analyse?
From the context "Our data consists of two sets used to train and evaluate our automatic speech recognition system. Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. These four collections contain radio and/or TV broadcasts aired between 1998 and 2013 which are used by most academic researchers in ASR.", so we know that this paper analyse ESTER1, ESTER2, ETAPE and REPERE.
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: a253749e3b4c4f340778235f640ce694642a4555
Question: Which corpora does this paper analyse?
From the context "Our data consists of two sets used to train and evaluate our automatic speech recognition system. Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. These four collections contain radio and/or TV broadcasts aired between 1998 and 2013 which are used by most academic researchers in ASR.", so we know that this paper analyse ESTER1, ESTER2, ETAPE and REPERE.
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: 1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891
Question: How many categories do authors define for speaker role?
From the context "We end up with two salient roles called Anchors and Punctual speakers: the Anchor speakers (A) are above the threshold of 1% for both criteria, meaning they are intervening often and for a long time thus holding an important place in interaction; the Punctual speakers (PS) on the contrary are below the threshold of 1% for both the total number of turns and the total speech time.", so we know that authors define two categories for speaker role.
Answer: 2
True answer:  two salient roles called Anchors and Punctual speakers

Validation ID: 777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47
Question: How big is imbalance in analyzed corpora?
From the context "Descriptive analysis of the broadcast data used to train our ASR system confirms the already known disparity, where 65% of the speakers are men, speaking more than 75% of the time.", so we know that imbalance in analyzed corpora is 65% of the speakers are men, speaking more than 75% of the time.
Answer: 65% of the speakers are men, speaking more than 75% of the time.
True answer: Women represent 33.16% of the speakers

Validation ID: 2da4c3679111dd92a1d0869dae353ebe5989dfd2
Question: What are four major corpora of French broadcast?
From the context "Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16", so we know that four major corpora of French broadcast are ESTER1, ESTER2, ETAPE and REPERE.
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: b7c3f3942a07c118e57130bc4c3ec4adc431d725
Question: What did the best systems use for their model?
From the context "We train the LM using a 90/10 training/validation split, reporting the validation loss and accuracy of next-word prediction on the validation set. For the LM, we selected an ASGD Weight-Dropped Long Short Term Memory (AWD_LSTM, described in Merity et al. BIBREF5 ) model included in Fast.ai. We replaced the typical Long Short Term Memory (LSTM) units with Quasi Recurrent Neural Network (QRNN, described in Bradbury et al. BIBREF6 ) units. Our network has 2304 hidden-states, 3 layers and a softmax layer to predict the next-word. We tied the embedding weights BIBREF7 on the encoder and decoder for training. We performed some simple tests with LSTM units and a Transformer Language model, finding all models were similar in performance during LM training. We thus chose to use QRNN units due to improved training speed compared to the alternatives. This model has about 60 million trainable parameters.", so we know that the best systems used ASGD Weight-Dropped Long Short Term Memory (AWD_LSTM) model.
Answer: ASGD Weight-Dropped Long Short Term Memory (AWD_LSTM) model
True answer: Unanswerable

Validation ID: a5505e25ee9ae84090e1442034ddbb3cedabcf04
Question: What were their results on the classification and regression tasks?
From the context "Table TABREF29 gives three results from our submissions in the competition. The first is the baseline NBSVM solution, with an F1 of 0.7548. Second is our first random seed selected for the classifier which produces a 0.8083 result. While better than the NBSVM solution, we pick the best validation F1 from the 20 seeds we tried. This produced our final submission of 0.8099. Our best model achieved an five-fold average F1 of 0.8254 on the validation set shown in Figure FIGREF27 but a test set F1 of 0.8099 - a drop of 0.0155 in F1 for the true out-of-sample data. Also note that our third place entry was 1.1% worse in F1 score than first place but 1.2% better in F1 than the 4th place entry.", so we know that their results on the classification and regression tasks are 0.8099 and 0.8254.
Answer: 0.8099 and 0.8254
True answer: F1 of 0.8099

Validation ID: a5505e25ee9ae84090e1442034ddbb3cedabcf04
Question: What were their results on the classification and regression tasks?
From the context "Table TABREF29 gives three results from our submissions in the competition. The first is the baseline NBSVM solution, with an F1 of 0.7548. Second is our first random seed selected for the classifier which produces a 0.8083 result. While better than the NBSVM solution, we pick the best validation F1 from the 20 seeds we tried. This produced our final submission of 0.8099. Our best model achieved an five-fold average F1 of 0.8254 on the validation set shown in Figure FIGREF27 but a test set F1 of 0.8099 - a drop of 0.0155 in F1 for the true out-of-sample data. Also note that our third place entry was 1.1% worse in F1 score than first place but 1.2% better in F1 than the 4th place entry.", so we know that their results on the classification and regression tasks are 0.8099 and 0.8254.
Answer: 0.8099 and 0.8254
True answer: F1 score result of 0.8099

Validation ID: 1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf
Question: Do the authors conduct experiments on the tasks mentioned?
From the context "The Winograd Schema Challenge (WSC) is a challenge for AI programs. The program is presented with a collection of sentences, each of which is one element of a Winograd schema, and is required to find the correct referent for the ambiguous pronoun. An AI program passes the challenge if its success rate is comparable to a human reader. The challenge is administered by commonsensereasoning.org and sponsored by Nuance Inc. It was offered for the first time at IJCAI-2016 (Morgenstern, Davis, and Ortiz, in preparation); the organizers plan to continue to offer it roughly once a year.", so we know that the authors conduct experiments on the tasks mentioned.
Answer: Yes
True answer: Yes

Validation ID: 1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf
Question: Do the authors conduct experiments on the tasks mentioned?
From the context "The Winograd Schema Challenge (WSC) is a challenge for AI programs. The program is presented with a collection of sentences, each of which is one element of a Winograd schema, and is required to find the correct referent for the ambiguous pronoun. An AI program passes the challenge if its success rate is comparable to a human reader. The challenge is administered by commonsensereasoning.org and sponsored by Nuance Inc. It was offered for the first time at IJCAI-2016 (Morgenstern, Davis, and Ortiz, in preparation); the organizers plan to continue to offer it roughly once a year.", so we know that the authors conduct experiments on the tasks mentioned.
Answer: Yes
True answer: No

Validation ID: 7fa3c2c0cf7f559d43e84076a9113a390c5ba03a
Question: Did they collect their own datasets?
From the context "The Winograd Schema Challenge (WSC) is a challenge for AI programs. The program is presented with a collection of sentences, each of which is one element of a Winograd schema, and is required to find the correct referent for the ambiguous pronoun", so we know that they collected their own datasets.
Answer: Yes
True answer: No

Validation ID: 9a7ba5ed1779c664d2cac92494a43517d3e87c96
Question: What data do they look at?
From the context "The Winograd Schema Challenge (WSC) is a challenge for AI programs. The program is presented with a collection of sentences, each of which is one element of a Winograd schema, and is required to find the correct referent for the ambiguous pronoun", so we know that the data they look at is a collection of sentences.
Answer: a collection of sentences
True answer: WSC collection

Validation ID: 662870a90890c620a964720b2ca122a1139410ea
Question: What language do they explore?
From the context "Broadly speaking, whenever a target language INLINEFORM0 requires some distinction that is optional or non-existent in source language INLINEFORM1, it is possible to create a sentence INLINEFORM2 in INLINEFORM3 where the missing information is not explicit but can be inferred from background knowledge. Translating INLINEFORM4 from INLINEFORM5 to INLINEFORM6 thus requires using the background knowledge to resolve the ambiguity, and will therefore be challenging for automatic machine translation.", so we know that they explore the language.
Answer: language
True answer: English, French, German 

Validation ID: 662870a90890c620a964720b2ca122a1139410ea
Question: What language do they explore?
From the context "Broadly speaking, whenever a target language INLINEFORM0 requires some distinction that is optional or non-existent in source language INLINEFORM1, it is possible to create a sentence INLINEFORM2 in INLINEFORM3 where the missing information is not explicit but can be inferred from background knowledge. Translating INLINEFORM4 from INLINEFORM5 to INLINEFORM6 thus requires using the background knowledge to resolve the ambiguity, and will therefore be challenging for automatic machine translation.", so we know that they explore the language.
Answer: language
True answer: French, English, Spanish, Italian, Portuguese, Hebrew, Arabic

Validation ID: 92d1a6df3041667dc662376938bc65527a5a1c3c
Question: Do they report results only on English datasets?
From the context "We have shown empirically the effectiveness of incorporating cluster membership features in the feature extraction pipeline of Named-Entity recognition, sentiment classification and quantification tasks", so we know that they report results only on English datasets.
Answer: Yes
True answer: Yes

Validation ID: 92d1a6df3041667dc662376938bc65527a5a1c3c
Question: Do they report results only on English datasets?
From the context "We have shown empirically the effectiveness of incorporating cluster membership features in the feature extraction pipeline of Named-Entity recognition, sentiment classification and quantification tasks", so we know that they report results only on English datasets.
Answer: Yes
True answer: Yes

Validation ID: 12159f04e0427fe33fa05af6ba8c950f1a5ce5ea
Question: Which hyperparameters were varied in the experiments on the four tasks?
From the context "Although our study does not identify a clear winner with respect to the type of word vectors (skipgram, cbow, or GloVe), our findings suggest that one should first try skip-gram embeddings of low dimensionality ( INLINEFORM0 ) and high number of clusters (e.g., INLINEFORM1 ) as the results obtained using these settings are consistently competitive", so we know that the hyperparameters were varied in the experiments on the four tasks are the type of word vectors and the number of clusters.
Answer: the type of word vectors and the number of clusters
True answer: number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding

Validation ID: 12159f04e0427fe33fa05af6ba8c950f1a5ce5ea
Question: Which hyperparameters were varied in the experiments on the four tasks?
From the context "Although our study does not identify a clear winner with respect to the type of word vectors (skipgram, cbow, or GloVe), our findings suggest that one should first try skip-gram embeddings of low dimensionality ( INLINEFORM0 ) and high number of clusters (e.g., INLINEFORM1 ) as the results obtained using these settings are consistently competitive", so we know that the hyperparameters were varied in the experiments on the four tasks are the type of word vectors and the number of clusters.
Answer: the type of word vectors and the number of clusters
True answer: different number of clusters, different embeddings

Validation ID: a4a1fcef760b133e9aa876ac28145ad98a609927
Question: Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?
From the context "Although our study does not identify a clear winner with respect to the type of word vectors (skipgram, cbow, or GloVe), our findings suggest that one should first try skip-gram embeddings of low dimensionality ( INLINEFORM0 ) and high number of clusters (e.g., INLINEFORM1 ) as the results obtained using these settings are consistently competitive.", so we know that the other hyperparameters are dimensionality and number of clusters.
Answer: dimensionality and number of clusters
True answer: selection of word vectors

Validation ID: 63bb2040fa107c5296351c2b5f0312336dad2863
Question: How were the cluster extracted??
From the context "We cluster the embeddings with INLINEFORM0 -Means. The k-means clusters are initialized using “k-means++” as proposed in BIBREF9, while the algorithm is run for 300 iterations. We try different values for INLINEFORM1. For each INLINEFORM2, we repeat the clustering experiment with different seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia.", so we know that the cluster extracted by k-means.
Answer: k-means
True answer: Word clusters are extracted using k-means on word embeddings

Validation ID: 01f4a0a19467947a8f3bdd7ec9fac75b5222d710
Question: what were the evaluation metrics?
From the context "Table TABREF23 shows the unlabeled INLINEFORM0 scores for our models and various baselines. All models soundly outperform right branching baselines, and we find that the neural PCFG/compound PCFG are strong models for grammar induction. In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chinese. We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization despite a thorough hyperparameter search. See lab:full for the full results (including corpus-level INLINEFORM1 ) broken down by sentence length.", so we know that the evaluation metrics are INLINEFORM0 and INLINEFORM1.
Answer: INLINEFORM0 and INLINEFORM1
True answer: INLINEFORM0 scores

Validation ID: 01f4a0a19467947a8f3bdd7ec9fac75b5222d710
Question: what were the evaluation metrics?
From the context "Table TABREF23 shows the unlabeled INLINEFORM0 scores for our models and various baselines. All models soundly outperform right branching baselines, and we find that the neural PCFG/compound PCFG are strong models for grammar induction. In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chinese. We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization despite a thorough hyperparameter search. See lab:full for the full results (including corpus-level INLINEFORM1 ) broken down by sentence length.", so we know that the evaluation metrics are INLINEFORM0 and INLINEFORM1.
Answer: INLINEFORM0 and INLINEFORM1
True answer: Unlabeled sentence-level F1, perplexity, grammatically judgment performance

Validation ID: 7784d321ccc64db5141113b6783e4ba92fdd4b20
Question: what are the state of the art methods?
From the context "More recently, neural network-based approaches to grammar induction have shown promising results on inducing parse trees directly from words. BIBREF14, BIBREF15 learn tree structures through soft gating layers within neural language models, while BIBREF16 combine recursive autoencoders with the inside-outside algorithm. BIBREF17 train unsupervised recurrent neural network grammars with a structured inference network to induce latent trees, and BIBREF78 utilize image captions to identify and ground constituents.", so we know that the state of the art methods are neural network-based approaches to grammar induction.
Answer: neural network-based approaches to grammar induction
True answer: Unanswerable

Validation ID: 7784d321ccc64db5141113b6783e4ba92fdd4b20
Question: what are the state of the art methods?
From the context "More recently, neural network-based approaches to grammar induction have shown promising results on inducing parse trees directly from words. BIBREF14, BIBREF15 learn tree structures through soft gating layers within neural language models, while BIBREF16 combine recursive autoencoders with the inside-outside algorithm. BIBREF17 train unsupervised recurrent neural network grammars with a structured inference network to induce latent trees, and BIBREF78 utilize image captions to identify and ground constituents.", so we know that the state of the art methods are neural network-based approaches to grammar induction.
Answer: neural network-based approaches to grammar induction
True answer: Unanswerable

Validation ID: 218615a005f7f00606223005fef22c07057d9d77
!!! Has error during processing !!!
Question: what english datasets were used?
Answer: From the context "We evaluate our models on the Penn Treebank (PTB) BIBREF91, BIBREF92, BIBREF93, BIBREF94, BIBREF95, BIBREF96, BIBREF97, BIBREF98, BIBREF99, BIBREF100, BIBREF101, BIBREF102, BIBREF103, BIBREF104, BIBREF105, BIBREF106, BIBREF107, BIBREF108, BIBREF109, BIBREF110, BIBREF111, BIBREF112, BIBREF113, BIBREF114, BIBREF115, BIBREF116, BIBREF117, BIBREF118, BIBREF119, BIBREF120, BIBREF121, BIBREF122, BIBREF123, BIBREF124, BIBREF125, BIBREF126, BIBREF127, BIBREF128, BIBREF129, BIBREF130, BIBREF131, BIBREF132, BIBREF133, BIBREF134, BIBREF135, BIBREF136, BIBREF137, BIBREF138, BIBREF139, BIBREF140, BIBREF141, BIBREF142, BIBREF143, BIBREF144, BIBREF145, BIBREF146, BIBREF147, BIBREF148, BIBREF149, BIBREF150, BIBREF151, BIBREF152, BIBREF153, BIBREF154, BIBREF155, BIBREF156, BIBREF157, BIBREF158, BIBREF159, BIBREF160, BIBREF161, BIBREF162, BIBREF163, BIBREF164, BIBREF165, BIBREF166, BIBREF167, BIBREF168, BIBREF169, BIBREF170, BIBREF171, BIBREF172, BIBREF173, BIBREF174, BIBREF175, BIBREF176, BIBREF177, BIBREF178, BIBREF179, BIBREF180, BIBREF181, BIBREF182, BIBREF183, BIBREF184, BIBREF185, BIBREF186, BIB
True answer: Answer with content missing: (Data section) Penn Treebank (PTB)

Validation ID: 867290103f762e1ddfa6f2ea30dd0a327f595182
Question: which chinese datasets were used?
From the context "We use the Penn Treebank (PTB) BIBREF91 for English and the Chinese Treebank (CTB) BIBREF92 for Chinese", so we know that the chinese datasets used are CTB.
Answer: CTB
True answer: Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)

Validation ID: 907b3af3cfaf68fe188de9467ed1260e52ec6cf1
Question: What were their distribution results?
From the context "The following results detail characteristics of these tweets along the previously mentioned dimensions. Table TABREF23 reports the actual differences (together with their associated p-values) of the distributions of viral tweets containing fake news and viral tweets not containing them for every variable considered.", so we know that the distribution results are in Table TABREF23.
Answer: Table TABREF23
True answer: Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different

Validation ID: 56a8826cbee49560592b2d4b47b18ada236a12b9
Question: How did they determine fake news tweets?
From the context "The 1327 `viral' tweets were manually annotated as containing fake news or not. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset", so we know that they determined fake news tweets manually.
Answer: manually
True answer: an expert annotator determined if the tweet fell under a specific category

Validation ID: 56a8826cbee49560592b2d4b47b18ada236a12b9
Question: How did they determine fake news tweets?
From the context "The 1327 `viral' tweets were manually annotated as containing fake news or not. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset", so we know that they manually annotated fake news tweets.
Answer: manually annotated
True answer: Exposure, Characterization, Polarization

Validation ID: 968b7c3553a668ba88da105eff067d57f393c63f
Question: What is their definition of tweets going viral?
From the context "We used the number of retweets to single-out those that went viral within our sample. Tweets within that subset (viral tweets hereafter) are varied and relate to different topics", so we know that tweets going viral are those tweets that have been retweeted more than 1000 times.
Answer: tweets that have been retweeted more than 1000 times
True answer: Viral tweets are the ones that are retweeted more than 1000 times

Validation ID: 968b7c3553a668ba88da105eff067d57f393c63f
Question: What is their definition of tweets going viral?
From the context "We used the number of retweets to single-out those that went viral within our sample. Tweets within that subset (viral tweets hereafter) are varied and relate to different topics", so we know that tweets going viral are those tweets that have been retweeted more than 1000 times.
Answer: tweets that have been retweeted more than 1000 times
True answer: those that contain a high number of retweets

Validation ID: f03df5d99b753dc4833ef27b32bb95ba53d790ee
Question: What are the characteristics of the accounts that spread fake news?
From the context "We found that 82 users within our sample were spreading fake news (i.e. they produced at least one tweet which was labelled as fake news). Out of those, 34 had verified accounts, and the rest were unverified. From the 48 unverified accounts, 6 have been suspended by Twitter at the date of writing, 3 tried to imitate legitimate accounts of others, and 4 accounts have been already deleted. Figure FIGREF28 shows the proportion of verified accounts to unverified accounts for viral tweets (containing fake news vs. not containing fake news). From the chart, it is clear that there is a higher chance of fake news coming from unverified accounts.", so we know that the characteristics of the accounts that spread fake news are:
1. There is a higher chance of fake news coming from unverified accounts.
True answer: Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio

Validation ID: f03df5d99b753dc4833ef27b32bb95ba53d790ee
Question: What are the characteristics of the accounts that spread fake news?
From the context "We found that 82 users within our sample were spreading fake news (i.e. they produced at least one tweet which was labelled as fake news). Out of those, 34 had verified accounts, and the rest were unverified. From the 48 unverified accounts, 6 have been suspended by Twitter at the date of writing, 3 tried to imitate legitimate accounts of others, and 4 accounts have been already deleted. Figure FIGREF28 shows the proportion of verified accounts to unverified accounts for viral tweets (containing fake news vs. not containing fake news). From the chart, it is clear that there is a higher chance of fake news coming from unverified accounts.", so we know that the characteristics of the accounts that spread fake news are:
1. They are unverified accounts
True answer: have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only

Validation ID: a8f51b4e334a917702422782329d97304a2fe139
Question: What is the threshold for determining that a tweet has gone viral?
From the context "For our study, we consider that a tweet went viral if it was retweeted more than 1000 times", so we know that the threshold for determining that a tweet has gone viral is 1000.
Answer: 1000
True answer: 1000

Validation ID: dca86fbe1d57b44986055b282a03c15ef7882e51
Question: How is the ground truth for fake news established?
From the context "The 1327 `viral' tweets were manually annotated as containing fake news or not. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset", so we know that the ground truth for fake news is established by manually annotated by a single person.
Answer: manually annotated by a single person
True answer: Ground truth is not established in the paper

Validation ID: 27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa
Question: what NMT models did they compare with?
From the context "In addition, we test the performance of the SMT and various NMT models on our built dataset and provide a strong NMT baseline for this task which achieves 27.16 BLEU score (4-gram).", so we know that they compared the performance of the SMT and various NMT models.
Answer: SMT and various NMT models
True answer: RNN-based NMT model, Transformer-NMT

Validation ID: b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72
Question: Where does the ancient Chinese dataset come from?
From the context "To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet", so we know that the ancient Chinese dataset comes from the internet.
Answer: the internet
True answer: ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era

Validation ID: b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72
Question: Where does the ancient Chinese dataset come from?
From the context "To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet", so we know that the ancient Chinese dataset comes from the internet.
Answer: the internet
True answer: Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet 

Validation ID: 808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc
Question: How many different characters were in dataset?
From the context "We collect dialogues from 327 major characters (a subset of the 45,821 characters we have HLA data for) in 38 TV shows from various existing sources of clean data on the internet, resulting in a total of 1,042,647 dialogue lines", so we know that there are 327 different characters in the dataset.
Answer: 327
True answer: 45,821 characters

Validation ID: 808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc
Question: How many different characters were in dataset?
From the context "We collect dialogues from 327 major characters (a subset of the 45,821 characters we have HLA data for) in 38 TV shows from various existing sources of clean data on the internet, resulting in a total of 1,042,647 dialogue lines", so we know that there are 327 different characters in the dataset.
Answer: 327
True answer: 45,821 characters

Validation ID: 36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd
Question: How does dataset model character's profiles?
From the context "We define a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise. We define a constant $\alpha $ that measures our confidence in observing various character-HLA pairs as positive. $\alpha $ controls how much the model penalizes the error if the ground truth is $P_{u,i} = 1$. If $P_{u,i} = 1$ and the model guesses incorrectly, we penalize by $\alpha $ times the loss. But if $P_{u,i} = 0$ and the model guesses a value greater than 0, we do not penalize as $\alpha $ has no impact. This is because $P_{u,i} = 0$ can either represent a true negative or be due to a lack of data, and hence is less reliable for penalization. See Equation DISPLAY_FORM8. We find that using $\alpha =20$ provides decent results.", so we know that dataset model character's profiles by using a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise.
Answer: dataset model character's profiles by using a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise.
True answer: attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics

Validation ID: f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6
Question: How big is the difference in performance between proposed model and baselines?
From the context "Table TABREF44 shows average results of our automatic and human evaluations. Table TABREF45 shows average Hits@1/20 scores by evaluation character. See Appendix F for detailed evaluation results. ALOHA is the model with HLA-OG during training and testing, and ALOHA (No HLA-OG) is the model with HLA-OG during training but tested with the four HLAs in the OBS marked as `none' (see Section SECREF17). See Appendix G for demo interactions between a human, BERT bi-ranker baseline, and ALOHA for all five evaluation characters.", so we know that the difference in performance between proposed model and baselines is 0.7815.
Answer: 0.7815
True answer: Metric difference between Aloha and best baseline score:
Hits@1/20: +0.061 (0.3642 vs 0.3032)
MRR: +0.0572(0.5114 vs 0.4542)
F1: -0.0484 (0.3901 vs 0.4385)
BLEU: +0.0474 (0.2867 vs 0.2393)

Validation ID: 357eb9f0c07fa45e482d998a8268bd737beb827f
Question: What baseline models are used?
From the context "We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20.", so we know that the baseline models are Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline.
Answer: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline
True answer: the Poly-encoder from BIBREF7 humeau2019real, Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously, We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20., a BERT bi-ranker

Validation ID: 357eb9f0c07fa45e482d998a8268bd737beb827f
Question: What baseline models are used?
From the context "We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20.", so we know that the baseline models are Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline.
Answer: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline
True answer: Kvmemnn,  Feed Yourself, Poly-encoder, BERT bi-ranker

Validation ID: ad08b215dca538930ef1f50b4e49cd25527028ad
Question: Was PolyReponse evaluated against some baseline?
From the context "The system offers both voice and text input and output. Speech-to-text and text-to-speech conversion in the PolyResponse system is currently supported by the off-the-shelf Google Cloud tools.", so we know that PolyReponse is evaluated against Google Cloud tools.
Answer: Google Cloud tools
True answer: No

Validation ID: ad08b215dca538930ef1f50b4e49cd25527028ad
Question: Was PolyReponse evaluated against some baseline?
From the context "The system offers both voice and text input and output. Speech-to-text and text-to-speech conversion in the PolyResponse system is currently supported by the off-the-shelf Google Cloud tools.", so we know that PolyReponse is evaluated against Google Cloud tools.
Answer: Google Cloud tools
True answer: No

Validation ID: 31101dc9937f108e27e08a5f34be44f0090b8b6b
Question: What metric is used to evaluate PolyReponse system?
From the context "The system offers both voice and text input and output. Speech-to-text and text-to-speech conversion in the PolyResponse system is currently supported by the off-the-shelf Google Cloud tools.", so we know that the PolyReponse system is evaluated by the off-the-shelf Google Cloud tools.
Answer: off-the-shelf Google Cloud tools
True answer: Unanswerable

Validation ID: e4a315e9c190cf96493eefe04ce4ba6ae6894550
Question: How does PolyResponse architecture look like?
From the context "The PolyResponse system is powered by a single large conversational search engine, trained on a large amount of conversational and image data, as shown in Figure FIGREF2", so we know that PolyResponse architecture is shown in Figure FIGREF2.
Answer: Figure FIGREF2
True answer: Henderson:2017, MobileNet model

Validation ID: 6263b2cba18207474786b303852d2f0d7068d4b6
Question: In what 8 languages is PolyResponse engine used for restourant search and booking system?
From the context "The PolyResponse restaurant search is currently available in 8 languages and for 8 cities around the world: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)", so we know that PolyResponse engine is used for restourant search and booking system in 8 languages: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.
Answer: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian
True answer: English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian

Validation ID: 6263b2cba18207474786b303852d2f0d7068d4b6
Question: In what 8 languages is PolyResponse engine used for restourant search and booking system?
From the context "The PolyResponse restaurant search is currently available in 8 languages and for 8 cities around the world: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)", so we know that PolyResponse engine is used for restourant search and booking system in 8 languages: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.
Answer: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian
True answer: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)

Validation ID: c1c44fd96c3fa6e16949ae8fa453e511c6435c68
Question: Why masking words in the decoder is helpful?
From the context "The main reason to introduce the refine process is to enhance the decoder using BERT's contextualized representations, so we do not modify the encoder and reuse it during this process. On the decoder side, we propose a new word-level refine decoder. The refine decoder receives a generated summary draft as input, and outputs a refined summary. It first masks each word in the summary draft one by one, then feeds the draft to BERT to generate context vectors. Finally it predicts a refined summary word using an $N$ layer Transformer decoder which is the same as the draft decoder. At t-th time step the n-th word of input summary is masked, and the decoder predicts the n-th refined word given other words of the summary.", so we know that the main reason to introduce the refine process is to enhance the decoder using BERT's contextualized representations.
Answer: To enhance the decoder using BERT's contextualized representations
True answer: ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.

Validation ID: d28d86524292506d4b24ae2d486725a6d57a3db3
Question: What is the ROUGE score of the highest performing model?
From the context "As the above sub-figure of Figure 2 shows, compared to Pointer-Generator with Coverage, on length interval 40-80(occupies about 70% of test set) the improvements of our model are higher than shorter samples, confirms that with better context representations, in longer documents our model can achieve higher performance.", so we know that the ROUGE score of the highest performing model is higher than shorter samples.
From the context "As the below sub-figure of Figure 2 shows, compared to extractive baseline: Lead-3 BIBREF0, the advantage of our model will fall when golden summary length is greater than 80. This probably because that we truncate the long documents and golden summaries and cannot get full information, it could also because that the training data in these intervals is too few to train an abstractive model, so simple extractive method will not fall too far behind.", so we know that the ROUGE score of the highest performing model is lower than longer samples.
True answer: 33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L 

Validation ID: d28d86524292506d4b24ae2d486725a6d57a3db3
Question: What is the ROUGE score of the highest performing model?
From the context "As the above sub-figure of Figure 2 shows, compared to Pointer-Generator with Coverage, on length interval 40-80(occupies about 70% of test set) the improvements of our model are higher than shorter samples, confirms that with better context representations, in longer documents our model can achieve higher performance.", so we know that the ROUGE score of the highest performing model is higher than shorter samples.
From the context "As the below sub-figure of Figure 2 shows, compared to extractive baseline: Lead-3 BIBREF0, the advantage of our model will fall when golden summary length is greater than 80. This probably because that we truncate the long documents and golden summaries and cannot get full information, it could also because that the training data in these intervals is too few to train an abstractive model, so simple extractive method will not fall too far behind.", so we know that the ROUGE score of the highest performing model is lower than longer samples.
True answer: 33.33

Validation ID: feafcc1c4026d7f55a2c8ce7850d7e12030b5c22
Question: How are the different components of the model trained? Is it trained end-to-end?
From the context "In this work, we propose a two-stage model based on sequence-to-sequence paradigm. Our model utilize BERT on both encoder and decoder sides, and introduce reinforce objective in learning process. We evaluate our model on two benchmark datasets CNN/Daily Mail and New York Times, the experimental results show that compared to previous systems our approach effectively improves performance.", so we know that the model is trained end-to-end.
Answer: end-to-end
True answer: the objective of our model is sum of the two processes, jointly trained using "teacher-forcing" algorithm, we feed the ground-truth summary to each decoder and minimize the objective, At test time, each time step we choose the predicted word by $\hat{y} = argmax_{y^{\prime }} P(y^{\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries., the model can be trained end-to-end

Validation ID: feafcc1c4026d7f55a2c8ce7850d7e12030b5c22
Question: How are the different components of the model trained? Is it trained end-to-end?
From the context "In this work, we propose a two-stage model based on sequence-to-sequence paradigm. Our model utilize BERT on both encoder and decoder sides, and introduce reinforce objective in learning process. We evaluate our model on two benchmark datasets CNN/Daily Mail and New York Times, the experimental results show that compared to previous systems our approach effectively improves performance.", so we know that the model is trained end-to-end.
Answer: end-to-end
True answer: the model can be trained end-to-end

Validation ID: 63488da6c7aff9e374561a24ba224e9ce7f65e40
Question: When is this paper published?
From the context "In this paper, we focus on single-document multi-sentence summarization and propose a supervised abstractive model based on the neural attentive sequence-to-sequence framework which consists of two parts: a neural network for the encoder and another network for the decoder. The encoder encodes the input sequence to intermediate representation and the decoder predicts one word at a time step given the input sequence representation vector and previous decoded output. The goal of the model is to maximize the probability of generating the correct target sequences. In the encoding and generation process, the attention mechanism is used to concentrate on the most important positions of text. The learning objective of most sequence-to-sequence models is to minimize the negative log likelihood of the generated sequence as following equation shows, where $y^*_i$ is the i-th ground-truth summary token.", so we know that the paper is published in 2019.
Answer: 2019
True answer: Unanswerable

Validation ID: c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c
Question: Can their indexing-based method be applied to create other QA datasets in other domains, and not just Wikipedia?
From the context "We propose an automatic way of mapping the answer contexts from all corpora to the same version of Wikipeda so they can be coherently used for answer retrieval.", so we know that their indexing-based method can be applied to create other QA datasets in other domains, and not just Wikipedia.
Answer: Yes
True answer: Unanswerable

Validation ID: a9337636b52de375c852682a2561af2c1db5ec63
Question: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?
From the context "Each paragraph in Wikipedia is first indexed by Lucene using {1,2,3}-grams, where the paragraphs are separated by WikiExtractor and segmented by NLP4J (28.7M+ paragraphs are indexed). Each answer sentence from the corpora in Table TABREF3 is then queried to Lucene, and the top-5 ranked paragraphs are retrieved. The cosine similarity between each sentence in these paragraphs and the answer sentence is measured for INLINEFORM0 -grams, say INLINEFORM1. A weight is assigned to each INLINEFORM2 -gram score, say INLINEFORM3, and the weighted sum is measured: INLINEFORM4. The fixed weights of INLINEFORM5 are used for our experiments, which can be improved.", so we know that they employ their indexing-based method to create a sample of a QA Wikipedia dataset.
Answer: Yes
True answer: Yes

Validation ID: a9337636b52de375c852682a2561af2c1db5ec63
Question: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?
From the context "Each paragraph in Wikipedia is first indexed by Lucene using {1,2,3}-grams, where the paragraphs are separated by WikiExtractor and segmented by NLP4J (28.7M+ paragraphs are indexed). Each answer sentence from the corpora in Table TABREF3 is then queried to Lucene, and the top-5 ranked paragraphs are retrieved. The cosine similarity between each sentence in these paragraphs and the answer sentence is measured for INLINEFORM0 -grams, say INLINEFORM1. A weight is assigned to each INLINEFORM2 -gram score, say INLINEFORM3, and the weighted sum is measured: INLINEFORM4. The fixed weights of INLINEFORM5 are used for our experiments, which can be improved.", so we know that they employ their indexing-based method to create a sample of a QA Wikipedia dataset.
Answer: Yes
True answer: No

Validation ID: 45a5961a4e1d1c22874c4918e5c98bd3c0a670b3
Question: How many question types do they find in the datasets analyzed?
From the context "Fig. FIGREF4 shows the distributions of seven question types grouped deterministically from the lexicons", so we know that they find seven question types in the datasets analyzed.
Answer: seven
True answer: seven 

Validation ID: 45a5961a4e1d1c22874c4918e5c98bd3c0a670b3
Question: How many question types do they find in the datasets analyzed?
From the context "Fig. FIGREF4 shows the distributions of seven question types grouped deterministically from the lexicons", so we know that they find seven question types in the datasets analyzed.
Answer: seven
True answer: 7

Validation ID: 30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1
Question: How do they analyze contextual similaries across datasets?
From the context "We first give a thorough intrinsic analysis regarding contextual similarities, question types, and answer categories (Section SECREF2 ).", so we know that they analyze contextual similaries across datasets in Section SECREF2.
Answer: Section SECREF2
True answer: They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.

Validation ID: 5c6fa86757410aee6f5a0762328637de03a569e9
Question: What were their performance results?
From the context "We have shown that DNN models can be used for cyberbullying detection on various topics across multiple SMPs using three datasets and four DNN models. These models coupled with transfer learning beat state of the art results for all three datasets.", so we know that their performance results are that DNN models can be used for cyberbullying detection on various topics across multiple SMPs using three datasets and four DNN models. These models coupled with transfer learning beat state of the art results for all three datasets.
Answer: DNN models can be used for cyberbullying detection on various topics across multiple SMPs using three datasets and four DNN models. These models coupled with transfer learning beat state of the art results for all three datasets.
True answer: best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset

Validation ID: 7e38e0279a620d3df05ab9b5e2795044f18d4471
Question: What cyberbulling topics did they address?
From the context "We cover three different types of social networks: teen oriented Q&A forum (Formspring), large microblogging platform (Twitter), and collaborative knowledge repository (Wikipedia talk pages). Each dataset addresses a different topic of cyberbullying. Twitter dataset contains examples of racism and sexism. Wikipedia dataset contains examples of personal attack. However, Formspring dataset is not specifically about any single topic.", so we know that they addressed three cyberbulling topics: racism, sexism, and personal attack.
Answer: racism, sexism, and personal attack
True answer: personal attack, racism, and sexism

Validation ID: 7e38e0279a620d3df05ab9b5e2795044f18d4471
Question: What cyberbulling topics did they address?
From the context "We cover three different types of social networks: teen oriented Q&A forum (Formspring), large microblogging platform (Twitter), and collaborative knowledge repository (Wikipedia talk pages). Each dataset addresses a different topic of cyberbullying. Twitter dataset contains examples of racism and sexism. Wikipedia dataset contains examples of personal attack. However, Formspring dataset is not specifically about any single topic.", so we know that they addressed three cyberbulling topics: racism, sexism, and personal attack.
Answer: racism, sexism, and personal attack
True answer: racism, sexism, personal attack, not specifically about any single topic

Validation ID: 8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92
Question: Were any of the pipeline components based on deep learning models?
From the context "The main goal of all the modules except lexicon matching is to identify events given in the text. These events are then used to populate an ontology. The lexicon matching, on the other hand, was created to link words that are found in the text source with the data available not only on Eurovoc BIBREF6 thesaurus but also on the EU's terminology database IATE BIBREF7 (see Section SECREF12 for details).", so we know that the pipeline components were not based on deep learning models.
Answer: No
True answer: No

Validation ID: 8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92
Question: Were any of the pipeline components based on deep learning models?
From the context "The main goal of all the modules except lexicon matching is to identify events given in the text. These events are then used to populate an ontology. The lexicon matching, on the other hand, was created to link words that are found in the text source with the data available not only on Eurovoc BIBREF6 thesaurus but also on the EU's terminology database IATE BIBREF7 (see Section SECREF12 for details).", so we know that the pipeline components were not based on deep learning models.
Answer: No
True answer: No

Validation ID: 03ebb29c08375afc42a957c7b2dc1a42bed7b713
Question: How is the effectiveness of this pipeline approach evaluated?
From the context "The obtained results support our claim that the proposed system can be used as a base tool for information extraction for the Portuguese language", so we know that the effectiveness of this pipeline approach is evaluated by the obtained results.
Answer: the obtained results
True answer: proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents.

Validation ID: 9cf070d6671ee4a6353f79a165aa648309e01295
Question: What is the size of the parallel corpus used to train the model constraints?
From the context "A part of the ILCI English-Hindi Tourism parallel corpus (1500 sentences) was used for training the classifiers", so we know that the size of the parallel corpus used to train the model constraints is 1500 sentences.
Answer: 1500 sentences
True answer: 1500 sentences

Validation ID: 9cf070d6671ee4a6353f79a165aa648309e01295
Question: What is the size of the parallel corpus used to train the model constraints?
From the context "A part of the ILCI English-Hindi Tourism parallel corpus (1500 sentences) was used for training the classifiers", so we know that the size of the parallel corpus used to train the model constraints is 1500 sentences.
Answer: 1500 sentences
True answer: 1500 sentences

Validation ID: 87bc6f83f7f90df3c6c37659139b92657c3f7a38
Question: How does enforcing agreement between parse trees work across different languages?
From the context "We first obtain the predicted parse trees for the English and Hindi sentences from the respective trained parser models as an initialsiation step. The DD algorithm then tries to enforce agreement between the two parse trees subject to the given alignments.", so we know that the DD algorithm tries to enforce agreement between the two parse trees subject to the given alignments.
Answer: The DD algorithm tries to enforce agreement between the two parse trees subject to the given alignments.
True answer: we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints

Validation ID: 01e2d10178347d177519f792f86f25575106ddc7
Question: What datasets are used to assess the performance of the system?
From the context "We have demonstrated that both UTD and AUD are viable technologies for producing effective tokenizations of speech that enable topic ID performance comparable to using standard ASR systems, while effectively removing the dependency on transcribed speech required by the ASR alternative. We find that when training data is severely limited the UTD-based classification is superior to AUD-based classification. As the amount of training data increases, performance improves across the board. Finally, with sufficient training data AUD-based CNNs with word2vec pre-training outperform AUD-based SVMs.", so we know that the datasets used to assess the performance of the system are Switchboard Telephone Speech Corpus, Turkish, Uzbek and Mandarin.
Answer: Switchboard Telephone Speech Corpus, Turkish, Uzbek and Mandarin
True answer: Switchboard Telephone Speech Corpus BIBREF21, LORELEI (Low Resource Languages for Emergent Incidents) Program

Validation ID: 01e2d10178347d177519f792f86f25575106ddc7
Question: What datasets are used to assess the performance of the system?
From the context "We have demonstrated that both UTD and AUD are viable technologies for producing effective tokenizations of speech that enable topic ID performance comparable to using standard ASR systems, while effectively removing the dependency on transcribed speech required by the ASR alternative. We find that when training data is severely limited the UTD-based classification is superior to AUD-based classification. As the amount of training data increases, performance improves across the board. Finally, with sufficient training data AUD-based CNNs with word2vec pre-training outperform AUD-based SVMs.", so we know that the datasets used to assess the performance of the system are Switchboard Telephone Speech Corpus, Turkish, Uzbek and Mandarin.
Answer: Switchboard Telephone Speech Corpus, Turkish, Uzbek and Mandarin
True answer: LORELEI datasets of Uzbek, Mandarin and Turkish

Validation ID: 021bfb7e180d67112b74f05ecb3fa13acc036c86
Question: How is the vocabulary of word-like or phoneme-like units automatically discovered?
From the context "Unsupervised term discovery (UTD) UTD aims to automatically identify and cluster repeated terms (e.g. words or phrases) from speech. To circumvent the exhaustive DTW-based search limited by INLINEFORM0 time BIBREF6, we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) BIBREF7, which permits search in INLINEFORM1 time. We briefly describe the UTD procedures in ZRTools by four steps below, and full details can be found in BIBREF7.", so we know that the vocabulary of word-like or phoneme-like units is automatically discovered by UTD.
Answer: UTD
True answer: Zero Resource Toolkit (ZRTools) BIBREF7

Validation ID: d201b9992809142fe59ae74508bc576f8ca538ff
Question: IS the graph representation supervised?
From the context "To start, we assign an initial embedding vector to every medical code $c_\ast \in \mathcal {O}_\ast $ with a learnable embedding matrix $\mathbf {W}_e \in \mathbb {R}^{|\mathcal {O}_\ast | \times d}$ where $d$ is the embedding dimension.", so we know that the graph representation is supervised.
Answer: Yes
True answer: The graph representation appears to be semi-supervised. It is included in the learning pipeline for the medical recommendation, where the attention model is learned. (There is some additional evidence that is unavailable in parsed text)

Validation ID: c4628d965983934d7a2a9797a2de6a411629d5bc
Question: Is the G-BERT model useful beyond the task considered?
From the context "In this paper we proposed a pre-training model named G-BERT for medical code representation and medication recommendation. To our best knowledge, G-BERT is the first that utilizes language model pre-training techniques in healthcare domain. It adapted BERT to the EHR data and integrated medical ontology information using graph neural networks. By additional pre-training on the EHR from patients who only have one hospital visit which are generally discarded before model training, G-BERT outperforms all baselines in prediction accuracy on medication", so we know that G-BERT is useful beyond the task considered.
Answer: Yes
True answer: There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding.

Validation ID: c4628d965983934d7a2a9797a2de6a411629d5bc
Question: Is the G-BERT model useful beyond the task considered?
From the context "In this paper we proposed a pre-training model named G-BERT for medical code representation and medication recommendation. To our best knowledge, G-BERT is the first that utilizes language model pre-training techniques in healthcare domain. It adapted BERT to the EHR data and integrated medical ontology information using graph neural networks. By additional pre-training on the EHR from patients who only have one hospital visit which are generally discarded before model training, G-BERT outperforms all baselines in prediction accuracy on medication", so we know that G-BERT is useful beyond the task considered.
Answer: Yes
True answer: It learns a representation of medical records. The learned representation (embeddings) can be used for other predictive tasks involving information from electronic health records.

Validation ID: bd419f4094186a5ce74ba6ac1622b24e29e553f4
Question: How well did the baseline perform?
From the context "Table TABREF8 shows the results for the baseline models as well as the HUMAN measured performance on the task. The human performance provides an upper bound for the RUN task performance, while the simple baselines provide lower bounds. The best baseline model is NO-MOVE, reaching an accuracy of 30.3% on single sentences and 0.3 on complete paragraphs. For the HUMAN case, paragraph accuracy reaches above 80.", so we know that the best baseline model is NO-MOVE, reaching an accuracy of 30.3% on single sentences and 0.3 on complete paragraphs.
Answer: 30.3%
True answer: accuracy of 30.3% on single sentences and 0.3 on complete paragraphs

Validation ID: 11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af
Question: What is the baseline?
From the context "We provide three simple baselines for the RUN task: (1) NO-MOVE: the only position considered is the starting point; (2) RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route; (3) JUMP: at each sentence, extract entities from the map and move between them in the order they appear. If the *WALK action is invalid we take a random *TURN action.", so we know that the baseline is NO-MOVE.
Answer: NO-MOVE
True answer: NO-MOVE, RANDOM, JUMP

Validation ID: 11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af
Question: What is the baseline?
From the context "We provide three simple baselines for the RUN task: (1) NO-MOVE: the only position considered is the starting point; (2) RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route; (3) JUMP: at each sentence, extract entities from the map and move between them in the order they appear. If the *WALK action is invalid we take a random *TURN action.", so we know that the baseline is the three simple baselines for the RUN task.
Answer: three simple baselines for the RUN task
True answer: NO-MOVE: the only position considered is the starting point, RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route, JUMP: at each sentence, extract entities from the map and move between them in the order they appear

Validation ID: 1269c5d8f61e821ee0029080c5ba2500421d5fa6
Question: what methods were used to reduce data sparsity effects?
From the context "In this section, we describe the data augmentation methods we use to increase the amount of training data in order to make our NMT systems suffer less from the low-resourced situation in Japanese INLINEFORM0 Vietnamese translation. Although NMT systems can predict and generate the translation of unseen words on their vocabularies, but they only perform this well if the parallel corpus for training are sufficiently large. For many under-resourced languages, unfortunately, it hardly presents. In reality, although the monolingual data of Vietnamese and Japanese are immensely available due to the popularity of their speakers, the bilingual Japanese-Vietnamese corpora are very limited and often in low quality or in narrowly technical domains. Therefore, data augmentation methods to exploit monolingual data for NMT systems are necessary to obtain more bilingual data, thus upgrading the translating quality.", so we know that the methods used to reduce data sparsity effects are data augmentation methods.
Answer: data augmentation methods
True answer: Back Translation, Mix-Source Approach

Validation ID: 1269c5d8f61e821ee0029080c5ba2500421d5fa6
Question: what methods were used to reduce data sparsity effects?
From the context "In this section, we describe the data augmentation methods we use to increase the amount of training data in order to make our NMT systems suffer less from the low-resourced situation in Japanese INLINEFORM0 Vietnamese translation. Although NMT systems can predict and generate the translation of unseen words on their vocabularies, but they only perform this well if the parallel corpus for training are sufficiently large. For many under-resourced languages, unfortunately, it hardly presents. In reality, although the monolingual data of Vietnamese and Japanese are immensely available due to the popularity of their speakers, the bilingual Japanese-Vietnamese corpora are very limited and often in low quality or in narrowly technical domains. Therefore, data augmentation methods to exploit monolingual data for NMT systems are necessary to obtain more bilingual data, thus upgrading the translating quality.", so we know that the methods used to reduce data sparsity effects are data augmentation methods.
Answer: data augmentation methods
True answer: data augmentation

Validation ID: e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb
Question: what was the baseline?
From the context "We has built the first Japanese INLINEFORM0 Vietnamese NMT systems and released the dataset as well as the associated training scripts. We have also shown that the proposed VNBPE algorithm can be used for Vietnamese word segmentation in order to conduct neural machine translation. Furthermore, by adapting Back Translation and Mix-Source, our NMT systems achieved the best improvement on the dataset.", so we know that the baseline is the first Japanese INLINEFORM0 Vietnamese NMT systems.
Answer: the first Japanese INLINEFORM0 Vietnamese NMT systems
True answer: traditional phrase-based statistical machine translation (SMT), NMT system

Validation ID: e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb
Question: what was the baseline?
From the context "We has built the first Japanese INLINEFORM0 Vietnamese NMT systems and released the dataset as well as the associated training scripts. We have also shown that the proposed VNBPE algorithm can be used for Vietnamese word segmentation in order to conduct neural machine translation. Furthermore, by adapting Back Translation and Mix-Source, our NMT systems achieved the best improvement on the dataset.", so we know that the baseline is the first Japanese INLINEFORM0 Vietnamese NMT systems.
Answer: the first Japanese INLINEFORM0 Vietnamese NMT systems
True answer: traditional phrase-based statistical machine translation (SMT), NMT system

Validation ID: b9ea841b817ba23281c95c7a769873b840dee8d5
Question: did they collect their own data?
From the context "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15. After removing blank and duplicate lines we obtained 106758 pairs of sentences.", so we know that they collected their own data.
Answer: Yes
True answer: No

Validation ID: 219af68afeaecabdfd279f439f10ba7c231736e4
Question: what japanese-vietnamese dataset do they use?
From the context "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15. After removing blank and duplicate lines we obtained 106758 pairs of sentences. The validation set used in all experiments is dev2010 and the test set is tst2010.", so we know that they use TED talks dataset.
Answer: TED talks dataset
True answer: WIT3's corpus

Validation ID: a66a275a817f980c36e0b67d2e00bd823f63abf8
Question: How do they measure style transfer success?
From the context "We evaluated the style transfer itself in the following way. For each pair of sentences, the average score given by three evaluators was calculated, in which the answer that the translated sentence is more formal counts as +1, more informal as -1, and neither as 0.", so we know that they measure style transfer success by the average score given by three evaluators.
Answer: average score given by three evaluators
True answer: Unanswerable

Validation ID: b6f466e0fdcb310ecd212fd90396d9d13e0c0504
Question: Do they introduce errors in the data or does the data already contain them?
From the context "The corpora we use for training the model are OpenSubtitles2018 BIBREF15, Europarl BIBREF16, JRC-Acquis and EMEA BIBREF17. We assume that there should be sufficient stylistic difference between these corpora, especially between the more informal OpenSubtitles2018 (comprised of movie and TV subtitles) on one hand and Europarl and JRC-Acquis (proceedings and documents of the European Parliament) on the other.", so we know that the data already contain them.
Answer: the data already contain them
True answer:  all three languages have error-corrected corpora for testing purposes

Validation ID: b6f466e0fdcb310ecd212fd90396d9d13e0c0504
Question: Do they introduce errors in the data or does the data already contain them?
From the context "The corpora we use for training the model are OpenSubtitles2018 BIBREF15, Europarl BIBREF16, JRC-Acquis and EMEA BIBREF17. We assume that there should be sufficient stylistic difference between these corpora, especially between the more informal OpenSubtitles2018 (comprised of movie and TV subtitles) on one hand and Europarl and JRC-Acquis (proceedings and documents of the European Parliament) on the other.", so we know that the data already contain them.
Answer: the data already contain them
True answer: Data already contain errors

Validation ID: 62ea141d0fb342dfb97c69b49d1c978665b93b3c
Question: What error types is their model more reliable for?
From the context "Our model handles punctuation, word order mistakes and grammatical errors well. For example the subject-verb disagreement in English UID16 and verb-object disagreement in Estonian UID19 have been corrected.", so we know that the error types that their model is more reliable for are punctuation, word order mistakes and grammatical errors.
Answer: punctuation, word order mistakes and grammatical errors
True answer: grammatical, spelling and word order errors

Validation ID: 62ea141d0fb342dfb97c69b49d1c978665b93b3c
Question: What error types is their model more reliable for?
From the context "Our model handles punctuation, word order mistakes and grammatical errors well. For example the subject-verb disagreement in English UID16 and verb-object disagreement in Estonian UID19 have been corrected.", so we know that the error types that their model is more reliable for are punctuation, word order mistakes and grammatical errors.
Answer: punctuation, word order mistakes and grammatical errors
True answer: spelling, word order and grammatical errors

Validation ID: a32c792a0cef03218bf66322245677fc2d5e5a31
Question: How does their parallel data differ in terms of style?
From the context "We use the zero-shot effect to achieve monolingual translation by training the model on bilingual examples in both directions, and then doing translation into the same language as the input: illustrated on Figure FIGREF1.", so we know that the parallel data differ in terms of style.
Answer: The parallel data differ in terms of style.
True answer: Unanswerable

Validation ID: 0101ebfbaba75fd47868ad0c796ac44ebc19c566
Question: How do they split text to obtain sentence levels?
From the context "We padded sentence number in each passage to 101, all word number in each sentence to 100. Word number of queries and choices were padded to 50.", so we know that they split text to obtain sentence levels by padding sentence number in each passage to 101.
Answer: padding sentence number in each passage to 101
True answer: Unanswerable

Validation ID: 50cb50657572e315fd452a89f3e0be465094b66f
Question: Do they experiment with their proposed model on any other dataset other than MovieQA?
From the context "The power of the model is verified on MovieQA dataset, which yielded the state of the art result on the dataset. In the future, we are now working on training our model based on our own trained embedding with TF-IDF BIBREF9 weighting. Furthermore, we would like to test our model on open-answer task like SQuaD by seeing the whole corpus as an “answer pool" and solve it like multiple choice question.", so we know that they are going to experiment with their proposed model on SQuaD.
Answer: SQuaD
True answer: Yes

Validation ID: 50cb50657572e315fd452a89f3e0be465094b66f
Question: Do they experiment with their proposed model on any other dataset other than MovieQA?
From the context "The power of the model is verified on MovieQA dataset, which yielded the state of the art result on the dataset. In the future, we are now working on training our model based on our own trained embedding with TF-IDF BIBREF9 weighting. Furthermore, we would like to test our model on open-answer task like SQuaD by seeing the whole corpus as an “answer pool" and solve it like multiple choice question.", so we know that they are going to experiment with their proposed model on SQuaD.
Answer: SQuaD
True answer: Yes

Validation ID: 981fd79dd69581659cb1d4e2b29178e82681eb4d
Question: What is the difference of the proposed model with a standard RNN encoder-decoder?
From the context "The proposed semantic RALSTM cell applied for Decoder side consists of three components: a Refinement cell, a traditional LSTM cell, and an Adjustment cell", so we know that the proposed model is an extension of the RNN encoder-decoder.
Answer: The proposed model is an extension of the RNN encoder-decoder.
True answer: Introduce a "Refinement Adjustment LSTM-based component" to the decoder

Validation ID: 03e9ac1a2d90152cd041342a11293a1ebd33bcc3
Question: Does the model evaluated on NLG datasets or dialog datasets?
From the context "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television", so we know that the model was evaluated on NLG datasets.
Answer: NLG datasets
True answer: NLG datasets

Validation ID: 03e9ac1a2d90152cd041342a11293a1ebd33bcc3
Question: Does the model evaluated on NLG datasets or dialog datasets?
From the context "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television", so we know that the model was evaluated on NLG datasets.
Answer: NLG datasets
True answer: NLG datasets

Validation ID: ef396a34436072cb3c40b0c9bc9179fee4a168ae
Question: What tasks do they experiment with?
From the context "We evaluate our models on two typical tasks: text classification and text semantic matching", so we know that they experiment with text classification and text semantic matching.
Answer: text classification and text semantic matching
True answer: text classification and text semantic matching

Validation ID: ef396a34436072cb3c40b0c9bc9179fee4a168ae
Question: What tasks do they experiment with?
From the context "We evaluate our models on two typical tasks: text classification and text semantic matching", so we know that they experiment with text classification and text semantic matching.
Answer: text classification and text semantic matching
True answer: text classification and text semantic matching

Validation ID: 04bde1d2b445f971e97bb46ade2d0290981c7a32
Question: What is the meta knowledge specifically?
From the context "Another thread of work is the idea of using one network to direct the learning of another network BIBREF16. BIBREF33 [ BIBREF33 ] introduce a meta neural network to provide another network with a step size and a direction", so we know that the meta knowledge is a step size and a direction.
Answer: a step size and a direction
True answer: Unanswerable

Validation ID: bfbd6040cb95b179118557352e8e3899ef25c525
Question: Are there elements, other than pitch, that can potentially result in out of key converted singing?
From the context "A pitch regression network is employed to render an adversarial loss separating pitch related information from the latent space in autoencoder. After the WaveNet-like encoder, a singer and pitch invariant representation is generated and then fed into the WaveNet decoder conditioning on the singer embedding and the extracted pitch to reconstruct the target singing voice.", so we know that the elements that can potentially result in out of key converted singing are singer embedding and the extracted pitch.
Answer: singer embedding and the extracted pitch
True answer: Unanswerable

Validation ID: d6e353e0231d09fd5dcba493544d53706f3fe1ab
Question: How is the quality of singing voice measured?
From the context "Mean Opinion Score (MOS) was used as a subjective metric to evaluate the quality of the converted audio. Two questions were asked: (1) what is quality of the audio? (naturalness) (2) How well does the converted version match the original? (similarity) A score of 1-5 would be given to answer the questions.", so we know that the quality of singing voice is measured by Mean Opinion Score (MOS).
Answer: Mean Opinion Score (MOS)
True answer: To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score.

Validation ID: d6e353e0231d09fd5dcba493544d53706f3fe1ab
Question: How is the quality of singing voice measured?
From the context "Mean Opinion Score (MOS) was used as a subjective metric to evaluate the quality of the converted audio. Two questions were asked: (1) what is quality of the audio? (naturalness) (2) How well does the converted version match the original? (similarity) A score of 1-5 would be given to answer the questions.", so we know that the quality of singing voice is measured by Mean Opinion Score (MOS).
Answer: Mean Opinion Score (MOS)
True answer: Automatic: Normalized cross correlation (NCC)
Manual: Mean Opinion Score (MOS)

Validation ID: 7bd6a6ec230e1efb27d691762cc0674237dc7967
Question: what data did they use?
From the context "We extend the language model, AWD-LSTM BIBREF0, by replacing LSTM layers with PRU. Our model uses 3-layers of PRU with an embedding size of 400. The number of parameters learned by state-of-the-art methods vary from 18M to 66M with majority of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair comparison with state-of-the-art methods, we fix the model size to 19M and vary the value of INLINEFORM0 and hidden layer sizes so that total number of learned parameters is similar across different configurations. We use 1000, 1200, and 1400 as hidden layer sizes for values of INLINEFORM1 =1,2, and 4, respectively. We use the same settings for the WT-2 dataset. We set the number of pyramidal levels INLINEFORM2 to two in our experiments and use average pooling for sub-sampling. These values are selected based on our ablation experiments on the validation set (Section SECREF39 ).", so we know that they use Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20.
Answer: Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20
True answer:  Penn Treebank, WikiText2

Validation ID: 7bd6a6ec230e1efb27d691762cc0674237dc7967
Question: what data did they use?
From the context "We extend the language model, AWD-LSTM BIBREF0, by replacing LSTM layers with PRU. Our model uses 3-layers of PRU with an embedding size of 400. The number of parameters learned by state-of-the-art methods vary from 18M to 66M with majority of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair comparison with state-of-the-art methods, we fix the model size to 19M and vary the value of INLINEFORM0 and hidden layer sizes so that total number of learned parameters is similar across different configurations. We use 1000, 1200, and 1400 as hidden layer sizes for values of INLINEFORM1 =1,2, and 4, respectively. We use the same settings for the WT-2 dataset. We set the number of pyramidal levels INLINEFORM2 to two in our experiments and use average pooling for sub-sampling. These values are selected based on our ablation experiments on the validation set (Section SECREF39 ).", so we know that they use Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20.
Answer: Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20
True answer: Penn Treebank (PTB) , WikiText2 (WT-2)

Validation ID: 6aaf12505add25dd133c7b0dafe8f4fe966d1f1d
Question: what previous RNN models do they compare with?
From the context "We extend the basic gating architecture of LSTM with the pyramidal and grouped linear transformations outlined above to produce the Pyramidal Recurrent Unit (PRU), whose improved sequence modeling capacity is evidenced in Section SECREF4.", so we know that they compare with LSTM.
Answer: LSTM
True answer: Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM

Validation ID: 73906462bd3415f23d6378590a5ba28709b17605
Question: What are examples of these artificats?
From the context "As such, one can expect both QA benchmarks to have more consistent translations than XNLI", so we know that QA benchmarks have more consistent translations than XNLI.
Answer: QA benchmarks
True answer: the degree of lexical overlap between them, presence of negation words

Validation ID: 73906462bd3415f23d6378590a5ba28709b17605
Question: What are examples of these artificats?
From the context "As such, one can expect both QA benchmarks to have more consistent translations than XNLI", so we know that QA benchmarks have more consistent translations than XNLI.
Answer: QA benchmarks
True answer: hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap

Validation ID: 5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11
Question: What are the languages they use in their experiment?
From the context "We experiment with two models that are representative of the state-of-the-art in monolingual and cross-lingual pre-training: (i) Roberta BIBREF28, which is an improved version of BERT that uses masked language modeling to pre-train an English Transformer model, and (ii) XLM-R BIBREF8, which is a multilingual extension of the former pre-trained on 100 languages", so we know that they use English and 100 languages in their experiment.
Answer: English and 100 languages
True answer: English
French
Spanish
German
Greek
Bulgarian
Russian
Turkish
Arabic
Vietnamese
Thai
Chinese
Hindi
Swahili
Urdu
Finnish

Validation ID: 5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11
Question: What are the languages they use in their experiment?
From the context "We experiment with two models that are representative of the state-of-the-art in monolingual and cross-lingual pre-training: (i) Roberta BIBREF28, which is an improved version of BERT that uses masked language modeling to pre-train an English Transformer model, and (ii) XLM-R BIBREF8, which is a multilingual extension of the former pre-trained on 100 languages", so we know that they use English and 100 languages in their experiment.
Answer: English and 100 languages
True answer: English, Spanish, Finnish

Validation ID: 88bf368491f9613767f696f84b4bb1f5a7d7cb48
Question: Does the professional translation or the machine translation introduce the artifacts?
From the context "The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates.", so we know that the professional translation or the machine translation introduce the artifacts.
Answer: The professional translation or the machine translation introduce the artifacts.
True answer: Yes

Validation ID: 0737954caf66f2b4c898b356d2a3c43748b9706b
Question: Do they recommend translating the premise and hypothesis together?
From the context "In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates", so we know that they recommend translating the premise and hypothesis together.
Answer: Yes
True answer: No

Validation ID: 0737954caf66f2b4c898b356d2a3c43748b9706b
Question: Do they recommend translating the premise and hypothesis together?
From the context "In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: Translate-Test systems are trained on original data and evaluated on machine translated test sets, Zero-Shot systems are trained on original data and evaluated on human translated test sets, and Translate-Train systems are trained on machine translated data and evaluated on human translated test sets.", so we know that the type of text a system is trained on does not typically match the type of text it is exposed to at test time.
From the context "This is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.", so we know that they recommend translating the premise and hypothesis together.
True answer: No

Validation ID: 664b3eadc12c8dde309e8bbd59e9af961a433cde
Question: Is the improvement over state-of-the-art statistically significant?
From the context "We have shown that both human and machine translation can alter superficial patterns in data, which requires reconsidering previous findings in cross-lingual transfer learning. Based on the gained insights, we have improved the state-of-the-art in XNLI for the Translate-Test and Zero-Shot approaches by a substantial margin.", so we know that the improvement over state-of-the-art is substantial.
Answer: substantial
True answer: Yes

Validation ID: b3307d5b68c57a074c483636affee41054be06d1
Question: What are examples of these artifacts?
From the context "As such, one can expect both QA benchmarks to have more consistent translations than XNLI", so we know that QA benchmarks have more consistent translations than XNLI.
Answer: QA benchmarks
True answer: hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap

Validation ID: bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31
Question: What languages do they use in their experiments?
From the context "We experiment with two models that are representative of the state-of-the-art in monolingual and cross-lingual pre-training: (i) Roberta BIBREF28, which is an improved version of BERT that uses masked language modeling to pre-train an English Transformer model, and (ii) XLM-R BIBREF8, which is a multilingual extension of the former pre-trained on 100 languages", so we know that they use English and 100 languages in their experiments.
Answer: English and 100 languages
True answer: English, Spanish, Finnish

Validation ID: 12d7055baf5bffb6e9e95e977c000ef2e77a4362
Question: How much higher quality is the resulting annotated data?
From the context "Simulations in which difficult examples are routed to experts and other instances to crowd annotators yields the best results, outperforming the strategy of randomly selecting data for expert annotation, and substantially improving upon the approach of relying exclusively on crowd annotations", so we know that the resulting annotated data is higher quality.
Answer: higher quality
True answer: improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added

Validation ID: 498c0229f831c82a5eb494cdb3547452112a66a0
Question: How do they match annotators to instances?
From the context "Simulations in which difficult examples are routed to experts and other instances to crowd annotators yields the best results, outperforming the strategy of randomly selecting data for expert annotation, and substantially improving upon the approach of relying exclusively on crowd annotations. In future work, routing strategies based on instance difficulty could be further investigated for budget-quality trade-off.", so we know that they match annotators to instances by routing strategies based on instance difficulty.
Answer: routing strategies based on instance difficulty
True answer: Annotations from experts are used if they have already been collected.

Validation ID: 8c48c726bb17a17d70ab29db4d65a93030dd5382
Question: How much data is needed to train the task-specific encoder?
From the context "We then use predicted spans as proxy `ground truth' annotations to calculate the difficulty score of sentences as described above; we normalize these to the [ $0, 1$ ] interval. We validate this approximation by comparing the proxy scores against reference scores over the test set, the Pearson's correlation coefficients are 0.57 for Population, 0.71 for Intervention and 0.68 for Outcome.", so we know that the task-specific encoder needs at least 57% of the data to train.
Answer: 57%
True answer: 57,505 sentences

Validation ID: 8c48c726bb17a17d70ab29db4d65a93030dd5382
Question: How much data is needed to train the task-specific encoder?
From the context "We then use predicted spans as proxy `ground truth' annotations to calculate the difficulty score of sentences as described above; we normalize these to the [ $0, 1$ ] interval. We validate this approximation by comparing the proxy scores against reference scores over the test set, the Pearson's correlation coefficients are 0.57 for Population, 0.71 for Intervention and 0.68 for Outcome.", so we know that the task-specific encoder needs at least 57% of the data to train.
Answer: 57%
True answer: 57,505 sentences

Validation ID: 89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f
Question: What kind of out-of-domain data?
From the context "We then use predicted spans as proxy `ground truth' annotations to calculate the difficulty score of sentences as described above; we normalize these to the [ $0, 1$ ] interval. We validate this approximation by comparing the proxy scores against reference scores over the test set, the Pearson's correlation coefficients are 0.57 for Population, 0.71 for Intervention and 0.68 for Outcome.", so we know that the out-of-domain data is the test set.
Answer: test set
True answer: Unanswerable

Validation ID: 89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f
Question: What kind of out-of-domain data?
From the context "We then use predicted spans as proxy `ground truth' annotations to calculate the difficulty score of sentences as described above; we normalize these to the [ $0, 1$ ] interval. We validate this approximation by comparing the proxy scores against reference scores over the test set, the Pearson's correlation coefficients are 0.57 for Population, 0.71 for Intervention and 0.68 for Outcome.", so we know that the out-of-domain data is the test set.
Answer: test set
True answer: Unanswerable

Validation ID: 06b5272774ec43ee5facfa7111033386f06cf448
Question: Is an instance a sentence or an IE tuple?
From the context "We then use predicted spans as proxy `ground truth' annotations to calculate the difficulty score of sentences as described above; we normalize these to the [ $0, 1$ ] interval.", so we know that an instance is a sentence.
Answer: sentence
True answer: sentence

Validation ID: 08b57deb237f15061e4029b6718f1393fa26acce
Question: Who are the crowdworkers?
From the context "The crowdworkers were located in the US and hired on the BIBREF22 platform", so we know that the crowdworkers are located in the US and hired on the BIBREF22 platform.
Answer: located in the US and hired on the BIBREF22 platform
True answer: people in the US that use Amazon Mechanical Turk

Validation ID: 08b57deb237f15061e4029b6718f1393fa26acce
Question: Who are the crowdworkers?
From the context "The crowdworkers were located in the US and hired on the BIBREF22 platform", so we know that the crowdworkers are located in the US and hired on the BIBREF22 platform.
Answer: located in the US and hired on the BIBREF22 platform
True answer: located in the US, hired on the BIBREF22 platform

Validation ID: 9b7655d39c7a19a23eb8944568eb5618042b9026
Question: Which toolkits do they use?
From the context "NLP toolkits typically have the following capabilities: tokenization, part-of-speech (PoS) tagging, chunking, named entity recognition and sentiment analysis. In a study by BIBREF3, it is shown that the well-known NLP toolkits NLTK BIBREF4, Stanford CoreNLP BIBREF5, and TwitterNLP BIBREF6 have tokenization, PoS tagging and NER modules in their pipelines.", so we know that they use NLTK, Stanford CoreNLP, and TwitterNLP.
Answer: NLTK, Stanford CoreNLP, and TwitterNLP
True answer: BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21

Validation ID: 9b7655d39c7a19a23eb8944568eb5618042b9026
Question: Which toolkits do they use?
From the context "NLP toolkits typically have the following capabilities: tokenization, part-of-speech (PoS) tagging, chunking, named entity recognition and sentiment analysis. In a study by BIBREF3, it is shown that the well-known NLP toolkits NLTK BIBREF4, Stanford CoreNLP BIBREF5, and TwitterNLP BIBREF6 have tokenization, PoS tagging and NER modules in their pipelines.", so we know that they use NLTK, Stanford CoreNLP, and TwitterNLP.
Answer: NLTK, Stanford CoreNLP, and TwitterNLP
True answer: BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26

Validation ID: cd06d775f491b4a17c9d616a8729fd45aa2e79bf
Question: Which sentiment class is the most accurately predicted by ELS systems?
From the context "In the sentiment analysis experiments, we found that a tweet may contain multiple sentiments. The groundtruth labels contain 210 positive sentiments, 521 neutral sentiments, and 305 negative sentiments to the candidates. We measured the CCR, across all tweets, to be 31.7% for Rosette Text Analytics, 43.2% for Google Cloud, 44.2% for TensiStrength, and 74.7% for the crowdworkers. This means the difference between the performance of the tools and the crowdworkers is significant – more than 30 percent points.", so we know that the sentiment class that is the most accurately predicted by ELS systems is neutral.
Answer: neutral
True answer: neutral sentiment

Validation ID: 1329280df5ee9e902b2742bde4a97bc3e6573ff3
Question: Is datasets for sentiment analysis balanced?
From the context "Our results show that existing NLP systems cannot accurately perform sentiment analysis of political tweets in the dataset we experimented with. Labeling by humans, even non-expert crowdworkers, yields accuracy results that are well above the results of existing automated NLP systems. In future work we will therefore use a crowdworker-labeled dataset to train a new machine-learning based NLP system for tweet analysis. We will ensure that the training data is balanced among classes.", so we know that datasets for sentiment analysis is not balanced.
Answer: No
True answer: No

Validation ID: 58c6737070ef559e9220a8d08adc481fdcd53a24
Question: What measures are used for evaluation?
From the context "We report the experimental results for our two tasks in terms of the correct classification rate (CCR). For sentiment analysis, we have a three-class problem (positive, negative, and neutral), where the classes are mutually exclusive. The CCR, averaged for a set of tweets, is defined to be the number of correctly-predicted sentiments over the number of groundtruth sentiments in these tweets. For NER, we consider that each tweet may reference up to four candidates, i.e., targeted entities. The CCR, averaged for a set of tweets, is the number of correctly predicted entities (candidates) over the number of groundtruth entities (candidates) in this set.", so we know that the measures used for evaluation are correct classification rate (CCR).
Answer: correct classification rate (CCR)
True answer: correct classification rate (CCR)

Validation ID: 0af16b164db20d8569df4ce688d5a62c861ace0b
Question: what were the baselines?
From the context "The baseline embedding methods are including bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), and neural-based word embedding. The classification models are including Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe BIBREF11, and our proposed model.", so we know that the baselines are BOW, TFIDF, neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN, GloVe.
Answer: BOW, TFIDF, neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN, GloVe
True answer: BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN

Validation ID: 0af16b164db20d8569df4ce688d5a62c861ace0b
Question: what were the baselines?
From the context "The baseline embedding methods are including bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), and neural-based word embedding. The classification models are including Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe BIBREF11, and our proposed model.", so we know that the baselines are BOW, TFIDF, neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN, GloVe.
Answer: BOW, TFIDF, neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN, GloVe
True answer: bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe

Validation ID: 78a4ec72d76f0a736a4a01369a42b092922203b6
Question: what datasets were used?
From the context "For pre-training FriendsBERT, we collect the completed scripts of all ten seasons of Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances. All the utterances are followed the preprocessing methods mentions above to compose the corpus for Masked language model pre-training task. The consequent utterances in the same scenes are considered as the consequent sentences to pre-train the Next Sentence Prediction task.", so we know that the datasets used for pre-training FriendsBERT are the completed scripts of all ten seasons of Friends TV shows from emorynlp.
Answer: the completed scripts of all ten seasons of Friends TV shows from emorynlp
True answer: Friends, EmotionPush

Validation ID: 78a4ec72d76f0a736a4a01369a42b092922203b6
Question: what datasets were used?
From the context "For pre-training FriendsBERT, we collect the completed scripts of all ten seasons of Friends TV shows from emorynlp which includes 3,107 scenes within 61,309 utterances. All the utterances are followed the preprocessing methods mentions above to compose the corpus for Masked language model pre-training task. The consequent utterances in the same scenes are considered as the consequent sentences to pre-train the Next Sentence Prediction task. In the pre-training process, the training loss is the sum of the mean likelihood of two pre-train tasks.", so we know that the datasets used for pre-training FriendsBERT are the completed scripts of all ten seasons of Friends TV shows from emorynlp.
Answer: the completed scripts of all ten seasons of Friends TV shows from emorynlp
True answer: EmotionLines BIBREF6

Validation ID: 6a14379fee26a39631aebd0e14511ce3756e42ad
Question: What BERT models are used?
From the context "The proposed approach is summarized in Figure FIGREF3, which aims to overcome these challenges. The framework could be separated into three steps and described as follow:", so we know that the BERT models are FriendsBERT and ChatBERT.
Answer: FriendsBERT and ChatBERT
True answer: BERT-base, BERT-large, BERT-uncased, BERT-cased

Validation ID: 81588e0e207303c2867c896f3911a54a1ef7c874
Question: What are the sources of the datasets?
From the context "The former comes from the scripts of the Friends TV sitcom. The other is made up of Facebook messenger chats", so we know that the source of the Friends dataset is the scripts of the Friends TV sitcom and the source of the EmotionPush dataset is Facebook messenger chats.
Answer: the scripts of the Friends TV sitcom and Facebook messenger chats
True answer: Friends TV sitcom, Facebook messenger chats

Validation ID: dd09db5eb321083dba16c2550676e60682f9a0cd
Question: What labels does the dataset have?
From the context "The EmotionX challenge consists of $1,000$ dialogues for both Friends and EmotionPush. In all of our experiments, each dataset is separated into top 800 dialogues for training and last 200 dialogues for validation. Since the EmotionX challenge considers only the four emotions (anger, joy, neutral, and sadness) in the evaluation stage, we ignore all the data point corresponding to other emotions directly. The details of emotions distribution are shown in Table TABREF18.", so we know that the dataset have four emotions (anger, joy, neutral, and sadness).
Answer: anger, joy, neutral, and sadness
True answer: Ekman’s six basic emotions,  neutral

Validation ID: 40c0f97c3547232d6aa039fcb330f142668dea4b
Question: Do they evaluate on English only datasets?
From the context "We apply the sefe on three datasets and compare it against the efe BIBREF10. Our quantitative results show that sharing the context vectors provides better results, and that amortization and hierarchical structure give further improvements.", so we know that they evaluate on English only datasets.
Answer: Yes
True answer: Unanswerable

Validation ID: 40c0f97c3547232d6aa039fcb330f142668dea4b
Question: Do they evaluate on English only datasets?
From the context "We apply the sefe on three datasets and compare it against the efe BIBREF10. Our quantitative results show that sharing the context vectors provides better results, and that amortization and hierarchical structure give further improvements.", so we know that they evaluate on English only datasets.
Answer: Yes
True answer: No

Validation ID: 777217e025132ddc173cf33747ee590628a8f62f
Question: What experiments are used to demonstrate the benefits of this approach?
From the context "On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34. In particular, we compare the following methods:", so we know that the experiments are used to demonstrate the benefits of this approach are four approaches based on sefe with two efe BIBREF10 baselines.
Answer: four approaches based on sefe with two efe BIBREF10 baselines
True answer: On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:

Validation ID: 777217e025132ddc173cf33747ee590628a8f62f
Question: What experiments are used to demonstrate the benefits of this approach?
From the context "On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34. In particular, we compare the following methods:", so we know that the experiments are used to demonstrate the benefits of this approach are four approaches based on sefe with two efe BIBREF10 baselines.
Answer: four approaches based on sefe with two efe BIBREF10 baselines
True answer: Calculate test log-likelihood on the three considered datasets

Validation ID: 2dbf6fe095cd879a9bf40f110b7b72c8bdde9475
Question: What hierarchical modelling approach is used?
From the context "We propose two methods to share statistical strength among the embedding vectors. The first approach is based on hierarchical modeling BIBREF13, which assumes that the group-specific embedding representations are tied through a global embedding", so we know that hierarchical modelling approach is used.
Answer: hierarchical modelling approach
True answer: the group-specific embedding representations are tied through a global embedding

Validation ID: 7d483077ed7f2f504d59f4fc2f162741fa5ac23b
Question: How do co-purchase patterns vary across seasons?
From the context "In this case, we use the Poisson conditional distribution, which is more appropriate for count data. In Poisson sefe, we also downweigh the contribution of the zeros in the objective function, which provides better results because it allows the inference to focus on the positive signal of the actual purchases BIBREF10, BIBREF2.", so we know that Poisson sefe downweighs the contribution of the zeros in the objective function.
Answer: Poisson sefe downweighs the contribution of the zeros in the objective function
True answer: Unanswerable

Validation ID: de830c534c23f103288c198eb19174c76bfd38a1
Question: Which words are used differently across ArXiv?
From the context "We treat each tag as a group and fit sefe with the goal of uncovering which words have the strongest shift in usage", so we know that the words used differently across ArXiv are the words have the strongest shift in usage.
Answer: the words have the strongest shift in usage
True answer: intelligence

Validation ID: b0d66760829f111b8fad0bd81ca331ddd943ef41
Question: What is future work planed?
From the context "Future work can be improving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in textGAN", so we know that future work planed is improving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in textGAN.
Answer: improving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in textGAN
True answer: ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE

Validation ID: ae7c93646aa5f3206cd759904965b4d484d12f83
Question: What is this method improvement over the best performing state-of-the-art?
From the context "Human evaluation confirms the effectiveness of both our sensationalism scorer and ARL to generate more sensational headlines", so we know that the improvement of this method is that it can generate more sensational headlines.
Answer: generate more sensational headlines
True answer: absolute improvement of 18.2% over the Pointer-Gen baseline

Validation ID: d1ec42b2b5a3c956ff528543636e024bfde5e5ba
Question: Which baselines are used for evaluation?
From the context "We experiment and compare with the following models. Pointer-Gen is the baseline model trained by optimizing $L_\text{MLE}$ in Equation DISPLAY_FORM13. Pointer-Gen+Pos is the baseline model by training Pointer-Gen only on positive examples whose sensationalism score is larger than 0.5 Pointer-Gen+Same-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.1 Pointer-Gen+Pos-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.5 Pointer-Gen+RL-ROUGE is the baseline model trained by optimizing $L_\text{RL-ROUGE}$ in Equation DISPLAY_FORM17, with ROUGE-L BIBREF9 as the reward. Pointer-Gen+RL-SEN is the baseline model trained by optimizing $L_\text{RL-SEN}$ in Equation DISPLAY_FORM17, with $\alpha _\text{sen}$ as the reward. Pointer-Gen+ARL-SEN is our model trained by optimizing $L_\text{ARL-SEN}$ in Equation DISPLAY_FORM19, with $\alpha _\text{sen}$ as the reward. Test set is the headlines from the test set.", so we know that the baselines are Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and Test set.
Answer: Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and Test set
True answer: Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN

Validation ID: 1dac4bc5af239024566fcb0f43bb9ff1c248ecec
Question: Did they used dataset from another domain for evaluation?
From the context "We use LCSTS BIBREF5 as our dataset to train the summarization model. The dataset is collected from the Chinese microblogging website Sina Weibo. It contains over 2 million Chinese short texts with corresponding headlines given by the author of each text. The dataset is split into 2,400,591 samples for training, 10,666 samples for validation and 725 samples for testing.", so we know that they used LCSTS dataset for evaluation.
Answer: LCSTS dataset
True answer: No

Validation ID: 1dac4bc5af239024566fcb0f43bb9ff1c248ecec
Question: Did they used dataset from another domain for evaluation?
From the context "We use LCSTS BIBREF5 as our dataset to train the summarization model. The dataset is collected from the Chinese microblogging website Sina Weibo. It contains over 2 million Chinese short texts with corresponding headlines given by the author of each text. The dataset is split into 2,400,591 samples for training, 10,666 samples for validation and 725 samples for testing.", so we know that they used LCSTS dataset for evaluation.
Answer: LCSTS dataset
True answer: No

Validation ID: 3bf0306e9bd044f723e38170c13455877b2aeec3
Question: How is sensationalism scorer trained?
From the context "We first briefly introduce MLE and RL objective functions, and a naive way to mix these two by a hyper-parameter $\lambda $. Then we point out the challenge of training with noisy reward, and propose ARL to address this issue.", so we know that sensationalism scorer is trained by ARL.
Answer: ARL
True answer: by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$

Validation ID: 3bf0306e9bd044f723e38170c13455877b2aeec3
Question: How is sensationalism scorer trained?
From the context "We first briefly introduce MLE and RL objective functions, and a naive way to mix these two by a hyper-parameter $\lambda $. Then we point out the challenge of training with noisy reward, and propose ARL to address this issue.", so we know that sensationalism scorer is trained by ARL.
Answer: ARL
True answer: classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss

Validation ID: 2858620e0498db2f2224bfbed5263432f0570832
Question: Which component is the least impactful?
From the context "We hypothesize that this is the case because statistical word co-occurrence networks tend to have similar structural properties, regardless of the topic, polarity, sentiment, etc. of the corresponding documents.", so we know that the structural properties of the word co-occurrence networks are similar.
From the context "First, our approach is inductive, not transductive. Indeed, while the node classification approach of BIBREF53 requires all test documents at training time, our graph classification model is able to perform inference on new, never-seen documents.", so we know that the node classification approach of BIBREF53 requires all test documents at training time.
True answer: Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.

Validation ID: 545e92833b0ad4ba32eac5997edecf97a366a244
Question: Which component has the greatest impact on performance?
From the context "We hypothesize that this is the case because statistical word co-occurrence networks tend to have similar structural properties, regardless of the topic, polarity, sentiment, etc. of the corresponding documents.", so we know that the component that has the greatest impact on performance is the structure of the word co-occurrence networks.
Answer: the structure of the word co-occurrence networks
True answer: Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations

Validation ID: 545e92833b0ad4ba32eac5997edecf97a366a244
Question: Which component has the greatest impact on performance?
From the context "We hypothesize that this is the case because statistical word co-occurrence networks tend to have similar structural properties, regardless of the topic, polarity, sentiment, etc. of the corresponding documents.", so we know that the component that has the greatest impact on performance is the structure of the word co-occurrence networks.
Answer: the structure of the word co-occurrence networks
True answer: Removing the master node deteriorates performance across all datasets

Validation ID: cb12c19f9d14bef7b2f778892d9071eea2d6c63d
Question: What is the state-of-the-art system?
From the context "Experiments conducted on 10 standard text classification datasets show that our architecture is competitive with the state-of-the-art", so we know that the state-of-the-art system is the system that has the best performance on 10 standard text classification datasets.
Answer: The system that has the best performance on 10 standard text classification datasets
True answer: doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT

Validation ID: 9193006f359c53eb937deff1248ee3317978e576
Question: Which datasets are used?
From the context "We evaluate the quality of the document embeddings learned by MPAD on 10 document classification datasets, covering the topic identification, coarse and fine sentiment analysis and opinion mining, and subjectivity detection tasks. We briefly introduce the datasets next. Their statistics are reported in Table TABREF21.", so we know that the datasets are Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013.
Answer: Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013
True answer: Reuters,  BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013

Validation ID: 9193006f359c53eb937deff1248ee3317978e576
Question: Which datasets are used?
From the context "We evaluate the quality of the document embeddings learned by MPAD on 10 document classification datasets, covering the topic identification, coarse and fine sentiment analysis and opinion mining, and subjectivity detection tasks. We briefly introduce the datasets next. Their statistics are reported in Table TABREF21.", so we know that the datasets are Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013.
Answer: Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013
True answer:  Reuters, BBCSport BIBREF30, Polarity BIBREF31, Subjectivity BIBREF32, MPQA BIBREF33, IMDB BIBREF34, TREC BIBREF35, SST-1 BIBREF36, SST-2 BIBREF36, Yelp2013 BIBREF26

Validation ID: bc67b91dd73acded2d52fd4fee732b7a9722ea8b
Question: What is the message passing framework?
From the context "The MP framework is based on the core idea of recursive neighborhood aggregation. That is, at every iteration, the representation of each vertex is updated based on messages received from its neighbors. All spectral GNNs can be described in terms of the MP framework.", so we know that the message passing framework is a framework based on the core idea of recursive neighborhood aggregation.
Answer: recursive neighborhood aggregation
True answer: It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors.

Validation ID: 49c32a2a64eb41381e5f12ccea4150cac9f3303d
Question: What other evaluation metrics are looked at?
From the context "We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall.", so we know that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall.
Answer: high-precision but a low recall
True answer: F-score, Kappa

Validation ID: 49c32a2a64eb41381e5f12ccea4150cac9f3303d
Question: What other evaluation metrics are looked at?
From the context "We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall.", so we know that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall.
Answer: high-precision but a low recall
True answer: Unanswerable

Validation ID: bbb77f2d6685c9257763ca38afaaef29044b4018
Question: What is the best reported system?
From the context "We augmented traditional linguistic features with cognitive features obtained from readers' eye-movement data in the form of simple gaze-based features and complex features derived from a graph structure. This extended feature-set improved the success rate of the sarcasm detector by 3.7%, over the best available system.", so we know that the best reported system is the system with 3.7% improvement.
Answer: the system with 3.7% improvement
True answer: Gaze Sarcasm using Multi Instance Logistic Regression.

Validation ID: bbb77f2d6685c9257763ca38afaaef29044b4018
Question: What is the best reported system?
From the context "We augmented traditional linguistic features with cognitive features obtained from readers' eye-movement data in the form of simple gaze-based features and complex features derived from a graph structure. This extended feature-set improved the success rate of the sarcasm detector by 3.7%, over the best available system.", so we know that the best reported system is the system with 3.7% improvement.
Answer: the system with 3.7% improvement
True answer: the MILR classifier

Validation ID: 22732cb9476e521452bf0538f3fdb94cf3867651
Question: What kind of stylistic features are obtained?
From the context "Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic/stylistic features alone.", so we know that the stylistic features are emoticons, laughter expressions such as “lol” etc.
Answer: emoticons, laughter expressions such as “lol” etc
True answer: Unanswerable

Validation ID: 4e748cb2b5e74d905d9b24b53be6cfdf326e8054
Question: What traditional linguistics features did they use?
From the context "We describe the features used for sarcasm detection in Table. The features enlisted under lexical,implicit incongruity and explicit incongruity are borrowed from various literature (predominantly from joshi2015harnessing). These features are essential to separate sarcasm from other forms semantic incongruity in text (for example ambiguity arising from semantic ambiguity or from metaphors).", so we know that they use lexical,implicit incongruity and explicit incongruity features.
Answer: lexical,implicit incongruity and explicit incongruity features
True answer: Unanswerable

Validation ID: 74b338d5352fe1a6fd592e38269a4c81fe79b866
Question: What cognitive features are used?
From the context "We hypothesize that distinctive eye-movement patterns, associated with reading sarcastic text, enables improved detection of sarcasm. We augmented traditional linguistic features with cognitive features obtained from readers' eye-movement data in the form of simple gaze-based features and complex features derived from a graph structure. This extended feature-set improved the success rate of the sarcasm detector by 3.7%, over the best available system.", so we know that the cognitive features are simple gaze-based features and complex features derived from a graph structure.
Answer: simple gaze-based features and complex features derived from a graph structure
True answer: Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half
to first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze
graph (ED),  Fixation Duration at Left/Source
(F1H, F1S),  Fixation Duration at Right/Target
(F2H, F2S),  Forward Saccade Word Count of
Source (PSH, PSS),  Forward SaccadeWord Count of Destination
(PSDH, PSDS), Regressive Saccade Word Count of
Source (RSH, RSS),  Regressive Saccade Word Count of
Destination (RSDH, RSDS)

Validation ID: d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf
Question: What approaches do they use towards text analysis?
From the context "We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results", so we know that they use the approaches of data selection, conceptualization, and operationalization, and analysis and the interpretation of results.
Answer: data selection, conceptualization, operationalization, analysis and the interpretation of results
True answer: Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use

Validation ID: d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf
Question: What approaches do they use towards text analysis?
From the context "We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results", so we know that they use the approaches of data selection, conceptualization, and operationalization, and analysis and the interpretation of results.
Answer: data selection, conceptualization, operationalization, analysis and the interpretation of results
True answer: Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis.

Validation ID: f903396d943541a8cc65edefb04ca37814ed30dd
Question: What dataset do they use for analysis?
From the context "The data used in this study are from the Internet Archive’s Wayback Machine, a web archive that contains snapshots of web pages from 1996 to the present. We collected all snapshots of the top 100,000 websites in the Alexa rankings from January 1, 2010 to December 31, 2016", so we know that the dataset they use for analysis is from the Internet Archive’s Wayback Machine.
Answer: Internet Archive’s Wayback Machine
True answer: Unanswerable

Validation ID: ba28ce9a2f7e8524243adf288cc3f11055e667bb
Question: Do they demonstrate why interdisciplinary insights are important?
From the context "We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original research questions, which may continue to evolve for much of the project.", so we know that interdisciplinary insights are important.
Answer: Yes
True answer: No

Validation ID: ba28ce9a2f7e8524243adf288cc3f11055e667bb
Question: Do they demonstrate why interdisciplinary insights are important?
From the context "We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original research questions, which may continue to evolve for much of the project.", so we know that interdisciplinary insights are important.
Answer: Yes
True answer: No

Validation ID: 975e60535724f4149c7488699a199ba2920a062c
Question: What background do they have?
From the context "The pre-processing steps have a big impact on the operationalizations, subsequent analyses and reproducibility efforts", so we know that the background of the pre-processing steps is operationalizations, subsequent analyses and reproducibility efforts.
Answer: operationalizations, subsequent analyses and reproducibility efforts
True answer: Unanswerable

Validation ID: b970f48d30775d3468952795bc72976baab3438e
Question: What kind of issues (that are not on the forefront of computational text analysis) do they tackle?
From the context "In this article, we take the reader through a typical research process that involves measuring social or cultural concepts using computational methods, discussing both the opportunities and complications that often arise", so we know that they tackle the issues that are not on the forefront of computational text analysis.
Answer: The issues that are not on the forefront of computational text analysis
True answer: identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines

