{
    "b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54": "pivoting, multilingual NMT (MNMT) and cross-lingual transfer without pretraining",
    "f5e6f43454332e0521a778db0b769481e23e7682": "pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16",
    "9a05a5f4351db75da371f7ac12eb0b03607c4b87": "",
    "5eda469a8a77f028d0c5f1acd296111085614537": "Europarl and MultiUN",
    "18c5d366b1da8447b5404eab71f4cc658ba12e6f": "Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF",
    "b5e4866f0685299f1d7af267bbcc4afe2aab806f": "ilur.am",
    "1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590": "crowdsourcing platform",
    "b6ae8e10c6a0d34c834f18f66ab730b670fb528c": "politics, business, science, AskReddit, and the Reddit frontpage",
    "a87a009c242d57c51fc94fe312af5e02070f898b": "A predictive model to discover behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users.",
    "ef4dba073d24042f24886580ae77add5326f2130": "higher",
    "2df4a045a9cd7b44874340b6fdf9308d3c55327a": "Amazon Mechanical Turk",
    "a313e98994fc039a82aa2447c411dda92c65a470": "CFILT-preorder system",
    "37861be6aecd9242c4fdccdfcd06e48f3f1f8f81": "Indian languages",
    "7e62a53823aba08bc26b2812db016f5ce6159565": "IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus and ILCI multilingual parallel corpus",
    "9eabb54c2408dac24f00f92cf1061258c7ea2e1a": "Information about text structure",
    "3d013f15796ae7fed5272183a166c45f16e24e39": "font type and font style",
    "9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc": "STAG",
    "d3aa0449708cc861a51551b128d73e11d62207d2": "The same entity linker as BIBREF4",
    "cfbec1ef032ac968560a7c76dec70faf1269b27c": "Knowledge Base Question Answering",
    "c0e341c4d2253eb42c8840381b082aae274eddad": "",
    "1ec152119cf756b16191b236c85522afeed11f59": "self-similarity, intra-sentence similarity, and maximum explainable variance",
    "891c2001d6baaaf0da4e65b647402acac621a7d2": "By taking the first principal component of its contextualized representations",
    "66c96c297c2cffdf5013bab5e95b59101cb38655": "high recall",
    "6b53e1f46ae4ba9b75117fc6e593abded89366be": "rule-based baseline, the CRF classifier, the spaCy entity tagger",
    "c0bee6539eb6956a7347daa9d2419b367bd02064": "Yes",
    "3de0487276bb5961586acc6e9f82934ef8cb668c": "NUBes-PHI and MEDDOCAN",
    "113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab": "the size of the model",
    "0752d71a0a1f73b3482a888313622ce9e9870d6e": "wFST",
    "55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3": "",
    "4eaf9787f51cd7cdc45eb85cf223d752328c6ee4": "the multilingual pronunciation corpus collected by deri2016grapheme",
    "fb2b536dc8e442dffab408db992b971e86548158": "49% for CS and 41% for Stat2015",
    "31735ec3d83c40b79d11df5c34154849aeb3fb47": "the authors' institution",
    "10d450960907091f13e0be55f40bcb96f44dd074": "Yes",
    "b5608076d91450b0d295ad14c3e3a90d7e168d0e": "Yes",
    "c21b87c97d1afac85ece2450ee76d01c946de668": "neural seq2seq models",
    "d087539e6a38c42f0a521ff2173ef42c0733878e": "The student vocabulary is not a complete subset of the teacher vocabulary, the two vocabularies may tokenize the same words differently. As a result, the outputs of the teacher and student model for the masked language modeling task may not align. Even with the high overlap between the two vocabularies, the need to train the student embedding from scratch, and the change in embedding dimension precludes existing knowledge distillation techniques, which rely on the alignment of both models' output spaces.",
    "efe9bad55107a6be7704ed97ecce948a8ca7b1d2": "Patient Knowledge Distillation (PKD) and NoKD",
    "71e4ba4e87e6596aeca187127c0d088df6570c57": "(1) suppress or (2) emphasize biases in human language",
    "7561a968470a8936d10e1ba722d2f38b5a9a4d38": "30,000",
    "6d4400f45bd97b812e946b8a682b018826e841f1": "",
    "26c2e1eb12143d985e4fb50543cf0d1eb4395e67": "linguistic bias and unwarranted inferences",
    "f17ca24b135f9fe6bb25dc5084b13e1637ec7744": "implicit discourse relation recognition",
    "bd5bd1765362c2d972a762ca12675108754aa437": "outperforms the best previous models for implicit discourse relation recognition on the PDTB dataset",
    "d9b6c61fc6d29ad399d27b931b6cb7b1117b314a": "generative approaches",
    "d27438b11bc70e706431dda0af2b1c0b0d209f96": "BIBREF1, BIBREF2 and BIBREF3",
    "8d4ac4afbf5b14f412171729ceb5e822afcfa3f4": "Yes",
    "3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f": "gender distribution",
    "07d15501a599bae7eb4a9ead63e9df3d55b3dc35": "Meaning Extraction Method (MEM)",
    "99e78c390932594bd833be0f5c890af5c605d808": "Random Top-3",
    "861187338c5ad445b9acddba8f2c7688785667b1": "Yes",
    "f161e6d5aecf8fae3a26374dcb3e4e1b40530c95": "ELMo, BERT and ClinicalBERT",
    "12c50dea84f9a8845795fa8b8c1679328bd66246": "CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus",
    "0810b43404686ddfe4ca84783477ae300fdd2ea4": "",
    "455d4ef8611f62b1361be4f6387b222858bb5e56": "conversational dialogs with workers on crowdsourcing platform CrowdFlower",
    "bc16ce6e9c61ae13d46970ebe6c4728a47f8f425": "4.5",
    "1ff0fccf0dca95a6630380c84b0422bed854269a": "retention rate of tokens and the accuracy of a scheme",
    "3d7d865e905295d11f1e85af5fa89b210e3e9fdf": "100",
    "2ad4d3d222f5237ed97923640bc8e199409cbe52": "100 crowdworkers on Amazon Mechanical Turk (AMT)",
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "Unif and Stopword",
    "ee417fea65f9b1029455797671da0840c8c1abbe": "handcrafted rules, Regular Expressions (RegEx) and Elasticsearch (ES) API",
    "ca5a82b54cb707c9b947aa8445aac51ea218b23a": "The IPA label data after interacting with users",
    "da55bd769721b878dd17f07f124a37a0a165db02": "",
    "feb448860918ef5b905bb25d7b855ba389117c1f": "All India Radio news channel",
    "4bc2784be43d599000cb71d31928908250d4cef3": "pooling approach",
    "75df70ce7aa714ec4c6456d0c51f82a16227f2cb": "7 Indian languages",
    "6424e442b34a576f904d9649d63acf1e4fdefdfc": "Wall Street Journal (WSJ) portion of the Penn Treebank",
    "5eabfc6cc8aa8a99e6e42514ef9584569cb75dec": "",
    "887c6727e9f25ade61b4853a869fe712fe0b703d": "the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility condition is naturally satisfied.",
    "6236762b5631d9e395f81e1ebccc4bf3ab9b24ac": "Yes",
    "31d695ba855d821d3e5cdb7bea638c7dbb7c87c7": "GRU",
    "b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab": "Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask",
    "a99fdd34422f4231442c220c97eafc26c76508dd": "No",
    "2c78993524ca62bf1f525b60f2220a374d0e3535": "rupnik2016news",
    "d604f5fb114169f75f9a38fab18c1e866c5ac28b": "recall score",
    "1d3e914d0890fc09311a70de0b20974bf7f0c9fe": "eight NER tasks",
    "16535db1d73a9373ffe9d6eedaa2369cefd91ac4": "CORD-19 and/or PubMed+PMC",
    "de0b650022ad8693465242ded169313419eed7d9": "Yes",
    "2b3cac7af10d358d4081083962d03ea2798cf622": "Yes",
    "897ba53ef44f658c128125edd26abf605060fb13": "Yes",
    "41ac23e32bf208b69414f4b687c4f324c6132464": "German-French",
    "e97186c51d4af490dba6faaf833d269c8256426c": "Yes",
    "5bb3c27606c59d73fd6944ba7382096de4fa58d8": "multiple choice question answering",
    "8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b": "2",
    "85590bb26fed01a802241bc537d85ba5ef1c6dc2": "comprehensive hyper-parameters searches that also manipulate the number of iterations and random seeds used",
    "75ff6e425ce304a35f18c0230c0d13d3913a31a9": "Yes",
    "5cb610d3d5d7d447b4cd5736d6a7d8262140af58": "randomly alternating between languages for every new minibatch",
    "c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a": "Track 1 and Track 2",
    "b9d168da5321a7d7b812c52bb102a05210fe45bd": "Yes",
    "0c234db3b380c27c4c70579a5d6948e1e3b24ff1": "LSTM",
    "fa527becb8e2551f4fd2ae840dbd4a68971349e0": "LSTM",
    "32a3c248b928d4066ce00bbb0053534ee62596e7": "the task to predict the MSD tag of the target form",
    "c9b8d3858c112859eabee54248b874331c48f71b": "universal morphological reinflection",
    "45e9533586199bde19313cd43b3d0ecadcaf7a33": "Yes",
    "d3dbb5c22ef204d85707d2d24284cc77fa816b6c": "R.M.-Reader + Verifier and DocQA",
    "a5e49cdb91d9fd0ca625cc1ede236d3d4672403c": "",
    "aefa333b2cf0a4000cd40566149816f5b36135e7": "the ratio of correct `translations' (matches)",
    "c5abe97625b9e1c8de8208e15d59c704a597b88c": "agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.",
    "eb2d5edcdfe18bd708348283f92a32294bb193a5": "KG-A2C, A2C, A2C-chained, A2C-Explore",
    "88ab7811662157680144ed3fdd00939e36552672": "KG-A2C-chained and KG-A2C-Explore",
    "cb196725edc9cdb2c54b72364f3bbf7c76471490": "Yes",
    "286078813136943dfafb5155ee15d2429e7601d9": "highly effective",
    "8f16dc7d7be0d284069841e456ebb2c69575b32b": "C-PR, C-PR-Guess, C-PR-NoInteraction, C-PR-NoLL, C-PR-NoLL-NoInteraction, C-PR-NoInteraction-NoLL",
    "a7d020120a45c39bee624f65443e09b895c10533": "performing inference using existing knowledge and asking questions to others to acquire related knowledge and use it in inference",
    "585626d18a20d304ae7df228c2128da542d248ff": "predictive performance and strategy formulation ability",
    "bfc2dc913e7b78f3bd45e5449d71383d0aa4a890": "RL model, lifelong prediction model, user interaction and knowledge retention",
    "6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de": "two labels",
    "b46c0015a122ee5fb95c2a45691cb97f80de1bb6": "",
    "5b7a4994bfdbf8882f391adf1cd2218dbc2255a0": "Naive, mSDA, NaiveNN, AuxNN, ADAN, and MMD",
    "9176d2ba1c638cdec334971c4c7f1bb959495a8e": "source domain is INLINEFORM2 and the target domain is INLINEFORM6",
    "0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c": "Yes",
    "5e324846a99a5573cd2e843d1657e87f4eb22fa6": "removing the bias towards frequent labels in the training data",
    "2ccc26e11df4eb26fcccdd1f446dc749aff5d572": "Yes",
    "f318a2851d7061f05a5b32b94251f943480fbd15": "automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community",
    "6bbbb9933aab97ce2342200447c6322527427061": "crowd-annotated news",
    "2007bfb8f66e88a235c3a8d8c0a3b3dd88734706": "word frequency and topic modeling with NMF",
    "d859cc37799a508bbbe4270ed291ca6394afce2c": "word frequency and topic modeling with NMF",
    "50e80cfa84200717921840fddcf3b051a9216ad8": "Yes",
    "b1bc9ae9d40e7065343c12f860a461c7c730a612": "ShapeWorldICE datasets",
    "63a1cbe66fd58ff0ead895a8bac1198c38c008aa": "Show&Tell model and LRCN1u model",
    "509af1f11bd6f3db59284258e18fdfebe86cae47": "the ratio of observed number versus optimal number",
    "23e16c1173b7def2c5cb56053b57047c9971e3bb": "LSTM",
    "d78f7f84a76a07b777d4092cb58161528ca3803c": "a backward greedy search over each sentence's label sequence to identify word boundaries",
    "9da1e124d28b488b0d94998d32aa2fa8a5ebec51": "BIBREF15, BIBREF19, BIBREF20",
    "37be0d479480211291e068d0d3823ad0c13321d3": "reasonable",
    "a3d9b101765048f4b61cbd3eaa2439582ebb5c77": "English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese",
    "009ce6f2bea67e7df911b3f93443b23467c9f4a1": "BiDAF model with self-attention and highway layers",
    "55569d0a4586d20c01268a80a7e31a17a18198e2": "semantic mapping between different languages",
    "7cd22ca9e107d2b13a7cc94252aaa9007976b338": "Yes",
    "adbf33c6144b2f5c40d0c6a328a92687a476f371": "Yes",
    "f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24": "PER, LOC, ORG, MISC",
    "a0543b4afda15ea47c1e623c7f00d4aaca045be0": "Yes",
    "1591068b747c94f45b948e12edafe74b5e721047": "10K",
    "193ee49ae0f8827a6e67388a10da59e137e7769f": "a task that learns to recover a document with a masked span of tokens",
    "ed2eb4e54b641b7670ab5a7060c7b16c628699ab": "SR",
    "beac555c4aea76c88f19db7cc901fa638765c250": "other relevant information",
    "91e326fde8b0a538bc34d419541b5990d8aae14b": "WMT15 German-to-English training data",
    "044f922604b4b3f42ae381419fd5cd5624fa0637": "",
    "f94b53db307685d572aefad52cd55f53d23769c2": "selecting a subset of the training data",
    "aa7d327ef98f9f9847b447d4def04889b4508d7a": "190 hours ( INLINEFORM1 100K instances)",
    "b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0": "EGL",
    "551457ed34ca7fc0878c85bc664b135c21059b58": "a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset",
    "0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8": "the previous syntactic tree-based models as well as other neural models",
    "4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94": "off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers",
    "a4d115220438c0ded06a91ad62337061389a6747": "Facebook status update messages",
    "2c7e94a65f5f532aa31d3e538dcab0468a43b264": "crowdsourcing task",
    "149da739b1c19a157880d9d4827f0b692006aa2c": "",
    "27de1d499348e17fec324d0ef00361a490659988": "23,700",
    "cfcdd73e712caf552ba44d0aa264d8dace65a589": "crowdsourcing",
    "23b2901264bda91045258b5d4120879ae292e950": "significant performance boost",
    "b5bc34e1e381dbf972d0b594fe8c66ff75305d71": "significant",
    "72f7ef55e150e16dcf97fe443aff9971a32414ef": "+1.86, +1.80 and +2.19",
    "20e38438471266ce021817c6364f6a46d01564f2": "multiplying the soft probability $p$ with a decaying factor $(1-p)$",
    "28067da818e3f61f8b5152c0d42a531bf0f987d4": "INLINEFORM1",
    "bf3b27a4f4be1f9ae31319877fd0c75c03126fd5": "",
    "ffa7f91d6406da11ddf415ef094aaf28f3c3872d": "",
    "b634ff1607ce5756655e61b9a6f18bc736f84c83": "Consumer Discretionary sector",
    "2f901dab6b757e12763b23ae8b37ae2e517a2271": "German and English",
    "b591853e938984e6069d738371500ebdec50d256": "IMDb movie review dataset",
    "a130306c6662ff489df13fb3f8faa7cba8c52a21": "f-pooling, fo-pooling, and ifo-pooling",
    "b1cf5739467ba90059add58d11b73d075a11ec86": "Yes",
    "2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd": "RNN, CNN, QRNN BIBREF2, Transformer BIBREF3, Highway network, Encoder Decoder architecture, etc. Furthermore, attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention BIBREF4, Bidirectional attention flow BIBREF5, etc. Meanwhile, regularization layers such as Dropout, Layer Norm, Batch Norm, etc are also supported for improving generalization ability.",
    "4f253dfced6a749bf57a1b4984dc962ce9550184": "",
    "dc1cec824507fc85ac1ba87882fe1e422ff6cffb": "3150 and 350 questions",
    "f428618ca9c017e0c9c2a23515dab30a7660f65f": "ml based approaches",
    "8ce11515634236165cdb06ba80b9a36a8b9099a2": "Yes",
    "6024039bbd1118c5dab86c41cce1175d99f10a25": "Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus",
    "de5b6c25e35b3a6c5e40e350fc5e52c160b33490": "outperforms",
    "b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f": "global context is the context of the whole document, while local context is the context of the topic segment that sentence falls into.",
    "6bfba3ddca5101ed15256fca75fcdc95a53cece7": "",
    "df5a4505edccc0ee11349ed6e7958cf6b84c9ed4": "A Data Pro",
    "fd753ab5177d7bd27db0e0afc12411876ee607df": "a very simple logistic regression classifier with default parameters",
    "88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42": "a matrix that contains the similarity score between the row word and column word using the similarity measures discussed above",
    "4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3": "four",
    "8b3d3953454c88bde88181897a7a2c0c8dd87e23": "CBOW and Skip\u2013gram",
    "784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f": "Yes",
    "7705dd04acedaefee30d8b2c9978537afb2040dc": "",
    "44497509fdf5e87cff05cdcbe254fbd288d857ad": "feed forward neural network",
    "0ee73909ac638903da4a0e5565c8571fc794ab96": "A group of 50 native people who were well-versed in both English and Tamil languages",
    "1f07e837574519f2b696f3d6fa3230af0b931e5d": "Yes",
    "5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76": "existing systems",
    "729694a9fe1e05d329b7a4078a596fe606bc5a95": "88% and 53%",
    "1c997c268c68149ae6fb43d83ffcd53f0e7fe57e": "ELMo embeddings and Wikidata",
    "5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde": "",
    "f9bf6bef946012dd42835bf0c547c0de9c1d229f": "annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses",
    "6a633811019e9323dc8549ad540550d27aa6d972": "Yes",
    "6b9b9e5d154cb963f6d921093539490daa5ebbae": "",
    "bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2": "STSB, WC, GSyn, Dep, and TopC",
    "d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7": "Clipping negative PMI is not optimal.",
    "6844683935d0d8f588fa06530f5068bf3e1ed0c0": "statistics from finite corpora are unreliable",
    "8acab64ba72831633e8cc174d5469afecccf3ae9": "telephone calls",
    "53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa": "low coverage of audio and difficulty in cross-speaker clustering",
    "72755c2d79210857cfff60bfbcb55f83c71ada51": "11 hours",
    "7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569": "LAN+P600 and ELAN+P600",
    "bd6dc38a9ac8d329114172194b0820766458dacc": "BIBREF0",
    "3ddff6b707767c3dd54d7104fe88b628765cae58": "Universal Dependencies v1.2 treebanks",
    "0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish",
    "06be47e2f50b902b05ebf1ff1c66051925f5c247": "ideal point data",
    "003d6f9722ddc2ee13e879fefafc315fb8e87cb9": "an actor",
    "c88a846197b72d25e04ec55f00ee3e72f655504c": "UN General Debate Corpus",
    "4d28c99750095763c81bcd5544491a0ba51d9070": "celebrities from three categories",
    "78292bc57ee68fdb93ed45430d80acca25a9e916": "manually inserting a negation element in each template or statement",
    "443d2448136364235389039cbead07e80922ec5c": "Sumy package, ILP-based summarization, phrase-based summary",
    "aa6d956c2860f58fc9baea74c353c9d985b05605": "ROUGE unigram score",
    "4c18081ae3b676cc7831403d11bc070c10120f8e": "CLUTO and Carrot2 Lingo",
    "fb3d30d59ed49e87f63d3735b876d45c4c6b8939": "accuracy, precision, recall and F-measure",
    "197b276d0610ebfacd57ab46b0b29f3033c96a40": "SVM, Naive Bayes, Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, and Pattern-based",
    "e025061e199b121f2ac8f3d9637d9bf987d65cd5": "15.5",
    "61652a3da85196564401d616d251084a25ab4596": "26972 sentences",
    "14b74ad5a6f5b0506511c9b454e9c464371ef8c4": "De-En, Ja-En, and Ro-En",
    "5c88d601e8fca96bffebfa9ef22331ecf31c6d75": "No",
    "71bd5db79635d48a0730163a9f2e8ef19a86cd66": "the queries in ReCoRd are explicitly independent from the passage",
    "9ecde59ffab3c57ec54591c3c7826a9188b2b270": "SQuAD, NewsQA, DROP, HotpotQA, ReCoRd and MultiRC",
    "005cca3c8ab6c3a166e315547a2259020f318ffb": "multi-label task",
    "af34051bf3e628c1e2a00b110bb84e5f018b419f": "many-to-many+pretrain, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many, many-to-one, many-to-many, and triangle+pretrain",
    "022c365a14fdec406c7a945a1a18e7e79df37f08": "MT",
    "5260cb56b7d127772425583c5c28958c37cb9bea": "The proposed model may implicitly capture the dialog context state for language modeling.",
    "9b97805a0c093df405391a85e4d3ab447671c86a": "Exact Match (EM) and Macro-averaged F1 scores (F1)",
    "38f58f13c7f23442d5952c8caf126073a477bac0": "2% EM score and over 1.5% F1 score",
    "7ee5c45b127fb284a4a9e72bb9b980a602f7445a": "the model that achieves the second best performance",
    "ddf5e1f600b9ce2e8f63213982ef4209bab01fd8": "Spoken-SQuAD and SQuAD",
    "27275fe9f6a9004639f9ac33c3a5767fea388a98": "Table TABREF2",
    "ef3567ce7301b28e34377e7b62c4ec9b496f00bf": "GMB",
    "7595260c5747aede0b32b7414e13899869209506": "IMDb",
    "c2d1387e08cf25cb6b1f482178cca58030e85b70": "",
    "5a22293b055f5775081d6acdc0450f7bd5f5de04": "our model",
    "03c967763e51ef2537793db7902e2c9c17e43e95": "Conditional Copy (CC) model",
    "26327ccebc620a73ba37a95aabe968864e3392b2": "self-coverage and the opponent-coverage",
    "ababb79dd3c301f4541beafa181f6a6726839a10": "Intelligence Squared Debates",
    "c2b8ee872b99f698b3d2082d57f9408a91e1b4c1": "the combination of letter-trigram word hashing with bidirectional LSTM",
    "8eefa116e3c3d3db751423cc4095d1c4153d3a5f": "standard data set",
    "133eb4aa4394758be5f41744c60c99901b2bc01c": "Yes",
    "3fff37b9f68697d080dbd9d9008a63907137644e": "90.58%",
    "a778b8204a415b295f73b93623d09599f242f202": "a method that explicitly maps data vectors to a space where linear separation is possible",
    "642e8cf1d39faa1cd985d16750cdc6696c52db2f": "attentional encoder-decoder networks",
    "493e971ee3f57a821ef1f67ef3cd47ade154e7c4": "CBOW, PV-DM, GloVe and EqEmb",
    "8dd8e5599fc56562f2acbc16dd8544689cddd938": "",
    "abe2393415e533cb06311e74ed1c5674cff8571f": "",
    "00c57e45ac6afbdfa67350a57e81b4fad0ed2885": "Twitter datasets",
    "22714f6cad2d5c54c28823e7285dc85e8d6bc109": "",
    "82642d3111287abf736b781043d49536fe48c350": "no evidence of depression or evidence of depression",
    "5a81732d52f64e81f1f83e8fd3514251227efbc7": "an existing, annotated Twitter dataset",
    "9a8b9ea3176d30da2453cac6e9347737c729a538": "the hybrid data enables the NER model perform better on both tagging the user queries and the clinical note sentences.",
    "4477bb513d56e57732fba126944073d414d1f75f": "",
    "1b23c4535a6c10eb70bbc95313c465e4a547db5e": "",
    "0a75a52450ed866df3a304077769e1725a995bb7": "previous output and context information",
    "fd0a3e9c210163a55d3ed791e95ae3875184b8f8": "WSJ dataset",
    "c37f65c9f0d543a35c784263b79236ccf1c44fac": "Inception V3",
    "584af673429c7f8621c6bf83362a37048daa0e5d": "the last hidden state of the encoder",
    "1be54c5b3ea67d837ffba2290a40c1e720d9587f": "Yes",
    "b08f88d1facefceb87e134ba2c1fa90035018e83": "Yes",
    "b06512c17d99f9339ffdab12cedbc63501ff527e": "No",
    "fd8e23947095fe2230ffe1a478945829b09c8c95": "performing several random walks on it",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "No",
    "3611a72f754de1e256fbd25b012197e1c24e8470": "Yes",
    "4c07c33dfaf4f3e6db55e377da6fa69825d0ba15": "300",
    "b1ce129678e37070e69f01332f1a8587e18e06b0": "3,216 tweets",
    "7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1": "RoBERTa",
    "0689904db9b00a814e3109fb1698086370a28fa2": "",
    "cc354c952b5aaed2d4d1e932175e008ff2d801dd": "race and gender",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "eleven sentence templates",
    "2ddb51b03163d309434ee403fef42d6b9aecc458": "other models trained using the same data",
    "e587559f5ab6e42f7d981372ee34aebdc92b646e": "SWB",
    "bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a": "improved on NLI",
    "7b4fb6da74e6bd1baea556788a02969134cf0800": "Yes",
    "bc31a3d2f7c608df8c019a64d64cb0ccc5669210": "BERTbase",
    "761de1610e934189850e8fda707dc5239dd58092": "PBSMT",
    "f8da63df16c4c42093e5778c01a8e7e9b270142e": "By counting the common pair intersection of the two lists",
    "c09a92e25e6a81369fcc4ae6045491f2690ccc10": "The crowd is as capable of evaluating lexicons, as experts.",
    "63c3550c6fb42f41a0c93133e9fca12ac00df9b3": "Yes",
    "603fee7314fa65261812157ddfc2c544277fcf90": "",
    "09a1173e971e0fcdbf2fbecb1b077158ab08f497": "significant",
    "70e9210fe64f8d71334e5107732d764332a81cb1": "INLINEFORM0 WER",
    "051df74dc643498e95d16e58851701628fdfd43e": "crawling and pre-processing an OSG web forum",
    "33554065284110859a8ea3ca7346474ab2cab100": "1,873 Twitter conversation threads",
    "57f23dfc264feb62f45d9a9e24c60bd73d7fe563": "50 samples",
    "54830abe73fef4e629a36866ceeeca10214bd2c8": "LDA and Gibbs sampling",
    "2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6": "WordNet and Wordnik",
    "ef7212075e80bf35b7889dc8dd52fcbae0d1400a": "low precision rates and design challenges in training datasets",
    "567dc9bad8428ea9a2658c88203a0ed0f8da0dc3": "BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS",
    "d51dc36fbf6518226b8e45d4c817e07e8f642003": "",
    "d8627ba08b7342e473b8a2b560baa8cdbae3c7fd": "No",
    "cb77d6a74065cb05318faf57e7ceca05e126a80d": "the model without stemming post-positions",
    "8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d": "BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF",
    "a1b3e2107302c5a993baafbe177684ae88d6f505": "The number of words in the dataset",
    "bb2de20ee5937da7e3e6230e942bec7b6e8f61ee": "Bal Krishna Bal, Kathmandu University Professor",
    "1170e4ee76fa202cabac9f621e8fbeb4a6c5f094": "",
    "1462eb312944926469e7cee067dfc7f1267a2a8c": "3",
    "f59f1f5b528a2eec5cfb1e49c87699e0c536cc45": "The new Nepali NER dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG).",
    "9bd080bb2a089410fd7ace82e91711136116af6c": "inflectional characteristics of Nepali language",
    "6d1217b3d9cfb04be7fcd2238666fa02855ce9c5": "BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF",
    "1e775cf30784e6b1c2b573294a82e145a3f959bb": "harassment and non-harassment tweets",
    "392fb87564c4f45d0d8d491a9bb217c4fce87f03": "the one with the Projected Layer",
    "203337c15bd1ee05763c748391d295a1f6415b9b": "multi-attention method having a projected layer",
    "d004ca2e999940ac5c1576046e30efa3059832fa": "multi-attention and single attention models",
    "21548433abd21346659505296fb0576e78287a74": "First workshop on categorizing different types of online harassment languages in social media",
    "f0b2289cb887740f9255909018f400f028b1ef26": "indirect harassment, physical and sexual harassment",
    "51b1142c1d23420dbf6d49446730b0e82b32137c": "F1 Score",
    "58355e2a782bf145b61ee2a3e0e426119985c179": "train set, validation set and test set",
    "25c1c4a91f5dedd4e06d14121af3b5921db125e9": "Yes",
    "f88036174b4a0dbf4fe70ddad884d16082c5748d": "No",
    "a267d620af319b48e56c191aa4c433ea3870f6fb": "the car models",
    "899ed05c460bf2aa0aa65101cad1986d4f622652": "$10,867$",
    "d53299fac8c94bd0179968eb868506124af407d1": "not good",
    "29f2954098f055fb19d9502572f085862d75bf61": "K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)",
    "6bf93968110c6e3e3640360440607744007a5228": "a car's physical attributes",
    "37a79be0148e1751ffb2daabe4c8ec6680036106": "anti-nuclear-power",
    "518dae6f936882152c162058895db4eca815e649": "",
    "e44a6bf67ce3fde0c6608b150030e44d87eb25e3": "abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)",
    "6a31db1aca57a818f36bba9002561724655372a7": "505,412",
    "e330e162ec29722f5ec9f83853d129c9e0693d65": "Yes",
    "d3093062aebff475b4deab90815004051e802aa6": "1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN); 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information; 7) UTCNN without the LDA model; 8) UTCNN without comments.",
    "4944cd597b836b62616a4e37c045ce48de8c82ca": "sentiment prediction, question-type classification, and argumentative excerpts from dialogs",
    "a29c071065d26e5ee3c3bcd877e7f215c59d1d33": "Spearman's rank correlation",
    "7f207549c75f5c4388efc15ed28822672b845663": "one epoch",
    "596aede2b311deb8cb0a82d2e7de314ef6e83e4e": "",
    "2e89ebd2e4008c67bb2413699589ee55f59c4f36": "NLI data",
    "e2db361ae9ad9dbaa9a85736c5593eb3a471983d": "",
    "252a645af9876241fb166e5822992ce17fec6eb6": "a string of characters",
    "ed67359889cf61fa11ee291d6c378cccf83d599d": "GloVe word vectors pre-trained on two datasets",
    "425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82": "accuracy",
    "955de9f7412ba98a0c91998919fa048d339b1d48": "Bag-of-Words and Support Vector Machine with linear kernel",
    "3b371ea554fa6639c76a364060258454e4b931d4": "NowThisNews Facebook page",
    "ddb23a71113cbc092cbc158066d891cae261e2c6": "BIBREF4",
    "e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38": "",
    "c7486d039304ca9d50d0571236429f4f6fbcfcf7": "",
    "f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9": "SemEval-2016 Challenge Task 5",
    "a103636c8d1dbfa53341133aeb751ffec269415c": "majority baseline and lexicon-based baseline",
    "55139fcfe04ce90aad407e2e5a0067a45f31e07e": "machine translation",
    "fbaf060004f196a286fef67593d2d76826f0304e": "hotel reviews",
    "7ae38f51243cb80b16a1df14872b72a1f8a2048f": "",
    "deb89bca0925657e0f91ab5daca78b9e548de2bd": "",
    "9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570": "CNN",
    "e6583c60b13b87fc37af75ffc975e7e316d4f4e0": "composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)",
    "c7b6e6cb997de1660fd24d31759fe6bb21c7863f": "",
    "f9f59c171531c452bd2767dc332dc74cadee5120": "14",
    "4ac2c3c259024d7cd8e449600b499f93332dab60": "Yes",
    "bc730e4d964b6a66656078e2da130310142ab641": "cyber security and death of politicians",
    "3941401a182a3d6234894a5c8a75d48c6116c45c": "CyberAttack and PoliticianDeath",
    "67e9e147b2cab5ba43572ce8a17fc863690172f0": "Crowd workers for estimating keyword-specific expectations and the disagreement between the model and the crowd for discovering new informative keywords",
    "a74190189a6ced2a2d5b781e445e36f4e527e82a": "$0.4\\%$ and $1.19\\%$",
    "43f074bacabd0a355b4e0f91a1afd538c0a6244f": "By asking workers to label a set of microposts containing the keyword into two classes",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "Yes",
    "78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d": "multi-turn, multi-modal, and mixed-initiative interactions",
    "375b281e7441547ba284068326dd834216e55c07": "a setup that can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research",
    "05c49b9f84772e6df41f530d86c1f7a1da6aa489": "File IO, Standard IO, and Telegram",
    "6ecb69360449bb9915ac73c0a816c8ac479cbbfc": "",
    "68df324e5fa697baed25c761d0be4c528f7f5cf7": "",
    "77c34f1033702278f7f044806c1eba0c6ecb8b04": "Yes",
    "2ee715c7c6289669f11a79743a6b2b696073805d": "AEP task",
    "61a9ea36ddc37c60d1a51dabcfff9445a2225725": "news external references in Wikipedia",
    "cc850bc8245a7ae790e1f59014371d4f35cd46d7": "the second step",
    "984fc3e726848f8f13dfe72b89e3770d00c3a1af": "",
    "fb1227b3681c69f60eb0539e16c5a8cd784177a7": "positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in",
    "8df35c24af9efc3348d3b8d746df116480dfe661": "Yes",
    "277a7e916e65dfefd44d2d05774f95257ac946ae": "four baselines",
    "2916bbdb95ef31ab26527ba67961cf5ec94d6afe": "8,275 sentences and 167,739 words in total",
    "f2e8497aa16327aa297a7f9f7d156e485fe33945": "WebAnno",
    "9b76f428b7c8c9fc930aa88ee585a03478bff9b3": "53",
    "dd6b378d89c05058e8f49e48fd48f5c458ea2ebc": "four baseline systems",
    "e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851": "two lists annotated in previous works",
    "c00ce1e3be14610fb4e1f0614005911bb5ff0302": "$relu$, $selu$, $tanh$",
    "71fe5822d9fccb1cb391c11283b223dc8aa1640c": "",
    "97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3": "The number of tweets in each account",
    "1062a0506c3691a93bb914171c2701d2ae9621cb": "chunk",
    "8e12b5c459fa963b3e549deadb864c244879fe82": "4",
    "483a699563efcb8804e1861b18809279f21c7610": "Yes",
    "d3ff2986ca8cb85a9a5cec039c266df756947b43": "words embeddings, style, and morality features",
    "3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4": "212 Twitter accounts",
    "2317ca8d475b01f6632537b95895608dc40c4415": "a list of tweets",
    "3e88fb3d28593309a307eb97e875575644a01463": "",
    "0767ca8ff1424f7a811222ca108a33b6411aaa8a": "strong",
    "e8f969ffd637b82d04d3be28c51f0f3ca6b3883e": "ROGUE-1, ROGUE-2 and ROGUE-L",
    "46227b4265f1d300a5ed71bf40822829de662bc2": "AMR Bank and CNN-Dailymail",
    "a6a48de63c1928238b37c2a01c924b852fe752f8": "ROGUE",
    "b65a83a24fc66728451bb063cf6ec50134c8bfb0": "finding the position of the most referred entity in the graph, then finding the closest verb to the entity and finally selecting the subtree hanging from that verb as the summary AMR",
    "8c852fc29bda014d28c3ee5b5a7e449ab9152d35": "linear SVM, BiLSTM, and CNN",
    "682e26262abba473412f68cbeb5f69aa3b9968d7": "",
    "5daeb8d4d6f3b8543ec6309a7a35523e160437eb": "English",
    "74fb77a624ea9f1821f58935a52cca3086bb0981": "14,100 tweets",
    "d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751": "aggression identification, bullying detection, hate speech, toxic comments, and offensive language",
    "55bd59076a49b19d3283af41c5e3ccb875f3eb0c": "CNN-based sentence classifier",
    "521280a87c43fcdf9f577da235e7072a23f0673e": "more than 2",
    "5a8cc8f80509ea77d8213ed28c5ead501c68c725": "",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "A, B, and C",
    "1b72aa2ec3ce02131e60626639f0cf2056ec23ca": "Table TABREF15",
    "c49ee6ac4dc812ff84d255886fd5aff794f53c39": "Yes",
    "3f856097be2246bde8244add838e83a2c793bd17": "comparing the overlaps of the retrieved results",
    "bf52c01bf82612d0c7bbf2e6a5bb2570c322936f": "",
    "74e866137b3452ec50fb6feaf5753c8637459e62": "semi-manual human judgments",
    "184b0082e10ce191940c1d24785b631828a9f714": "",
    "c59078efa7249acfb9043717237c96ae762c0a8c": "",
    "73bddaaf601a4f944a3182ca0f4de85a19cdc1d2": "Daily Mail news articles",
    "d4e5e3f37679ff68914b55334e822ea18e60a6cf": "",
    "5f60defb546f35d25a094ff34781cddd4119e400": "",
    "90d946ccc3abf494890e147dd85bd489b8f3f0e8": "",
    "b962cc817a4baf6c56150f0d97097f18ad6cd9ed": "8",
    "fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f": "4 models",
    "52f8a3e3cd5d42126b5307adc740b71510a6bdf5": "4 models",
    "2236386729105f5cf42f73cc055ce3acdea2d452": "English",
    "18942ab8c365955da3fd8fc901dfb1a3b65c1be1": "TripAdvisor",
    "7b4992e2d26577246a16ac0d1efc995ab4695d24": "the original annotated dataset",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "substantially improved",
    "9a9d225f9ac35ed35ea02f554f6056af3b42471d": "the original and corrected sentences in the corpus",
    "ea56148a8356a1918bedcf0a99ae667c27792cfe": "the corrected version of the same FCE training set on which the system is trained (450K tokens) and example sentences extracted from the English Vocabulary Profile (270K tokens)",
    "cd32a38e0f33b137ab590e1677e8fb073724df7f": "FCE and CoNLL 2014",
    "2c6b50877133a499502feb79a682f4023ddab63e": "text simplification",
    "f651cd144b7749e82aa1374779700812f64c8799": "",
    "4625cfba3083346a96e573af5464bc26c34ec943": "substantial gains",
    "326588b1de9ba0fd049ab37c907e6e5413e14acd": "Dress",
    "ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb": "89,042 sentence pairs and 296,402 sentence pairs",
    "55507f066073b29c1736b684c09c045064053ba9": "",
    "e838275bb0673fba0d67ac00e4307944a2c17be3": "topics, dialects and gender",
    "8dda1ef371933811e2a25a286529c31623cca0c6": "4",
    "b3de9357c569fb1454be8f2ac5fcecaea295b967": "10,000",
    "59e58c6fc63cf5b54b632462465bfbd85b1bf3dd": "The offensive dataset is not biased by topic, dialect or target.",
    "5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83": "Our model outperforms other generative models and our rewards are effective.",
    "3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f": "irony reward and sentiment reward",
    "14b8ae5656e7d4ee02237288372d9e682b24fdb8": "Ironies in the latter two categories are obscure and hard to understand.",
    "e3a2d8886f03e78ed5e138df870f48635875727e": "",
    "62f27fe08ddb67f16857fab2a8a721926ecbb6fb": "The model outperforms other generative models and our rewards are effective.",
    "9ca447c8959a693a3f7bdd0a2c516f4b86f95718": "Favor or Against",
    "05887a8466e0a2f0df4d6a5ffc5815acd7d9066a": "SVM approach using unigram-based and hashtag-related features",
    "c87fcc98625e82fdb494ff0f5309319620d69040": "the existence of hashtags in tweets",
    "500a8ec1c56502529d6e59ba6424331f797f31f0": "1 million",
    "ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc": "Galatasaray and Fenerbah\u00e7e",
    "f2155dc4aeab86bf31a838c8ff388c85440fce6e": "No",
    "ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc": "3",
    "4d706ce5bde82caf40241f5b78338ea5ee5eb01e": "linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases",
    "86bf75245358f17e35fc133e46a92439ac86d472": "modest",
    "9132d56e26844dc13b3355448d0f14b95bd2178a": "chunk boundary information",
    "f3c204723da53c7c8ef4dc1018ffbee545e81056": "Yes",
    "0602a974a879e6eae223cdf048410b5a0111665e": "",
    "56b034c303983b2e276ed6518d6b080f7b8abe6a": "FSD BIBREF12, Twitter, and Google datasets",
    "15e481e668114e4afe0c78eefb716ffe1646b494": "Gibbs sampling",
    "3d7a982c718ea6bc7e770d8c5da564fbb9d11951": "The generator network is used to learn the projection function between the document-event distribution and the four event related word distributions (entity distribution, location distribution, keyword distribution and date distribution).",
    "692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1": "all the users that had the industry profile element completed",
    "935d6a6187e6a0c9c0da8e53a42697f853f5c248": "the field that a person works in",
    "3b77b4defc8a139992bd0b07b5cf718382cb1a5f": "contextualized word embeddings",
    "01a41c0a4a7365cd37d28690735114f2ff5229f2": "Blogger",
    "cd2878c5a52542ddf080b20bec005d9a74f2d916": "",
    "fd2c6c26fd0ab3c10aae4f2550c5391576a77491": "Yes",
    "6b6d498546f856ac20958f666fc3fd55811347e2": "",
    "de3b1145cb4111ea2d4e113f816b537d052d9814": "CrowdFlower dataset annotated for emotions",
    "132f752169adf6dc5ade3e4ca773c11044985da4": "CrowdFlower emotional tweets dataset",
    "1d9aeeaa6efa1367c22be0718f5a5635a73844bd": "the sequential nature of the data and the text as a whole",
    "012b8a89aea27485797373adbcda32f16f9d7b54": "data harvesting, building standardised datasets and creating shared tasks for South Africa and Africa",
    "c598028815066089cc1e131b96d6966d2610467a": "No",
    "ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed": "The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets.",
    "0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50": "classification accuracy",
    "92dfacbbfa732ecea006e251be415a6f89fb4ec6": "Nguni languages and Sotho languages",
    "c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9": "NCHLT text corpora",
    "307e8ab37b67202fe22aedd9a98d9d06aaa169c5": "Yes",
    "6415f38a06c2f99e8627e8ba6251aa4b364ade2d": "14 languages over 6 language groups",
    "e5c8e9e54e77960c8c26e8e238168a603fcdfcc6": "No",
    "50be4a737dc0951b35d139f51075011095d77f2a": "prior knowledge to label unlabeled instances and then apply a standard learning algorithm",
    "6becff2967fe7c5256fe0b00231765be5b9db9f1": "",
    "76121e359dfe3f16c2a352bd35f28005f2a40da3": "",
    "02428a8fec9788f6dc3a86b5d5f3aa679935678d": "The performance of the model",
    "7793805982354947ea9fc742411bec314a6998f6": "automatic",
    "007b13f05d234d37966d1aa7d85b5fd78564ff45": "",
    "2ceced87af4c8fdebf2dc959aa700a5c95bd518f": "No",
    "72ed5fed07ace5e3ffe9de6c313625705bc8f0c7": "150 to 250 tokens",
    "2e37e681942e28b5b05639baaff4cd5129adb5fb": "large",
    "b49598b05358117ab1471b8ebd0b042d2f04b2a4": "NBOW, LSTM and attentive LSTM",
    "932b39fd6c47c6a880621a62e6a978491d881d60": "TransE, TransH, TransR, TransD, PTransE, RTransE and KG2E",
    "b36f867fcda5ad62c46d23513369337352aa01d2": "WN18 and FB15K",
    "c6a0b9b5dabcefda0233320dd1548518a0ae758e": "CJFA encoder",
    "1e185a3b8cac1da939427b55bf1ba7e768c5dae4": "VAE baseline, the CJFS encoder and the CJFA encoder",
    "26e2d4d0e482e6963a76760323b8e1c26b6eee91": "block E,F,G,H",
    "b80a3fbeb49a8968e149955bdcf199556478eeff": "CJFA encoder performs significantly better in both phone classification and speaker recognition tasks compared with other two approaches.",
    "badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9": "the model with the INLINEFORM0 tagging scheme",
    "67b66fe67a3cb2ce043070513664203e564bdcbd": "conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27, n-grams, label transitions, word suffixes and relative position to the end of the text are considered.",
    "f56d07f73b31a9c72ea737b40103d7004ef6a079": "homographic dataset and heterographic dataset",
    "38e4aaeabf06a63a067b272f8950116733a7895c": "",
    "1d197cbcac7b3f4015416f0152a6692e881ada6c": "OpenIE",
    "92294820ac0d9421f086139e816354970f066d8a": "big",
    "477d9d3376af4d938bb01280fe48d9ae7c9cf7f7": "BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET) and ROUGE-L (R-L)",
    "f225a9f923e4cdd836dd8fe097848da06ec3e0cc": "SQuAD dataset",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "answering always YES",
    "e807d347742b2799bc347c0eff19b4c270449fee": "SQuAD 2.0",
    "31b92c03d5b9be96abcc1d588d10651703aff716": "0.7033",
    "9ec1f88ceec84a10dc070ba70e90a792fba8ce71": "0.6103",
    "384bf1f55c34b36cb03f916f50bbefade6c86a75": "",
    "aef607d2ac46024be17b1ddd0ed3f13378c563a6": "Detecting the under-translated words",
    "93beae291b455e5d3ecea6ac73b83632a3ae7ec7": "IG is a gradient-based method, which exploits the handy model gradient information by integrating first-order derivatives.",
    "6c91d44d5334a4ac80100eead4e105d34e99a284": "Transformer and RNN-Search model",
    "a69a59b6c0ab27bcee1a780d6867df21e30aec08": "",
    "b3d01ac226ee979e188a4141877a6d2a5482de98": "",
    "af5730d82535464cedfa707a03415ac2e7a21295": "Wikipedia and HotpotQA",
    "ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2": "Yes",
    "b249b60a8c94d0e40d65f1ffdfcac527dab57516": "Yes",
    "0f567251a6566f65170a1329eeeb5105932036b2": "A maximum entropy classifier (MaxEnt) trained on the STAN INLINEFORM0 training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current state-of-the-art BIBREF14.",
    "4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f": "gazeteer and rule based, word boundary detection, and ranking with language model and other features",
    "60ce4868af45753c9e124e64e518c32376f12694": "Stanford Sentiment Analysis Dataset",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "",
    "2c85865a65acd429508f50b5e4db9674813d67f2": "recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital",
    "73a7acf33b26f5e9475ee975ba00d14fd06f170f": "templates and expression pools",
    "dd53baf26dad3d74872f2d8956c9119a27269bd5": "linguistic analysis followed by manual verification",
    "218bc82796eb8d91611996979a4a42500131a936": "MLP, Eusboost and MWMOTE",
    "b21bc09193699dc9cfad523f3d5542b0b2ff1b8e": "MLP",
    "352bc6de5c5068c6c19062bad1b8f644919b1145": "two",
    "d667731ea20605580c398a1224a0094d1155ebbb": "No",
    "8bb0011ad1d63996d5650770f3be18abdd9f7fc6": "Yes",
    "b0dbe75047310fec4d4ce787be5c32935fc4e37b": "AddSent and AddOneSent",
    "d64383e39357bd4177b49c02eb48e12ba7ffd4fb": "a model that integrates the neural networks of MRC models with the general knowledge of human beings",
    "52f9cd05d8312ae3c7a43689804bac63f7cac34b": "Yes",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "four different classifiers",
    "955cbea7e5ead36fb89cd6229a97ccb3febcf8bc": "",
    "04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39": "three",
    "15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8": "tweets",
    "ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797": "BIBREF26",
    "6ca938324dc7e1742a840d0a54dc13cc207394a1": "WMT'18 English-German (en-de) news translation task and WMT'18 English-Turkish (en-tr) news task",
    "4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802": "BIBREF26",
    "4d47bef19afd70c10bbceafd1846516546641a2f": "bi-directional language model and uni-directional model",
    "506d21501d54a12d0c9fd3dbbf19067802439a04": "subway and manhattan",
    "701571680724c05ca70c11bc267fb1160ea1460a": "No",
    "600b097475b30480407ce1de81c28c54a0b3b2f8": "layer normalization and data-specific trained word embeddings for text generation [9] and sentiment analysis categorization",
    "ee7e9a948ee6888aa5830b1a3d0d148ff656d864": "40,000",
    "5fda8539a97828e188ba26aad5cda1b9dd642bc8": "Our model performs segmentation faster than any previous models and gives new higher or comparable segmentation performance against previous state-of-the-art models.",
    "709feae853ec0362d4e883db8af41620da0677fe": "",
    "186b7978ee33b563a37139adff1da7d51a60f581": "All the data for learning should not be beyond the given training set.",
    "fabcd71644bb63559d34b38d78f6ef87c256d475": "previous state-of-the-art models",
    "da9c0637623885afaf023a319beee87898948fe9": "Yes",
    "8a1c0ef69b6022a0642ca131a8eacb5c97016640": "context tweets",
    "48088a842f7a433d3290eb45eb0d4c6ab1d8f13c": "",
    "4907096cf16d506937e592c50ae63b642da49052": "humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language.",
    "8748e8f64af57560d124c7b518b853bf2711c13e": "Yes",
    "893ec40b678a72760b6802f6abf73b8f487ae639": "The model can capture some biases in data annotation and collection.",
    "c81f215d457bdb913a5bade2b4283f19c4ee826c": "Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9",
    "e101e38efaa4b931f7dd75757caacdc945bb32b4": "Waseem and Hovy BIBREF5 and Davidson et al. BIBREF9",
    "afb77b11da41cd0edcaa496d3f634d18e48d7168": "examining the effect of different layers of BERT in hate speech detection task",
    "41b2355766a4260f41b477419d44c3fd37f3547d": "",
    "96a4091f681872e6d98d0efee777d9e820cb8dae": "biases in the process of collecting or annotating datasets",
    "81a35b9572c9d574a30cc2164f47750716157fc8": "Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9",
    "f4496316ddd35ee2f0ccc6475d73a66abf87b611": "20 Newsgroups dataset",
    "e8a32460fba149003566969f92ab5dd94a8754a4": "Concept Raw Context model (CRC) and Concept-Concept Context model (3C)",
    "2a6003a74d051d0ebbe62e8883533a5f5e55078b": "",
    "1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b": "hundreds of concepts",
    "9c44df7503720709eac933a15569e5761b378046": "PPMI-SVD and GloVe",
    "b7381927764536bd97b099b6a172708125364954": "by comparing the impact on the resulting IV and OOV word vectors",
    "df95b3cb6aa0187655fd4856ae2b1f503d533583": "n-gram subwords and unsupervised morphemes",
    "f7ed3b9ed469ed34f46acde86b8a066c52ecf430": "n-gram subwords and unsupervised morphemes",
    "c7eb71683f53ab7acffd691a36cad6edc7f5522e": "Yes",
    "17a1eff7993c47c54eddc7344e7454fbe64191cd": "average interpretability scores across dimensions",
    "a5e5cda1f6195ab1336855f1e39a609d61326d62": "all dimensions",
    "32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9": "",
    "eda4869c67fe8bbf83db632275f053e7e0241e8c": "tweet dataset",
    "2c7494d47b2a69f182e83455fe4c75ae3b2893e9": "Yes",
    "4d7ff4e5d06902de85b0e9a364dc455196d06a7d": "character-level models",
    "ecc63972b2783ee39b3e522653cfb6dc5917d522": "They encourage understanding of literature as part of their objective function",
    "8d074aabf4f51c8455618c5bf7689d3f62c4da1d": "",
    "fe2666ace293b4bfac3182db6d0c6f03ea799277": "to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy. In addition, applying linguistics knowledge on word context to extract useful features also enhances prediction performance.",
    "70a1b0f9f26f1b82c14783f1b76dfb5400444aa4": "successful",
    "d3ca5f1814860a88ff30761fec3d860d35e39167": "Maximum Matching, Hidden Markov Model (HMM), Maximum Entropy (ME), Conditional Random Fields and Support Vector Machines",
    "dd20d93166c14f1e57644cd7fa7b5e5738025cd0": "disinformation news and mainstream news",
    "dc2a2c177cd5df6da5d03e6e74262bf424850ec9": "The political bias of different sources is included in the model.",
    "ae90c5567746fe25af2fcea0cc5f355751e05c71": "US dataset and Italian dataset",
    "d7644c674887ca9708eb12107acd964ae53b216d": "",
    "a3bb9a936f61bafb509fa12ac0a61f91abcc5106": "TREC-50 and TREC-6",
    "df6d327e176740da9edcc111a06374c54c8e809c": "TREC-50 and TREC-6",
    "49764eee7fb523a6a28375cc699f5e0220b81766": "Yes",
    "3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1": "standardized 3rd to 9th grade science questions from 12 US states from the past decade",
    "bb3267c3f0a12d8014d51105de5d81686afe5f1b": "five different benchmarks",
    "114934e1a1e818630ff33ac5c4cd4be6c6f75bb2": "effective",
    "2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72": "by comparing the performance of NCEL with other collective alternatives on five public available benchmarks",
    "b8d0e4e0e820753ffc107c1847fe1dfd48883989": "adjacent entity mentions",
    "5aa12b4063d6182a71870c98e4e1815ff3dc8a72": "Yes",
    "22815878083ebd2f9e08bc33a5e733063dac7a0f": "Russian and English",
    "220d11a03897d85af91ec88a9b502815c7d2b6f3": "Because English has simple morphology",
    "d509081673f5667060400eb325a8050fa5db7cc8": "the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC)",
    "c2e475adeddcdc4d637ef0d4f5065b6a9b299827": "BLEU-4, NIST-4 and ROUGE-4",
    "cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5": "Yes",
    "6cd25c637c6b772ce29e8ee81571e8694549c5ab": "WikiBio",
    "1088255980541382a2aa2c0319427702172bbf84": "",
    "0d9fcc715dee0ec85132b3f4a730d7687b6a06f4": "the expected number of unique outputs it assigns to a set of adversarial perturbations",
    "8910ee2236a497c92324bbbc77c596dba39efe46": "sentiment analysis and paraphrase detection",
    "2c59528b6bc5b5dc28a7b69b33594b274908cca6": "",
    "6b367775a081f4d2423dc756c9b65b6eef350345": "Yes",
    "bc01853512eb3c11528e33003ceb233d7c1d7038": "dropping, adding, and swapping internal characters within words",
    "67ec8ef85844e01746c13627090dc2706bb2a4f3": "They experiment with RNNs instead of transformers for this task.",
    "ba539cab80d25c3e20f39644415ed48b9e4e4185": "falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK",
    "6bf5620f295b5243230bc97b340fae6e92304595": "assigning a semantic role to a constituent based on its syntactic function",
    "4986f420884f917d1f60d3cea04dc8e64d3b5bf1": "crosslingual latent variables (CLVs)",
    "747b847d687f703cc20a87877c5b138f26ff137d": "CoNLL 2009 corpus and Europarl corpus",
    "111afb77cfbf4c98e0458606378fa63a0e965e36": "No",
    "6568a31241167f618ef5ede939053feaa2fb0d7e": "Yes",
    "50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed": "the monolingual model and additional crosslingual latent variables",
    "0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c": "Yes",
    "4dc268e3d482e504ca80d2ab514e68fd9b1c3af1": "2,896",
    "ab54cd2dc83141bad3cb3628b3f0feee9169a556": "",
    "249c805ee6f2ebe4dbc972126b3d82fb09fa3556": "the combination of the two vocabularies",
    "b4f881331b975e6e4cab1868267211ed729d782d": "",
    "79413ff5d98957c31866f22179283902650b5bb6": "2,896 e-books",
    "29c014baf99fb9f40b5171aab3e2c7f12a748f79": "19 different algorithms",
    "09c86ef78e567033b725fc56b85c5d2602c1a7c3": "average the predictions from the constituent single models",
    "d67c01d9b689c052045f3de1b0918bab18c3f174": "INLINEFORM3",
    "e5bc73974c79d96eee2b688e578a9de1d0eb38fd": "There is still space for improvement.",
    "2cd37743bcc7ea3bd405ce6d91e79e5339d7642e": "Yes",
    "eac9dae3492e17bc49c842fb566f464ff18c049b": "argument components",
    "7697baf8d8d582c1f664a614f6332121061f87db": "",
    "1cb100182508cf55b3509283c0e2bbcd527d625e": "several target domains from educational controversies, such as homeschooling, single-sex education, or mainstreaming",
    "206739417251064b910ae9e5ff096e867ee10fb8": "The argumentation phenomena encounter in actual data are now accounted for by this work.",
    "d6401cece55a14d2a35ba797a0878dfe2deabedc": "different registers and domains",
    "ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff": "",
    "fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c": "Android application",
    "b1a068c1050e2bed12d5c9550c73e59cd5b1f78d": "different accents",
    "f9edd8f9c13b54d8b1253ed30e7decc1999602da": "each part of the database",
    "d93c0e78a3fe890cd534a11276e934be68583f4b": "",
    "30af1926559079f59b0df055da76a3a34df8336f": "Sharif DeepMine company",
    "ceb767e33fde4b927e730f893db5ece947ffb0d8": "demographics, diagnosis history, and symptoms/signs",
    "c2cb6c4500d9e02fc9a1bdffd22c3df69655189f": "No",
    "c571deefe93f0a41b60f9886db119947648e967c": "515 annotated history of present illness notes",
    "06eb9f2320451df83e27362c22eb02f4a426a018": "the effectiveness of the document preprocessing",
    "e54257585cc75564341eb02bdc63ff8111992f82": "five keyphrase extraction models",
    "2a3e36c220e7b47c1b652511a4fdd7238a74a68f": "244",
    "9658b5ffb5c56e5a48a3fea0342ad8fc99741908": "Yes",
    "46c9e5f335b2927db995a55a18b7c7621fd3d051": "13",
    "ce0e2a8675055a5468c4c54dbb099cfd743df8a7": "",
    "3a6e843c6c81244c14730295cfb8b865cd7ede46": "pre-trained features and the baseline features",
    "fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf": "the publicly available word2vec vectors",
    "1beb4a590fa6127a138f4ed1dd13d5d51cc96809": "the features that outperform the pre-trained features for sarcasm detection",
    "5c5aeee83ea3b34f5936404f5855ccb9869356c1": "machine translation tasks",
    "f8c1b17d265a61502347c9a937269b38fc3fcab1": "better",
    "5913930ce597513299e4b630df5e5153f3618038": "Their model improves interpretability compared to softmax transformers.",
    "81d193672090295e687bc4f4ac1b7a9c76ea35df": "distant supervised method",
    "cf171fad0bea5ab985c53d11e48e7883c23cdc44": "The Twitter dataset is formed of tweets about Turkish mobile network operators, there are 1,716 tweets, 973 of them are negative and 743 of them are positive, these tweets are manually annotated by two humans, where the labels are either positive or negative, the Cohen's Kappa inter-annotator agreement score is 0.82.",
    "2a564b092916f2fabbfe893cf13de169945ef2e1": "20,244 reviews, the average number of words in reviews is 39, the polarity scores are between the values 0.5 and 5, at intervals of 0.5, and we have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.",
    "0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d": "minimum, mean, and maximum polarity scores",
    "73e83c54251f6a07744413ac8b8bed6480b2294f": "",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "(DISPLAY_FORM4)",
    "e48e750743aef36529fbea4328b8253dbe928b4d": "dev data set",
    "c08aab979dcdc8f4fe8ec1337c3c8290ab13414e": "14",
    "8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f": "pretrained word embeddings",
    "cc608df2884e1e82679f663ed9d9d67a4b6c03f3": "precision, recall, and F1-score",
    "3e432d71512ffbd790a482c716e7079ee78ce732": "large corpora of annotated task-specific dialogs",
    "dd76130ec5fac477123fe8880472d03fbafddef6": "conversational systems in three perspectives",
    "43eecc576348411b0634611c81589f618cd4fddf": "SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, and DPGAN",
    "79f9468e011670993fd162543d1a4b3dd811ac5d": "Better performance on three text generation tasks",
    "c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf": "Yes",
    "902b3123aec0f3a39319ffa9d05ab8e08a2eb567": "skip-gram with negative sampling (SGNS)",
    "1038542243efe5ab3e65c89385e53c4831cd9981": "DTA corpus",
    "e2b0cd30cf56a4b13f96426489367024310c3a05": "comparing the model's output with the gold ranking of the DURel data set",
    "e831041d50f3922265330fcbee5a980d0e2586dd": "a task that participants were instructed to read the sentences naturally, without any specific task other than comprehension",
    "7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93": "Yes",
    "ac7f6497be4bcca64e75f28934b207c9e8097576": "Wikipedia corpus",
    "87bb3105e03ed6ac5abfde0a7ca9b8de8985663c": "the techniques are cheaper to implement",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "copy, copy-marked and copy-dummies",
    "9225b651e0fed28d4b6261a9f6b443b52597e401": "an efficient solution for domain adaptation",
    "565189b672efee01d22f4fc6b73cd5287b2ee72c": "in-domain target data",
    "b6f7fadaa1bb828530c2d6780289f12740229d84": "English-French and English-German",
    "7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613": "English",
    "4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f": "Yes",
    "6c96e910bd98c9fd58ba2050f99b9c9bac69840a": "whether a given question after a time period $t$ will be answered or not",
    "9af3142630b350c93875441e1e1767312df76d17": "usefulness of the answer",
    "e374169ee10f835f660ab8403a5701114586f167": "username, display name, profile image, location and description",
    "82595ca5d11e541ed0c3353b41e8698af40a479b": "",
    "d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f": "The profile changes vary for influential leads and their followers over the social movement.",
    "53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e": "BLEU-1, Meteor and Rouge-L",
    "869feb7f47606105005efdb6bea1c549824baea0": "two question-answer pairs for each tweet",
    "c497e8701060583d91bb64b9f9202d40047effc4": "by checking if the tweets are used in news articles",
    "8060a773f6a136944f7b59758d08cc6f2a59693b": "",
    "1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306": "14%",
    "c0af8b7bf52dc15e0b33704822c4a34077e09cd1": "2-layers regular trained model",
    "9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e": "They determine text from the audio.",
    "e0122fc7b0143d5cbcda2120be87a012fb987627": "68.8% to 71.8%",
    "5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4": "using a feed-forward neural model to predict the emotion class",
    "37edc25e39515ffc2d92115d2fcd9e6ceb18898b": "BIBREF2",
    "e431661f17347607c3d3d9764928385a8f3d9650": "greatly",
    "876700622bd6811d903e65314ac75971bbe23dcc": "two different twitter sentiment classification problems",
    "312e9cc11b9036a6324bdcb64eca6814053ffa17": "0.8",
    "1c0ba6958da09411deded4a14dfea5be55687619": "1,949",
    "1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b": "TF-IDF features",
    "d915b401bb96c9f104a0353bef9254672e6f5a47": "",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "qualitative metrics",
    "664db503509b8236bc4d3dc39cebb74498365750": "1.0",
    "64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8": "the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends",
    "b0a18628289146472aa42f992d0db85c200ec64b": "F1 score",
    "72ce05546c81ada05885026470f4c8c218805055": "English",
    "5b551ba47d582f2e6467b1b91a8d4d6a30c343ec": "BLEU-1/4 and ROUGE-L",
    "3cf1edfa6d53a236cf4258afd87c87c0a477e243": "English",
    "9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa": "930",
    "34dc0838632d643f33c8dbfe7bd4b656586582a2": "NN and Enc-Dec",
    "c77359fb9d3ef96965a9af0396b101f82a0a9de6": "Food.com",
    "1bdc990c7e948724ab04e70867675a334fdd3051": "Food.com",
    "78536da059b884d6ad04680baeb894895458055c": "Khandelwal and Sawant (BIBREF12)",
    "96b07373756d7854bccc3c12e8d41454ab8741f5": "No",
    "511517efc96edcd3e91e7783821c9d6d5a6562af": "BF, BA, SFU and Sherlock",
    "9122de265577e8f6b5160cd7d28be9e22da752b2": "3.16 F1 points on BF, 0.06 F1 points on BA and 0.3 F1 points on SFU",
    "e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d": "",
    "45be665a4504f0c7f458cf3f75a95d5a75eefd42": "11871 sentences",
    "22b740cc3c8598247ee102279f96575bdb10d53f": "Yes",
    "74b4779de437c697fe702e51f23e2b0538b0f631": "paraphrase information and context",
    "435570723b37ee1f5898c1a34ef86a0b2e8701bb": "Zhang2008b and Xiong2010",
    "aa2948209cc33b071dbf294822e72bb136678345": "better",
    "d9412dda3279729e95fcb35cbed09e61577a896e": "precision, recall, F1 and accuracy",
    "41b70699514703820435b00efbc3aac4dd67560a": "civil field",
    "e3c9e4bc7bb93461856e1f4354f33010bc7d28d5": "baselines",
    "06cc8fcafc0880cf69a2514bb7341642b9833041": "INLINEFORM0",
    "d650101712e36594bd77b45930a990402a455222": "China Judgments Online",
    "cb384dc5366b693f28680374d31ff45356af0461": "Yes",
    "d41e20ec716b5904a272938e5a8f5f3f15a7779e": "the journalists' words in news articles, direct quotes, paraphrases from interviews, and published opinion articles",
    "0682bf049f96fa603d50f0fdad0b79a5c55f6c97": "Yes",
    "97d1ac71eed13d4f51f29aac0e1a554007907df8": "They use interval segment embeddings to distinguish multiple sentences within a document.",
    "c17b609b0b090d7e8f99de1445be04f8f66367d4": "ROUGE-1, ROUGE-2 and ROUGE-L",
    "53014cfb506f6fffb22577bf580ae6f4d5317ce5": "CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum",
    "fa30a938b58fc05131c3854f12efe376cbad887f": "competitive results without relying on any handcrafted resource",
    "f875337f2ecd686cd7789e111174d0f14972638d": "",
    "de53af4eddbc30c808d90b8a11a29217d377569e": "different Facebook pages",
    "dac087e1328e65ca08f66d8b5307d6624bf3943f": "Yes",
    "a1645d0ba50e4c29f0feb806521093e7b1459081": "",
    "3cd185b7adc835e1c4449eff81222f5fc15c8500": "a novel feature extraction method",
    "f03112b868b658c954db62fc64430bebbaa7d9e0": "ROUGE",
    "5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4": "great",
    "a6d3e57de796172c236e33a6ceb4cca793dc2315": "LEAD",
    "395b61d368e8766014aa960fde0192e4196bcb85": "imdb400, yelp50 and yelp200",
    "92bb41cf7bd1f7886784796a8220ed5aa07bc49b": "the architecture of the target classifier, dataset features such as length of text, training data size (for target classifiers) and input domains",
    "4ef11518b40cc55d86c485f14e24732123b0d907": "FGM, FGVM, DeepFool, HotFlip and TYC",
    "6a219d7c58451842aa5d6819a7cdf51c55e9fc0f": "11 languages into English",
    "cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad": "Transformer base architecture with 3 encoder layers, 3 decoder layers and 0.3 dropout",
    "f8f4e4a50d2b3fbd193327e79ea32d8d057e1414": "Common Voice",
    "bc84c5a58c57038910f7720d7a784560054d3e1a": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese",
    "29923a824c98b3ba85ced964a0e6a2af35758abe": "sanity check",
    "559c68802ee2bb8b11e2188127418ca3a6155ba7": "Yes",
    "8dc707a0daf7bff61a97d9d854283e65c0c85064": "",
    "ffde866b1203a01580eb33237a0bb9da71c75ecf": "2,000 hours",
    "6cd8bad8a031ce6d802ded90f9754088e0c8d653": "0.9",
    "30eacb4595014c9c0e5ee9669103d003cfdfe1e5": "the relation classification dataset of the SemEval 2010 task 8",
    "0f7867f888109b9e000ef68965df4dde2511a55f": "combine CNNs and RNNs",
    "e2e977d7222654ee8d983fd8ba63b930e9a5a691": "",
    "0cfe0e33fbb100751fc0916001a5a19498ae8cb5": "splitting the context into two parts",
    "35b3ce3a7499070e9b280f52e2cb0c29b0745380": "",
    "71ba1b09bb03f5977d790d91702481cc406b3767": "The performance of the baseline is the accuracy of the global majority class baseline and the target-wise majority class baseline.",
    "612c3675b6c55b60ae6d24265ed8e20f62cb117e": "",
    "bd40f33452da7711b65faaa248aca359b27fddb6": "state of the art",
    "787c4d4628eac00dbceb1c96020bff0090edca46": "(A) a natural language question concerning a political issue and (B) a natural language commentary on a specific stance towards the question",
    "3c3807f226ba72fc41f59f0338f12a49a0c35605": "Yes",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "Yes",
    "81d607fc206198162faa54a796717c2805282d9b": "seven experts with legal training",
    "51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad": "BERT",
    "f0848e7a339da0828278f6803ed7990366c975f0": "Yes",
    "b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8": "Yes",
    "792f6d76d2befba2af07198584aac1b189583ae4": "Established task",
    "127d5ddfabec5c58832e5865cbd8ed0978c25a13": "a simple word-level encoder for tweets",
    "b91671715ad4fad56c67c28ce6f29e180fe08595": "precision 1",
    "a6d37b5975050da0b1959232ae756fc09e5f87e8": "a simple word-level encoder for tweets",
    "e82fa03f1638a8c59ceb62bb9a6b41b498950e1f": "BERT",
    "7ab9c0b4ceca1c142ff068f85015a249b14282d0": "No",
    "00050f7365e317dc0487e282a4c33804b58b1fb3": "No",
    "c5b0ed5db65051eebd858beaf303809aa815e8e5": "small BERT",
    "10fb7dc031075946153baf0a0599e126de29e3a4": "by constructing context-gloss pairs from glosses of all possible senses (in WordNet) of the target word",
    "e438445cf823893c841b2bc26cdce32ccc3f5cbe": "the fonts in the top right corner",
    "12f7fac818f0006cf33269c9eafd41bbb8979a48": "biLSTM",
    "d5a8fd8bb48dd1f75927e874bdea582b4732a0cd": "Yes",
    "1097768b89f8bd28d6ef6443c94feb04c1a1318e": "Yes",
    "fc1679c714eab822431bbe96f0e9cf4079cd8b8d": "2.9%",
    "23e2971c962bb6486bc0a66ff04242170dd22a1d": "visual features",
    "c9bc6f53b941863e801280343afa14248521ce43": "GloVe and uniform distribution $U(-1, 1)$",
    "07b70b2b799b9efa630e8737df8b1dd1284f032c": "29,794 articles",
    "71a0c4f19be4ce1b1bae58a6e8f2a586e125d074": "Wikipedia reviewers or any registered user",
    "c2eb743c9d0baf1781c3c0df9533fab588250af3": "natural language inference, paraphrase identification, and sentiment classification",
    "c35806cf68220b2b9bb082b62f493393b9bdff86": "",
    "f7d0fa52017a642a9f70091a252857fccca31f12": "NLI, PI, sentiment classification, gate values, and model variants",
    "01209a3bead7c87bcdc628be2a5a26b41abde9d1": "SNLI and MultiNLI",
    "2740e3d7d33173664c1c5ab292c7ec75ff6e0802": "",
    "db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21": "multiple systems",
    "48bd71477d5f89333fa7ce5c4556e4d950fb16ed": "surface-level features",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "From the context \"We used 4 feature types, namely",
    "ad1be65c4f0655ac5c902d17f05454c0d4c4a15d": "dataset statistics",
    "2eb9280d72cde9de3aabbed993009a98a5fe0990": "13,939",
    "154a721ccc1d425688942e22e75af711b423e086": "Amazon Mechanical Turk",
    "84bad9a821917cb96584cf5383c6d2a035358d7c": "crowdsourcing",
    "c9305e5794b65b33399c22ac8e4e024f6b757a30": "",
    "56b7319be68197727baa7d498fa38af0a8440fe4": "extracted features",
    "2268c9044e868ba0a16e92d2063ada87f68b5d03": "",
    "6b7354d7d715bad83183296ce2f3ddf2357cb449": "LSTM-CRF",
    "e949b28f6d1f20e18e82742e04d68158415dc61e": "ranked 1st and 2nd in FLC and SLC tasks, respectively",
    "a1ac2a152710335519c9a907eec60d9f468b19db": "propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification)",
    "ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25": "Yes",
    "49eb52b3ec0647e165a5e41488088c80a20cc78f": "the ultimate, penultimate and any post attention models",
    "9bb7ae50bff91571a945c1af025ed2e67714a788": "BIBREF7",
    "81dbe9a9ddaa5d02b02e01a306d898015a56ffb6": "ultimate, penultimate and any post attention models",
    "348886b4762db063711ef8b7a10952375fbdcb57": "English dataset",
    "1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b": "multimodal machine translation",
    "f9aa055bf73185ba939dfb03454384810eb17ad1": "Monolingual data from Wikipedias",
    "d571e0b0f402a3d36fb30d70cdcd2911df883bc7": "Yes",
    "ce2b921e4442a21555d65d8ce4ef7e3bde931dfc": "French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)",
    "2275b0e195cd9cb25f50c5c570da97a4cce5dca8": "adapting autoencoding pre-trained LMs",
    "37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6": "XNLI and dependency parsing",
    "d01c51155e4719bf587d114bcd403b273c77246f": "XNLI dataset and Universal Dependencies v2.4",
    "9b4dc790e4ff49562992aae4fad3a38621fadd8b": "manual evaluations of image content and bag-of-words representations",
    "a1dac888f63c9efaf159d9bdfde7c938636f07b1": "the same as BIBREF7",
    "1e4dbfc556cf237accb8b370de2f164fa723687b": "two metrics",
    "fff5c24dca92bc7d5435a2600e6764f039551787": "from the public StackExchange data",
    "b2ecfd5480a2a4be98730e2d646dfb84daedab17": "a collection of text-based adventure games",
    "a3efe43a72b76b8f5e5111b54393d00e6a5c97ab": "",
    "f1e90a553a4185a4b0299bd179f4f156df798bce": "four non-neural extractive models and CopyRNN",
    "19b7312cfdddb02c3d4eaa40301a67143a72a35a": "two metrics",
    "22744c3bc68f120669fc69490f8e539b09e34b94": "Yes",
    "dcea88698949da4a1bd00277c06df06c33f6a5ff": "The proposed task is a good proxy for the general-purpose sequence to sequence tasks",
    "d7b60abb0091246e29d1a9c28467de598e090c20": "7.8%",
    "bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e": "93.7% of the accounts which have over 75% of their comments tagged as offensive are throwaways and 1.3% are trolls.",
    "5a6926de13a8cc25ce687c22741ba97a6e63d4ee": "Other political events",
    "dcc1115aeaf87118736e86f3e3eb85bf5541281c": "Random forest classifier",
    "c74185bced810449c5f438f11ed6a578d1e359b4": "available",
    "88e5d37617e14d6976cc602a168332fc23644f19": "Wikipedia conversations dataset and ChangeMyView (CMV)",
    "45f7c03a686b68179cadb1413c5f3c1d373328bd": "a dataset containing over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses",
    "a2015f02dfb376bf9b218d1c897018f4e70424d7": "over 45,000 scholarly articles",
    "f697d00a82750b14376fe20a5a2b249e98bebe9b": "Bi-LSTM-CRF",
    "e0e379e546f1da9da874a2e90c79b41c60feb817": "",
    "70148c8d0f345ea36200d5ba19d021924d98e759": "The phenomenon by which the perception of what we hear can be influenced by what we see",
    "27cf16bc9ef71761b9df6217f00f39f21130ce15": "Yes",
    "627b8d7b5b985394428c974aca5ba0c1bbbba377": "700",
    "126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0": "the data released by BIBREF11",
    "7e4ef0a4debc048b244b61b4f7dc2518b5b466c0": "long contexts",
    "b68f72aed961d5ba152e9dc50345e1e832196a76": "0.7 BLEU",
    "cf874cd9023d901e10aa8664b813d32501e7e4d2": "Named Entity Recognition",
    "42084c41343e5a6ae58a22e5bfc5ce987b5173de": "Yes",
    "b637d6393ef3af7462917b81861531022b291933": "Yes",
    "8b9c12df9f89040f1485b3847a29f11b5c9262e0": "Python",
    "72e4e26d0dd79c590c28b10938952a9f9497ff1e": "",
    "63b92dcc701ec77fdb3355ede5d37d2fbf057bcc": "a CNN-RNN based image-to-poem net combined with a seq2seq model with parallel text corpus for text style transfer synthesizes Shakespeare-style prose for a given painting.",
    "58ee0cbf1d8e3711c617b1cd3d7aca8620e26187": "the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.",
    "f71b52e00e0be80c926f153b9fe0a06dd93af11e": "3.9",
    "8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe": "large",
    "54e945ea4b014e11ed4e1e61abc2aa9e68fea310": "29.65",
    "df0257ab04686ddf1c6c4d9b0529a7632330b98e": "a more complex Hierarchical-Seq2Seq model or a better encoder representation based on knowledge graphs",
    "568fb7989a133564d84911e7cb58e4d8748243ef": "Go-Explore",
    "2c947447d81252397839d58c75ebcc71b34379b5": "CoinCollector and CookingWorld",
    "c01784b995f6594fdb23d7b62f20a35ae73eaa77": "By comparing the performance on unseen games",
    "3415762847ed13acc3c90de60e3ef42612bc49af": "improved",
    "223dc2b9ea34addc0f502003c2e1c1141f6b36a7": "gradient-based reward learning",
    "e1ab11885f72b4658263a60751d956ba661c1d61": "four Spanish subtasks",
    "c85b6f9bafc4c64fc538108ab40a0590a2f5768e": "second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc)",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "machine translation platform Apertium",
    "0f6216b9e4e59252b0c1adfd1a848635437dfcdc": "a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment",
    "22ccee453e37536ddb0c1c1d17b0dbac04c6c607": "English",
    "d00bbeda2a45495e6261548710afa6b21ea32870": "applied",
    "71b1af123fe292fd9950b8439db834212f0b0e32": "human raters",
    "a616a3f0d244368ec588f04dfbc37d77fda01b4c": "English and eleven other languages",
    "8e44c02c2d9fa56fb74ace35ee70a5add50b52ae": "Yes",
    "1522ccedbb1f668958f24cca070f640274bc2549": "baseline method",
    "97466a37525536086ed5d6e5ed143df085682318": "In this section",
    "e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a": "low-context importance annotation",
    "d6191c4643201262a770947fc95a613f57bedb6b": "49 clusters of 100 web pages on educational topics",
    "ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8": "A labeled graph showing concepts as nodes and relationships between them as edges",
    "fc4ae12576ea3a85ea6d150b46938890d63a7d18": "",
    "19cf7884c0c509c189b1e74fe92c149ff59e444b": "from the approximated joint probability",
    "ecd5770cf8cb12cb34285e26ab834301c17c53e1": "LSTM-CNN",
    "4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729": "The data used for training is annotated by keeping 248,349 visual questions (i.e., Training questions 2015 v1.0) for training and the remaining 121,512 visual questions (i.e., Validation questions 2015 v1.0) for testing.",
    "5529f26f72ce47440c2a64248063a6d5892b9fde": "loss function analysis",
    "f85ca6135b101736f5c16c5b5d40895280016023": "transformer and our model",
    "5fa36dc8f7c4e65acb962fc484989d20b8fdaeec": "Yes",
    "d98847340e46ffe381992f1a594e75d3fb8d385e": "RQE",
    "7006c66a15477b917656f435d66f63760d33a304": "",
    "a15bc19674d48cd9919ad1cf152bf49c88f4417d": "user utterances of the training data set",
    "440faf8d0af8291d324977ad0f68c8d661fe365e": "Reuters-8 dataset",
    "0ec56e15005a627d0b478a67fd627a9d85c3920e": "data that is close to a normal distribution",
    "a712718e6596ba946f29a99838d82f95b9ebb1ce": "big",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "TF-IDF features, Doc2vec model",
    "dfca00be3284cc555a6a4eac4831471fb1f5875b": "balanced",
    "a9a532399237b514c1227f2d6be8601474e669be": "UM Inventory",
    "26126068d72408555bcb52977cd669faf660bdf7": "",
    "660284b0a21fe3801e64dc9e0e51da5400223fe3": "",
    "c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd": "WER scores",
    "f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3": "To find out the impact of gender balance on ASR performance trained on broadcast recordings",
    "a253749e3b4c4f340778235f640ce694642a4555": "ESTER1, ESTER2, ETAPE and REPERE",
    "1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891": "2",
    "777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47": "65% of the speakers are men, speaking more than 75% of the time.",
    "2da4c3679111dd92a1d0869dae353ebe5989dfd2": "ESTER1, ESTER2, ETAPE and REPERE",
    "b7c3f3942a07c118e57130bc4c3ec4adc431d725": "ASGD Weight-Dropped Long Short Term Memory (AWD_LSTM) model",
    "a5505e25ee9ae84090e1442034ddbb3cedabcf04": "0.8099 and 0.8254",
    "1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf": "Yes",
    "7fa3c2c0cf7f559d43e84076a9113a390c5ba03a": "Yes",
    "9a7ba5ed1779c664d2cac92494a43517d3e87c96": "a collection of sentences",
    "662870a90890c620a964720b2ca122a1139410ea": "language",
    "92d1a6df3041667dc662376938bc65527a5a1c3c": "Yes",
    "12159f04e0427fe33fa05af6ba8c950f1a5ce5ea": "the type of word vectors and the number of clusters",
    "a4a1fcef760b133e9aa876ac28145ad98a609927": "dimensionality and number of clusters",
    "63bb2040fa107c5296351c2b5f0312336dad2863": "k-means",
    "01f4a0a19467947a8f3bdd7ec9fac75b5222d710": "INLINEFORM0 and INLINEFORM1",
    "7784d321ccc64db5141113b6783e4ba92fdd4b20": "neural network-based approaches to grammar induction",
    "218615a005f7f00606223005fef22c07057d9d77": "From the context \"We evaluate our models on the Penn Treebank (PTB) BIBREF91, BIBREF92, BIBREF93, BIBREF94, BIBREF95, BIBREF96, BIBREF97, BIBREF98, BIBREF99, BIBREF100, BIBREF101, BIBREF102, BIBREF103, BIBREF104, BIBREF105, BIBREF106, BIBREF107, BIBREF108, BIBREF109, BIBREF110, BIBREF111, BIBREF112, BIBREF113, BIBREF114, BIBREF115, BIBREF116, BIBREF117, BIBREF118, BIBREF119, BIBREF120, BIBREF121, BIBREF122, BIBREF123, BIBREF124, BIBREF125, BIBREF126, BIBREF127, BIBREF128, BIBREF129, BIBREF130, BIBREF131, BIBREF132, BIBREF133, BIBREF134, BIBREF135, BIBREF136, BIBREF137, BIBREF138, BIBREF139, BIBREF140, BIBREF141, BIBREF142, BIBREF143, BIBREF144, BIBREF145, BIBREF146, BIBREF147, BIBREF148, BIBREF149, BIBREF150, BIBREF151, BIBREF152, BIBREF153, BIBREF154, BIBREF155, BIBREF156, BIBREF157, BIBREF158, BIBREF159, BIBREF160, BIBREF161, BIBREF162, BIBREF163, BIBREF164, BIBREF165, BIBREF166, BIBREF167, BIBREF168, BIBREF169, BIBREF170, BIBREF171, BIBREF172, BIBREF173, BIBREF174, BIBREF175, BIBREF176, BIBREF177, BIBREF178, BIBREF179, BIBREF180, BIBREF181, BIBREF182, BIBREF183, BIBREF184, BIBREF185, BIBREF186, BIB",
    "867290103f762e1ddfa6f2ea30dd0a327f595182": "CTB",
    "907b3af3cfaf68fe188de9467ed1260e52ec6cf1": "Table TABREF23",
    "56a8826cbee49560592b2d4b47b18ada236a12b9": "manually annotated",
    "968b7c3553a668ba88da105eff067d57f393c63f": "tweets that have been retweeted more than 1000 times",
    "f03df5d99b753dc4833ef27b32bb95ba53d790ee": "",
    "a8f51b4e334a917702422782329d97304a2fe139": "1000",
    "dca86fbe1d57b44986055b282a03c15ef7882e51": "manually annotated by a single person",
    "27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa": "SMT and various NMT models",
    "b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72": "the internet",
    "808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc": "327",
    "36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd": "dataset model character's profiles by using a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise.",
    "f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6": "0.7815",
    "357eb9f0c07fa45e482d998a8268bd737beb827f": "Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline",
    "ad08b215dca538930ef1f50b4e49cd25527028ad": "Google Cloud tools",
    "31101dc9937f108e27e08a5f34be44f0090b8b6b": "off-the-shelf Google Cloud tools",
    "e4a315e9c190cf96493eefe04ce4ba6ae6894550": "Figure FIGREF2",
    "6263b2cba18207474786b303852d2f0d7068d4b6": "English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian",
    "c1c44fd96c3fa6e16949ae8fa453e511c6435c68": "To enhance the decoder using BERT's contextualized representations",
    "d28d86524292506d4b24ae2d486725a6d57a3db3": "",
    "feafcc1c4026d7f55a2c8ce7850d7e12030b5c22": "end-to-end",
    "63488da6c7aff9e374561a24ba224e9ce7f65e40": "2019",
    "c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c": "Yes",
    "a9337636b52de375c852682a2561af2c1db5ec63": "Yes",
    "45a5961a4e1d1c22874c4918e5c98bd3c0a670b3": "seven",
    "30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1": "Section SECREF2",
    "5c6fa86757410aee6f5a0762328637de03a569e9": "DNN models can be used for cyberbullying detection on various topics across multiple SMPs using three datasets and four DNN models. These models coupled with transfer learning beat state of the art results for all three datasets.",
    "7e38e0279a620d3df05ab9b5e2795044f18d4471": "racism, sexism, and personal attack",
    "8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92": "No",
    "03ebb29c08375afc42a957c7b2dc1a42bed7b713": "the obtained results",
    "9cf070d6671ee4a6353f79a165aa648309e01295": "1500 sentences",
    "87bc6f83f7f90df3c6c37659139b92657c3f7a38": "The DD algorithm tries to enforce agreement between the two parse trees subject to the given alignments.",
    "01e2d10178347d177519f792f86f25575106ddc7": "Switchboard Telephone Speech Corpus, Turkish, Uzbek and Mandarin",
    "021bfb7e180d67112b74f05ecb3fa13acc036c86": "UTD",
    "d201b9992809142fe59ae74508bc576f8ca538ff": "Yes",
    "c4628d965983934d7a2a9797a2de6a411629d5bc": "Yes",
    "bd419f4094186a5ce74ba6ac1622b24e29e553f4": "30.3%",
    "11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af": "three simple baselines for the RUN task",
    "1269c5d8f61e821ee0029080c5ba2500421d5fa6": "data augmentation methods",
    "e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb": "the first Japanese INLINEFORM0 Vietnamese NMT systems",
    "b9ea841b817ba23281c95c7a769873b840dee8d5": "Yes",
    "219af68afeaecabdfd279f439f10ba7c231736e4": "TED talks dataset",
    "a66a275a817f980c36e0b67d2e00bd823f63abf8": "average score given by three evaluators",
    "b6f466e0fdcb310ecd212fd90396d9d13e0c0504": "the data already contain them",
    "62ea141d0fb342dfb97c69b49d1c978665b93b3c": "punctuation, word order mistakes and grammatical errors",
    "a32c792a0cef03218bf66322245677fc2d5e5a31": "The parallel data differ in terms of style.",
    "0101ebfbaba75fd47868ad0c796ac44ebc19c566": "padding sentence number in each passage to 101",
    "50cb50657572e315fd452a89f3e0be465094b66f": "SQuaD",
    "981fd79dd69581659cb1d4e2b29178e82681eb4d": "The proposed model is an extension of the RNN encoder-decoder.",
    "03e9ac1a2d90152cd041342a11293a1ebd33bcc3": "NLG datasets",
    "ef396a34436072cb3c40b0c9bc9179fee4a168ae": "text classification and text semantic matching",
    "04bde1d2b445f971e97bb46ade2d0290981c7a32": "a step size and a direction",
    "bfbd6040cb95b179118557352e8e3899ef25c525": "singer embedding and the extracted pitch",
    "d6e353e0231d09fd5dcba493544d53706f3fe1ab": "Mean Opinion Score (MOS)",
    "7bd6a6ec230e1efb27d691762cc0674237dc7967": "Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20",
    "6aaf12505add25dd133c7b0dafe8f4fe966d1f1d": "LSTM",
    "73906462bd3415f23d6378590a5ba28709b17605": "QA benchmarks",
    "5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11": "English and 100 languages",
    "88bf368491f9613767f696f84b4bb1f5a7d7cb48": "The professional translation or the machine translation introduce the artifacts.",
    "0737954caf66f2b4c898b356d2a3c43748b9706b": "",
    "664b3eadc12c8dde309e8bbd59e9af961a433cde": "substantial",
    "b3307d5b68c57a074c483636affee41054be06d1": "QA benchmarks",
    "bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31": "English and 100 languages",
    "12d7055baf5bffb6e9e95e977c000ef2e77a4362": "higher quality",
    "498c0229f831c82a5eb494cdb3547452112a66a0": "routing strategies based on instance difficulty",
    "8c48c726bb17a17d70ab29db4d65a93030dd5382": "57%",
    "89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f": "test set",
    "06b5272774ec43ee5facfa7111033386f06cf448": "sentence",
    "08b57deb237f15061e4029b6718f1393fa26acce": "located in the US and hired on the BIBREF22 platform",
    "9b7655d39c7a19a23eb8944568eb5618042b9026": "NLTK, Stanford CoreNLP, and TwitterNLP",
    "cd06d775f491b4a17c9d616a8729fd45aa2e79bf": "neutral",
    "1329280df5ee9e902b2742bde4a97bc3e6573ff3": "No",
    "58c6737070ef559e9220a8d08adc481fdcd53a24": "correct classification rate (CCR)",
    "0af16b164db20d8569df4ce688d5a62c861ace0b": "BOW, TFIDF, neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN, GloVe",
    "78a4ec72d76f0a736a4a01369a42b092922203b6": "the completed scripts of all ten seasons of Friends TV shows from emorynlp",
    "6a14379fee26a39631aebd0e14511ce3756e42ad": "FriendsBERT and ChatBERT",
    "81588e0e207303c2867c896f3911a54a1ef7c874": "the scripts of the Friends TV sitcom and Facebook messenger chats",
    "dd09db5eb321083dba16c2550676e60682f9a0cd": "anger, joy, neutral, and sadness",
    "40c0f97c3547232d6aa039fcb330f142668dea4b": "Yes",
    "777217e025132ddc173cf33747ee590628a8f62f": "four approaches based on sefe with two efe BIBREF10 baselines",
    "2dbf6fe095cd879a9bf40f110b7b72c8bdde9475": "hierarchical modelling approach",
    "7d483077ed7f2f504d59f4fc2f162741fa5ac23b": "Poisson sefe downweighs the contribution of the zeros in the objective function",
    "de830c534c23f103288c198eb19174c76bfd38a1": "the words have the strongest shift in usage",
    "b0d66760829f111b8fad0bd81ca331ddd943ef41": "improving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in textGAN",
    "ae7c93646aa5f3206cd759904965b4d484d12f83": "generate more sensational headlines",
    "d1ec42b2b5a3c956ff528543636e024bfde5e5ba": "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and Test set",
    "1dac4bc5af239024566fcb0f43bb9ff1c248ecec": "LCSTS dataset",
    "3bf0306e9bd044f723e38170c13455877b2aeec3": "ARL",
    "2858620e0498db2f2224bfbed5263432f0570832": "",
    "545e92833b0ad4ba32eac5997edecf97a366a244": "the structure of the word co-occurrence networks",
    "cb12c19f9d14bef7b2f778892d9071eea2d6c63d": "The system that has the best performance on 10 standard text classification datasets",
    "9193006f359c53eb937deff1248ee3317978e576": "Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013",
    "bc67b91dd73acded2d52fd4fee732b7a9722ea8b": "recursive neighborhood aggregation",
    "49c32a2a64eb41381e5f12ccea4150cac9f3303d": "high-precision but a low recall",
    "bbb77f2d6685c9257763ca38afaaef29044b4018": "the system with 3.7% improvement",
    "22732cb9476e521452bf0538f3fdb94cf3867651": "emoticons, laughter expressions such as \u201clol\u201d etc",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "lexical,implicit incongruity and explicit incongruity features",
    "74b338d5352fe1a6fd592e38269a4c81fe79b866": "simple gaze-based features and complex features derived from a graph structure",
    "d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf": "data selection, conceptualization, operationalization, analysis and the interpretation of results",
    "f903396d943541a8cc65edefb04ca37814ed30dd": "Internet Archive\u2019s Wayback Machine",
    "ba28ce9a2f7e8524243adf288cc3f11055e667bb": "Yes",
    "975e60535724f4149c7488699a199ba2920a062c": "operationalizations, subsequent analyses and reproducibility efforts",
    "b970f48d30775d3468952795bc72976baab3438e": "The issues that are not on the forefront of computational text analysis"
}