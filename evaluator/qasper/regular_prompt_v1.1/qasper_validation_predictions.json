{
    "b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54": "multilingual NMT",
    "f5e6f43454332e0521a778db0b769481e23e7682": "pivoting, pivot-synthetic",
    "9a05a5f4351db75da371f7ac12eb0b03607c4b87": "Europarl and MultiUN",
    "5eda469a8a77f028d0c5f1acd296111085614537": "Fr-En-Es, De-En-Fr, Ro-En-De",
    "18c5d366b1da8447b5404eab71f4cc658ba12e6f": "Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF",
    "b5e4866f0685299f1d7af267bbcc4afe2aab806f": "ilur.am",
    "1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590": "No",
    "b6ae8e10c6a0d34c834f18f66ab730b670fb528c": "politics, business, science, AskReddit, and the Reddit frontpage",
    "a87a009c242d57c51fc94fe312af5e02070f898b": "A predictive model that analyzes new content.",
    "ef4dba073d24042f24886580ae77add5326f2130": "0.9",
    "2df4a045a9cd7b44874340b6fdf9308d3c55327a": "Amazon Mechanical Turk",
    "a313e98994fc039a82aa2447c411dda92c65a470": "Using a bilingual dictionary (Google Translate word translation in our case).",
    "37861be6aecd9242c4fdccdfcd06e48f3f1f8f81": "5",
    "7e62a53823aba08bc26b2812db016f5ce6159565": "IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus, ILCI multilingual parallel corpus",
    "9eabb54c2408dac24f00f92cf1061258c7ea2e1a": "Information on text structure, typography, and images",
    "3d013f15796ae7fed5272183a166c45f16e24e39": "Font type and font style",
    "9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc": "WebQSP and SimpleQuestions",
    "d3aa0449708cc861a51551b128d73e11d62207d2": "HR-BiLSTM",
    "cfbec1ef032ac968560a7c76dec70faf1269b27c": "Knowledge Base Question Answering",
    "c0e341c4d2253eb42c8840381b082aae274eddad": "Relation Detection",
    "1ec152119cf756b16191b236c85522afeed11f59": "The authors propose to measure the self-similarity of words in different layers.",
    "891c2001d6baaaf0da4e65b647402acac621a7d2": "By taking the first principal component of its contextualized representations in a given layer.",
    "66c96c297c2cffdf5013bab5e95b59101cb38655": "0.979",
    "6b53e1f46ae4ba9b75117fc6e593abded89366be": "CRF, spaCy",
    "c0bee6539eb6956a7347daa9d2419b367bd02064": "Yes",
    "3de0487276bb5961586acc6e9f82934ef8cb668c": "NUBes-PHI and MEDDOCAN",
    "113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab": "The power of these embeddings is demonstrated by what happens when one feeds the same input word to the model with different language tokens, as is seen in Table TABREF30. Impressively, this even works when the source sequence is in the wrong script for the language, as is seen in the entry for Arabic.",
    "0752d71a0a1f73b3482a888313622ce9e9870d6e": "wFST",
    "55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3": "Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)",
    "4eaf9787f51cd7cdc45eb85cf223d752328c6ee4": "Wiktionary",
    "fb2b536dc8e442dffab408db992b971e86548158": "49% for CS and 41% for Stat2015",
    "31735ec3d83c40b79d11df5c34154849aeb3fb47": "20 evaluators were recruited from our institution",
    "10d450960907091f13e0be55f40bcb96f44dd074": "Yes",
    "b5608076d91450b0d295ad14c3e3a90d7e168d0e": "Yes",
    "c21b87c97d1afac85ece2450ee76d01c946de668": "pointer networks with coverage mechanism (PG-net)",
    "d087539e6a38c42f0a521ff2173ef42c0733878e": "Because the student and teacher model output spaces are not identical, intermediate model outputs may prove hard to align.",
    "efe9bad55107a6be7704ed97ecce948a8ca7b1d2": "Model weight quantization techniques",
    "71e4ba4e87e6596aeca187127c0d088df6570c57": "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.",
    "7561a968470a8936d10e1ba722d2f38b5a9a4d38": "30,000 images",
    "6d4400f45bd97b812e946b8a682b018826e841f1": "I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.",
    "26c2e1eb12143d985e4fb50543cf0d1eb4395e67": "Stereotypes",
    "f17ca24b135f9fe6bb25dc5084b13e1637ec7744": "Implicit-Comparison",
    "bd5bd1765362c2d972a762ca12675108754aa437": "3.02",
    "d9b6c61fc6d29ad399d27b931b6cb7b1117b314a": "In this paper",
    "d27438b11bc70e706431dda0af2b1c0b0d209f96": "BIBREF1, BIBREF2, BIBREF3",
    "8d4ac4afbf5b14f412171729ceb5e822afcfa3f4": "No",
    "3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f": "Gender, Industry, and Location",
    "07d15501a599bae7eb4a9ead63e9df3d55b3dc35": "By analyzing the words they use",
    "99e78c390932594bd833be0f5c890af5c605d808": "The baseline is the performance of a model that is not trained on the task.",
    "861187338c5ad445b9acddba8f2c7688785667b1": "Yes",
    "f161e6d5aecf8fae3a26374dcb3e4e1b40530c95": "ELMo, BERT, and ClinicalBERT",
    "12c50dea84f9a8845795fa8b8c1679328bd66246": "CSAT, 20newsgroups and Fisher",
    "0810b43404686ddfe4ca84783477ae300fdd2ea4": "transformer layer",
    "455d4ef8611f62b1361be4f6387b222858bb5e56": "CrowdFlower",
    "bc16ce6e9c61ae13d46970ebe6c4728a47f8f425": "8533",
    "1ff0fccf0dca95a6630380c84b0422bed854269a": "Accuracy and efficiency",
    "3d7d865e905295d11f1e85af5fa89b210e3e9fdf": "100",
    "2ad4d3d222f5237ed97923640bc8e199409cbe52": "crowdworkers on Amazon Mechanical Turk (AMT)",
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "Unif and Stopword",
    "ee417fea65f9b1029455797671da0840c8c1abbe": "Yes, they use off-the-shelf NLP systems to build their assitant.",
    "ca5a82b54cb707c9b947aa8445aac51ea218b23a": "It augments the resources with structured and to some extent labeled training data for further possible implementation of learnable dialogue components.",
    "da55bd769721b878dd17f07f124a37a0a165db02": "The assistant handles repetitive and time-consuming activities like entering data into a system, registering into accounts, and accomplishing straightforward but repetitive workflows.",
    "feb448860918ef5b905bb25d7b855ba389117c1f": "All India Radio news channel",
    "4bc2784be43d599000cb71d31928908250d4cef3": "It is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). Which means that we compute the matrix V for both normal cluster K and ghost clusters G, but we will not include the vectors belongs to ghost cluster from V during concatenation of the features. Due to which, during feature aggregation stage the contribution of the noisy and unwanted features to normal VLAD clusters are assigned less weights while Ghost clusters absorb most of the weight. We illustrate this in Figure 1(Right Side), where the ghost clusters are shown in red color. We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into",
    "75df70ce7aa714ec4c6456d0c51f82a16227f2cb": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English",
    "6424e442b34a576f904d9649d63acf1e4fdefdfc": "Wall Street Journal (WSJ) portion of the Penn Treebank",
    "5eabfc6cc8aa8a99e6e42514ef9584569cb75dec": "Yes",
    "887c6727e9f25ade61b4853a869fe712fe0b703d": "(1) INLINEFORM0 and (2) INLINEFORM1 exists.",
    "6236762b5631d9e395f81e1ebccc4bf3ab9b24ac": "Yes, we have provided two examples in the paper.",
    "31d695ba855d821d3e5cdb7bea638c7dbb7c87c7": "GRU",
    "b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab": "Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask",
    "a99fdd34422f4231442c220c97eafc26c76508dd": "No",
    "2c78993524ca62bf1f525b60f2220a374d0e3535": "Reuters, TDT",
    "d604f5fb114169f75f9a38fab18c1e866c5ac28b": "F1 score",
    "1d3e914d0890fc09311a70de0b20974bf7f0c9fe": "BC5CDR, BC5CDR-disease, BC5CDR-chemical, BC5CDR-gene, BC5CDR-species, JNLPBA, LINNAEUS and NCBI-disease.",
    "16535db1d73a9373ffe9d6eedaa2369cefd91ac4": "PubMed+PMC",
    "de0b650022ad8693465242ded169313419eed7d9": "No",
    "2b3cac7af10d358d4081083962d03ea2798cf622": "No",
    "897ba53ef44f658c128125edd26abf605060fb13": "No",
    "41ac23e32bf208b69414f4b687c4f324c6132464": "German and French",
    "e97186c51d4af490dba6faaf833d269c8256426c": "Yes. We show that our datasets are challenging for state-of-the-art models, and that they are not subject to systematic biases.",
    "5bb3c27606c59d73fd6944ba7382096de4fa58d8": "multiple choice question answering",
    "8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b": "8-15%",
    "85590bb26fed01a802241bc537d85ba5ef1c6dc2": "They use a baseline model that only uses the answer choices to predict the correct answer.",
    "75ff6e425ce304a35f18c0230c0d13d3913a31a9": "Yes, but only for 1-hop reasoning.",
    "5cb610d3d5d7d447b4cd5736d6a7d8262140af58": "They train models for 50 different random combinations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2.",
    "c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a": "English, French, German, Russian, Spanish, Swedish, Finnish",
    "b9d168da5321a7d7b812c52bb102a05210fe45bd": "Yes",
    "0c234db3b380c27c4c70579a5d6948e1e3b24ff1": "LSTM",
    "fa527becb8e2551f4fd2ae840dbd4a68971349e0": "LSTM",
    "32a3c248b928d4066ce00bbb0053534ee62596e7": "MSD prediction is the auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process\u2014the task is to predict the MSD tag of the target form.",
    "c9b8d3858c112859eabee54248b874331c48f71b": "Morphosyntactic",
    "45e9533586199bde19313cd43b3d0ecadcaf7a33": "Yes",
    "d3dbb5c22ef204d85707d2d24284cc77fa816b6c": "R.M.-Reader + Verifier",
    "a5e49cdb91d9fd0ca625cc1ede236d3d4672403c": "SAN",
    "aefa333b2cf0a4000cd40566149816f5b36135e7": "Accuracy",
    "c5abe97625b9e1c8de8208e15d59c704a597b88c": "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.",
    "eb2d5edcdfe18bd708348283f92a32294bb193a5": "KG-A2C, A2C, A2C-chained, A2C-Explore",
    "88ab7811662157680144ed3fdd00939e36552672": "KG-A2C-chained and KG-A2C-Explore",
    "cb196725edc9cdb2c54b72364f3bbf7c76471490": "Yes",
    "286078813136943dfafb5155ee15d2429e7601d9": "LiLi is much better than the baseline.",
    "8f16dc7d7be0d284069841e456ebb2c69575b32b": "We use the path-ranking algorithm (PR) BIBREF11, BIBREF17 as the baseline.",
    "a7d020120a45c39bee624f65443e09b895c10533": "LiLi mimics how humans acquire knowledge and perform inference during an interactive conversation by formulating a query-specific inference strategy and executing it.",
    "585626d18a20d304ae7df228c2128da542d248ff": "Matthews correlation coefficient (MCC)",
    "bfc2dc913e7b78f3bd45e5449d71383d0aa4a890": "The general knowledge learning engine consists of two interconnected models",
    "6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de": "4",
    "b46c0015a122ee5fb95c2a45691cb97f80de1bb6": "CNN",
    "5b7a4994bfdbf8882f391adf1cd2218dbc2255a0": "Naive, mSDA, NaiveNN, AuxNN, ADAN, MMD",
    "9176d2ba1c638cdec334971c4c7f1bb959495a8e": "Book (B), DVDs (D), Electronics (E), and Kitchen (K)",
    "0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c": "No",
    "5e324846a99a5573cd2e843d1657e87f4eb22fa6": "They modify the Bayesian classifier, removing the bias towards frequent labels in the training data.",
    "2ccc26e11df4eb26fcccdd1f446dc749aff5d572": "Yes, they report results only on English data.",
    "f318a2851d7061f05a5b32b94251f943480fbd15": "The authors conclude that the emotional appeal of ISIS and Catholic materials are similar.",
    "6bbbb9933aab97ce2342200447c6322527427061": "Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words.",
    "2007bfb8f66e88a235c3a8d8c0a3b3dd88734706": "Topic modeling and emotion detection methods are used to analyze the texts from violent and non-violent religious groups.",
    "d859cc37799a508bbbe4270ed291ca6394afce2c": "Topic modeling",
    "50e80cfa84200717921840fddcf3b051a9216ad8": "No",
    "b1bc9ae9d40e7065343c12f860a461c7c730a612": "ShapeWorldICE",
    "63a1cbe66fd58ff0ead895a8bac1198c38c008aa": "Show&Tell model and LRCN1u model",
    "509af1f11bd6f3db59284258e18fdfebe86cae47": "By comparing the number of observed language constructions to the number of possible language constructions.",
    "23e16c1173b7def2c5cb56053b57047c9971e3bb": "LSTM",
    "d78f7f84a76a07b777d4092cb58161528ca3803c": "A backward greedy search over each sentence's label sequence to identify word boundaries.",
    "9da1e124d28b488b0d94998d32aa2fa8a5ebec51": "BIBREF15, BIBREF19, BIBREF20",
    "37be0d479480211291e068d0d3823ad0c13321d3": "The model performance on target language reading comprehension is not good.",
    "a3d9b101765048f4b61cbd3eaa2439582ebb5c77": "English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese",
    "009ce6f2bea67e7df911b3f93443b23467c9f4a1": "QANet",
    "55569d0a4586d20c01268a80a7e31a17a18198e2": "The model learns to encode semantic and syntactic information in language-agnostic ways.",
    "7cd22ca9e107d2b13a7cc94252aaa9007976b338": "No",
    "adbf33c6144b2f5c40d0c6a328a92687a476f371": "Yes",
    "f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24": "PER, LOC, ORG, MISC",
    "a0543b4afda15ea47c1e623c7f00d4aaca045be0": "Yes.",
    "1591068b747c94f45b948e12edafe74b5e721047": "10K",
    "193ee49ae0f8827a6e67388a10da59e137e7769f": "Masked Document Generation (MDG) learns to recover a document with a masked span of tokens.",
    "ed2eb4e54b641b7670ab5a7060c7b16c628699ab": "Sentence Reordering",
    "beac555c4aea76c88f19db7cc901fa638765c250": "Other relevant information",
    "91e326fde8b0a538bc34d419541b5990d8aae14b": "WMT15 German-to-English",
    "044f922604b4b3f42ae381419fd5cd5624fa0637": "Attention is different from alignment in the case of verbs.",
    "f94b53db307685d572aefad52cd55f53d23769c2": "They use Fisher Information Matrix",
    "aa7d327ef98f9f9847b447d4def04889b4508d7a": "190 hours ( INLINEFORM1 100K instances)",
    "b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0": "RNN",
    "551457ed34ca7fc0878c85bc664b135c21059b58": "unlabeled dataset",
    "0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8": "Tree-LSTM, TG-RNN, TE-RNN, TE-RNTN, DC-TreeLSTM, SPINN, and Latent Tree-LSTM",
    "4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94": "They considered off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers.",
    "a4d115220438c0ded06a91ad62337061389a6747": "Facebook",
    "2c7e94a65f5f532aa31d3e538dcab0468a43b264": "Crowdsourcing",
    "149da739b1c19a157880d9d4827f0b692006aa2c": "SVM, MLP, FastText, CNN, BERT, Platforms",
    "27de1d499348e17fec324d0ef00361a490659988": "23,700 queries",
    "cfcdd73e712caf552ba44d0aa264d8dace65a589": "https://github.com/clinc/oos-eval",
    "23b2901264bda91045258b5d4120879ae292e950": "+0.58 for MRPC and +0.73 for QQP",
    "b5bc34e1e381dbf972d0b594fe8c66ff75305d71": "+0.29 and +0.96",
    "72f7ef55e150e16dcf97fe443aff9971a32414ef": "+1.86, +1.80, +2.19",
    "20e38438471266ce021817c6364f6a46d01564f2": "We multiply the soft probability $p$ with a decaying factor $(1-p)$, changing Eq.DISPLAY_FORM22 to the following form:",
    "28067da818e3f61f8b5152c0d42a531bf0f987d4": "Ngrams of length 1",
    "bf3b27a4f4be1f9ae31319877fd0c75c03126fd5": "500",
    "ffa7f91d6406da11ddf415ef094aaf28f3c3872d": "0.1",
    "b634ff1607ce5756655e61b9a6f18bc736f84c83": "Consumer Discretionary",
    "2f901dab6b757e12763b23ae8b37ae2e517a2271": "German\u2013English",
    "b591853e938984e6069d738371500ebdec50d256": "IMDb movie review dataset",
    "a130306c6662ff489df13fb3f8faa7cba8c52a21": "f-pooling",
    "b1cf5739467ba90059add58d11b73d075a11ec86": "Yes",
    "2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd": "Embedding Layer, Neural Network Layers, Loss Function, Metrics",
    "4f253dfced6a749bf57a1b4984dc962ce9550184": "The authors conducted a survey among engineers and identified a spectrum of three typical personas.",
    "dc1cec824507fc85ac1ba87882fe1e422ff6cffb": "Bengali question corpus",
    "f428618ca9c017e0c9c2a23515dab30a7660f65f": "Multi Layer Perceptron (MLP), Support Vector Machine (SVM), Naive Bayesian Classifier (NBC), Stochastic Gradient Descent (SGD), Gradient Boosting Classifier (GBC), K Nearest Neighbour (K-NN) and Random Forest (RF)",
    "8ce11515634236165cdb06ba80b9a36a8b9099a2": "Yes",
    "6024039bbd1118c5dab86c41cce1175d99f10a25": "ASPEC and NTCIR",
    "de5b6c25e35b3a6c5e40e350fc5e52c160b33490": "Their model outperforms existing models by a wide margin.",
    "b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f": "Global context is the context of the whole document, while local context is the context of the topic segment that sentence falls into.",
    "6bfba3ddca5101ed15256fca75fcdc95a53cece7": "Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans, Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating clich\u00e9, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man",
    "df5a4505edccc0ee11349ed6e7958cf6b84c9ed4": "The corpus for the task was annotated by A Data Pro, a company that performs high-quality manual annotations.",
    "fd753ab5177d7bd27db0e0afc12411876ee607df": "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature",
    "88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42": "A second order co-ocurrence matrix is a matrix that contains the co-occurrence frequencies of two words in a corpus.",
    "4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3": "4",
    "8b3d3953454c88bde88181897a7a2c0c8dd87e23": "word2vec",
    "784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f": "Yes, they do.",
    "7705dd04acedaefee30d8b2c9978537afb2040dc": "The word level model is a two-layer LSTM with 200 hidden units. The character level model is a two-layer gated-feedback LSTM with 200 hidden units.",
    "44497509fdf5e87cff05cdcbe254fbd288d857ad": "AER scores",
    "0ee73909ac638903da4a0e5565c8571fc794ab96": "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive). For the comparison process, the RNNMorph and the RNNSearch + Word2Vec models\u2019 sentence level translations were individually ranked between each other, permitting the two translations to have ties in the ranking. The intra-annotator values were computed for these metrics and the scores are shown in Table TABREF32 BIBREF12, BIBREF13.",
    "1f07e837574519f2b696f3d6fa3230af0b931e5d": "No",
    "5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76": "They compare their model with existing systems without being trained or tuned on that particular dataset.",
    "729694a9fe1e05d329b7a4078a596fe606bc5a95": "The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%.",
    "1c997c268c68149ae6fb43d83ffcd53f0e7fe57e": "They use Wikidata",
    "5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde": "Speech synthesis",
    "f9bf6bef946012dd42835bf0c547c0de9c1d229f": "Annotations",
    "6a633811019e9323dc8549ad540550d27aa6d972": "Yes",
    "6b9b9e5d154cb963f6d921093539490daa5ebbae": "$\\mathit {CPMI_z}$ and $\\mathit {NNEGPMI}$",
    "bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2": "Semantic Textual Similarity (STSB) task, Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) probing tasks, part-of-speech (POS) tagging",
    "d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7": "It discards scale information.",
    "6844683935d0d8f588fa06530f5068bf3e1ed0c0": "Because $\\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus.",
    "8acab64ba72831633e8cc174d5469afecccf3ae9": "CALLHOME Spanish-English speech translation corpus",
    "53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa": "",
    "72755c2d79210857cfff60bfbcb55f83c71ada51": "11 hours",
    "7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569": "ELAN and P600, LAN and P600",
    "bd6dc38a9ac8d329114172194b0820766458dacc": "BIBREF0",
    "3ddff6b707767c3dd54d7104fe88b628765cae58": "16 datasets",
    "0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7": "16 languages",
    "06be47e2f50b902b05ebf1ff1c66051925f5c247": "No",
    "003d6f9722ddc2ee13e879fefafc315fb8e87cb9": "A country",
    "c88a846197b72d25e04ec55f00ee3e72f655504c": "UN General Debate Corpus",
    "4d28c99750095763c81bcd5544491a0ba51d9070": "They obtain tweets from celebrities from various domains, including politics, entertainment, and sports.",
    "78292bc57ee68fdb93ed45430d80acca25a9e916": "They created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. They did the same for a subset of ConceptNet that is easy to negate. They selected this subset by filtering for sentence length and extracting common queries.",
    "443d2448136364235389039cbead07e80922ec5c": "Sumy package",
    "aa6d956c2860f58fc9baea74c353c9d985b05605": "ROUGE unigram score",
    "4c18081ae3b676cc7831403d11bc070c10120f8e": "CLUTO and Carrot2 Lingo",
    "fb3d30d59ed49e87f63d3735b876d45c4c6b8939": "Accuracy, Precision, Recall, F-measure",
    "197b276d0610ebfacd57ab46b0b29f3033c96a40": "SVM with ADWS kernel",
    "e025061e199b121f2ac8f3d9637d9bf987d65cd5": "15.5",
    "61652a3da85196564401d616d251084a25ab4596": "4528 employees",
    "14b74ad5a6f5b0506511c9b454e9c464371ef8c4": "German-English, Japanese-English, and Romanian-English",
    "5c88d601e8fca96bffebfa9ef22331ecf31c6d75": "No",
    "71bd5db79635d48a0730163a9f2e8ef19a86cd66": "Semantics-altering grammatical modifiers",
    "9ecde59ffab3c57ec54591c3c7826a9188b2b270": "SQuAD, NewsQA, MultiRC, ReCoRd, HotpotQA, DROP",
    "005cca3c8ab6c3a166e315547a2259020f318ffb": "The proposed annotation schema is shown in Figure FIGREF10.",
    "af34051bf3e628c1e2a00b110bb84e5f018b419f": "Vanilla ST baseline, Pre-training baselines, Multi-task baselines",
    "022c365a14fdec406c7a945a1a18e7e79df37f08": "MT",
    "5260cb56b7d127772425583c5c28958c37cb9bea": "5 turns",
    "9b97805a0c093df405391a85e4d3ab447671c86a": "Exact Match (EM) and Macro-averaged F1 scores (F1)",
    "38f58f13c7f23442d5952c8caf126073a477bac0": "2% EM score and over 1.5% F1 score",
    "7ee5c45b127fb284a4a9e72bb9b980a602f7445a": "Dr.QA",
    "ddf5e1f600b9ce2e8f63213982ef4209bab01fd8": "Spoken-SQuAD",
    "27275fe9f6a9004639f9ac33c3a5767fea388a98": "dimension size, training epochs, window size and vocabulary size",
    "ef3567ce7301b28e34377e7b62c4ec9b496f00bf": "GMB",
    "7595260c5747aede0b32b7414e13899869209506": "IMDb",
    "c2d1387e08cf25cb6b1f482178cca58030e85b70": "Yes",
    "5a22293b055f5775081d6acdc0450f7bd5f5de04": "Nie et al.",
    "03c967763e51ef2537793db7902e2c9c17e43e95": "Conditional Copy (CC)",
    "26327ccebc620a73ba37a95aabe968864e3392b2": "promoting one's own points and addressing the opponent's points",
    "ababb79dd3c301f4541beafa181f6a6726839a10": "Intelligence Squared",
    "c2b8ee872b99f698b3d2082d57f9408a91e1b4c1": "84\u201394%",
    "8eefa116e3c3d3db751423cc4095d1c4153d3a5f": "CoNLL2003",
    "133eb4aa4394758be5f41744c60c99901b2bc01c": "No",
    "3fff37b9f68697d080dbd9d9008a63907137644e": "The results are not state-of-the-art.",
    "a778b8204a415b295f73b93623d09599f242f202": "Random Kitchen Sink approach is a method to explicitly map data vectors to a space where linear separation is possible.",
    "642e8cf1d39faa1cd985d16750cdc6696c52db2f": "attentional encoder-decoder networks",
    "493e971ee3f57a821ef1f67ef3cd47ade154e7c4": "CBOW, PV-DM, GloVe and EqEmb",
    "8dd8e5599fc56562f2acbc16dd8544689cddd938": "Euclidean distance computed between the context vector representations of the equations",
    "abe2393415e533cb06311e74ed1c5674cff8571f": "Word error rate (edit distance)",
    "00c57e45ac6afbdfa67350a57e81b4fad0ed2885": "No",
    "22714f6cad2d5c54c28823e7285dc85e8d6bc109": "Reduction, Selection, Rank",
    "82642d3111287abf736b781043d49536fe48c350": "Each tweet is annotated as no evidence of depression (e.g., \u201cCitizens fear an economic depression\") or evidence of depression (e.g., \u201cdepressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., \u201cfeeling down in the dumps\"), disturbed sleep (e.g., \u201canother restless night\"), or fatigue or loss of energy (e.g., \u201cthe fatigue is unbearable\") BIBREF10. For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0.",
    "5a81732d52f64e81f1f83e8fd3514251227efbc7": "Twitter dataset",
    "9a8b9ea3176d30da2453cac6e9347737c729a538": "The hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes.",
    "4477bb513d56e57732fba126944073d414d1f75f": "2010 i2b2/VA",
    "1b23c4535a6c10eb70bbc95313c465e4a547db5e": "Convolutional, NIN and Bi-LSTM layers",
    "0a75a52450ed866df3a304077769e1725a995bb7": "Attention-based Encoder Decoder for Raw Speech Recognition",
    "fd0a3e9c210163a55d3ed791e95ae3875184b8f8": "WSJ",
    "c37f65c9f0d543a35c784263b79236ccf1c44fac": "Inception V3",
    "584af673429c7f8621c6bf83362a37048daa0e5d": "The model's first component is a Recurrent Neural Network (RNN), more precisely an LSTM that summarizes the sequence of images. At every timestep $t$ the network takes as input an image $I_i$ where $i\\in \\lbrace 1,2,3,4,5\\rbrace $ from the sequence. At time $t=5$, the LSTM has encoded the 5 images and provides the sequence's context through its last hidden state denoted by $h_e^{(t)}$. The representation of the images was obtained through Inception V3.",
    "1be54c5b3ea67d837ffba2290a40c1e720d9587f": "Yes",
    "b08f88d1facefceb87e134ba2c1fa90035018e83": "No",
    "b06512c17d99f9339ffdab12cedbc63501ff527e": "No. We can simply use these embeddings as a drop-in replacement for existing embedding layers.",
    "fd8e23947095fe2230ffe1a478945829b09c8c95": "Random walk",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "No.",
    "3611a72f754de1e256fbd25b012197e1c24e8470": "No",
    "4c07c33dfaf4f3e6db55e377da6fa69825d0ba15": "300",
    "b1ce129678e37070e69f01332f1a8587e18e06b0": "3,216 tweets",
    "7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1": "RoBERTa",
    "0689904db9b00a814e3109fb1698086370a28fa2": "Distributed Bag of Words (DBOW)",
    "cc354c952b5aaed2d4d1e932175e008ff2d801dd": "African American and female",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "The sentences are selected based on the following criteria:",
    "2ddb51b03163d309434ee403fef42d6b9aecc458": "wav2letter",
    "e587559f5ab6e42f7d981372ee34aebdc92b646e": "on other benchmarks",
    "bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a": "4%",
    "7b4fb6da74e6bd1baea556788a02969134cf0800": "Yes",
    "bc31a3d2f7c608df8c019a64d64cb0ccc5669210": "BERTBase",
    "761de1610e934189850e8fda707dc5239dd58092": "the baseline was the M2M Transformer model trained from scratch on the mixture of six-way pseudo-parallel data generated by VII and the original parallel data.",
    "f8da63df16c4c42093e5778c01a8e7e9b270142e": "The evaluation is based on the correspondence of word pairs representing a border.",
    "c09a92e25e6a81369fcc4ae6045491f2690ccc10": "They compare lexicons by using the same data set.",
    "63c3550c6fb42f41a0c93133e9fca12ac00df9b3": "Yes",
    "603fee7314fa65261812157ddfc2c544277fcf90": "The training sets are much larger.",
    "09a1173e971e0fcdbf2fbecb1b077158ab08f497": "0.01",
    "70e9210fe64f8d71334e5107732d764332a81cb1": "INLINEFORM6",
    "051df74dc643498e95d16e58851701628fdfd43e": "They crawled and pre-processed an OSG web forum.",
    "33554065284110859a8ea3ca7346474ab2cab100": "1.873",
    "57f23dfc264feb62f45d9a9e24c60bd73d7fe563": "The augmented dataset is the same size as the original dataset.",
    "54830abe73fef4e629a36866ceeeca10214bd2c8": "They applied LDA algorithm and Gibbs sampling on ISWC and WWW conference's publications from 2013-2017.",
    "2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6": "WordNet",
    "ef7212075e80bf35b7889dc8dd52fcbae0d1400a": "The extracted entities may be too ambiguous and coarse to be considered relevant to the summary.",
    "567dc9bad8428ea9a2658c88203a0ed0f8da0dc3": "BiLSTM+CNN(grapheme-level)",
    "d51dc36fbf6518226b8e45d4c817e07e8f642003": "6946",
    "d8627ba08b7342e473b8a2b560baa8cdbae3c7fd": "No",
    "cb77d6a74065cb05318faf57e7ceca05e126a80d": "BiLSTM+CNN(grapheme-level)",
    "8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d": "BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF",
    "a1b3e2107302c5a993baafbe177684ae88d6f505": "72782",
    "bb2de20ee5937da7e3e6230e942bec7b6e8f61ee": "ILPRL Lab, KU and KEIV Technologies",
    "1170e4ee76fa202cabac9f621e8fbeb4a6c5f094": "No",
    "1462eb312944926469e7cee067dfc7f1267a2a8c": "4",
    "f59f1f5b528a2eec5cfb1e49c87699e0c536cc45": "It contains around 10,000 sentences",
    "9bd080bb2a089410fd7ace82e91711136116af6c": "10%",
    "6d1217b3d9cfb04be7fcd2238666fa02855ce9c5": "BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF",
    "1e775cf30784e6b1c2b573294a82e145a3f959bb": "English",
    "392fb87564c4f45d0d8d491a9bb217c4fce87f03": "RNN",
    "203337c15bd1ee05763c748391d295a1f6415b9b": "MultiAttentionRNN with Projected Layer",
    "d004ca2e999940ac5c1576046e30efa3059832fa": "Multi-attention and single attention models",
    "21548433abd21346659505296fb0576e78287a74": "The dataset from Twitter",
    "f0b2289cb887740f9255909018f400f028b1ef26": "indirect harassment, information threat, sexual harassment, physical harassment and not sexist",
    "51b1142c1d23420dbf6d49446730b0e82b32137c": "RNN",
    "58355e2a782bf145b61ee2a3e0e426119985c179": "Twitter",
    "25c1c4a91f5dedd4e06d14121af3b5921db125e9": "Yes",
    "f88036174b4a0dbf4fe70ddad884d16082c5748d": "Yes",
    "a267d620af319b48e56c191aa4c433ea3870f6fb": "car models",
    "899ed05c460bf2aa0aa65101cad1986d4f622652": "3,209 reviews",
    "d53299fac8c94bd0179968eb868506124af407d1": "KNN",
    "29f2954098f055fb19d9502572f085862d75bf61": "K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)",
    "6bf93968110c6e3e3640360440607744007a5228": "It could be the horsepower, or it could be the car's form factor (how the car looks).",
    "37a79be0148e1751ffb2daabe4c8ec6680036106": "Nuclear power plants",
    "518dae6f936882152c162058895db4eca815e649": "3",
    "e44a6bf67ce3fde0c6608b150030e44d87eb25e3": "Abortion, gay rights, Obama, and marijuana",
    "6a31db1aca57a818f36bba9002561724655372a7": "505,412 unique users",
    "e330e162ec29722f5ec9f83853d129c9e0693d65": "Yes",
    "d3093062aebff475b4deab90815004051e802aa6": "SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, Recurrent Convolutional Neural Networks (RCNN), SVM with comment information, UTCNN without user information, UTCNN without the LDA model, UTCNN without comments",
    "4944cd597b836b62616a4e37c045ce48de8c82ca": "SentEval",
    "a29c071065d26e5ee3c3bcd877e7f215c59d1d33": "Spearman's rank correlation",
    "7f207549c75f5c4388efc15ed28822672b845663": "20 minutes",
    "596aede2b311deb8cb0a82d2e7de314ef6e83e4e": "1",
    "2e89ebd2e4008c67bb2413699589ee55f59c4f36": "The siamese networks are trained using the classification objective function.",
    "e2db361ae9ad9dbaa9a85736c5593eb3a471983d": "InferSent, Universal Sentence Encoder, average GloVe embeddings",
    "252a645af9876241fb166e5822992ce17fec6eb6": "20",
    "ed67359889cf61fa11ee291d6c378cccf83d599d": "GloVe",
    "425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82": "Accuracy",
    "955de9f7412ba98a0c91998919fa048d339b1d48": "Support Vector Machine with linear kernel",
    "3b371ea554fa6639c76a364060258454e4b931d4": "Facebook",
    "ddb23a71113cbc092cbc158066d891cae261e2c6": "Social media platform",
    "e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38": "Dutch",
    "c7486d039304ca9d50d0571236429f4f6fbcfcf7": "Russian",
    "f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9": "SemEval-2016 Challenge Task 5",
    "a103636c8d1dbfa53341133aeb751ffec269415c": "majority baseline and lexicon-based baseline",
    "55139fcfe04ce90aad407e2e5a0067a45f31e07e": "Google translation API",
    "fbaf060004f196a286fef67593d2d76826f0304e": "Amazon reviews",
    "7ae38f51243cb80b16a1df14872b72a1f8a2048f": "They show that their model outperforms the baseline methods by a large average margin of 22.51%.",
    "deb89bca0925657e0f91ab5daca78b9e548de2bd": "presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels",
    "9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570": "By computing the channel cross-covariance",
    "e6583c60b13b87fc37af75ffc975e7e316d4f4e0": "Phonemes",
    "c7b6e6cb997de1660fd24d31759fe6bb21c7863f": "64",
    "f9f59c171531c452bd2767dc332dc74cadee5120": "14",
    "4ac2c3c259024d7cd8e449600b499f93332dab60": "Yes",
    "bc730e4d964b6a66656078e2da130310142ab641": "Logistic Regression and Multilayer Perceptron",
    "3941401a182a3d6234894a5c8a75d48c6116c45c": "CyberAttack and PoliticianDeath",
    "67e9e147b2cab5ba43572ce8a17fc863690172f0": "The approach is demonstrated to be interpretable by the fact that it provides human-understandable explanations for the improved model.",
    "a74190189a6ced2a2d5b781e445e36f4e527e82a": "The accuracy merits of the approach are demonstrated by the fact that the approach significantly outperforms the state of the art.",
    "43f074bacabd0a355b4e0f91a1afd538c0a6244f": "The keyword specific expectation is elicited from the crowd by asking them to label a set of tweets containing the keyword.",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "Yes, the paper provides a case study in Section 5.",
    "78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d": "Macaw supports multi-turn, multi-modal, and mixed-initiative interactions.",
    "375b281e7441547ba284068326dd834216e55c07": "A wizard of oz setup is a setup in which a real user interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.",
    "05c49b9f84772e6df41f530d86c1f7a1da6aa489": "Telegram",
    "6ecb69360449bb9915ac73c0a816c8ac479cbbfc": "multi-modal",
    "68df324e5fa697baed25c761d0be4c528f7f5cf7": "The different modules in Macaw are:",
    "77c34f1033702278f7f044806c1eba0c6ecb8b04": "Yes, they do.",
    "2ee715c7c6289669f11a79743a6b2b696073805d": "B1",
    "61a9ea36ddc37c60d1a51dabcfff9445a2225725": "The news article sources are the news external references in Wikipedia.",
    "cc850bc8245a7ae790e1f59014371d4f35cd46d7": "They use the class of the entity to generate section templates.",
    "984fc3e726848f8f13dfe72b89e3770d00c3a1af": "The novelty of news articles to entity pages is measured by the KL divergence between the language models of the news article and the entity profile.",
    "fb1227b3681c69f60eb0539e16c5a8cd784177a7": "The features used to represent the salience and relative authority of entities are:",
    "8df35c24af9efc3348d3b8d746df116480dfe661": "Yes",
    "277a7e916e65dfefd44d2d05774f95257ac946ae": "CRF, BiLSTM-CRF, Multi-Task Learning, BioBERT",
    "2916bbdb95ef31ab26527ba67961cf5ec94d6afe": "53 documents",
    "f2e8497aa16327aa297a7f9f7d156e485fe33945": "Using WebAnno",
    "9b76f428b7c8c9fc930aa88ee585a03478bff9b3": "53",
    "dd6b378d89c05058e8f49e48fd48f5c458ea2ebc": "Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT",
    "e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851": "They used a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). They discarded the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, they used a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. They discarded some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, they removed the duplicate, media-based, and link-only tweets. For each account, they collected the maximum amount of tweets allowed by Twitter API.",
    "c00ce1e3be14610fb4e1f0614005911bb5ff0302": "relu",
    "71fe5822d9fccb1cb391c11283b223dc8aa1640c": "LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets",
    "97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3": "Chunks are defined as a sequence of tweets labeled by the label of its corresponding account.",
    "1062a0506c3691a93bb914171c2701d2ae9621cb": "Emotion, sentiment, morality, style, and words embeddings",
    "8e12b5c459fa963b3e549deadb864c244879fe82": "2",
    "483a699563efcb8804e1861b18809279f21c7610": "No",
    "d3ff2986ca8cb85a9a5cec039c266df756947b43": "Words embeddings, style, and morality features",
    "3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4": "180 Twitter accounts",
    "2317ca8d475b01f6632537b95895608dc40c4415": "A chunk of posts is a sequence of tweets labeled by the label of its corresponding account.",
    "3e88fb3d28593309a307eb97e875575644a01463": "LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets",
    "0767ca8ff1424f7a811222ca108a33b6411aaa8a": "We provide strong baseline for text summarization using AMR for possible future works.",
    "e8f969ffd637b82d04d3be28c51f0f3ca6b3883e": "ROGUE metric",
    "46227b4265f1d300a5ed71bf40822829de662bc2": "CNN-Dailymail",
    "a6a48de63c1928238b37c2a01c924b852fe752f8": "SummaRunNer",
    "b65a83a24fc66728451bb063cf6ec50134c8bfb0": "We select the first sentence from the story.",
    "8c852fc29bda014d28c3ee5b5a7e449ab9152d35": "SVM, BiLSTM, CNN",
    "682e26262abba473412f68cbeb5f69aa3b9968d7": "This is the first dataset to contain annotation of type and target of offenses in social media.",
    "5daeb8d4d6f3b8543ec6309a7a35523e160437eb": "English",
    "74fb77a624ea9f1821f58935a52cca3086bb0981": "14,100 tweets",
    "d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751": "abusive language, (cyber-)aggression, (cyber-)bullying, toxic comments, hate speech, offensive language",
    "55bd59076a49b19d3283af41c5e3ccb875f3eb0c": "CNN",
    "521280a87c43fcdf9f577da235e7072a23f0673e": "6",
    "5a8cc8f80509ea77d8213ed28c5ead501c68c725": "Posts that do not contain offense or profanity",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "Level A, Level B, Level C",
    "1b72aa2ec3ce02131e60626639f0cf2056ec23ca": "14,100 tweets",
    "c49ee6ac4dc812ff84d255886fd5aff794f53c39": "Yes",
    "3f856097be2246bde8244add838e83a2c793bd17": "By comparing the overlaps of the retrieved results.",
    "bf52c01bf82612d0c7bbf2e6a5bb2570c322936f": "Pearson, Spearman and Kendall",
    "74e866137b3452ec50fb6feaf5753c8637459e62": "semi-manual Pyramid evaluation framework",
    "184b0082e10ce191940c1d24785b631828a9f714": "Rouge is the best metric for all summarization tasks",
    "c59078efa7249acfb9043717237c96ae762c0a8c": "CDA and REG",
    "73bddaaf601a4f944a3182ca0f4de85a19cdc1d2": "Daily Mail news articles",
    "d4e5e3f37679ff68914b55334e822ea18e60a6cf": "gender pairs",
    "5f60defb546f35d25a094ff34781cddd4119e400": "by measuring the difference between the occurrences of all gender-neutral words with female and male words.",
    "90d946ccc3abf494890e147dd85bd489b8f3f0e8": "Co-occurrence bias, Conditional co-occurrence bias, Causal occupation bias, Embedding bias",
    "b962cc817a4baf6c56150f0d97097f18ad6cd9ed": "Yes/No questions, rating questions, aspect questions, best aspect questions, worst aspect questions, aspect comparison questions, aspect-aspect comparison questions, aspect-aspect-aspect comparison questions",
    "fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f": "Logistic regression, LSTM, MemN2N, Deep projective reader",
    "52f8a3e3cd5d42126b5307adc740b71510a6bdf5": "8 tasks",
    "2236386729105f5cf42f73cc055ce3acdea2d452": "English",
    "18942ab8c365955da3fd8fc901dfb1a3b65c1be1": "TripAdvisor",
    "7b4992e2d26577246a16ac0d1efc995ab4695d24": "Felice2014a",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "Making use of artificial data provided improvements for all data generation methods.",
    "9a9d225f9ac35ed35ea02f554f6056af3b42471d": "(incorrect phrase, correct phrase)",
    "ea56148a8356a1918bedcf0a99ae667c27792cfe": "FCE",
    "cd32a38e0f33b137ab590e1677e8fb073724df7f": "English",
    "2c6b50877133a499502feb79a682f4023ddab63e": "English",
    "f651cd144b7749e82aa1374779700812f64c8799": "BLEU, FKGL, SARI",
    "4625cfba3083346a96e573af5464bc26c34ec943": "6.37 BLEU",
    "326588b1de9ba0fd049ab37c907e6e5413e14acd": "PBMT-R, Hybrid, SBMT-SARI, NMT, Dress",
    "ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb": "WikiLarge has 296,402 sentence pairs, and WikiSmall has 89,042 sentence pairs.",
    "55507f066073b29c1736b684c09c045064053ba9": "Direct name calling, simile and metaphor, indirect speech, wishing evil, name alteration, societal stratification, and sexually related.",
    "e838275bb0673fba0d67ac00e4307944a2c17be3": "They used the annotated tweets to ascertain the distribution of",
    "8dda1ef371933811e2a25a286529c31623cca0c6": "1",
    "b3de9357c569fb1454be8f2ac5fcecaea295b967": "10,000",
    "59e58c6fc63cf5b54b632462465bfbd85b1bf3dd": "The dataset is not biased by topic, dialect or target because the dataset is built using a pattern that is not associated with any specific topic or genre, and it appears in all Arabic dialects.",
    "5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83": "We first systematically define irony generation based on style transfer. Because of the lack of irony data, we make use of twitter and build a large-scale dataset. In order to control irony accuracy, sentiment preservation and content preservation at the same time, we also design a combination of rewards for reinforcement learning and incorporate reinforcement learning with a pre-training process. Experimental results demonstrate that our model outperforms other generative models and our rewards are effective. Although our model design is effective, there are still many errors and we systematically analyze them. In the future, we are interested in exploring these directions and our work may extend to other kinds of ironies which are more difficult to model.",
    "3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f": "irony reward and sentiment reward",
    "14b8ae5656e7d4ee02237288372d9e682b24fdb8": "The ironic pattern is hard to model because it is a complicated style and hard to describe with some specific style attribute words.",
    "e3a2d8886f03e78ed5e138df870f48635875727e": "They crawled over 2M tweets from twitter using GetOldTweets-python.",
    "62f27fe08ddb67f16857fab2a8a721926ecbb6fb": "Annotators",
    "9ca447c8959a693a3f7bdd0a2c516f4b86f95718": "The tweets were annotated with the target and stance information.",
    "05887a8466e0a2f0df4d6a5ffc5815acd7d9066a": "SVM classifiers using unigram and bigram features in addition to using the existence of hashtags as another feature.",
    "c87fcc98625e82fdb494ff0f5309319620d69040": "Hashtag features are the features that are used in the SVM classifiers.",
    "500a8ec1c56502529d6e59ba6424331f797f31f0": "700",
    "ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc": "Galatasaray and Fenerbah\u00e7e",
    "f2155dc4aeab86bf31a838c8ff388c85440fce6e": "No",
    "ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc": "3",
    "4d706ce5bde82caf40241f5b78338ea5ee5eb01e": "BIBREF1",
    "86bf75245358f17e35fc133e46a92439ac86d472": "modest gains",
    "9132d56e26844dc13b3355448d0f14b95bd2178a": "Chunk labels",
    "f3c204723da53c7c8ef4dc1018ffbee545e81056": "Yes",
    "0602a974a879e6eae223cdf048410b5a0111665e": "K-means, LEM, DPEMM",
    "56b034c303983b2e276ed6518d6b080f7b8abe6a": "FSD, Twitter, Google",
    "15e481e668114e4afe0c78eefb716ffe1646b494": "CUDA acceleration",
    "3d7a982c718ea6bc7e770d8c5da564fbb9d11951": "The generator network is designed to learn the projection function between the document-event distribution and the four document-level word distributions (entity distribution, location distribution, keyword distribution and date distribution).",
    "692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1": "20,000",
    "935d6a6187e6a0c9c0da8e53a42697f853f5c248": "A person's industry is the aggregate of enterprises in a particular field.",
    "3b77b4defc8a139992bd0b07b5cf718382cb1a5f": "Naive Bayes",
    "01a41c0a4a7365cd37d28690735114f2ff5229f2": "Sina Weibo",
    "cd2878c5a52542ddf080b20bec005d9a74f2d916": "Technology, Environment, Real Estate, Tourism, Religion, Media, Finance, Education, Healthcare, Government, Law, and Automotive",
    "fd2c6c26fd0ab3c10aae4f2550c5391576a77491": "Yes",
    "6b6d498546f856ac20958f666fc3fd55811347e2": "batch size 250, epochs 20, binary cross-entropy as the objective function and Adam optimizer",
    "de3b1145cb4111ea2d4e113f816b537d052d9814": "BOW model and maximum entropy classifier",
    "132f752169adf6dc5ade3e4ca773c11044985da4": "CrowdFlower emotional tweets dataset",
    "1d9aeeaa6efa1367c22be0718f5a5635a73844bd": "The sequential nature of the data",
    "012b8a89aea27485797373adbcda32f16f9d7b54": "Hierarchical stacked classifiers",
    "c598028815066089cc1e131b96d6966d2610467a": "No",
    "ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed": "They use a naive Bayesian classifier with character (2, 4 & 6)-grams, word unigram and word bigram features with a hierarchical lexicon based classifier.",
    "0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50": "Accuracy",
    "92dfacbbfa732ecea006e251be415a6f89fb4ec6": "Nguni languages",
    "c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9": "NCHLT text corpora",
    "307e8ab37b67202fe22aedd9a98d9d06aaa169c5": "Yes",
    "6415f38a06c2f99e8627e8ba6251aa4b364ade2d": "14 languages over 6 language groups",
    "e5c8e9e54e77960c8c26e8e238168a603fcdfcc6": "Yes",
    "50be4a737dc0951b35d139f51075011095d77f2a": "Prior knowledge",
    "6becff2967fe7c5256fe0b00231765be5b9db9f1": "(1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.",
    "76121e359dfe3f16c2a352bd35f28005f2a40da3": "text categorization, sentiment classification",
    "02428a8fec9788f6dc3a86b5d5f3aa679935678d": "The model is robust if it can perform well even when the prior knowledge is biased.",
    "7793805982354947ea9fc742411bec314a6998f6": "automatic",
    "007b13f05d234d37966d1aa7d85b5fd78564ff45": "Yes",
    "2ceced87af4c8fdebf2dc959aa700a5c95bd518f": "No",
    "72ed5fed07ace5e3ffe9de6c313625705bc8f0c7": "150 to 250 tokens",
    "2e37e681942e28b5b05639baaff4cd5129adb5fb": "The text descriptions of entities are variable-length sentences.",
    "b49598b05358117ab1471b8ebd0b042d2f04b2a4": "NBOW, LSTM and attentive LSTM",
    "932b39fd6c47c6a880621a62e6a978491d881d60": "TransE",
    "b36f867fcda5ad62c46d23513369337352aa01d2": "WN18 and FB15K",
    "c6a0b9b5dabcefda0233320dd1548518a0ae758e": "CJFA (contextual joint factor analysis) encoder",
    "1e185a3b8cac1da939427b55bf1ba7e768c5dae4": "VAE baseline",
    "26e2d4d0e482e6963a76760323b8e1c26b6eee91": "Test set",
    "b80a3fbeb49a8968e149955bdcf199556478eeff": "The proposed unsupervised approaches obtain embeddings and can be improved with unlabelled out-of-domain data, the classification tasks benefits even though the labelled data remains the same.",
    "badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9": "Empirical results on the benchmark datasets prove the effectiveness of the proposed approach that the two tasks of pun detection and location can be addressed by a single model from a sequence labeling perspective.",
    "67b66fe67a3cb2ce043070513664203e564bdcbd": "CRF",
    "f56d07f73b31a9c72ea737b40103d7004ef6a079": "homographic dataset and heterographic dataset",
    "38e4aaeabf06a63a067b272f8950116733a7895c": "INLINEFORM0",
    "1d197cbcac7b3f4015416f0152a6692e881ada6c": "They use OpenIE to extract relations from sentences.",
    "92294820ac0d9421f086139e816354970f066d8a": "significant and consistent",
    "477d9d3376af4d938bb01280fe48d9ae7c9cf7f7": "BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19.",
    "f225a9f923e4cdd836dd8fe097848da06ec3e0cc": "SQuAD",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "The baseline model was a simple rule-based system that answered always YES to Yes/No questions and returned the first sentence of the snippet as the answer to factoid questions.",
    "e807d347742b2799bc347c0eff19b4c270449fee": "BioASQ",
    "31b92c03d5b9be96abcc1d588d10651703aff716": "0.7033",
    "9ec1f88ceec84a10dc070ba70e90a792fba8ce71": "0.6103",
    "384bf1f55c34b36cb03f916f50bbefade6c86a75": "Yes, they do.",
    "aef607d2ac46024be17b1ddd0ed3f13378c563a6": "They use the gradient-based method to estimate the word importance, and then perturb the input sentence by masking the most important words.",
    "93beae291b455e5d3ecea6ac73b83632a3ae7ec7": "The model decides how much importance to give to the output words by using the gradient-based method.",
    "6c91d44d5334a4ac80100eead4e105d34e99a284": "Transformer and RNN-Search",
    "a69a59b6c0ab27bcee1a780d6867df21e30aec08": "No",
    "b3d01ac226ee979e188a4141877a6d2a5482de98": "The weaknesses found by non-expert annotators of current state-of-the-art NLI models are:",
    "af5730d82535464cedfa707a03415ac2e7a21295": "Wikipedia, News, Fiction, Formal spoken text, and causal or procedural text",
    "ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2": "Yes",
    "b249b60a8c94d0e40d65f1ffdfcac527dab57516": "Yes",
    "0f567251a6566f65170a1329eeeb5105932036b2": "Maximum entropy classifier",
    "4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f": "(a) gazeteer and rule based, (b) word boundary detection, and (c) ranking with language model and other features",
    "60ce4868af45753c9e124e64e518c32376f12694": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "1,200 turns",
    "2c85865a65acd429508f50b5e4db9674813d67f2": "Seed data",
    "73a7acf33b26f5e9475ee975ba00d14fd06f170f": "symptom, attribute, entity",
    "dd53baf26dad3d74872f2d8956c9119a27269bd5": "They randomly sample 1,200 turns from the seed dataset and manually categorize them to different types.",
    "218bc82796eb8d91611996979a4a42500131a936": "MLP",
    "b21bc09193699dc9cfad523f3d5542b0b2ff1b8e": "MLP",
    "352bc6de5c5068c6c19062bad1b8f644919b1145": "4",
    "d667731ea20605580c398a1224a0094d1155ebbb": "No",
    "8bb0011ad1d63996d5650770f3be18abdd9f7fc6": "Yes.",
    "b0dbe75047310fec4d4ce787be5c32935fc4e37b": "They evaluate the performance of the model on the adversarial sets.",
    "d64383e39357bd4177b49c02eb48e12ba7ffd4fb": "End-to-end MRC model",
    "52f9cd05d8312ae3c7a43689804bac63f7cac34b": "Yes.",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "Support vector machine",
    "955cbea7e5ead36fb89cd6229a97ccb3febcf8bc": "10-fold cross validation on 3543 code-mixed tweets was carried out by dividing the corpus into 10 equal parts with nine parts as training corpus and rest one for testing. Mean accuracy is calculated by taking the average of the accuracy obtained in each iteration of the testing process.",
    "04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39": "3",
    "15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8": "Twitter",
    "ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797": "BIBREF26",
    "6ca938324dc7e1742a840d0a54dc13cc207394a1": "newscrawl",
    "4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802": "BIBREF26",
    "4d47bef19afd70c10bbceafd1846516546641a2f": "bi-directional language model and uni-directional model",
    "506d21501d54a12d0c9fd3dbbf19067802439a04": "subway, manhattan",
    "701571680724c05ca70c11bc267fb1160ea1460a": "No",
    "600b097475b30480407ce1de81c28c54a0b3b2f8": "The baseline is the standard GAN model.",
    "ee7e9a948ee6888aa5830b1a3d0d148ff656d864": "160-acre",
    "5fda8539a97828e188ba26aad5cda1b9dd642bc8": "Our model gives new higher or comparable segmentation performance against previous state-of-the-art models.",
    "709feae853ec0362d4e883db8af41620da0677fe": "Gaussian-masked directional multi-head attention is a variant of multi-head attention. It uses a Gaussian mask to adjust the weight between characters and their adjacent characters to a larger value which stands for the effect of adjacent characters.",
    "186b7978ee33b563a37139adff1da7d51a60f581": "Closed test limits all the data for learning should not be beyond the given training set.",
    "fabcd71644bb63559d34b38d78f6ef87c256d475": "BIBREF4, BIBREF6, BIBREF7, BIBREF9, BIBREF11, BIBREF17, BIBREF18, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34, BIBREF35, BIBREF36, BIBREF37, BIBREF38, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF43, BIBREF44, BIBREF45, BIBREF46, BIBREF47, BIBREF48, BIBREF49, BIBREF50, BIBREF51, BIBREF52, BIBREF53, BIBREF54, BIBREF55, BIBREF56, BIBREF57, BIBREF58, BIBREF59, BIBREF60, BIBREF61, BIBREF62, BIBREF63, B",
    "da9c0637623885afaf023a319beee87898948fe9": "Yes",
    "8a1c0ef69b6022a0642ca131a8eacb5c97016640": "Context tweets",
    "48088a842f7a433d3290eb45eb0d4c6ab1d8f13c": "CNN, RNN, LTC",
    "4907096cf16d506937e592c50ae63b642da49052": "The authors give two examples of the difficulties presented by the context-dependent nature of online aggression. The first example is a tweet that uses vulgar language, but the intention of the user can be better understood with its context tweet. The second example is a tweet that is abusive, but the target of the malice can be better understood with its context tweet.",
    "8748e8f64af57560d124c7b518b853bf2711c13e": "Yes",
    "893ec40b678a72760b6802f6abf73b8f487ae639": "The evaluation results indicate that our model outperforms previous works by profiting the syntactical and contextual information embedded in different transformer encoder layers of the BERT model using a CNN-based fine-tuning strategy. Furthermore, examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT\u2019s layers and a set of features associated to the different type of biases in data.",
    "c81f215d457bdb913a5bade2b4283f19c4ee826c": "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9",
    "e101e38efaa4b931f7dd75757caacdc945bb32b4": "BERT",
    "afb77b11da41cd0edcaa496d3f634d18e48d7168": "Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer",
    "41b2355766a4260f41b477419d44c3fd37f3547d": "Bias in the process of collecting or annotating datasets",
    "96a4091f681872e6d98d0efee777d9e820cb8dae": "Bias in the process of collecting or annotating datasets",
    "81a35b9572c9d574a30cc2164f47750716157fc8": "They compare to the following approaches:",
    "f4496316ddd35ee2f0ccc6475d73a66abf87b611": "20 Newsgroups",
    "e8a32460fba149003566969f92ab5dd94a8754a4": "Concept Raw Context model (CRC) and Concept-Concept Context model (3C)",
    "2a6003a74d051d0ebbe62e8883533a5f5e55078b": "Concept Raw Context model (CRC)",
    "1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b": "It scales linearly with the # of the BOC dimensions.",
    "9c44df7503720709eac933a15569e5761b378046": "English",
    "b7381927764536bd97b099b6a172708125364954": "They evaluate the resulting word embeddings by using intrinsic evaluation via word similarity and word analogy tasks, as well as downstream tasks.",
    "df95b3cb6aa0187655fd4856ae2b1f503d533583": "simple n-grams and unsupervised morphemes",
    "f7ed3b9ed469ed34f46acde86b8a066c52ecf430": "stochastic gradient descent",
    "c7eb71683f53ab7acffd691a36cad6edc7f5522e": "Yes",
    "17a1eff7993c47c54eddc7344e7454fbe64191cd": "Word similarity test",
    "a5e5cda1f6195ab1336855f1e39a609d61326d62": "32nd dimension",
    "32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9": "INLINEFORM0",
    "eda4869c67fe8bbf83db632275f053e7e0241e8c": "PPDB",
    "2c7494d47b2a69f182e83455fe4c75ae3b2893e9": "Yes",
    "4d7ff4e5d06902de85b0e9a364dc455196d06a7d": "CNN-LSTM",
    "ecc63972b2783ee39b3e522653cfb6dc5917d522": "They group the models based on the objective function it optimizes.",
    "8d074aabf4f51c8455618c5bf7689d3f62c4da1d": "The limitations of existing Vietnamese word segmentation systems are:",
    "fe2666ace293b4bfac3182db6d0c6f03ea799277": "Because Vietnamese is an isolating language and one branch of Mon-Khmer language group.",
    "70a1b0f9f26f1b82c14783f1b76dfb5400444aa4": "The accuracy of the approaches used to solve word segmentation in Vietnamese is around 94-97%.",
    "d3ca5f1814860a88ff30761fec3d860d35e39167": "Maximum Matching, Maximum Entropy, Hidden Markov Model, Conditional Random Fields, and Support Vector Machines",
    "dd20d93166c14f1e57644cd7fa7b5e5738025cd0": "Mainstream and disinformation news",
    "dc2a2c177cd5df6da5d03e6e74262bf424850ec9": "We used the procedure described in BIBREF2 to label different outlets.",
    "ae90c5567746fe25af2fcea0cc5f355751e05c71": "US dataset and Italian dataset",
    "d7644c674887ca9708eb12107acd964ae53b216d": "Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d), Structural virality of the largest weakly connected component (SV)",
    "a3bb9a936f61bafb509fa12ac0a61f91abcc5106": "We evaluate on the ARC dataset, the TREC question classification dataset, and the MLBioMedLAT and GARD datasets.",
    "df6d327e176740da9edcc111a06374c54c8e809c": "They compare their model to the following methods:",
    "49764eee7fb523a6a28375cc699f5e0220b81766": "No",
    "3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1": "The dataset was collected from the ARC corpus.",
    "bb3267c3f0a12d8014d51105de5d81686afe5f1b": "CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW",
    "114934e1a1e818630ff33ac5c4cd4be6c6f75bb2": "NCEL outperforms the state-of-the-art collective methods across five different datasets.",
    "2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72": "We train NCEL using collected Wikipedia hyperlinks instead of specific annotated data. We then evaluate the trained model on five different benchmarks to verify the linking precision as well as the generalization ability.",
    "b8d0e4e0e820753ffc107c1847fe1dfd48883989": "They only use adjacent entity mentions.",
    "5aa12b4063d6182a71870c98e4e1815ff3dc8a72": "Yes, they do.",
    "22815878083ebd2f9e08bc33a5e733063dac7a0f": "Russian",
    "220d11a03897d85af91ec88a9b502815c7d2b6f3": "Because English has simple morphology.",
    "d509081673f5667060400eb325a8050fa5db7cc8": "English Wikipedia dump from February 2017 and Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC).",
    "c2e475adeddcdc4d637ef0d4f5065b6a9b299827": "BLEU-4, NIST-4 and ROUGE-4",
    "cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5": "Yes, they use Glove embeddings.",
    "6cd25c637c6b772ce29e8ee81571e8694549c5ab": "WikiBio dataset",
    "1088255980541382a2aa2c0319427702172bbf84": "A bifocal attention mechanism is a mechanism that computes an attention over fields at a macro level and over values at a micro level.",
    "0d9fcc715dee0ec85132b3f4a730d7687b6a06f4": "The expected number of unique outputs it assigns to a set of adversarial perturbations.",
    "8910ee2236a497c92324bbbc77c596dba39efe46": "Sentiment analysis and paraphrase detection",
    "2c59528b6bc5b5dc28a7b69b33594b274908cca6": "A semicharacter architecture is a neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step.",
    "6b367775a081f4d2423dc756c9b65b6eef350345": "We did not experiment with this, but it is a good idea.",
    "bc01853512eb3c11528e33003ceb233d7c1d7038": "Because spammers continually bombard email servers, subtly misspelling words in efforts to evade spam detection while preserving the emails' intended meaning.",
    "67ec8ef85844e01746c13627090dc2706bb2a4f3": "RNNs are more efficient than transformers.",
    "ba539cab80d25c3e20f39644415ed48b9e4e4185": "Backoff strategies are used when the word recognizer predicts UNK. The backoff strategies are:",
    "6bf5620f295b5243230bc97b340fae6e92304595": "The baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.",
    "4986f420884f917d1f60d3cea04dc8e64d3b5bf1": "Crosslingual latent variables",
    "747b847d687f703cc20a87877c5b138f26ff137d": "Europarl corpus",
    "111afb77cfbf4c98e0458606378fa63a0e965e36": "No",
    "6568a31241167f618ef5ede939053feaa2fb0d7e": "No, they add one latent variable for each aligned argument.",
    "50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed": "Bayesian model",
    "0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c": "Yes",
    "4dc268e3d482e504ca80d2ab514e68fd9b1c3af1": "19",
    "ab54cd2dc83141bad3cb3628b3f0feee9169a556": "INLINEFORM7",
    "249c805ee6f2ebe4dbc972126b3d82fb09fa3556": "We calculate recommendation diversity as the average dissimilarity of all pairs of tags in the list of recommended tags.",
    "b4f881331b975e6e4cab1868267211ed729d782d": "48,705",
    "79413ff5d98957c31866f22179283902650b5bb6": "Amazon review keywords",
    "29c014baf99fb9f40b5171aab3e2c7f12a748f79": "Doc2Vec",
    "09c86ef78e567033b725fc56b85c5d2602c1a7c3": "They start with the best performing model according to validation performance. Then in each step they try adding the best performing model that had not been previously tried. They keep it in the ensemble if it does improve its validation performance and discard it otherwise. This way they gradually try each model once. They call the resulting model a greedy ensemble. They use the 20k BookTest validation dataset for this procedure.",
    "d67c01d9b689c052045f3de1b0918bab18c3f174": "The Attention-Sum Reader model trained on the BookTest dataset improves over the best Attention-Sum Reader model trained on the CBT dataset by INLINEFORM1 absolute on the CBT NE validation dataset and INLINEFORM2 absolute on the CBT CN validation dataset.",
    "e5bc73974c79d96eee2b688e578a9de1d0eb38fd": "They show that humans can answer questions that the model cannot.",
    "2cd37743bcc7ea3bd405ce6d91e79e5339d7642e": "Yes",
    "eac9dae3492e17bc49c842fb566f464ff18c049b": "claim, premise, conclusion",
    "7697baf8d8d582c1f664a614f6332121061f87db": "SVM, CRF, and Naive Bayes",
    "1cb100182508cf55b3509283c0e2bbcd527d625e": "The data in the new corpus come sourced from the Web.",
    "206739417251064b910ae9e5ff096e867ee10fb8": "The phenomena that are accounted for by this work are:",
    "d6401cece55a14d2a35ba797a0878dfe2deabedc": "The linguistic variability of the education topics represents a challenge for NLP.",
    "ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff": "the transcriber",
    "fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c": "crowdsourcing",
    "b1a068c1050e2bed12d5c9550c73e59cd5b1f78d": "Persian and English",
    "f9edd8f9c13b54d8b1253ed30e7decc1999602da": "We provided several evaluation protocols for each part of the database. The protocols allow researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration and phrase text on the performance.",
    "d93c0e78a3fe890cd534a11276e934be68583f4b": "18-60",
    "30af1926559079f59b0df055da76a3a34df8336f": "The data collection project was mainly supported by Sharif DeepMine company.",
    "ceb767e33fde4b927e730f893db5ece947ffb0d8": "demographics, diagnosis history, and symptoms/signs",
    "c2cb6c4500d9e02fc9a1bdffd22c3df69655189f": "No",
    "c571deefe93f0a41b60f9886db119947648e967c": "MIMIC-III",
    "06eb9f2320451df83e27362c22eb02f4a426a018": "four levels of preprocessing",
    "e54257585cc75564341eb02bdc63ff8111992f82": "TopicRank, TF-IDF, KP-Miner, WINGNUS, and TextRank",
    "2a3e36c220e7b47c1b652511a4fdd7238a74a68f": "244",
    "9658b5ffb5c56e5a48a3fea0342ad8fc99741908": "No",
    "46c9e5f335b2927db995a55a18b7c7621fd3d051": "15",
    "ce0e2a8675055a5468c4c54dbb099cfd743df8a7": "None, Unsure, Alcoholism, Depression, Diabetes, Drug Abuse, Hypertension, Obesity, Psychiatric Disorder, Pulmonary Disease",
    "3a6e843c6c81244c14730295cfb8b865cd7ede46": "BIBREF9, BIBREF8",
    "fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf": "Semeval 2014 Twitter Sentiment Analysis Dataset, corpus developed by BIBREF35, corpus developed by BIBREF36",
    "1beb4a590fa6127a138f4ed1dd13d5d51cc96809": "The network's baseline features are the features extracted from the fully-connected layer of the baseline CNN.",
    "5c5aeee83ea3b34f5936404f5855ccb9869356c1": "machine translation",
    "f8c1b17d265a61502347c9a937269b38fc3fcab1": "The method performs better than the baselines.",
    "5913930ce597513299e4b630df5e5153f3618038": "The softmax transformers assign non-zero attention weights to all context words, while the sparsemax transformers assign zero attention weights to irrelevant words.",
    "81d193672090295e687bc4f4ac1b7a9c76ea35df": "word2vec",
    "cf171fad0bea5ab985c53d11e48e7883c23cdc44": "It has nearly 1.6 million tweets annotated through a distant supervised method.",
    "2a564b092916f2fabbfe893cf13de169945ef2e1": "The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39.",
    "0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d": "3-feats",
    "73e83c54251f6a07744413ac8b8bed6480b2294f": "word2vec and TDK",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "The supervised polarity score per word is calculated as in (DISPLAY_FORM4).",
    "e48e750743aef36529fbea4328b8253dbe928b4d": "Edinburgh corpus",
    "c08aab979dcdc8f4fe8ec1337c3c8290ab13414e": "14",
    "8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f": "GloVe, Edinburgh embeddings, Emoji embeddings",
    "cc608df2884e1e82679f663ed9d9d67a4b6c03f3": "We looked at the following metrics:",
    "3e432d71512ffbd790a482c716e7079ee78ce732": "The datasets used are the ones from the DSTC2 BIBREF38 and DSTC3 BIBREF39 challenges.",
    "dd76130ec5fac477123fe8880472d03fbafddef6": "The state of the art described in the paper is the lack of methods and tools to design and engineer the coordination and mediation among chatbots and humans.",
    "43eecc576348411b0634611c81589f618cd4fddf": "SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN",
    "79f9468e011670993fd162543d1a4b3dd811ac5d": "Better performance on three text generation tasks.",
    "c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf": "Yes",
    "902b3123aec0f3a39319ffa9d05ab8e08a2eb567": "Skip-Gram with negative sampling (SGNS)",
    "1038542243efe5ab3e65c89385e53c4831cd9981": "DTA18 and DTA19",
    "e2b0cd30cf56a4b13f96426489367024310c3a05": "Spearman's $\\rho $",
    "e831041d50f3922265330fcbee5a980d0e2586dd": "Reading without any specific task other than comprehension",
    "7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93": "Yes",
    "ac7f6497be4bcca64e75f28934b207c9e8097576": "Wikipedia sentences",
    "87bb3105e03ed6ac5abfde0a7ca9b8de8985663c": "because they do not require to train a backward translation engine.",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "back-translation, forward-translation, copy, copy-marked, copy-dummies, copy+noise, copy+noise+GANs",
    "9225b651e0fed28d4b6261a9f6b443b52597e401": "monotonicity is a facilitating factor",
    "565189b672efee01d22f4fc6b73cd5287b2ee72c": "Europarl",
    "b6f7fadaa1bb828530c2d6780289f12740229d84": "English-French and English-German",
    "7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613": "English",
    "4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f": "No",
    "6c96e910bd98c9fd58ba2050f99b9c9bac69840a": "We have a total of 822,040 questions in our dataset. We randomly split the dataset into training and test sets in the ratio of 80:20.",
    "9af3142630b350c93875441e1e1767312df76d17": "Yes, they do.",
    "e374169ee10f835f660ab8403a5701114586f167": "We consider 5 major profile attributes for the analysis, including, Profile Name, Username, Description, Profile Image, and Location.",
    "82595ca5d11e541ed0c3353b41e8698af40a479b": "Organic ways include mentioning party name in the profile attributes. Inorganic ways include adding election campaign related keywords to the profile attributes.",
    "d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f": "We find that the political handles made more changes to profile attributes than the follower handles. About $80\\%$ changes made by the followers in the period of data collection was related to #LokSabhaElectios2019. While the profile handles made changes, the changes mostly included going back and forth to same profile attribute, with the change in attribute being active support to a political party or political leader.",
    "53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e": "BLEU-1, Meteor, Rouge-L",
    "869feb7f47606105005efdb6bea1c549824baea0": "The dataset includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.",
    "c497e8701060583d91bb64b9f9202d40047effc4": "We crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets.",
    "8060a773f6a136944f7b59758d08cc6f2a59693b": "14%",
    "1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306": "14%",
    "c0af8b7bf52dc15e0b33704822c4a34077e09cd1": "2-layers regular trained model",
    "9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e": "They use datasets with transcribed text.",
    "e0122fc7b0143d5cbcda2120be87a012fb987627": "68.8% to 71.8%",
    "5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4": "They concatenate the final encoding vector of the audio-RNN with the final encoding vector of the text-RNN.",
    "37edc25e39515ffc2d92115d2fcd9e6ceb18898b": "BIBREF2",
    "e431661f17347607c3d3d9764928385a8f3d9650": "several points",
    "876700622bd6811d903e65314ac75971bbe23dcc": "SemEval-2016",
    "312e9cc11b9036a6324bdcb64eca6814053ffa17": "92%",
    "1c0ba6958da09411deded4a14dfea5be55687619": "3",
    "1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b": "TF-IDF features",
    "d915b401bb96c9f104a0353bef9254672e6f5a47": "1. The model could be improved by using a more complex decoder, such as the one proposed in BIBREF12.",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "BLEU score",
    "664db503509b8236bc4d3dc39cebb74498365750": "BLEU score",
    "64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8": "Propaganda is a social phenomenon and takes place as an act of communication.",
    "b0a18628289146472aa42f992d0db85c200ec64b": "F1 score",
    "72ce05546c81ada05885026470f4c8c218805055": "English",
    "5b551ba47d582f2e6467b1b91a8d4d6a30c343ec": "We use BLEU-1/4, ROUGE-L, Distinct-1/2, and perplexity.",
    "3cf1edfa6d53a236cf4258afd87c87c0a477e243": "English",
    "9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa": "They achieved an average recipe-level coherence score of 1.78-1.82, surpassing the baseline at 1.77.",
    "34dc0838632d643f33c8dbfe7bd4b656586582a2": "The baseline models are the encoder-decoder model and the nearest-neighbor model.",
    "c77359fb9d3ef96965a9af0396b101f82a0a9de6": "They scraped the data from Food.com.",
    "1bdc990c7e948724ab04e70867675a334fdd3051": "They are scraped from Food.com.",
    "78536da059b884d6ad04680baeb894895458055c": "Maximum Entropy Classifiers, Support Vector Machines, Recursive Neural Networks, Convolutional Neural Networks, Bidirectional Encoder Representation from Transformers (BERT)",
    "96b07373756d7854bccc3c12e8d41454ab8741f5": "No",
    "511517efc96edcd3e91e7783821c9d6d5a6562af": "BF, BA, SFU",
    "9122de265577e8f6b5160cd7d28be9e22da752b2": "We obtained a gain of 0.42 F1 points on BF, 1.98 F1 points on BA and 0.29 F1 points on SFU, while on the scope resolution task, we obtained a gain of 8.06 F1 points on BF, 4.27 F1 points on BA and 11.87 F1 points on SFU, when trained on a single dataset. While training on multiple datasets, we observed a gain of 10.6 F1 points on BF and 1.94 F1 points on BA on the speculation detection task and 2.16 F1 points on BF and 0.25 F1 points on SFU on the scope resolution task over the single dataset training approach. We thus significantly advance the state-of-the-art for speculation detection and scope resolution. On the negation scope resolution task, we applied the XLNet and RoBERTa and obtained a gain of 3.16 F1 points on BF, 0.06 F1 points on BA and 0.3 F1 points on SFU. Thus, we demonstrated the usefulness of transformer-based architectures in the field of negation and speculation detection and scope resolution.",
    "e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d": "2670 sentences",
    "45be665a4504f0c7f458cf3f75a95d5a75eefd42": "11871 sentences",
    "22b740cc3c8598247ee102279f96575bdb10d53f": "No",
    "74b4779de437c697fe702e51f23e2b0538b0f631": "context-dependent",
    "435570723b37ee1f5898c1a34ef86a0b2e8701bb": "phrase-based MT system",
    "aa2948209cc33b071dbf294822e72bb136678345": "The results are shown in Table TABREF37.",
    "d9412dda3279729e95fcb35cbed09e61577a896e": "precision, recall, F1 and accuracy",
    "41b70699514703820435b00efbc3aac4dd67560a": "divorce proceedings",
    "e3c9e4bc7bb93461856e1f4354f33010bc7d28d5": "CNN/GRU+law",
    "06cc8fcafc0880cf69a2514bb7341642b9833041": "INLINEFORM0",
    "d650101712e36594bd77b45930a990402a455222": "divorce proceedings",
    "cb384dc5366b693f28680374d31ff45356af0461": "Yes",
    "d41e20ec716b5904a272938e5a8f5f3f15a7779e": "They use a predetermined list of LGBTQ terms.",
    "0682bf049f96fa603d50f0fdad0b79a5c55f6c97": "Yes",
    "97d1ac71eed13d4f51f29aac0e1a554007907df8": "It is able to encode a document and obtain representations for its sentences.",
    "c17b609b0b090d7e8f99de1445be04f8f66367d4": "0.9",
    "53014cfb506f6fffb22577bf580ae6f4d5317ce5": "CNN/DailyMail, NYT, XSum",
    "fa30a938b58fc05131c3854f12efe376cbad887f": "competitive results",
    "f875337f2ecd686cd7789e111174d0f14972638d": "Affective Text dataset, Fairy Tales dataset, and the ISEAR dataset.",
    "de53af4eddbc30c808d90b8a11a29217d377569e": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney",
    "dac087e1328e65ca08f66d8b5307d6624bf3943f": "Yes",
    "a1645d0ba50e4c29f0feb806521093e7b1459081": "Social Honeypot dataset and yes",
    "3cd185b7adc835e1c4449eff81222f5fc15c8500": "By using the LDA model, we obtain the topic probability for each Twitter user. By utilizing the topic probability result, we extract our two topic-based features",
    "f03112b868b658c954db62fc64430bebbaa7d9e0": "Yes, they use BLEU.",
    "5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4": "0.57",
    "a6d3e57de796172c236e33a6ceb4cca793dc2315": "LEAD",
    "395b61d368e8766014aa960fde0192e4196bcb85": "IMDB, Yelp",
    "92bb41cf7bd1f7886784796a8220ed5aa07bc49b": "dataset features such as length of text, training data size (for target classifiers) and input domains",
    "4ef11518b40cc55d86c485f14e24732123b0d907": "FGM, FGVM, DeepFool, HotFlip and TYC",
    "6a219d7c58451842aa5d6819a7cdf51c55e9fc0f": "Tourism, Bible readings",
    "cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad": "Transformer",
    "f8f4e4a50d2b3fbd193327e79ea32d8d057e1414": "Crowdsourcing",
    "bc84c5a58c57038910f7720d7a784560054d3e1a": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese",
    "29923a824c98b3ba85ced964a0e6a2af35758abe": "We compute sentence-level BLEU with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system.",
    "559c68802ee2bb8b11e2188127418ca3a6155ba7": "No",
    "8dc707a0daf7bff61a97d9d854283e65c0c85064": "No",
    "ffde866b1203a01580eb33237a0bb9da71c75ecf": "1000 hours",
    "6cd8bad8a031ce6d802ded90f9754088e0c8d653": "1.3",
    "30eacb4595014c9c0e5ee9669103d003cfdfe1e5": "SemEval 2010 task 8",
    "0f7867f888109b9e000ef68965df4dde2511a55f": "We apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly.",
    "e2e977d7222654ee8d983fd8ba63b930e9a5a691": "Connectionist Bi-directional RNNs",
    "0cfe0e33fbb100751fc0916001a5a19498ae8cb5": "By splitting the context into three disjoint regions based on the two relation arguments",
    "35b3ce3a7499070e9b280f52e2cb0c29b0745380": "Yes.",
    "71ba1b09bb03f5977d790d91702481cc406b3767": "The baseline achieves a macro-average F1-score of 0.845 on the supervised test set.",
    "612c3675b6c55b60ae6d24265ed8e20f62cb117e": "Yes, they did.",
    "bd40f33452da7711b65faaa248aca359b27fddb6": "The performance of multilingual BERT was good.",
    "787c4d4628eac00dbceb1c96020bff0090edca46": "Stance.",
    "3c3807f226ba72fc41f59f0338f12a49a0c35605": "No. The experts are legal experts, and are not representative of the general public.",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "Yes, the answers are double annotated.",
    "81d607fc206198162faa54a796717c2805282d9b": "The experts were recruited from the University of Michigan Law School.",
    "51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad": "BERT",
    "f0848e7a339da0828278f6803ed7990366c975f0": "Yes, we tested a number of baselines, including SVM and CNN.",
    "b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8": "Yes",
    "792f6d76d2befba2af07198584aac1b189583ae4": "It is a new task.",
    "127d5ddfabec5c58832e5865cbd8ed0978c25a13": "Word-level baseline is a simple word-level encoder for tweets.",
    "b91671715ad4fad56c67c28ce6f29e180fe08595": "Tracking infectious diseases",
    "a6d37b5975050da0b1959232ae756fc09e5f87e8": "word level baseline is a simple word-level encoder for tweets. The input tweet is first split into tokens along white-spaces. A more sophisticated tokenizer may be used, but for a fair comparison we wanted to keep language specific preprocessing to a minimum. The encoder is essentially the same as tweet2vec, with the input as words instead of characters. A lookup table stores word vectors for the $V$ (20K here) most common words, and the rest are grouped together under the `UNK' token.",
    "e82fa03f1638a8c59ceb62bb9a6b41b498950e1f": "BERT",
    "7ab9c0b4ceca1c142ff068f85015a249b14282d0": "Yes",
    "00050f7365e317dc0487e282a4c33804b58b1fb3": "No",
    "c5b0ed5db65051eebd858beaf303809aa815e8e5": "small",
    "10fb7dc031075946153baf0a0599e126de29e3a4": "It doesn't.",
    "e438445cf823893c841b2bc26cdce32ccc3f5cbe": "Fonts that are easy to read",
    "12f7fac818f0006cf33269c9eafd41bbb8979a48": "Inception V3",
    "d5a8fd8bb48dd1f75927e874bdea582b4732a0cd": "Yes",
    "1097768b89f8bd28d6ef6443c94feb04c1a1318e": "No",
    "fc1679c714eab822431bbe96f0e9cf4079cd8b8d": "59.4%",
    "23e2971c962bb6486bc0a66ff04242170dd22a1d": "Both are useful, but visual features are more useful.",
    "c9bc6f53b941863e801280343afa14248521ce43": "English",
    "07b70b2b799b9efa630e8737df8b1dd1284f032c": "29,794",
    "71a0c4f19be4ce1b1bae58a6e8f2a586e125d074": "Wikipedia",
    "c2eb743c9d0baf1781c3c0df9533fab588250af3": "(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, and (iv) models that integrate lower contexts via peephole connections.",
    "c35806cf68220b2b9bb082b62f493393b9bdff86": "SNLI",
    "f7d0fa52017a642a9f70091a252857fccca31f12": "The baselines were the models that used plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrated lower contexts via peephole connections.",
    "01209a3bead7c87bcdc628be2a5a26b41abde9d1": "SNLI, MultiNLI, Quora Question Pairs, SST",
    "2740e3d7d33173664c1c5ab292c7ec75ff6e0802": "WikiNews dataset",
    "db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21": "Farasa, MADAMIRA, RDI, MIT, Microsoft ATKS",
    "48bd71477d5f89333fa7ce5c4556e4d950fb16ed": "word surface forms, stems, prefixes, and suffixes",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "CHAR, SEG, PRIOR, CASE",
    "ad1be65c4f0655ac5c902d17f05454c0d4c4a15d": "The number of questions, the number of questions that require commonsense knowledge, the number of questions that can be answered from the text, the number of questions that can be answered from the text and the number of questions that can be answered from the text.",
    "2eb9280d72cde9de3aabbed993009a98a5fe0990": "2,100 texts and a total of approx. 14,000 questions",
    "154a721ccc1d425688942e22e75af711b423e086": "Amazon Mechanical Turk",
    "84bad9a821917cb96584cf5383c6d2a035358d7c": "We collected the data using Amazon Mechanical Turk.",
    "c9305e5794b65b33399c22ac8e4e024f6b757a30": "BERT",
    "56b7319be68197727baa7d498fa38af0a8440fe4": "linguistic, layout and topical features",
    "2268c9044e868ba0a16e92d2063ada87f68b5d03": "Yes, ensemble schemes helped in boosting performance by 0.02 F1.",
    "6b7354d7d715bad83183296ce2f3ddf2357cb449": "LSTM-CRF",
    "e949b28f6d1f20e18e82742e04d68158415dc61e": "None",
    "a1ac2a152710335519c9a907eec60d9f468b19db": "multi-tasking with another auxiliary task of PFD",
    "ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25": "Yes",
    "49eb52b3ec0647e165a5e41488088c80a20cc78f": "The instructor's reply is based on the context of the previous post.",
    "9bb7ae50bff91571a945c1af025ed2e67714a788": "BIBREF7",
    "81dbe9a9ddaa5d02b02e01a306d898015a56ffb6": "A series of linear contiguous posts",
    "348886b4762db063711ef8b7a10952375fbdcb57": "No",
    "1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b": "Multi30K",
    "f9aa055bf73185ba939dfb03454384810eb17ad1": "None",
    "d571e0b0f402a3d36fb30d70cdcd2911df883bc7": "Yes",
    "ce2b921e4442a21555d65d8ce4ef7e3bde931dfc": "French, Russian, Arabic, Chinese, Hindi, and Vietnamese",
    "2275b0e195cd9cb25f50c5c570da97a4cce5dca8": "We develop a technique to rapidly transfer an existing pre-trained model from English to other languages in an energy efficient way.",
    "37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6": "LAS",
    "d01c51155e4719bf587d114bcd403b273c77246f": "XNLI and Universal Dependencies v2.4",
    "9b4dc790e4ff49562992aae4fad3a38621fadd8b": "",
    "a1dac888f63c9efaf159d9bdfde7c938636f07b1": "Flickr tags",
    "1e4dbfc556cf237accb8b370de2f164fa723687b": "We use two metrics, one quantitative and one qualitative.",
    "fff5c24dca92bc7d5435a2600e6764f039551787": "We use the public StackExchange data dump.",
    "b2ecfd5480a2a4be98730e2d646dfb84daedab17": "TextWorld is a new benchmark for text-based games. It contains a set of procedurally generated text-based games, which are used to train and evaluate reinforcement learning agents.",
    "a3efe43a72b76b8f5e5111b54393d00e6a5c97ab": "6.8 million",
    "f1e90a553a4185a4b0299bd179f4f156df798bce": "CopyRNN, KEA, Maui, TextRank, SingleRank, PositionRank, TopicRank, Topical PageRank, MultipartiteRank, TF-IDF, TextTeaser, LexRank, SumBasic, LSA, LDA, PLSA, PLSA-LDA, PLSA-LDA-LSA, PLSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-LSA, PLSA-LDA-LSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA-LSA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA-LSA, PLSA-L",
    "19b7312cfdddb02c3d4eaa40301a67143a72a35a": "F INLINEFORM0 @ INLINEFORM1 and F INLINEFORM2 @ INLINEFORM3",
    "22744c3bc68f120669fc69490f8e539b09e34b94": "Yes",
    "dcea88698949da4a1bd00277c06df06c33f6a5ff": "The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.",
    "d7b60abb0091246e29d1a9c28467de598e090c20": "7.8%",
    "bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e": "89.6%",
    "5a6926de13a8cc25ce687c22741ba97a6e63d4ee": "US presidential primaries, Democratic and Republican National Conventions",
    "dcc1115aeaf87118736e86f3e3eb85bf5541281c": "Random Forest",
    "c74185bced810449c5f438f11ed6a578d1e359b4": "Wikipedia",
    "88e5d37617e14d6976cc602a168332fc23644f19": "Wikipedia and CMV",
    "45f7c03a686b68179cadb1413c5f3c1d373328bd": "CORD-19 dataset is a collection of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses.",
    "a2015f02dfb376bf9b218d1c897018f4e70424d7": "45,000 scholarly articles",
    "f697d00a82750b14376fe20a5a2b249e98bebe9b": "Bi-LSTM-CRF",
    "e0e379e546f1da9da874a2e90c79b41c60feb817": "Cross-View Training",
    "70148c8d0f345ea36200d5ba19d021924d98e759": "The McGurk effect is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see.",
    "27cf16bc9ef71761b9df6217f00f39f21130ce15": "Yes",
    "627b8d7b5b985394428c974aca5ba0c1bbbba377": "700",
    "126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0": "the baseline was the model used for back-translation",
    "7e4ef0a4debc048b244b61b4f7dc2518b5b466c0": "VP ellipsis",
    "b68f72aed961d5ba152e9dc50345e1e832196a76": "0.7",
    "cf874cd9023d901e10aa8664b813d32501e7e4d2": "Named Entity Recognition",
    "42084c41343e5a6ae58a22e5bfc5ce987b5173de": "Yes",
    "b637d6393ef3af7462917b81861531022b291933": "Yes",
    "8b9c12df9f89040f1485b3847a29f11b5c9262e0": "No",
    "72e4e26d0dd79c590c28b10938952a9f9497ff1e": "CNN-RNN based image-to-poem net and seq2seq model with parallel text corpus",
    "63b92dcc701ec77fdb3355ede5d37d2fbf057bcc": "Generating Shakespearean prose for a given painting",
    "58ee0cbf1d8e3711c617b1cd3d7aca8620e26187": "The generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score.",
    "f71b52e00e0be80c926f153b9fe0a06dd93af11e": "3.9",
    "8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe": "18,395 sentences",
    "54e945ea4b014e11ed4e1e61abc2aa9e68fea310": "29.65",
    "df0257ab04686ddf1c6c4d9b0529a7632330b98e": "Our experiments show promising results in three settings, with improved generalization and sample efficiency compared to existing methods.",
    "568fb7989a133564d84911e7cb58e4d8748243ef": "Go-Explore",
    "2c947447d81252397839d58c75ebcc71b34379b5": "CoinCollector, CookingWorld",
    "c01784b995f6594fdb23d7b62f20a35ae73eaa77": "They show that their learned policy generalize better than existing solutions to unseen games by comparing the performance of their learned policy on unseen games with the performance of existing solutions on unseen games.",
    "3415762847ed13acc3c90de60e3ef42612bc49af": "6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000",
    "223dc2b9ea34addc0f502003c2e1c1141f6b36a7": "BIBREF7",
    "e1ab11885f72b4658263a60751d956ba661c1d61": "Four Spanish subtasks",
    "c85b6f9bafc4c64fc538108ab40a0590a2f5768e": "second, second, fourth and fifth place",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "Machine translation",
    "0f6216b9e4e59252b0c1adfd1a848635437dfcdc": "DISC",
    "22ccee453e37536ddb0c1c1d17b0dbac04c6c607": "English",
    "d00bbeda2a45495e6261548710afa6b21ea32870": "The DISC corpus was used.",
    "71b1af123fe292fd9950b8439db834212f0b0e32": "The translated pairs must preserve the semantic relations between the two words when possible. This means that, when multiple translations are possible, the translation that best conveys the semantic relation between the two words found in the original English pair is selected.",
    "a616a3f0d244368ec588f04dfbc37d77fda01b4c": "English, German, Spanish, French, Italian, Russian, Mandarin, Kiswahili, Welsh, Turkish, Farsi, and Yue Chinese",
    "8e44c02c2d9fa56fb74ace35ee70a5add50b52ae": "Yes",
    "1522ccedbb1f668958f24cca070f640274bc2549": "extrinsic",
    "97466a37525536086ed5d6e5ed143df085682318": "We use the same approach as in BIBREF40, which is based on the idea that important concepts are mentioned more often than unimportant ones. We first extract all concepts from the documents and then count their occurrences. The most frequent concepts are then selected as the summary.",
    "e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a": "They were asked to read a short description of the topic and then select important elements from a list of propositions.",
    "d6191c4643201262a770947fc95a613f57bedb6b": "DIP corpus",
    "ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8": "A concept map is a labeled graph showing concepts as nodes and relationships between them as edges.",
    "fc4ae12576ea3a85ea6d150b46938890d63a7d18": "No",
    "19cf7884c0c509c189b1e74fe92c149ff59e444b": "Pseudo-perplexity is defined as the inverse probability of the set of the tokens and taking the $T^{th}$ root were $T$ is the number of tokens",
    "ecd5770cf8cb12cb34285e26ab834301c17c53e1": "LSTM-CNN",
    "4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729": "The data is annotated by humans.",
    "5529f26f72ce47440c2a64248063a6d5892b9fde": "We show the loss function analysis of transformer and our model. Figure FIGREF23 shows the validation performance of transformer against global training steps. Figure FIGREF21 show the validation performance of our model for the same number of global steps. Figure FIGREF22 shows that NMT loss is going down with the number of steps, while Figure FIGREF20 shows the degradation of the loss of our proposed RAT task.",
    "f85ca6135b101736f5c16c5b5d40895280016023": "transformer",
    "5fa36dc8f7c4e65acb962fc484989d20b8fdaeec": "Yes, they report results only on English data.",
    "d98847340e46ffe381992f1a594e75d3fb8d385e": "Logistic Regression and Deep Learning",
    "7006c66a15477b917656f435d66f63760d33a304": "4.4%",
    "a15bc19674d48cd9919ad1cf152bf49c88f4417d": "DSTC2",
    "440faf8d0af8291d324977ad0f68c8d661fe365e": "Reuters-8",
    "0ec56e15005a627d0b478a67fd627a9d85c3920e": "Text data",
    "a712718e6596ba946f29a99838d82f95b9ebb1ce": "We have an improvement of 12.27% on the accuracy and 14.86% on F1 score.",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "CNN, LSTM, LSTM-soft, LSTM-self",
    "dfca00be3284cc555a6a4eac4831471fb1f5875b": "14.56 samples",
    "a9a532399237b514c1227f2d6be8601474e669be": "UM Inventory",
    "26126068d72408555bcb52977cd669faf660bdf7": "Table TABREF9 shows the qualitative results of GM$\\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.",
    "660284b0a21fe3801e64dc9e0e51da5400223fe3": "The proposed approach is different from other WSD approaches employing word embeddings in the following ways:",
    "c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd": "They used the same tasks for both male and female speakers.",
    "f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3": "The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to understand how women under-representation in broadcast data can lead to bias in ASR performances.",
    "a253749e3b4c4f340778235f640ce694642a4555": "ESTER1, ESTER2, ETAPE and REPERE",
    "1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891": "2",
    "777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47": "65% of the speakers are men, speaking more than 75% of the time.",
    "2da4c3679111dd92a1d0869dae353ebe5989dfd2": "ESTER1, ESTER2, ETAPE and REPERE",
    "b7c3f3942a07c118e57130bc4c3ec4adc431d725": "ULMFiT",
    "a5505e25ee9ae84090e1442034ddbb3cedabcf04": "3rd and 2nd",
    "1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf": "Yes",
    "7fa3c2c0cf7f559d43e84076a9113a390c5ba03a": "No",
    "9a7ba5ed1779c664d2cac92494a43517d3e87c96": "A collection of sentences",
    "662870a90890c620a964720b2ca122a1139410ea": "French",
    "92d1a6df3041667dc662376938bc65527a5a1c3c": "Yes",
    "12159f04e0427fe33fa05af6ba8c950f1a5ce5ea": "The number of clusters",
    "a4a1fcef760b133e9aa876ac28145ad98a609927": "Dimensionality of the word vectors",
    "63bb2040fa107c5296351c2b5f0312336dad2863": "We cluster the embeddings with INLINEFORM0 -Means. The k-means clusters are initialized using \u201ck-means++\u201d as proposed in BIBREF9, while the algorithm is run for 300 iterations. We try different values for INLINEFORM1. For each INLINEFORM2, we repeat the clustering experiment with different seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia.",
    "01f4a0a19467947a8f3bdd7ec9fac75b5222d710": "perplexity, syntactic evaluation, grammaticality judgment",
    "7784d321ccc64db5141113b6783e4ba92fdd4b20": "neural network-based approaches",
    "218615a005f7f00606223005fef22c07057d9d77": "PTB",
    "867290103f762e1ddfa6f2ea30dd0a327f595182": "PTB",
    "907b3af3cfaf68fe188de9467ed1260e52ec6cf1": "The distribution of the number of followers, the number of URLs on tweets, and the verification of the users.",
    "56a8826cbee49560592b2d4b47b18ada236a12b9": "They manually inspected the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before).",
    "968b7c3553a668ba88da105eff067d57f393c63f": "Retweeted more than 1000 times",
    "f03df5d99b753dc4833ef27b32bb95ba53d790ee": "Accounts spreading fake news appear to have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only. Together with the fact that, on average, tweets containing fake news have more URLs than those spreading viral content, it is possible to hypothesize that, both, the ratio of friends/followers of the account producing a viral tweet and number of URLs contained in such a tweet could be useful to single-out fake news in Twitter. Not only that, but our finding related to the number of URLs is in line with intuitions behind the incentives to create fake news commonly found in the literature BIBREF9 (in particular that of obtaining revenue through click-through advertising).",
    "a8f51b4e334a917702422782329d97304a2fe139": "1000 retweets",
    "dca86fbe1d57b44986055b282a03c15ef7882e51": "The dataset BIBREF8, manually labelled by an expert, has been publicly released and is available to researchers and interested parties.",
    "27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa": "RNN-based NMT, Transformer-NMT",
    "b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72": "The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.",
    "808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc": "327",
    "36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd": "HLAs",
    "f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6": "Significant",
    "357eb9f0c07fa45e482d998a8268bd737beb827f": "Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset.",
    "ad08b215dca538930ef1f50b4e49cd25527028ad": "Yes, it was evaluated against a baseline.",
    "31101dc9937f108e27e08a5f34be44f0090b8b6b": "Cosine similarity",
    "e4a315e9c190cf96493eefe04ce4ba6ae6894550": "PolyResponse",
    "6263b2cba18207474786b303852d2f0d7068d4b6": "English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian",
    "c1c44fd96c3fa6e16949ae8fa453e511c6435c68": "Because the decoder can only see the left context, so it is hard to learn the right context. Masking words in the decoder can help the decoder learn the right context.",
    "d28d86524292506d4b24ae2d486725a6d57a3db3": "33.33",
    "feafcc1c4026d7f55a2c8ce7850d7e12030b5c22": "Yes, it is trained end-to-end.",
    "63488da6c7aff9e374561a24ba224e9ce7f65e40": "2019",
    "c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c": "Yes, it can be applied to other domains.",
    "a9337636b52de375c852682a2561af2c1db5ec63": "Yes",
    "45a5961a4e1d1c22874c4918e5c98bd3c0a670b3": "7",
    "30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1": "They use a CNN model to classify the question types.",
    "5c6fa86757410aee6f5a0762328637de03a569e9": "F1 score of 0.94 for both these datasets using BLSTM with attention and feature level transfer learning.",
    "7e38e0279a620d3df05ab9b5e2795044f18d4471": "Personal attack, racism, and sexism",
    "8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92": "No",
    "03ebb29c08375afc42a957c7b2dc1a42bed7b713": "The obtained results support our claim that the proposed system can be used as a base tool for information extraction for the Portuguese language.",
    "9cf070d6671ee4a6353f79a165aa648309e01295": "1500",
    "87bc6f83f7f90df3c6c37659139b92657c3f7a38": "We make use of the parallel Hindi sentences to disambiguate the PP attachments for English sentences.",
    "01e2d10178347d177519f792f86f25575106ddc7": "Switchboard, LORELEI",
    "021bfb7e180d67112b74f05ecb3fa13acc036c86": "Unsupervised term discovery (UTD)",
    "d201b9992809142fe59ae74508bc576f8ca538ff": "No",
    "c4628d965983934d7a2a9797a2de6a411629d5bc": "Yes",
    "bd419f4094186a5ce74ba6ac1622b24e29e553f4": "30.3%",
    "11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af": "The baseline is the lowest level of performance that can be achieved by a system.",
    "1269c5d8f61e821ee0029080c5ba2500421d5fa6": "Back Translation and Mix-Source",
    "e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb": "phrase-based and tree-to-string models",
    "b9ea841b817ba23281c95c7a769873b840dee8d5": "Yes",
    "219af68afeaecabdfd279f439f10ba7c231736e4": "TED talks",
    "a66a275a817f980c36e0b67d2e00bd823f63abf8": "We evaluated the style transfer itself in the following way. For each pair of sentences, the average score given by three evaluators was calculated, in which the answer that the translated sentence is more formal counts as +1, more informal as -1, and neither as 0.",
    "b6f466e0fdcb310ecd212fd90396d9d13e0c0504": "The data already contains them.",
    "62ea141d0fb342dfb97c69b49d1c978665b93b3c": "spelling, word order and grammatical errors",
    "a32c792a0cef03218bf66322245677fc2d5e5a31": "The parallel data is different in terms of style.",
    "0101ebfbaba75fd47868ad0c796ac44ebc19c566": "They split text to obtain sentence levels by using the sentence tokenizer in NLTK.",
    "50cb50657572e315fd452a89f3e0be465094b66f": "No",
    "981fd79dd69581659cb1d4e2b29178e82681eb4d": "The proposed model is an extension of the RNN encoder-decoder model by introducing a Refinement cell and an Adjustment cell.",
    "03e9ac1a2d90152cd041342a11293a1ebd33bcc3": "NLG datasets",
    "ef396a34436072cb3c40b0c9bc9179fee4a168ae": "Text classification and text semantic matching",
    "04bde1d2b445f971e97bb46ade2d0290981c7a32": "The parameters of compositional function vary from position to position and from sample to sample, allowing for more sophisticated operations on the input.",
    "bfbd6040cb95b179118557352e8e3899ef25c525": "Yes",
    "d6e353e0231d09fd5dcba493544d53706f3fe1ab": "Mean Opinion Score (MOS)",
    "7bd6a6ec230e1efb27d691762cc0674237dc7967": "Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20",
    "6aaf12505add25dd133c7b0dafe8f4fe966d1f1d": "LSTM",
    "73906462bd3415f23d6378590a5ba28709b17605": "lexical overlap, sentence length, negation words",
    "5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11": "English, Spanish, Finnish",
    "88bf368491f9613767f696f84b4bb1f5a7d7cb48": "Machine translation",
    "0737954caf66f2b4c898b356d2a3c43748b9706b": "No",
    "664b3eadc12c8dde309e8bbd59e9af961a433cde": "Yes.",
    "b3307d5b68c57a074c483636affee41054be06d1": "lexical overlap, sentence length, negation words",
    "bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31": "English, Spanish, Finnish",
    "12d7055baf5bffb6e9e95e977c000ef2e77a4362": "$\\sim $ 10%",
    "498c0229f831c82a5eb494cdb3547452112a66a0": "They use a routing strategy in which difficult examples are routed to experts and other instances to crowd annotators.",
    "8c48c726bb17a17d70ab29db4d65a93030dd5382": "We used the large version of the universal sentence encoder with a transformer. We did not update the pretrained sentence encoder parameters during training.",
    "89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f": "common crawl data",
    "06b5272774ec43ee5facfa7111033386f06cf448": "Sentence",
    "08b57deb237f15061e4029b6718f1393fa26acce": "Crowdworkers were located in the US and hired on the BIBREF22 platform.",
    "9b7655d39c7a19a23eb8944568eb5618042b9026": "NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, Rosette Text Analytics, Google Cloud, spaCy, CogComp-NLP, Stanford NLP NER",
    "cd06d775f491b4a17c9d616a8729fd45aa2e79bf": "Neutral",
    "1329280df5ee9e902b2742bde4a97bc3e6573ff3": "No",
    "58c6737070ef559e9220a8d08adc481fdcd53a24": "CCR",
    "0af16b164db20d8569df4ce688d5a62c861ace0b": "BOW, TFIDF, TextCNN, C-TextCNN",
    "78a4ec72d76f0a736a4a01369a42b092922203b6": "Friends and EmotionPush",
    "6a14379fee26a39631aebd0e14511ce3756e42ad": "FriendsBERT and ChatBERT",
    "81588e0e207303c2867c896f3911a54a1ef7c874": "Friends is a speech-based dataset and EmotionPush is a chat-based dataset.",
    "dd09db5eb321083dba16c2550676e60682f9a0cd": "Joy, Sadness, Anger, Neutral",
    "40c0f97c3547232d6aa039fcb330f142668dea4b": "Yes",
    "777217e025132ddc173cf33747ee590628a8f62f": "We compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34. In particular, we compare the following methods:",
    "2dbf6fe095cd879a9bf40f110b7b72c8bdde9475": "Hierarchical embedding structure",
    "7d483077ed7f2f504d59f4fc2f162741fa5ac23b": "We fit a Poisson sefe model to the shopping data, grouped by season. We use the same context as in the efe model, i.e., the set of items purchased in the same shopping trip, excluding the item of interest. We use the same vocabulary as in the efe model, i.e., the 15k most frequent items. We use the same hyperparameters as in the efe model, i.e., INLINEFORM0 and INLINEFORM1.",
    "de830c534c23f103288c198eb19174c76bfd38a1": "artificial, ai, abilities, consciousness",
    "b0d66760829f111b8fad0bd81ca331ddd943ef41": "Improving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in textGAN.",
    "ae7c93646aa5f3206cd759904965b4d484d12f83": "It is a novel loss function, ARL, to automatically balance RL with MLE.",
    "d1ec42b2b5a3c956ff528543636e024bfde5e5ba": "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set",
    "1dac4bc5af239024566fcb0f43bb9ff1c248ecec": "No",
    "3bf0306e9bd044f723e38170c13455877b2aeec3": "We regard headlines receiving lots of comments as sensational samples and the headlines generated by a summarization model as non-sensational samples.",
    "2858620e0498db2f2224bfbed5263432f0570832": "No master node",
    "545e92833b0ad4ba32eac5997edecf97a366a244": "Number of MP iterations",
    "cb12c19f9d14bef7b2f778892d9071eea2d6c63d": "MPAD",
    "9193006f359c53eb937deff1248ee3317978e576": "Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013",
    "bc67b91dd73acded2d52fd4fee732b7a9722ea8b": "The message passing framework is a framework under which many of the recently introduced GNNs can be reformulated.",
    "49c32a2a64eb41381e5f12ccea4150cac9f3303d": "Kappa statistics",
    "bbb77f2d6685c9257763ca38afaaef29044b4018": "joshi2015harnessing",
    "22732cb9476e521452bf0538f3fdb94cf3867651": "Emoticons, laughter expressions such as \u201clol\u201d etc",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "Unigrams and Pragmatic features",
    "74b338d5352fe1a6fd592e38269a4c81fe79b866": "Simple Gaze Based Features and Complex Gaze Based Features",
    "d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf": "Computational approaches",
    "f903396d943541a8cc65edefb04ca37814ed30dd": "Reddit",
    "ba28ce9a2f7e8524243adf288cc3f11055e667bb": "Yes",
    "975e60535724f4149c7488699a199ba2920a062c": "They are all from the same school.",
    "b970f48d30775d3468952795bc72976baab3438e": ""
}