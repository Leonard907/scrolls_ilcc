Validation ID: b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54
Question: which multilingual approaches do they compare with?
Answer: multilingual NMT
True answer: BIBREF19, BIBREF20

Validation ID: b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54
Question: which multilingual approaches do they compare with?
Answer: multilingual NMT
True answer: multilingual NMT (MNMT) BIBREF19

Validation ID: f5e6f43454332e0521a778db0b769481e23e7682
Question: what are the pivot-based baselines?
Answer: pivoting, pivot-synthetic
True answer: pivoting, pivoting$_{\rm m}$

Validation ID: f5e6f43454332e0521a778db0b769481e23e7682
Question: what are the pivot-based baselines?
Answer: pivoting, pivot-synthetic
True answer: firstly translates a source language into the pivot language which is later translated to the target language

Validation ID: 9a05a5f4351db75da371f7ac12eb0b03607c4b87
Question: which datasets did they experiment with?
Answer: Europarl and MultiUN
True answer: Europarl, MultiUN

Validation ID: 9a05a5f4351db75da371f7ac12eb0b03607c4b87
Question: which datasets did they experiment with?
Answer: Europarl and MultiUN
True answer: Europarl BIBREF31, MultiUN BIBREF32

Validation ID: 5eda469a8a77f028d0c5f1acd296111085614537
Question: what language pairs are explored?
Answer: Fr-En-Es, De-En-Fr, Ro-En-De
True answer: De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru

Validation ID: 5eda469a8a77f028d0c5f1acd296111085614537
Question: what language pairs are explored?
Answer: Fr-En-Es, De-En-Fr, Ro-En-De
True answer: French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation

Validation ID: 18c5d366b1da8447b5404eab71f4cc658ba12e6f
Question: what ner models were evaluated?
Answer: Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF
True answer: Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer

Validation ID: 18c5d366b1da8447b5404eab71f4cc658ba12e6f
Question: what ner models were evaluated?
Answer: Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13, BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF
True answer: Stanford NER, spaCy 2.0, recurrent model with a CRF top layer

Validation ID: b5e4866f0685299f1d7af267bbcc4afe2aab806f
Question: what is the source of the news sentences?
Answer: ilur.am
True answer: ilur.am

Validation ID: b5e4866f0685299f1d7af267bbcc4afe2aab806f
Question: what is the source of the news sentences?
Answer: ilur.am
True answer: links between Wikipedia articles to generate sequences of named-entity annotated tokens

Validation ID: 1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590
Question: did they use a crowdsourcing platform for manual annotations?
Answer: No
True answer: No

Validation ID: 1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590
Question: did they use a crowdsourcing platform for manual annotations?
Answer: No
True answer: No

Validation ID: b6ae8e10c6a0d34c834f18f66ab730b670fb528c
Question: what are the topics pulled from Reddit?
Answer: politics, business, science, AskReddit, and the Reddit frontpage
True answer: politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. 

Validation ID: b6ae8e10c6a0d34c834f18f66ab730b670fb528c
Question: what are the topics pulled from Reddit?
Answer: politics, business, science, AskReddit, and the Reddit frontpage
True answer: training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit

Validation ID: a87a009c242d57c51fc94fe312af5e02070f898b
Question: What predictive model do they build?
Answer: A predictive model that analyzes new content.
True answer: logistic regression models

Validation ID: a87a009c242d57c51fc94fe312af5e02070f898b
Question: What predictive model do they build?
Answer: A predictive model that analyzes new content.
True answer: logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.

Validation ID: ef4dba073d24042f24886580ae77add5326f2130
Question: What accuracy does the proposed system achieve?
Answer: 0.9
True answer: F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data 

Validation ID: ef4dba073d24042f24886580ae77add5326f2130
Question: What accuracy does the proposed system achieve?
Answer: 0.9
True answer: F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)

Validation ID: 2df4a045a9cd7b44874340b6fdf9308d3c55327a
Question: What crowdsourcing platform is used?
Answer: Amazon Mechanical Turk
True answer: Unanswerable

Validation ID: 2df4a045a9cd7b44874340b6fdf9308d3c55327a
Question: What crowdsourcing platform is used?
Answer: Amazon Mechanical Turk
True answer: They did not use any platform, instead they hired undergraduate students to do the annotation.

Validation ID: a313e98994fc039a82aa2447c411dda92c65a470
Question: How do they match words before reordering them?
Answer: Using a bilingual dictionary (Google Translate word translation in our case).
True answer: Unanswerable

Validation ID: a313e98994fc039a82aa2447c411dda92c65a470
Question: How do they match words before reordering them?
Answer: Using a bilingual dictionary (Google Translate word translation in our case).
True answer: CFILT-preorder system

Validation ID: 37861be6aecd9242c4fdccdfcd06e48f3f1f8f81
Question: On how many language pairs do they show that preordering assisting language sentences helps translation quality?
Answer: 5
True answer: 5

Validation ID: 37861be6aecd9242c4fdccdfcd06e48f3f1f8f81
Question: On how many language pairs do they show that preordering assisting language sentences helps translation quality?
Answer: 5
True answer: Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.

Validation ID: 7e62a53823aba08bc26b2812db016f5ce6159565
Question: Which dataset(s) do they experiment with?
Answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus, ILCI multilingual parallel corpus
True answer: IITB English-Hindi parallel corpus BIBREF22, ILCI English-Hindi parallel corpus

Validation ID: 7e62a53823aba08bc26b2812db016f5ce6159565
Question: Which dataset(s) do they experiment with?
Answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus, ILCI multilingual parallel corpus
True answer: IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus

Validation ID: 9eabb54c2408dac24f00f92cf1061258c7ea2e1a
Question: Which information about text structure is included in the corpus?
Answer: Information on text structure, typography, and images
True answer: paragraphs, lines, Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation

Validation ID: 9eabb54c2408dac24f00f92cf1061258c7ea2e1a
Question: Which information about text structure is included in the corpus?
Answer: Information on text structure, typography, and images
True answer: paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))

Validation ID: 3d013f15796ae7fed5272183a166c45f16e24e39
Question: Which information about typography is included in the corpus?
Answer: Font type and font style
True answer: font type, font style, Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page

Validation ID: 3d013f15796ae7fed5272183a166c45f16e24e39
Question: Which information about typography is included in the corpus?
Answer: Font type and font style
True answer: font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer, A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer

Validation ID: 9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc
Question: On which benchmarks they achieve the state of the art?
Answer: WebQSP and SimpleQuestions
True answer: SimpleQuestions, WebQSP

Validation ID: 9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc
Question: On which benchmarks they achieve the state of the art?
Answer: WebQSP and SimpleQuestions
True answer: WebQSP, SimpleQuestions

Validation ID: d3aa0449708cc861a51551b128d73e11d62207d2
Question: What they use in their propsoed framework?
Answer: HR-BiLSTM
True answer: break the relation names into word sequences,  relation-level and word-level relation representations, bidirectional LSTMs (BiLSTMs),  residual learning method

Validation ID: d3aa0449708cc861a51551b128d73e11d62207d2
Question: What they use in their propsoed framework?
Answer: HR-BiLSTM
True answer: break the relation names into word sequences for question-relation matching, build both relation-level and word-level relation representations, use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations, residual learning method for sequence matching, a simple KBQA implementation composed of two-step relation detection

Validation ID: cfbec1ef032ac968560a7c76dec70faf1269b27c
Question: What does KBQA abbreviate for?
Answer: Knowledge Base Question Answering
True answer: Knowledge Base Question Answering

Validation ID: cfbec1ef032ac968560a7c76dec70faf1269b27c
Question: What does KBQA abbreviate for?
Answer: Knowledge Base Question Answering
True answer: Knowledge Base Question Answering 

Validation ID: c0e341c4d2253eb42c8840381b082aae274eddad
Question: What is te core component for KBQA?
Answer: Relation Detection
True answer: answer questions by obtaining information from KB tuples 

Validation ID: c0e341c4d2253eb42c8840381b082aae274eddad
Question: What is te core component for KBQA?
Answer: Relation Detection
True answer: hierarchical matching between questions and relations with residual learning

Validation ID: 1ec152119cf756b16191b236c85522afeed11f59
Question: What experiments are proposed to test that upper layers produce context-specific embeddings?
Answer: The authors propose to measure the self-similarity of words in different layers.
True answer: They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.

Validation ID: 1ec152119cf756b16191b236c85522afeed11f59
Question: What experiments are proposed to test that upper layers produce context-specific embeddings?
Answer: The authors propose to measure the self-similarity of words in different layers.
True answer: They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  
They plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.
They plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another.

Validation ID: 891c2001d6baaaf0da4e65b647402acac621a7d2
Question: How do they calculate a static embedding for each word?
Answer: By taking the first principal component of its contextualized representations in a given layer.
True answer: They use the first principal component of a word's contextualized representation in a given layer as its static embedding.

Validation ID: 891c2001d6baaaf0da4e65b647402acac621a7d2
Question: How do they calculate a static embedding for each word?
Answer: By taking the first principal component of its contextualized representations in a given layer.
True answer:  by taking the first principal component (PC) of its contextualized representations in a given layer

Validation ID: 66c96c297c2cffdf5013bab5e95b59101cb38655
Question: What is the performance of BERT on the task?
Answer: 0.979
True answer: F1 scores are:
HUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)
Medoccan: Detection(0.972), Classification (0.967)

Validation ID: 66c96c297c2cffdf5013bab5e95b59101cb38655
Question: What is the performance of BERT on the task?
Answer: 0.979
True answer: BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table 

Validation ID: 6b53e1f46ae4ba9b75117fc6e593abded89366be
Question: What are the other algorithms tested?
Answer: CRF, spaCy
True answer: NER model, CRF classifier trained with sklearn-crfsuite, classifier has been developed that consists of regular-expressions and dictionary look-up

Validation ID: 6b53e1f46ae4ba9b75117fc6e593abded89366be
Question: What are the other algorithms tested?
Answer: CRF, spaCy
True answer: As the simplest baseline, a sensitive data recogniser and classifier, Conditional Random Fields (CRF), spaCy 

Validation ID: c0bee6539eb6956a7347daa9d2419b367bd02064
Question: Does BERT reach the best performance among all the algorithms compared?
Answer: Yes
True answer: No

Validation ID: c0bee6539eb6956a7347daa9d2419b367bd02064
Question: Does BERT reach the best performance among all the algorithms compared?
Answer: Yes
True answer: No

Validation ID: 3de0487276bb5961586acc6e9f82934ef8cb668c
Question: What are the clinical datasets used in the paper?
Answer: NUBes-PHI and MEDDOCAN
True answer: MEDDOCAN, NUBes-PHI

Validation ID: 3de0487276bb5961586acc6e9f82934ef8cb668c
Question: What are the clinical datasets used in the paper?
Answer: NUBes-PHI and MEDDOCAN
True answer: MEDDOCAN, NUBes 

Validation ID: 113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab
Question: how is model compactness measured?
Answer: The power of these embeddings is demonstrated by what happens when one feeds the same input word to the model with different language tokens, as is seen in Table TABREF30. Impressively, this even works when the source sequence is in the wrong script for the language, as is seen in the entry for Arabic.
True answer: Using file size on disk

Validation ID: 113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab
Question: how is model compactness measured?
Answer: The power of these embeddings is demonstrated by what happens when one feeds the same input word to the model with different language tokens, as is seen in Table TABREF30. Impressively, this even works when the source sequence is in the wrong script for the language, as is seen in the entry for Arabic.
True answer: 15.4 MB

Validation ID: 0752d71a0a1f73b3482a888313622ce9e9870d6e
Question: what was the baseline?
Answer: wFST
True answer: system presented by deri2016grapheme

Validation ID: 0752d71a0a1f73b3482a888313622ce9e9870d6e
Question: what was the baseline?
Answer: wFST
True answer: wFST

Validation ID: 55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3
Question: what evaluation metrics were used?
Answer: Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)
True answer: Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)

Validation ID: 55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3
Question: what evaluation metrics were used?
Answer: Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)
True answer: PER, WER, WER 100

Validation ID: 4eaf9787f51cd7cdc45eb85cf223d752328c6ee4
Question: what datasets did they use?
Answer: Wiktionary
True answer: the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary

Validation ID: 4eaf9787f51cd7cdc45eb85cf223d752328c6ee4
Question: what datasets did they use?
Answer: Wiktionary
True answer: multilingual pronunciation corpus collected by deri2016grapheme

Validation ID: fb2b536dc8e442dffab408db992b971e86548158
Question: What is the interannotator agreement for the human evaluation?
Answer: 49% for CS and 41% for Stat2015
True answer: Unanswerable

Validation ID: fb2b536dc8e442dffab408db992b971e86548158
Question: What is the interannotator agreement for the human evaluation?
Answer: 49% for CS and 41% for Stat2015
True answer: Unanswerable

Validation ID: 31735ec3d83c40b79d11df5c34154849aeb3fb47
Question: Who were the human evaluators used?
Answer: 20 evaluators were recruited from our institution
True answer: 20 evaluators were recruited from our institution and asked to each perform 20 annotations

Validation ID: 31735ec3d83c40b79d11df5c34154849aeb3fb47
Question: Who were the human evaluators used?
Answer: 20 evaluators were recruited from our institution
True answer: 20 annotatos from author's institution

Validation ID: 10d450960907091f13e0be55f40bcb96f44dd074
Question: Is the template-based model realistic? ?
Answer: Yes
True answer: Yes

Validation ID: 10d450960907091f13e0be55f40bcb96f44dd074
Question: Is the template-based model realistic? ?
Answer: Yes
True answer: Yes

Validation ID: b5608076d91450b0d295ad14c3e3a90d7e168d0e
Question: Is the student reflection data very different from the newspaper data? ?
Answer: Yes
True answer: Yes

Validation ID: b5608076d91450b0d295ad14c3e3a90d7e168d0e
Question: Is the student reflection data very different from the newspaper data? ?
Answer: Yes
True answer: Yes

Validation ID: c21b87c97d1afac85ece2450ee76d01c946de668
Question: What is the recent abstractive summarization method in this paper?
Answer: pointer networks with coverage mechanism (PG-net)
True answer: pointer networks with coverage mechanism (PG-net)

Validation ID: c21b87c97d1afac85ece2450ee76d01c946de668
Question: What is the recent abstractive summarization method in this paper?
Answer: pointer networks with coverage mechanism (PG-net)
True answer:  pointer networks with coverage mechanism (PG-net)BIBREF0

Validation ID: d087539e6a38c42f0a521ff2173ef42c0733878e
Question: Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models? ?
Answer: Because the student and teacher model output spaces are not identical, intermediate model outputs may prove hard to align.
True answer: While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.

Validation ID: d087539e6a38c42f0a521ff2173ef42c0733878e
Question: Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models? ?
Answer: Because the student and teacher model output spaces are not identical, intermediate model outputs may prove hard to align.
True answer: distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.

Validation ID: efe9bad55107a6be7704ed97ecce948a8ca7b1d2
Question: What state-of-the-art compression techniques were used in the comparison?
Answer: Model weight quantization techniques
True answer: baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)

Validation ID: efe9bad55107a6be7704ed97ecce948a8ca7b1d2
Question: What state-of-the-art compression techniques were used in the comparison?
Answer: Model weight quantization techniques
True answer: NoKD, PKD, BERTBASE teacher model

Validation ID: 71e4ba4e87e6596aeca187127c0d088df6570c57
Question: What evaluations methods do they take?
Answer: I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.
True answer: Unanswerable

Validation ID: 71e4ba4e87e6596aeca187127c0d088df6570c57
Question: What evaluations methods do they take?
Answer: I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.
True answer: Unanswerable

Validation ID: 7561a968470a8936d10e1ba722d2f38b5a9a4d38
Question: What is the size of the dataset?
Answer: 30,000 images
True answer: 30,000

Validation ID: 7561a968470a8936d10e1ba722d2f38b5a9a4d38
Question: What is the size of the dataset?
Answer: 30,000 images
True answer: collection of over 30,000 images with 5 crowdsourced descriptions each

Validation ID: 6d4400f45bd97b812e946b8a682b018826e841f1
Question: Which methods are considered to find examples of biases and unwarranted inferences??
Answer: I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.
True answer: spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering

Validation ID: 6d4400f45bd97b812e946b8a682b018826e841f1
Question: Which methods are considered to find examples of biases and unwarranted inferences??
Answer: I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce.
True answer: Looking for adjectives marking the noun "baby" and also looking for most-common adjectives related to certain nouns using POS-tagging

Validation ID: 26c2e1eb12143d985e4fb50543cf0d1eb4395e67
Question: What biases are found in the dataset?
Answer: Stereotypes
True answer: Ethnic bias

Validation ID: 26c2e1eb12143d985e4fb50543cf0d1eb4395e67
Question: What biases are found in the dataset?
Answer: Stereotypes
True answer: adjectives are used to create “more narrow labels [or subtypes] for individuals who do not fit with general social category expectations”

Validation ID: f17ca24b135f9fe6bb25dc5084b13e1637ec7744
Question: What discourse relations does it work best/worst for?
Answer: Implicit-Comparison
True answer: explicit discourse relations

Validation ID: f17ca24b135f9fe6bb25dc5084b13e1637ec7744
Question: What discourse relations does it work best/worst for?
Answer: Implicit-Comparison
True answer: Best: Expansion (Exp). Worst: Comparison (Comp).

Validation ID: bd5bd1765362c2d972a762ca12675108754aa437
Question: How much does this model improve state-of-the-art?
Answer: 3.02
True answer: the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 )., full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent., Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. 

Validation ID: bd5bd1765362c2d972a762ca12675108754aa437
Question: How much does this model improve state-of-the-art?
Answer: 3.02
True answer: 1 percent

Validation ID: d9b6c61fc6d29ad399d27b931b6cb7b1117b314a
Question: Where is a question generation model used?
Answer: In this paper
True answer: The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. 

Validation ID: d9b6c61fc6d29ad399d27b931b6cb7b1117b314a
Question: Where is a question generation model used?
Answer: In this paper
True answer: framework consisting of both a question answering model and a question generation model

Validation ID: d27438b11bc70e706431dda0af2b1c0b0d209f96
Question: Were any of these tasks evaluated in any previous work?
Answer: BIBREF1
True answer: Yes

Validation ID: d27438b11bc70e706431dda0af2b1c0b0d209f96
Question: Were any of these tasks evaluated in any previous work?
Answer: BIBREF1, BIBREF2, BIBREF3
True answer: Yes

Validation ID: 8d4ac4afbf5b14f412171729ceb5e822afcfa3f4
Question: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?
Answer: No
True answer: No

Validation ID: 8d4ac4afbf5b14f412171729ceb5e822afcfa3f4
Question: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?
Answer: No
True answer: No

Validation ID: 3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f
Question: Which demographic dimensions of people do they obtain?
Answer: Gender, Industry, and Location
True answer: occupation, industry, profile information, language use, gender 

Validation ID: 3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f
Question: Which demographic dimensions of people do they obtain?
Answer: Gender, Industry, and Location
True answer: density of users, gender distribution

Validation ID: 07d15501a599bae7eb4a9ead63e9df3d55b3dc35
Question: How do they obtain psychological dimensions of people?
Answer: By analyzing the words they use
True answer: using the Meaning Extraction Method

Validation ID: 07d15501a599bae7eb4a9ead63e9df3d55b3dc35
Question: How do they obtain psychological dimensions of people?
Answer: By analyzing the words they use
True answer: Unanswerable

Validation ID: 99e78c390932594bd833be0f5c890af5c605d808
Question: What is the baseline?
Answer: The baseline is the performance of a model that is not trained on the task.
True answer: QA PGNet, Multi-decoder QA PGNet with lookup table embedding

Validation ID: 99e78c390932594bd833be0f5c890af5c605d808
Question: What is the baseline?
Answer: The baseline is the performance of a model that is not trained on the task.
True answer: QA PGNet and Multi-decoder QA PGNet

Validation ID: 861187338c5ad445b9acddba8f2c7688785667b1
Question: Is the data de-identified?
Answer: Yes
True answer: Yes

Validation ID: 861187338c5ad445b9acddba8f2c7688785667b1
Question: Is the data de-identified?
Answer: Yes
True answer: Yes

Validation ID: f161e6d5aecf8fae3a26374dcb3e4e1b40530c95
Question: What embeddings are used?
Answer: ELMo, BERT, and ClinicalBERT
True answer:  simple lookup table embeddings learned from scratch, using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13

Validation ID: f161e6d5aecf8fae3a26374dcb3e4e1b40530c95
Question: What embeddings are used?
Answer: ELMo, BERT, and ClinicalBERT
True answer: ELMO BIBREF11, BERT BIBREF12 and ClinicalBERT BIBREF13

Validation ID: 12c50dea84f9a8845795fa8b8c1679328bd66246
Question: What datasets did they use for evaluation?
Answer: CSAT, 20newsgroups and Fisher
True answer: CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus

Validation ID: 12c50dea84f9a8845795fa8b8c1679328bd66246
Question: What datasets did they use for evaluation?
Answer: CSAT, 20newsgroups and Fisher
True answer: CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus

Validation ID: 0810b43404686ddfe4ca84783477ae300fdd2ea4
Question: On top of BERT does the RNN layer work better or the transformer layer?
Answer: transformer layer
True answer: Transformer over BERT (ToBERT)

Validation ID: 0810b43404686ddfe4ca84783477ae300fdd2ea4
Question: On top of BERT does the RNN layer work better or the transformer layer?
Answer: transformer layer
True answer: The transformer layer

Validation ID: 455d4ef8611f62b1361be4f6387b222858bb5e56
Question: How was this data collected?
Answer: CrowdFlower
True answer: CrowdFlower

Validation ID: 455d4ef8611f62b1361be4f6387b222858bb5e56
Question: How was this data collected?
Answer: CrowdFlower
True answer: The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English.

Validation ID: bc16ce6e9c61ae13d46970ebe6c4728a47f8f425
Question: What is the average length of dialog?
Answer: 8533
True answer: 4.49 turns

Validation ID: bc16ce6e9c61ae13d46970ebe6c4728a47f8f425
Question: What is the average length of dialog?
Answer: 8533
True answer: 4.5 turns per dialog (8533 turns / 1900 dialogs)

Validation ID: 1ff0fccf0dca95a6630380c84b0422bed854269a
Question: How are models evaluated in this human-machine communication game?
Answer: Accuracy and efficiency
True answer: by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews

Validation ID: 1ff0fccf0dca95a6630380c84b0422bed854269a
Question: How are models evaluated in this human-machine communication game?
Answer: Accuracy and efficiency
True answer: efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence

Validation ID: 3d7d865e905295d11f1e85af5fa89b210e3e9fdf
Question: How many participants were trying this communication game?
Answer: 100
True answer: 100 

Validation ID: 3d7d865e905295d11f1e85af5fa89b210e3e9fdf
Question: How many participants were trying this communication game?
Answer: 100
True answer: 100 crowdworkers 

Validation ID: 2ad4d3d222f5237ed97923640bc8e199409cbe52
Question: What user variations have been tested?
Answer: crowdworkers on Amazon Mechanical Turk (AMT)
True answer: completion times and accuracies 

Validation ID: 2ad4d3d222f5237ed97923640bc8e199409cbe52
Question: What user variations have been tested?
Answer: crowdworkers on Amazon Mechanical Turk (AMT)
True answer: Unanswerable

Validation ID: 3fad42be0fb2052bb404b989cc7d58b440cd23a0
Question: What are the baselines used?
Answer: Unif and Stopword
True answer: Unif and Stopword

Validation ID: 3fad42be0fb2052bb404b989cc7d58b440cd23a0
Question: What are the baselines used?
Answer: Unif and Stopword
True answer: Unif and Stopword

Validation ID: ee417fea65f9b1029455797671da0840c8c1abbe
Question: Do they use off-the-shelf NLP systems to build their assitant?
Answer: Yes, they use off-the-shelf NLP systems to build their assitant.
True answer: No

Validation ID: ee417fea65f9b1029455797671da0840c8c1abbe
Question: Do they use off-the-shelf NLP systems to build their assitant?
Answer: Yes, they use off-the-shelf NLP systems to build their assitant.
True answer: No

Validation ID: ca5a82b54cb707c9b947aa8445aac51ea218b23a
Question: How does the IPA label data after interacting with users?
Answer: It augments the resources with structured and to some extent labeled training data for further possible implementation of learnable dialogue components.
True answer: It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).

Validation ID: ca5a82b54cb707c9b947aa8445aac51ea218b23a
Question: How does the IPA label data after interacting with users?
Answer: It augments the resources with structured and to some extent labeled training data for further possible implementation of learnable dialogue components.
True answer: Plain dialogues with unique dialogue indexes, Plain Information Dictionary information (e.g., extracted entities) collected for the whole dialogue, Pairs of questions (i.e., user requests) and responses (i.e., bot responses), Triples in the form of (User Request, Next Action, Response)

Validation ID: da55bd769721b878dd17f07f124a37a0a165db02
Question: What kind of repetitive and time-consuming activities does their assistant handle?
Answer: The assistant handles repetitive and time-consuming activities like entering data into a system, registering into accounts, and accomplishing straightforward but repetitive workflows.
True answer: Unanswerable

Validation ID: da55bd769721b878dd17f07f124a37a0a165db02
Question: What kind of repetitive and time-consuming activities does their assistant handle?
Answer: The assistant handles repetitive and time-consuming activities like entering data into a system, registering into accounts, and accomplishing straightforward but repetitive workflows.
True answer:  What kind of topic (or sub-topic) a student has a problem with, At which examination mode (i.e., quiz, chapter level training or exercise, section level training or exercise, or final examination) the student is working right now,  the exact question number and exact problem formulation

Validation ID: feb448860918ef5b905bb25d7b855ba389117c1f
Question: How was the audio data gathered?
Answer: All India Radio news channel
True answer: Through the All India Radio new channel where actors read news.

Validation ID: feb448860918ef5b905bb25d7b855ba389117c1f
Question: How was the audio data gathered?
Answer: All India Radio news channel
True answer:  $\textbf {All India Radio}$ news channel

Validation ID: 4bc2784be43d599000cb71d31928908250d4cef3
Question: What is the GhostVLAD approach?
Answer: It is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). Which means that we compute the matrix V for both normal cluster K and ghost clusters G, but we will not include the vectors belongs to ghost cluster from V during concatenation of the features. Due to which, during feature aggregation stage the contribution of the noisy and unwanted features to normal VLAD clusters are assigned less weights while Ghost clusters absorb most of the weight. We illustrate this in Figure 1(Right Side), where the ghost clusters are shown in red color. We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into
True answer: extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters

Validation ID: 4bc2784be43d599000cb71d31928908250d4cef3
Question: What is the GhostVLAD approach?
Answer: It is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). Which means that we compute the matrix V for both normal cluster K and ghost clusters G, but we will not include the vectors belongs to ghost cluster from V during concatenation of the features. Due to which, during feature aggregation stage the contribution of the noisy and unwanted features to normal VLAD clusters are assigned less weights while Ghost clusters absorb most of the weight. We illustrate this in Figure 1(Right Side), where the ghost clusters are shown in red color. We use Ghost clusters when we are computing the V matrix, but they are excluded during the concatenation stage. These concatenated features are fed into
True answer: An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content.

Validation ID: 75df70ce7aa714ec4c6456d0c51f82a16227f2cb
Question: Which 7 Indian languages do they experiment with?
Answer: Kannada, Hindi, Telugu, Malayalam, Bengali, and English
True answer: Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam

Validation ID: 75df70ce7aa714ec4c6456d0c51f82a16227f2cb
Question: Which 7 Indian languages do they experiment with?
Answer: Kannada, Hindi, Telugu, Malayalam, Bengali, and English
True answer: Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)

Validation ID: 6424e442b34a576f904d9649d63acf1e4fdefdfc
Question: What datasets do they evaluate on?
Answer: Wall Street Journal (WSJ) portion of the Penn Treebank
True answer:  Wall Street Journal (WSJ) portion of the Penn Treebank

Validation ID: 6424e442b34a576f904d9649d63acf1e4fdefdfc
Question: What datasets do they evaluate on?
Answer: Wall Street Journal (WSJ) portion of the Penn Treebank
True answer: Unanswerable

Validation ID: 5eabfc6cc8aa8a99e6e42514ef9584569cb75dec
Question: Do they evaluate only on English datasets?
Answer: Yes
True answer: Yes

Validation ID: 5eabfc6cc8aa8a99e6e42514ef9584569cb75dec
Question: Do they evaluate only on English datasets?
Answer: Yes
True answer: Unanswerable

Validation ID: 887c6727e9f25ade61b4853a869fe712fe0b703d
Question: What is the invertibility condition?
Answer: (1) INLINEFORM0 and (2) INLINEFORM1 exists.
True answer: The neural projector must be invertible.

Validation ID: 887c6727e9f25ade61b4853a869fe712fe0b703d
Question: What is the invertibility condition?
Answer: (1) INLINEFORM0 and (2) INLINEFORM1 exists.
True answer: we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists

Validation ID: 6236762b5631d9e395f81e1ebccc4bf3ab9b24ac
Question: Do they show on which examples how conflict works better than attention?
Answer: Yes, we have provided two examples in the paper.
True answer: Yes

Validation ID: 6236762b5631d9e395f81e1ebccc4bf3ab9b24ac
Question: Do they show on which examples how conflict works better than attention?
Answer: Yes, we have provided two examples in the paper.
True answer: Yes

Validation ID: 31d695ba855d821d3e5cdb7bea638c7dbb7c87c7
Question: Which neural architecture do they use as a base for their attention conflict mechanisms?
Answer: GRU
True answer: GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.

Validation ID: 31d695ba855d821d3e5cdb7bea638c7dbb7c87c7
Question: Which neural architecture do they use as a base for their attention conflict mechanisms?
Answer: GRU
True answer: two stacked GRU layers, attention for one model while for the another one it consists of attention and conflict combined, fully-connected layers

Validation ID: b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab
Question: On which tasks do they test their conflict method?
Answer: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask
True answer: Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions

Validation ID: b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab
Question: On which tasks do they test their conflict method?
Answer: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask
True answer: Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask

Validation ID: a99fdd34422f4231442c220c97eafc26c76508dd
Question: Do they use graphical models?
Answer: No
True answer: No

Validation ID: a99fdd34422f4231442c220c97eafc26c76508dd
Question: Do they use graphical models?
Answer: No
True answer: No

Validation ID: 2c78993524ca62bf1f525b60f2220a374d0e3535
Question: What are the sources of the datasets?
Answer: Reuters, TDT
True answer: rupnik2016news

Validation ID: 2c78993524ca62bf1f525b60f2220a374d0e3535
Question: What are the sources of the datasets?
Answer: Reuters, TDT
True answer: rupnik2016news, Deutsche Welle's news website

Validation ID: d604f5fb114169f75f9a38fab18c1e866c5ac28b
Question: What metric is used for evaluation?
Answer: F1 score
True answer: F1, precision, recall, accuracy

Validation ID: d604f5fb114169f75f9a38fab18c1e866c5ac28b
Question: What metric is used for evaluation?
Answer: F1 score
True answer: Precision, recall, F1, accuracy

Validation ID: 1d3e914d0890fc09311a70de0b20974bf7f0c9fe
Question: Which eight NER tasks did they evaluate on?
Answer: BC5CDR, BC5CDR-disease, BC5CDR-chemical, BC5CDR-gene, BC5CDR-species, JNLPBA, LINNAEUS and NCBI-disease.
True answer: BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800

Validation ID: 1d3e914d0890fc09311a70de0b20974bf7f0c9fe
Question: Which eight NER tasks did they evaluate on?
Answer: BC5CDR, BC5CDR-disease, BC5CDR-chemical, BC5CDR-gene, BC5CDR-species, JNLPBA, LINNAEUS and NCBI-disease.
True answer: BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800

Validation ID: 16535db1d73a9373ffe9d6eedaa2369cefd91ac4
Question: What in-domain text did they use?
Answer: PubMed+PMC
True answer: PubMed+PMC

Validation ID: 16535db1d73a9373ffe9d6eedaa2369cefd91ac4
Question: What in-domain text did they use?
Answer: PubMed+PMC
True answer: PubMed+PMC (the data used for BioBERTv1.0) and/or CORD-19 (Covid-19 Open Research Dataset)

Validation ID: de0b650022ad8693465242ded169313419eed7d9
Question: Does their framework automatically optimize for hyperparameters?
Answer: No
True answer: Unanswerable

Validation ID: de0b650022ad8693465242ded169313419eed7d9
Question: Does their framework automatically optimize for hyperparameters?
Answer: No
True answer: Unanswerable

Validation ID: 2b3cac7af10d358d4081083962d03ea2798cf622
Question: Does their framework always generate purely attention-based models?
Answer: No
True answer: Yes

Validation ID: 2b3cac7af10d358d4081083962d03ea2798cf622
Question: Does their framework always generate purely attention-based models?
Answer: No
True answer: Unanswerable

Validation ID: 897ba53ef44f658c128125edd26abf605060fb13
Question: Do they test their framework performance on commonly used language pairs, such as English-to-German?
Answer: No
True answer: Yes

Validation ID: 897ba53ef44f658c128125edd26abf605060fb13
Question: Do they test their framework performance on commonly used language pairs, such as English-to-German?
Answer: No
True answer: Yes

Validation ID: 41ac23e32bf208b69414f4b687c4f324c6132464
Question: Which languages do they test on for the under-resourced scenario?
Answer: German and French
True answer: English, German

Validation ID: 41ac23e32bf208b69414f4b687c4f324c6132464
Question: Which languages do they test on for the under-resourced scenario?
Answer: German and French
True answer: small portion of the large parallel corpus for English-German is used as a simulation

Validation ID: e97186c51d4af490dba6faaf833d269c8256426c
Question: Are the automatically constructed datasets subject to quality control?
Answer: Yes. We show that our datasets are challenging for state-of-the-art models, and that they are not subject to systematic biases that would artificially inflate model scores.
True answer: No

Validation ID: e97186c51d4af490dba6faaf833d269c8256426c
Question: Are the automatically constructed datasets subject to quality control?
Answer: Yes. We show that our datasets are challenging for state-of-the-art models, and that they are not subject to systematic biases.
True answer: No

Validation ID: 5bb3c27606c59d73fd6944ba7382096de4fa58d8
Question: Do they focus on Reading Comprehension or multiple choice question answering?
Answer: multiple choice question answering
True answer: MULTIPLE CHOICE QUESTION ANSWERING

Validation ID: 5bb3c27606c59d73fd6944ba7382096de4fa58d8
Question: Do they focus on Reading Comprehension or multiple choice question answering?
Answer: multiple choice question answering
True answer: multiple-choice

Validation ID: 8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b
Question: After how many hops does accuracy decrease?
Answer: 8-15%
True answer: 1-hop links to 2-hops

Validation ID: 8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b
Question: After how many hops does accuracy decrease?
Answer: 8-15%
True answer: one additional hop

Validation ID: 85590bb26fed01a802241bc537d85ba5ef1c6dc2
Question: How do they control for annotation artificats?
Answer: They use a baseline model that only uses the answer choices to predict the correct answer.
True answer:  we use several of the MCQA baseline models first introduced in BIBREF0

Validation ID: 85590bb26fed01a802241bc537d85ba5ef1c6dc2
Question: How do they control for annotation artificats?
Answer: They use a baseline model that only uses the answer choices to predict the correct answer.
True answer: Choice-Only model, which is a variant of the well-known hypothesis-only baseline, Choice-to-choice model, tries to single out a given answer choice relative to other choices, Question-to-choice model, in contrast, uses the contextual representations for each question and individual choice and an attention model Att model to get a score

Validation ID: 75ff6e425ce304a35f18c0230c0d13d3913a31a9
Question: Is WordNet useful for taxonomic reasoning for this task?
Answer: Yes, but only for 1-hop reasoning.
True answer: Unanswerable

Validation ID: 75ff6e425ce304a35f18c0230c0d13d3913a31a9
Question: Is WordNet useful for taxonomic reasoning for this task?
Answer: Yes, but only for 1-hop reasoning.
True answer: Yes

Validation ID: 5cb610d3d5d7d447b4cd5736d6a7d8262140af58
Question: How do they perform multilingual training?
Answer: They train models for 50 different random combinations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2.
True answer: Multilingual training is performed by randomly alternating between languages for every new minibatch

Validation ID: 5cb610d3d5d7d447b4cd5736d6a7d8262140af58
Question: How do they perform multilingual training?
Answer: They train models for 50 different random combinations of two to three languages in Track 1, and 50 monolingual models for each language in Track 2.
True answer: by randomly alternating between languages for every new minibatch

Validation ID: c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a
Question: What languages are evaluated?
Answer: English, French, German, Russian, Spanish, Swedish, Finnish
True answer: German, English, Spanish, Finnish, French, Russian,  Swedish.

Validation ID: c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a
Question: What languages are evaluated?
Answer: English, French, German, Russian, Spanish, Swedish, Finnish
True answer: Unanswerable

Validation ID: b9d168da5321a7d7b812c52bb102a05210fe45bd
Question: Does the model have attention?
Answer: Yes
True answer: Yes

Validation ID: b9d168da5321a7d7b812c52bb102a05210fe45bd
Question: Does the model have attention?
Answer: Yes
True answer: Yes

Validation ID: 0c234db3b380c27c4c70579a5d6948e1e3b24ff1
Question: What architecture does the decoder have?
Answer: LSTM
True answer: LSTM

Validation ID: 0c234db3b380c27c4c70579a5d6948e1e3b24ff1
Question: What architecture does the decoder have?
Answer: LSTM
True answer: LSTM

Validation ID: fa527becb8e2551f4fd2ae840dbd4a68971349e0
Question: What architecture does the encoder have?
Answer: LSTM
True answer: LSTM

Validation ID: fa527becb8e2551f4fd2ae840dbd4a68971349e0
Question: What architecture does the encoder have?
Answer: LSTM
True answer: LSTM

Validation ID: 32a3c248b928d4066ce00bbb0053534ee62596e7
Question: What is MSD prediction?
Answer: MSD prediction is the auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form.
True answer: The task of predicting MSD tags: V, PST, V.PCTP, PASS.

Validation ID: 32a3c248b928d4066ce00bbb0053534ee62596e7
Question: What is MSD prediction?
Answer: MSD prediction is the auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form.
True answer: morphosyntactic descriptions (MSD)

Validation ID: c9b8d3858c112859eabee54248b874331c48f71b
Question: What type of inflections are considered?
Answer: Morphosyntactic
True answer: Unanswerable

Validation ID: c9b8d3858c112859eabee54248b874331c48f71b
Question: What type of inflections are considered?
Answer: Morphosyntactic
True answer: Unanswerable

Validation ID: 45e9533586199bde19313cd43b3d0ecadcaf7a33
Question: Do they use attention?
Answer: Yes
True answer: Yes

Validation ID: 45e9533586199bde19313cd43b3d0ecadcaf7a33
Question: Do they use attention?
Answer: Yes
True answer: Yes

Validation ID: d3dbb5c22ef204d85707d2d24284cc77fa816b6c
Question: What other models do they compare to?
Answer: R.M.-Reader + Verifier
True answer: SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo

Validation ID: d3dbb5c22ef204d85707d2d24284cc77fa816b6c
Question: What other models do they compare to?
Answer: R.M.-Reader + Verifier
True answer: BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo

Validation ID: a5e49cdb91d9fd0ca625cc1ede236d3d4672403c
Question: What is the architecture of the span detector?
Answer: SAN
True answer: adopt a multi-turn answer module for the span detector BIBREF1

Validation ID: a5e49cdb91d9fd0ca625cc1ede236d3d4672403c
Question: What is the architecture of the span detector?
Answer: SAN
True answer: Unanswerable

Validation ID: aefa333b2cf0a4000cd40566149816f5b36135e7
Question: What evaluation metric do they use?
Answer: Accuracy
True answer: Accuracy

Validation ID: aefa333b2cf0a4000cd40566149816f5b36135e7
Question: What evaluation metric do they use?
Answer: Accuracy
True answer: ratio of correct `translations'

Validation ID: c5abe97625b9e1c8de8208e15d59c704a597b88c
Question: What are the results from these proposed strategies?
Answer: KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.
True answer: Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.

Validation ID: c5abe97625b9e1c8de8208e15d59c704a597b88c
Question: What are the results from these proposed strategies?
Answer: KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it.
True answer: KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40

Validation ID: eb2d5edcdfe18bd708348283f92a32294bb193a5
Question: What are the baselines?
Answer: KG-A2C, A2C, A2C-chained, A2C-Explore
True answer: a score of 40

Validation ID: eb2d5edcdfe18bd708348283f92a32294bb193a5
Question: What are the baselines?
Answer: KG-A2C, A2C, A2C-chained, A2C-Explore
True answer: KG-A2C, A2C, A2C-chained, A2C-Explore

Validation ID: 88ab7811662157680144ed3fdd00939e36552672
Question: What are the two new strategies?
Answer: KG-A2C-chained and KG-A2C-Explore
True answer: a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state, to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space

Validation ID: 88ab7811662157680144ed3fdd00939e36552672
Question: What are the two new strategies?
Answer: KG-A2C-chained and KG-A2C-Explore
True answer: KG-A2C-chained, KG-A2C-Explore

Validation ID: cb196725edc9cdb2c54b72364f3bbf7c76471490
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: cb196725edc9cdb2c54b72364f3bbf7c76471490
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: 286078813136943dfafb5155ee15d2429e7601d9
Question: How much better than the baseline is LiLi?
Answer: LiLi is much better than the baseline.
True answer: In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. 


Validation ID: 286078813136943dfafb5155ee15d2429e7601d9
Question: How much better than the baseline is LiLi?
Answer: LiLi is much better than the baseline.
True answer: Unanswerable

Validation ID: 8f16dc7d7be0d284069841e456ebb2c69575b32b
Question: What baseline is used in the experiments?
Answer: We use the path-ranking algorithm (PR) BIBREF11, BIBREF17 as the baseline.
True answer: versions of LiLi

Validation ID: 8f16dc7d7be0d284069841e456ebb2c69575b32b
Question: What baseline is used in the experiments?
Answer: We use the path-ranking algorithm (PR) BIBREF11, BIBREF17 as the baseline.
True answer: various versions of LiLi as baselines, Single, Sep, F-th, BG, w/o PTS

Validation ID: a7d020120a45c39bee624f65443e09b895c10533
Question: In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?
Answer: LiLi mimics how humans acquire knowledge and perform inference during an interactive conversation by formulating a query-specific inference strategy and executing it.
True answer: newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning

Validation ID: a7d020120a45c39bee624f65443e09b895c10533
Question: In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?
Answer: LiLi mimics how humans acquire knowledge and perform inference during an interactive conversation by formulating a query-specific inference strategy and executing it.
True answer: Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. 

Validation ID: 585626d18a20d304ae7df228c2128da542d248ff
Question: What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation??
Answer: Matthews correlation coefficient (MCC)
True answer: Coverage, Avg. MCC and avg. +ve F1 score

Validation ID: 585626d18a20d304ae7df228c2128da542d248ff
Question: What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation??
Answer: Matthews correlation coefficient (MCC)
True answer: strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score

Validation ID: bfc2dc913e7b78f3bd45e5449d71383d0aa4a890
Question: What are the components of the general knowledge learning engine?
Answer: The general knowledge learning engine consists of two interconnected models: (1) a RL model that learns to formulate a query-specific inference strategy for performing the OKBC task, and (2) a lifelong prediction model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR.
True answer: Answer with content missing: (list)
LiLi should have the following capabilities:
1. to formulate an inference strategy for a given query that embeds processing and interactive actions.
2. to learn interaction behaviors (deciding what to ask and when to ask the user).
3. to leverage the acquired knowledge in the current and future inference process.
4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.

Validation ID: bfc2dc913e7b78f3bd45e5449d71383d0aa4a890
Question: What are the components of the general knowledge learning engine?
Answer: The general knowledge learning engine consists of two interconnected models: (1) a RL model that learns to formulate a query-specific inference strategy for performing the OKBC task, and (2) a lifelong prediction model to predict whether a triple should be in the KB, which is invoked by an action while executing the inference strategy and is learned for each relation as in C-PR.
True answer: Knowledge Store (KS) , Knowledge Graph ( INLINEFORM0 ),  Relation-Entity Matrix ( INLINEFORM2 ), Task Experience Store ( INLINEFORM15 ), Incomplete Feature DB ( INLINEFORM29 )

Validation ID: 6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de
Question: How many labels do the datasets have?
Answer: 4
True answer: 719313

Validation ID: 6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de
Question: How many labels do the datasets have?
Answer: 4
True answer: Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.

Validation ID: b46c0015a122ee5fb95c2a45691cb97f80de1bb6
Question: What is the architecture of the model?
Answer: CNN
True answer: one-layer CNN structure from previous works BIBREF22 , BIBREF4

Validation ID: b46c0015a122ee5fb95c2a45691cb97f80de1bb6
Question: What is the architecture of the model?
Answer: CNN
True answer:  one-layer CNN

Validation ID: 5b7a4994bfdbf8882f391adf1cd2218dbc2255a0
Question: What are the baseline methods?
Answer: Naive, mSDA, NaiveNN, AuxNN, ADAN, MMD
True answer: (1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD

Validation ID: 5b7a4994bfdbf8882f391adf1cd2218dbc2255a0
Question: What are the baseline methods?
Answer: Naive, mSDA, NaiveNN, AuxNN, ADAN, MMD
True answer: non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized

Validation ID: 9176d2ba1c638cdec334971c4c7f1bb959495a8e
Question: What are the source and target domains?
Answer: Book (B), DVDs (D), Electronics (E), and Kitchen (K)
True answer: Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen

Validation ID: 9176d2ba1c638cdec334971c4c7f1bb959495a8e
Question: What are the source and target domains?
Answer: Book (B), DVDs (D), Electronics (E), and Kitchen (K)
True answer: we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)

Validation ID: 0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c
Question: Did they use a crowdsourcing platform for annotations?
Answer: No
True answer: No

Validation ID: 0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c
Question: Did they use a crowdsourcing platform for annotations?
Answer: No
True answer: Unanswerable

Validation ID: 5e324846a99a5573cd2e843d1657e87f4eb22fa6
Question: How do they deal with unknown distribution senses?
Answer: They modify the Bayesian classifier, removing the bias towards frequent labels in the training data.
True answer: The Näive-Bayes classifier is corrected so it is not biased to most frequent classes

Validation ID: 5e324846a99a5573cd2e843d1657e87f4eb22fa6
Question: How do they deal with unknown distribution senses?
Answer: They modify the Bayesian classifier, removing the bias towards frequent labels in the training data.
True answer: Bayesian classifier has been modified, removing the bias towards frequent labels in the training data

Validation ID: 2ccc26e11df4eb26fcccdd1f446dc749aff5d572
Question: Do they report results only on English data?
Answer: Yes, they report results only on English data.
True answer: Yes

Validation ID: 2ccc26e11df4eb26fcccdd1f446dc749aff5d572
Question: Do they report results only on English data?
Answer: Yes, they report results only on English data.
True answer: Yes

Validation ID: f318a2851d7061f05a5b32b94251f943480fbd15
Question: What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?
Answer: The authors conclude that the emotional appeal of ISIS and Catholic materials are similar.
True answer: both corpuses used words that aim to inspire readers while avoiding fear, actual words that lead to these effects are very different in the two contexts, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda

Validation ID: f318a2851d7061f05a5b32b94251f943480fbd15
Question: What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?
Answer: The authors conclude that the emotional appeal of ISIS and Catholic materials are similar.
True answer: By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores

Validation ID: 6bbbb9933aab97ce2342200447c6322527427061
Question: How id Depechemood trained?
Answer: Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words.
True answer: By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. 

Validation ID: 6bbbb9933aab97ce2342200447c6322527427061
Question: How id Depechemood trained?
Answer: Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words.
True answer: researchers asked subjects to report their emotions after reading each article, multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words, Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories

Validation ID: 2007bfb8f66e88a235c3a8d8c0a3b3dd88734706
Question: How are similarities and differences between the texts from violent and non-violent religious groups analyzed?
Answer: Topic modeling and emotion detection methods are used to analyze the texts from violent and non-violent religious groups.
True answer: By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum

Validation ID: 2007bfb8f66e88a235c3a8d8c0a3b3dd88734706
Question: How are similarities and differences between the texts from violent and non-violent religious groups analyzed?
Answer: Topic modeling and emotion detection methods are used to analyze the texts from violent and non-violent religious groups.
True answer: A comparison of common words, We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource

Validation ID: d859cc37799a508bbbe4270ed291ca6394afce2c
Question: How are prominent topics idenified in Dabiq and Rumiyah?
Answer: Topic modeling
True answer: LDA, non-negative matrix factorization (NMF)

Validation ID: d859cc37799a508bbbe4270ed291ca6394afce2c
Question: How are prominent topics idenified in Dabiq and Rumiyah?
Answer: Topic modeling
True answer: Using NMF based topic modeling and their coherence prominent topics are identified

Validation ID: 50e80cfa84200717921840fddcf3b051a9216ad8
Question: Are the images from a specific domain?
Answer: No
True answer: Yes

Validation ID: 50e80cfa84200717921840fddcf3b051a9216ad8
Question: Are the images from a specific domain?
Answer: No
True answer: Yes

Validation ID: b1bc9ae9d40e7065343c12f860a461c7c730a612
Question: Which datasets are used?
Answer: ShapeWorldICE
True answer: Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE

Validation ID: b1bc9ae9d40e7065343c12f860a461c7c730a612
Question: Which datasets are used?
Answer: ShapeWorldICE
True answer: ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio

Validation ID: 63a1cbe66fd58ff0ead895a8bac1198c38c008aa
Question: Which existing models are evaluated?
Answer: Show&Tell model and LRCN1u model
True answer: Show&Tell and LRCN1u

Validation ID: 63a1cbe66fd58ff0ead895a8bac1198c38c008aa
Question: Which existing models are evaluated?
Answer: Show&Tell model and LRCN1u model
True answer: Show&Tell model, LRCN1u

Validation ID: 509af1f11bd6f3db59284258e18fdfebe86cae47
Question: How is diversity measured?
Answer: By comparing the number of observed language constructions to the number of possible language constructions.
True answer: diversity score as the ratio of observed number versus optimal number

Validation ID: 509af1f11bd6f3db59284258e18fdfebe86cae47
Question: How is diversity measured?
Answer: By comparing the number of observed language constructions to the number of possible language constructions.
True answer:  we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number

Validation ID: 23e16c1173b7def2c5cb56053b57047c9971e3bb
Question: What state-of-the-art deep neural network is used?
Answer: LSTM
True answer: LSTM model

Validation ID: 23e16c1173b7def2c5cb56053b57047c9971e3bb
Question: What state-of-the-art deep neural network is used?
Answer: LSTM
True answer: BIBREF15, BIBREF19, BIBREF20 

Validation ID: d78f7f84a76a07b777d4092cb58161528ca3803c
Question: What boundary assembling method is used?
Answer: A backward greedy search over each sentence's label sequence to identify word boundaries.
True answer: This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition.

Validation ID: d78f7f84a76a07b777d4092cb58161528ca3803c
Question: What boundary assembling method is used?
Answer: A backward greedy search over each sentence's label sequence to identify word boundaries.
True answer: backward greedy search over each sentence's label sequence to identify word boundaries

Validation ID: 9da1e124d28b488b0d94998d32aa2fa8a5ebec51
Question: What are previous state of the art results?
Answer: BIBREF15, BIBREF19, BIBREF20
True answer: Overall F1 score:
- He and Sun (2017) 58.23
- Peng and Dredze (2017) 58.99
- Xu et al. (2018) 59.11

Validation ID: 9da1e124d28b488b0d94998d32aa2fa8a5ebec51
Question: What are previous state of the art results?
Answer: BIBREF15, BIBREF19, BIBREF20
True answer: For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%

Validation ID: 37be0d479480211291e068d0d3823ad0c13321d3
Question: What is the model performance on target language reading comprehension?
Answer: The model performance on target language reading comprehension is not good.
True answer: Table TABREF6, Table TABREF8

Validation ID: 37be0d479480211291e068d0d3823ad0c13321d3
Question: What is the model performance on target language reading comprehension?
Answer: The model performance on target language reading comprehension is not good.
True answer: when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: English , Chinese

Validation ID: a3d9b101765048f4b61cbd3eaa2439582ebb5c77
Question: What source-target language pairs were used in this work??
Answer: English-Chinese, English-Korean, Chinese-English, Chinese-Korean, Korean-English, Korean-Chinese
True answer: English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate

Validation ID: 009ce6f2bea67e7df911b3f93443b23467c9f4a1
Question: What model is used as a baseline? ?
Answer: QANet
True answer: pre-trained multi-BERT

Validation ID: 009ce6f2bea67e7df911b3f93443b23467c9f4a1
Question: What model is used as a baseline? ?
Answer: QANet
True answer: QANet , BIBREF14,  fine-tuned a BERT model

Validation ID: 55569d0a4586d20c01268a80a7e31a17a18198e2
Question: what does the model learn in zero-shot setting?
Answer: The model learns to encode semantic and syntactic information in language-agnostic ways.
True answer: we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged

Validation ID: 7cd22ca9e107d2b13a7cc94252aaa9007976b338
Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?
Answer: No
True answer: Yes

Validation ID: 7cd22ca9e107d2b13a7cc94252aaa9007976b338
Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?
Answer: No
True answer: Yes

Validation ID: adbf33c6144b2f5c40d0c6a328a92687a476f371
Question: Does their NER model learn NER from both text and images?
Answer: Yes
True answer: Yes

Validation ID: adbf33c6144b2f5c40d0c6a328a92687a476f371
Question: Does their NER model learn NER from both text and images?
Answer: Yes
True answer: Yes

Validation ID: f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24
Question: Which types of named entities do they recognize?
Answer: PER, LOC, ORG, MISC
True answer: PER, LOC, ORG, MISC

Validation ID: f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24
Question: Which types of named entities do they recognize?
Answer: PER, LOC, ORG, MISC
True answer: PER, LOC, ORG, MISC

Validation ID: a0543b4afda15ea47c1e623c7f00d4aaca045be0
Question: Can named entities in SnapCaptions be discontigious?
Answer: Yes
True answer: No

Validation ID: a0543b4afda15ea47c1e623c7f00d4aaca045be0
Question: Can named entities in SnapCaptions be discontigious?
Answer: Yes.
True answer: Unanswerable

Validation ID: 1591068b747c94f45b948e12edafe74b5e721047
Question: How large is their MNER SnapCaptions dataset?
Answer: 10K
True answer: 10K user-generated image (snap) and textual caption pairs

Validation ID: 1591068b747c94f45b948e12edafe74b5e721047
Question: How large is their MNER SnapCaptions dataset?
Answer: 10K
True answer: 10000

Validation ID: 193ee49ae0f8827a6e67388a10da59e137e7769f
Question: What is masked document generation?
Answer: Masked Document Generation (MDG) learns to recover a document with a masked span of tokens.
True answer: A task for seq2seq model pra-training that recovers a masked document to its original form.

Validation ID: 193ee49ae0f8827a6e67388a10da59e137e7769f
Question: What is masked document generation?
Answer: Masked Document Generation (MDG) learns to recover a document with a masked span of tokens.
True answer: recovers a masked document to its original form

Validation ID: ed2eb4e54b641b7670ab5a7060c7b16c628699ab
Question: Which of the three pretraining tasks is the most helpful?
Answer: Sentence Reordering
True answer: SR

Validation ID: ed2eb4e54b641b7670ab5a7060c7b16c628699ab
Question: Which of the three pretraining tasks is the most helpful?
Answer: Sentence Reordering
True answer: SR

Validation ID: beac555c4aea76c88f19db7cc901fa638765c250
Question: What useful information does attention capture?
Answer: Other relevant information
True answer: it captures other information rather than only the translational equivalent in the case of verbs

Validation ID: beac555c4aea76c88f19db7cc901fa638765c250
Question: What useful information does attention capture?
Answer: Other relevant information
True answer: Alignment points of the POS tags.

Validation ID: 91e326fde8b0a538bc34d419541b5990d8aae14b
Question: What datasets are used?
Answer: WMT15 German-to-English
True answer: WMT15 German-to-English, RWTH German-English dataset

Validation ID: 91e326fde8b0a538bc34d419541b5990d8aae14b
Question: What datasets are used?
Answer: WMT15 German-to-English
True answer: RWTH German-English dataset

Validation ID: 044f922604b4b3f42ae381419fd5cd5624fa0637
Question: In what cases is attention different from alignment?
Answer: Attention is different from alignment in the case of verbs.
True answer: For certain POS tags, e.g. VERB, PRON.

Validation ID: 044f922604b4b3f42ae381419fd5cd5624fa0637
Question: In what cases is attention different from alignment?
Answer: Attention is different from alignment in the case of verbs.
True answer: most word alignments only involve one or a few words, attention can be distributed more freely

Validation ID: f94b53db307685d572aefad52cd55f53d23769c2
Question: How do they calculate variance from the model outputs?
Answer: They use Fisher Information Matrix
True answer: reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3

Validation ID: f94b53db307685d572aefad52cd55f53d23769c2
Question: How do they calculate variance from the model outputs?
Answer: They use Fisher Information Matrix
True answer:  Fisher Information Ratio

Validation ID: aa7d327ef98f9f9847b447d4def04889b4508d7a
Question: How much data samples do they start with before obtaining the initial model labels?
Answer: 190 hours ( INLINEFORM1 100K instances)
True answer: 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset

Validation ID: aa7d327ef98f9f9847b447d4def04889b4508d7a
Question: How much data samples do they start with before obtaining the initial model labels?
Answer: 190 hours ( INLINEFORM1 100K instances)
True answer: INLINEFORM2 is queried for the “most informative” instance(s) INLINEFORM3

Validation ID: b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0
Question: Which model do they use for end-to-end speech recognition?
Answer: RNN
True answer: RNN

Validation ID: b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0
Question: Which model do they use for end-to-end speech recognition?
Answer: RNN
True answer:  Recurrent Neural Network (RNN)

Validation ID: 551457ed34ca7fc0878c85bc664b135c21059b58
Question: Which dataset do they use?
Answer: unlabeled dataset
True answer: 190 hours ( INLINEFORM1 100K instances)

Validation ID: 551457ed34ca7fc0878c85bc664b135c21059b58
Question: Which dataset do they use?
Answer: unlabeled dataset
True answer: trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data, selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset

Validation ID: 0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8
Question: Which baselines did they compare against?
Answer: Tree-LSTM, TG-RNN, TE-RNN, TE-RNTN, DC-TreeLSTM, SPINN, and Latent Tree-LSTM
True answer: Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks

Validation ID: 0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8
Question: Which baselines did they compare against?
Answer: Tree-LSTM, TG-RNN, TE-RNN, TE-RNTN, DC-TreeLSTM, SPINN, and Latent Tree-LSTM
True answer: Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). 
Stanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).

Validation ID: 4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94
Question: What baselines did they consider?
Answer: They considered off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers.
True answer: state-of-the-art PDTB taggers

Validation ID: 4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94
Question: What baselines did they consider?
Answer: They considered off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest and LSTM classifiers.
True answer: Linear SVM, RBF SVM, and Random Forest

Validation ID: a4d115220438c0ded06a91ad62337061389a6747
Question: What types of social media did they consider?
Answer: Facebook
True answer: Facebook status update messages

Validation ID: a4d115220438c0ded06a91ad62337061389a6747
Question: What types of social media did they consider?
Answer: Facebook
True answer: Facebook status update messages

Validation ID: 2c7e94a65f5f532aa31d3e538dcab0468a43b264
Question: How was the dataset annotated?
Answer: Crowdsourcing
True answer: intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task

Validation ID: 2c7e94a65f5f532aa31d3e538dcab0468a43b264
Question: How was the dataset annotated?
Answer: Crowdsourcing
True answer: manually 

Validation ID: 149da739b1c19a157880d9d4827f0b692006aa2c
Question: Which classifiers are evaluated?
Answer: SVM, MLP, FastText, CNN, BERT, Platforms
True answer: SVM, MLP, FastText, CNN, BERT, Google's DialogFlow, Rasa NLU

Validation ID: 149da739b1c19a157880d9d4827f0b692006aa2c
Question: Which classifiers are evaluated?
Answer: SVM, MLP, FastText, CNN, BERT, Platforms
True answer: SVM, MLP, FastText, CNN, BERT, DialogFlow, Rasa NLU

Validation ID: 27de1d499348e17fec324d0ef00361a490659988
Question: What is the size of this dataset?
Answer: 23,700 queries
True answer: 23,700 

Validation ID: 27de1d499348e17fec324d0ef00361a490659988
Question: What is the size of this dataset?
Answer: 23,700 queries
True answer:  23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries.

Validation ID: cfcdd73e712caf552ba44d0aa264d8dace65a589
Question: Where does the data come from?
Answer: https://github.com/clinc/oos-eval
True answer: crowsourcing platform

Validation ID: cfcdd73e712caf552ba44d0aa264d8dace65a589
Question: Where does the data come from?
Answer: https://github.com/clinc/oos-eval
True answer: For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. 
For out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere.

Validation ID: 23b2901264bda91045258b5d4120879ae292e950
Question: What are method improvements of F1 for paraphrase identification?
Answer: +0.58 for MRPC and +0.73 for QQP
True answer: Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP

Validation ID: 23b2901264bda91045258b5d4120879ae292e950
Question: What are method improvements of F1 for paraphrase identification?
Answer: +0.58 for MRPC and +0.73 for QQP
True answer: +0.58

Validation ID: b5bc34e1e381dbf972d0b594fe8c66ff75305d71
Question: What are method's improvements of F1 for NER task for English and Chinese datasets?
Answer: +0.29 and +0.96
True answer: English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively

Validation ID: b5bc34e1e381dbf972d0b594fe8c66ff75305d71
Question: What are method's improvements of F1 for NER task for English and Chinese datasets?
Answer: +0.29 and +0.96
True answer: For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively

Validation ID: 72f7ef55e150e16dcf97fe443aff9971a32414ef
Question: What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?
Answer: +1.86, +1.80, +2.19
True answer: +1.86 in terms of F1 score on CTB5, +1.80 on CTB6, +2.19 on UD1.4

Validation ID: 72f7ef55e150e16dcf97fe443aff9971a32414ef
Question: What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?
Answer: +1.86, +1.80, +2.19
True answer:  +1.86

Validation ID: 20e38438471266ce021817c6364f6a46d01564f2
Question: How are weights dynamically adjusted?
Answer: We multiply the soft probability $p$ with a decaying factor $(1-p)$, changing Eq.DISPLAY_FORM22 to the following form:
True answer: One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.

Validation ID: 20e38438471266ce021817c6364f6a46d01564f2
Question: How are weights dynamically adjusted?
Answer: We multiply the soft probability $p$ with a decaying factor $(1-p)$, changing Eq.DISPLAY_FORM22 to the following form:
True answer: associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds

Validation ID: 28067da818e3f61f8b5152c0d42a531bf0f987d4
Question: Ngrams of which length are aligned using PARENT?
Answer: Ngrams of length 1
True answer: Unanswerable

Validation ID: 28067da818e3f61f8b5152c0d42a531bf0f987d4
Question: Ngrams of which length are aligned using PARENT?
Answer: Ngrams of length 1
True answer: Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4

Validation ID: bf3b27a4f4be1f9ae31319877fd0c75c03126fd5
Question: How many people participated in their evaluation study of table-to-text models?
Answer: 500
True answer: about 500

Validation ID: bf3b27a4f4be1f9ae31319877fd0c75c03126fd5
Question: How many people participated in their evaluation study of table-to-text models?
Answer: 500
True answer: Unanswerable

Validation ID: ffa7f91d6406da11ddf415ef094aaf28f3c3872d
Question: By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?
Answer: 0.1
True answer: Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.

Validation ID: ffa7f91d6406da11ddf415ef094aaf28f3c3872d
Question: By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?
Answer: 0.1
True answer: Their average correlation tops the best other model by 0.155 on WikiBio.

Validation ID: b634ff1607ce5756655e61b9a6f18bc736f84c83
Question: Which stock market sector achieved the best performance?
Answer: Consumer Discretionary
True answer: Energy with accuracy of 0.538

Validation ID: b634ff1607ce5756655e61b9a6f18bc736f84c83
Question: Which stock market sector achieved the best performance?
Answer: Consumer Discretionary
True answer: Energy

Validation ID: 2f901dab6b757e12763b23ae8b37ae2e517a2271
Question: What languages pairs are used in machine translation?
Answer: German–English
True answer: German–English

Validation ID: 2f901dab6b757e12763b23ae8b37ae2e517a2271
Question: What languages pairs are used in machine translation?
Answer: German–English
True answer: German–English

Validation ID: b591853e938984e6069d738371500ebdec50d256
Question: What sentiment classification dataset is used?
Answer: IMDb movie review dataset
True answer: the IMDb movie review dataset BIBREF17

Validation ID: b591853e938984e6069d738371500ebdec50d256
Question: What sentiment classification dataset is used?
Answer: IMDb movie review dataset
True answer: IMDb movie review

Validation ID: a130306c6662ff489df13fb3f8faa7cba8c52a21
Question: What pooling function is used?
Answer: f-pooling
True answer: dynamic average pooling

Validation ID: a130306c6662ff489df13fb3f8faa7cba8c52a21
Question: What pooling function is used?
Answer: f-pooling
True answer:  f-pooling, fo-pooling, and ifo-pooling 

Validation ID: b1cf5739467ba90059add58d11b73d075a11ec86
Question: Do they report results only on English?
Answer: Yes
True answer: Unanswerable

Validation ID: b1cf5739467ba90059add58d11b73d075a11ec86
Question: Do they report results only on English?
Answer: Yes
True answer: Yes

Validation ID: 2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd
Question: What neural network modules are included in NeuronBlocks?
Answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics
True answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics

Validation ID: 2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd
Question: What neural network modules are included in NeuronBlocks?
Answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics
True answer: Embedding Layer, Neural Network Layers, Loss Function, Metrics

Validation ID: 4f253dfced6a749bf57a1b4984dc962ce9550184
Question: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?
Answer: The authors conducted a survey among engineers and identified a spectrum of three typical personas.
True answer: By conducting a survey among engineers

Validation ID: 4f253dfced6a749bf57a1b4984dc962ce9550184
Question: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?
Answer: The authors conducted a survey among engineers and identified a spectrum of three typical personas.
True answer: Unanswerable

Validation ID: dc1cec824507fc85ac1ba87882fe1e422ff6cffb
Question: what datasets did they use?
Answer: Bengali question corpus
True answer: Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.

Validation ID: dc1cec824507fc85ac1ba87882fe1e422ff6cffb
Question: what datasets did they use?
Answer: Bengali question corpus
True answer: 3500 questions collected from the internet and books.

Validation ID: f428618ca9c017e0c9c2a23515dab30a7660f65f
Question: what ml based approaches were compared?
Answer: Multi Layer Perceptron (MLP), Support Vector Machine (SVM), Naive Bayesian Classifier (NBC), Stochastic Gradient Descent (SGD), Gradient Boosting Classifier (GBC), K Nearest Neighbour (K-NN) and Random Forest (RF)
True answer: Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)

Validation ID: f428618ca9c017e0c9c2a23515dab30a7660f65f
Question: what ml based approaches were compared?
Answer: Multi Layer Perceptron (MLP), Support Vector Machine (SVM), Naive Bayesian Classifier (NBC), Stochastic Gradient Descent (SGD), Gradient Boosting Classifier (GBC), K Nearest Neighbour (K-NN) and Random Forest (RF)
True answer: Multi-Layer Perceptron, Naive Bayes Classifier, Support Vector Machine, Gradient Boosting Classifier, Stochastic Gradient Descent, K Nearest Neighbour, Random Forest

Validation ID: 8ce11515634236165cdb06ba80b9a36a8b9099a2
Question: Is pre-training effective in their evaluation?
Answer: Yes
True answer: Yes

Validation ID: 8ce11515634236165cdb06ba80b9a36a8b9099a2
Question: Is pre-training effective in their evaluation?
Answer: Yes
True answer: Yes

Validation ID: 6024039bbd1118c5dab86c41cce1175d99f10a25
Question: What parallel corpus did they use?
Answer: ASPEC and NTCIR
True answer: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1

Validation ID: 6024039bbd1118c5dab86c41cce1175d99f10a25
Question: What parallel corpus did they use?
Answer: ASPEC and NTCIR
True answer: Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus 

Validation ID: de5b6c25e35b3a6c5e40e350fc5e52c160b33490
Question: How much does their model outperform existing models?
Answer: Their model outperforms existing models by a wide margin.
True answer: Best proposed model result vs best previous result:
Arxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)
Pubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)

Validation ID: de5b6c25e35b3a6c5e40e350fc5e52c160b33490
Question: How much does their model outperform existing models?
Answer: Their model outperforms existing models by a wide margin.
True answer: On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.


Validation ID: b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f
Question: What do they mean by global and local context?
Answer: Global context is the context of the whole document, while local context is the context of the topic segment that sentence falls into.
True answer: global (the whole document), local context (e.g., the section/topic)

Validation ID: b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f
Question: What do they mean by global and local context?
Answer: Global context is the context of the whole document, while local context is the context of the topic segment that sentence falls into.
True answer: global (the whole document) and the local context (e.g., the section/topic) 

Validation ID: 6bfba3ddca5101ed15256fca75fcdc95a53cece7
Question: What are the 18 propaganda techniques?
Answer: Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans, Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating cliché, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man
True answer: Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans,  Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating cliché, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man

Validation ID: 6bfba3ddca5101ed15256fca75fcdc95a53cece7
Question: What are the 18 propaganda techniques?
Answer: Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans, Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating cliché, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man
True answer: 1. Loaded language, 2. Name calling or labeling, 3. Repetition, 4. Exaggeration or minimization, 5. Doubt, 6. Appeal to fear/prejudice, 7. Flag-waving, 8. Causal oversimplification, 9. Slogans, 10. Appeal to authority, 11. Black-and-white fallacy, dictatorship, 12. Thought-terminating cliché, 13. Whataboutism, 14. Reductio ad Hitlerum, 15. Red herring, 16. Bandwagon, 17. Obfuscation, intentional vagueness, confusion, 18. Straw man

Validation ID: df5a4505edccc0ee11349ed6e7958cf6b84c9ed4
Question: What dataset was used?
Answer: The corpus for the task was annotated by A Data Pro, a company that performs high-quality manual annotations.
True answer:  news articles in free-text format

Validation ID: df5a4505edccc0ee11349ed6e7958cf6b84c9ed4
Question: What dataset was used?
Answer: The corpus for the task was annotated by A Data Pro, a company that performs high-quality manual annotations.
True answer: collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators

Validation ID: fd753ab5177d7bd27db0e0afc12411876ee607df
Question: What was the baseline for this task?
Answer: The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.
True answer: The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.

Validation ID: fd753ab5177d7bd27db0e0afc12411876ee607df
Question: What was the baseline for this task?
Answer: The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.
True answer: SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly

Validation ID: 88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42
Question: What is a second order co-ocurrence matrix?
Answer: A second order co-ocurrence matrix is a matrix that contains the frequencies of the other words which occur with both of them (i.e., second order co–occurrences).
True answer: frequencies of the other words which occur with both of them (i.e., second order co–occurrences)

Validation ID: 88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42
Question: What is a second order co-ocurrence matrix?
Answer: A second order co-ocurrence matrix is a matrix that contains the co-occurrence frequencies of two words in a corpus.
True answer: The matrix containing co-occurrences of the words which occur with the both words of every given pair of words.

Validation ID: 4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3
Question: How many humans participated?
Answer: 4
True answer: Unanswerable

Validation ID: 4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3
Question: How many humans participated?
Answer: 4
True answer: 16

Validation ID: 8b3d3953454c88bde88181897a7a2c0c8dd87e23
Question: What embedding techniques are explored in the paper?
Answer: word2vec
True answer: Skip–gram, CBOW

Validation ID: 8b3d3953454c88bde88181897a7a2c0c8dd87e23
Question: What embedding techniques are explored in the paper?
Answer: word2vec
True answer: integrated vector-res, vector-faith, Skip–gram, CBOW

Validation ID: 784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f
Question: Do the authors also try the model on other datasets?
Answer: Yes, they do.
True answer: No

Validation ID: 784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f
Question: Do the authors also try the model on other datasets?
Answer: Yes, they do.
True answer: No

Validation ID: 7705dd04acedaefee30d8b2c9978537afb2040dc
Question: What word level and character level model baselines are used?
Answer: The word level model is a two-layer LSTM with 200 hidden units. The character level model is a two-layer gated-feedback LSTM with 200 hidden units.
True answer: None

Validation ID: 7705dd04acedaefee30d8b2c9978537afb2040dc
Question: What word level and character level model baselines are used?
Answer: The word level model is a two-layer LSTM with 200 hidden units. The character level model is a two-layer gated-feedback LSTM with 200 hidden units.
True answer: Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015)

Validation ID: 44497509fdf5e87cff05cdcbe254fbd288d857ad
Question: By how much do they improve the efficacy of the attention mechanism?
Answer: AER scores
True answer: Unanswerable

Validation ID: 44497509fdf5e87cff05cdcbe254fbd288d857ad
Question: By how much do they improve the efficacy of the attention mechanism?
Answer: AER scores
True answer: Unanswerable

Validation ID: 0ee73909ac638903da4a0e5565c8571fc794ab96
Question: How were the human judgements assembled?
Answer: A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive). For the comparison process, the RNNMorph and the RNNSearch + Word2Vec models’ sentence level translations were individually ranked between each other, permitting the two translations to have ties in the ranking. The intra-annotator values were computed for these metrics and the scores are shown in Table TABREF32 BIBREF12, BIBREF13.
True answer: 50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.

Validation ID: 0ee73909ac638903da4a0e5565c8571fc794ab96
Question: How were the human judgements assembled?
Answer: A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive). For the comparison process, the RNNMorph and the RNNSearch + Word2Vec models’ sentence level translations were individually ranked between each other, permitting the two translations to have ties in the ranking. The intra-annotator values were computed for these metrics and the scores are shown in Table TABREF32 BIBREF12, BIBREF13.
True answer: adequacy, precision and ranking values

Validation ID: 1f07e837574519f2b696f3d6fa3230af0b931e5d
Question: Did they only experiment with one language pair?
Answer: No
True answer: Yes

Validation ID: 1f07e837574519f2b696f3d6fa3230af0b931e5d
Question: Did they only experiment with one language pair?
Answer: No
True answer: Yes

Validation ID: 5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76
Question: Which other approaches do they compare their model with?
Answer: They compare their model with existing systems without being trained or tuned on that particular dataset.
True answer: Akbik et al. (2018), Link et al. (2012)

Validation ID: 5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76
Question: Which other approaches do they compare their model with?
Answer: They compare their model with existing systems without being trained or tuned on that particular dataset.
True answer: They compare to Akbik et al. (2018) and Link et al. (2012).

Validation ID: 729694a9fe1e05d329b7a4078a596fe606bc5a95
Question: What results do they achieve using their proposed approach?
Answer: The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%.
True answer: F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).

Validation ID: 729694a9fe1e05d329b7a4078a596fe606bc5a95
Question: What results do they achieve using their proposed approach?
Answer: The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%.
True answer:  total F-1 score on the OntoNotes dataset is 88%, total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%

Validation ID: 1c997c268c68149ae6fb43d83ffcd53f0e7fe57e
Question: How do they combine a deep learning model with a knowledge base?
Answer: They use Wikidata
True answer: Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.

Validation ID: 1c997c268c68149ae6fb43d83ffcd53f0e7fe57e
Question: How do they combine a deep learning model with a knowledge base?
Answer: They use Wikidata
True answer: ELMo embeddings are then used with a residual LSTM to learn informative morphological representations from the character sequence of each token

Validation ID: 5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde
Question: What are the models used for the baseline of the three NLP tasks?
Answer: Speech synthesis: Clustergen
True answer: state-of-the-art Transformer architecture, Kaldi, speech clustergen statistical speech synthesizer

Validation ID: 5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde
Question: What are the models used for the baseline of the three NLP tasks?
Answer: Speech synthesis: Clustergen
True answer: For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15.

Validation ID: f9bf6bef946012dd42835bf0c547c0de9c1d229f
Question: How is non-standard pronunciation identified?
Answer: Annotations
True answer: Unanswerable

Validation ID: f9bf6bef946012dd42835bf0c547c0de9c1d229f
Question: How is non-standard pronunciation identified?
Answer: Annotations
True answer: Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.

Validation ID: 6a633811019e9323dc8549ad540550d27aa6d972
Question: Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?
Answer: Yes
True answer: Unanswerable

Validation ID: 6a633811019e9323dc8549ad540550d27aa6d972
Question: Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?
Answer: Yes
True answer: No

Validation ID: 6b9b9e5d154cb963f6d921093539490daa5ebbae
Question: What novel PMI variants are introduced?
Answer: $\mathit {CPMI_z}$ and $\mathit {NNEGPMI}$
True answer: clipped PMI; NNEGPMI

Validation ID: 6b9b9e5d154cb963f6d921093539490daa5ebbae
Question: What novel PMI variants are introduced?
Answer: $\mathit {CPMI_z}$ and $\mathit {NNEGPMI}$
True answer: clipped $\mathit {PMI}$, $\mathit {NNEGPMI}$

Validation ID: bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2
Question: What semantic and syntactic tasks are used as probes?
Answer: Semantic Textual Similarity (STSB) task, Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) probing tasks, part-of-speech (POS) tagging
True answer: Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks

Validation ID: bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2
Question: What semantic and syntactic tasks are used as probes?
Answer: Semantic Textual Similarity (STSB) task, Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) probing tasks, part-of-speech (POS) tagging
True answer: SimLex, Rare Word, Google Semantic, Semantic Textual Similarity, Word Content (WC) probing, Google Syntactic analogies, Depth, Top Constituent, part-of-speech (POS) tagging

Validation ID: d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7
Question: What are the disadvantages to clipping negative PMI?
Answer: It discards scale information.
True answer: It may lead to poor rare word representations and word analogies.

Validation ID: d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7
Question: What are the disadvantages to clipping negative PMI?
Answer: It discards scale information.
True answer: Unanswerable

Validation ID: 6844683935d0d8f588fa06530f5068bf3e1ed0c0
Question: Why are statistics from finite corpora unreliable?
Answer: Because $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus.
True answer: $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus

Validation ID: 6844683935d0d8f588fa06530f5068bf3e1ed0c0
Question: Why are statistics from finite corpora unreliable?
Answer: Because $\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus.
True answer: A finite corpora may entirely omit rare word combinations

Validation ID: 8acab64ba72831633e8cc174d5469afecccf3ae9
Question: what is the domain of the corpus?
Answer: CALLHOME Spanish-English speech translation corpus
True answer: Unanswerable

Validation ID: 8acab64ba72831633e8cc174d5469afecccf3ae9
Question: what is the domain of the corpus?
Answer: CALLHOME Spanish-English speech translation corpus
True answer: telephone calls

Validation ID: 53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa
Question: what challenges are identified?
Answer: 1. Assigning wrong words to a cluster
True answer: Assigning wrong words to a cluster, Splitting words across different clusters, sparse, giving low coverage

Validation ID: 53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa
Question: what challenges are identified?
Answer: 
True answer: low coverage of audio, difficulty in cross-speaker clustering

Validation ID: 72755c2d79210857cfff60bfbcb55f83c71ada51
Question: what is the size of the speech corpus?
Answer: 11 hours
True answer: 104 telephone calls, transcripts contain 168,195 Spanish word tokens,  translations contain 159,777 English word tokens

Validation ID: 72755c2d79210857cfff60bfbcb55f83c71ada51
Question: what is the size of the speech corpus?
Answer: 11 hours
True answer: 104 telephone calls, which pair 11 hours of audio

Validation ID: 7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569
Question: Which two pairs of ERPs from the literature benefit from joint training?
Answer: ELAN and P600, LAN and P600
True answer: Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.
Select:
- ELAN, LAN
- PNP ERP

Validation ID: 7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569
Question: Which two pairs of ERPs from the literature benefit from joint training?
Answer: ELAN and P600, LAN and P600
True answer: Unanswerable

Validation ID: bd6dc38a9ac8d329114172194b0820766458dacc
Question: What datasets are used?
Answer: BIBREF0
True answer: Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.
Select:
- ERP data collected and computed by Frank et al. (2015)
- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)

Validation ID: bd6dc38a9ac8d329114172194b0820766458dacc
Question: What datasets are used?
Answer: BIBREF0
True answer: the ERP data: BIBREF0

Validation ID: 3ddff6b707767c3dd54d7104fe88b628765cae58
Question: which datasets did they experiment with?
Answer: 16 datasets
True answer: Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,
Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish

Validation ID: 3ddff6b707767c3dd54d7104fe88b628765cae58
Question: which datasets did they experiment with?
Answer: 16 datasets
True answer: Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2

Validation ID: 0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7
Question: which languages are explored?
Answer: 16 languages
True answer: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish

Validation ID: 0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7
Question: which languages are explored?
Answer: 16 languages
True answer: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish

Validation ID: 06be47e2f50b902b05ebf1ff1c66051925f5c247
Question: Do they use number of votes as an indicator of preference?
Answer: No
True answer: No

Validation ID: 06be47e2f50b902b05ebf1ff1c66051925f5c247
Question: Do they use number of votes as an indicator of preference?
Answer: No
True answer: Yes

Validation ID: 003d6f9722ddc2ee13e879fefafc315fb8e87cb9
Question: What does a node in the network approach repesent?
Answer: A country
True answer: Unanswerable

Validation ID: 003d6f9722ddc2ee13e879fefafc315fb8e87cb9
Question: What does a node in the network approach repesent?
Answer: A country
True answer: Unanswerable

Validation ID: c88a846197b72d25e04ec55f00ee3e72f655504c
Question: Which dataset do they use?
Answer: UN General Debate Corpus
True answer: corpus of state speeches delivered during the annual UN General Debate

Validation ID: c88a846197b72d25e04ec55f00ee3e72f655504c
Question: Which dataset do they use?
Answer: UN General Debate Corpus
True answer: corpus of state speeches delivered during the annual UN General Debate

Validation ID: 4d28c99750095763c81bcd5544491a0ba51d9070
Question: What kind of celebrities do they obtain tweets from?
Answer: They obtain tweets from celebrities from various domains, including politics, entertainment, and sports.
True answer: Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,
Ellen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey

Validation ID: 4d28c99750095763c81bcd5544491a0ba51d9070
Question: What kind of celebrities do they obtain tweets from?
Answer: They obtain tweets from celebrities from various domains, including politics, entertainment, and sports.
True answer: Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. 

Validation ID: 78292bc57ee68fdb93ed45430d80acca25a9e916
Question: How did they extend LAMA evaluation framework to focus on negation?
Answer: They created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. They did the same for a subset of ConceptNet that is easy to negate. They selected this subset by filtering for sentence length and extracting common queries.
True answer: To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement

Validation ID: 78292bc57ee68fdb93ed45430d80acca25a9e916
Question: How did they extend LAMA evaluation framework to focus on negation?
Answer: They created negated versions of Google-RE, T-REx and SQuAD by manually inserting a negation element in each template or statement. They did the same for a subset of ConceptNet that is easy to negate. They selected this subset by filtering for sentence length and extracting common queries.
True answer: Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions.

Validation ID: 443d2448136364235389039cbead07e80922ec5c
Question: What summarization algorithms did the authors experiment with?
Answer: Sumy package
True answer: LSA, TextRank, LexRank and ILP-based summary.

Validation ID: 443d2448136364235389039cbead07e80922ec5c
Question: What summarization algorithms did the authors experiment with?
Answer: Sumy package
True answer: LSA, TextRank, LexRank

Validation ID: aa6d956c2860f58fc9baea74c353c9d985b05605
Question: What evaluation metrics were used for the summarization task?
Answer: ROUGE unigram score
True answer: ROUGE BIBREF22 unigram score

Validation ID: aa6d956c2860f58fc9baea74c353c9d985b05605
Question: What evaluation metrics were used for the summarization task?
Answer: ROUGE unigram score
True answer: ROUGE

Validation ID: 4c18081ae3b676cc7831403d11bc070c10120f8e
Question: What clustering algorithms were used?
Answer: CLUTO and Carrot2 Lingo
True answer: CLUTO, Carrot2 Lingo

Validation ID: 4c18081ae3b676cc7831403d11bc070c10120f8e
Question: What clustering algorithms were used?
Answer: CLUTO and Carrot2 Lingo
True answer: simple clustering algorithm which uses the cosine similarity between word embeddings

Validation ID: fb3d30d59ed49e87f63d3735b876d45c4c6b8939
Question: What evaluation metrics are looked at for classification tasks?
Answer: Accuracy, Precision, Recall, F-measure
True answer: Precision, Recall, F-measure, accuracy

Validation ID: fb3d30d59ed49e87f63d3735b876d45c4c6b8939
Question: What evaluation metrics are looked at for classification tasks?
Answer: Accuracy, Precision, Recall, F-measure
True answer: Precision, Recall and F-measure

Validation ID: 197b276d0610ebfacd57ab46b0b29f3033c96a40
Question: What methods were used for sentence classification?
Answer: SVM with ADWS kernel
True answer: Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based

Validation ID: 197b276d0610ebfacd57ab46b0b29f3033c96a40
Question: What methods were used for sentence classification?
Answer: SVM with ADWS kernel
True answer: Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach

Validation ID: e025061e199b121f2ac8f3d9637d9bf987d65cd5
Question: What is the average length of the sentences?
Answer: 15.5
True answer: 15.5

Validation ID: e025061e199b121f2ac8f3d9637d9bf987d65cd5
Question: What is the average length of the sentences?
Answer: 15.5
True answer: average:15.5

Validation ID: 61652a3da85196564401d616d251084a25ab4596
Question: What is the size of the real-life dataset?
Answer: 4528 employees
True answer: 26972

Validation ID: 61652a3da85196564401d616d251084a25ab4596
Question: What is the size of the real-life dataset?
Answer: 4528 employees
True answer: 26972 sentences

Validation ID: 14b74ad5a6f5b0506511c9b454e9c464371ef8c4
Question: What are the language pairs explored in this paper?
Answer: German-English, Japanese-English, and Romanian-English
True answer: De-En, Ja-En, Ro-En

Validation ID: 14b74ad5a6f5b0506511c9b454e9c464371ef8c4
Question: What are the language pairs explored in this paper?
Answer: German-English, Japanese-English, and Romanian-English
True answer: De-En, Ja-En, Ro-En

Validation ID: 5c88d601e8fca96bffebfa9ef22331ecf31c6d75
Question: Have they made any attempt to correct MRC gold standards according to their findings??
Answer: No
True answer: Yes

Validation ID: 5c88d601e8fca96bffebfa9ef22331ecf31c6d75
Question: Have they made any attempt to correct MRC gold standards according to their findings??
Answer: No
True answer: No

Validation ID: 71bd5db79635d48a0730163a9f2e8ef19a86cd66
Question: What features are absent from MRC gold standards that can result in potential lexical ambiguity?
Answer: Semantics-altering grammatical modifiers
True answer: Restrictivity , Factivity , Coreference 

Validation ID: 71bd5db79635d48a0730163a9f2e8ef19a86cd66
Question: What features are absent from MRC gold standards that can result in potential lexical ambiguity?
Answer: Semantics-altering grammatical modifiers
True answer: semantics-altering grammatical modifiers

Validation ID: 9ecde59ffab3c57ec54591c3c7826a9188b2b270
Question: What modern MRC gold standards are analyzed?
Answer: SQuAD, NewsQA, MultiRC, ReCoRd, HotpotQA, DROP
True answer: fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations

Validation ID: 9ecde59ffab3c57ec54591c3c7826a9188b2b270
Question: What modern MRC gold standards are analyzed?
Answer: SQuAD, NewsQA, MultiRC, ReCoRd, HotpotQA, DROP
True answer: MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.

Validation ID: 005cca3c8ab6c3a166e315547a2259020f318ffb
Question: How does proposed qualitative annotation schema looks like?
Answer: The proposed annotation schema is shown in Figure FIGREF10.
True answer: The resulting taxonomy of the framework is shown in Figure FIGREF10

Validation ID: 005cca3c8ab6c3a166e315547a2259020f318ffb
Question: How does proposed qualitative annotation schema looks like?
Answer: The proposed annotation schema is shown in Figure FIGREF10.
True answer: FIGREF10

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
Answer: Vanilla ST baseline, Pre-training baselines, Multi-task baselines
True answer: Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
Answer: Vanilla ST baseline, Pre-training baselines, Multi-task baselines
True answer: Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train

Validation ID: af34051bf3e628c1e2a00b110bb84e5f018b419f
Question: What are the baselines?
Answer: Vanilla ST baseline, Pre-training baselines, Multi-task baselines
True answer: Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.

Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.

Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\alpha _{st}=0.75$ while $\alpha _{asr}=0.25$ or $\alpha _{mt}=0.25$. For many-to-many setting, we use $\alpha _{st}=0.6, \alpha _{asr}=0.2$ and $\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.

Many-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. 

Validation ID: 022c365a14fdec406c7a945a1a18e7e79df37f08
Question: What is the attention module pretrained on?
Answer: MT
True answer: the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.

Validation ID: 5260cb56b7d127772425583c5c28958c37cb9bea
Question: How long of dialog history is captured?
Answer: 5 turns
True answer: two previous turns

Validation ID: 5260cb56b7d127772425583c5c28958c37cb9bea
Question: How long of dialog history is captured?
Answer: 5 turns
True answer: 160

Validation ID: 9b97805a0c093df405391a85e4d3ab447671c86a
Question: What evaluation metrics were used?
Answer: Exact Match (EM) and Macro-averaged F1 scores (F1)
True answer: Exact Match (EM), Macro-averaged F1 scores (F1)

Validation ID: 9b97805a0c093df405391a85e4d3ab447671c86a
Question: What evaluation metrics were used?
Answer: Exact Match (EM) and Macro-averaged F1 scores (F1)
True answer: Exact Match (EM) and Macro-averaged F1 scores (F1) 

Validation ID: 38f58f13c7f23442d5952c8caf126073a477bac0
Question: What was the score of the proposed model?
Answer: 2% EM score and over 1.5% F1 score
True answer: Best results authors obtain is EM 51.10 and F1 63.11

Validation ID: 38f58f13c7f23442d5952c8caf126073a477bac0
Question: What was the score of the proposed model?
Answer: 2% EM score and over 1.5% F1 score
True answer: EM Score of 51.10

Validation ID: 7ee5c45b127fb284a4a9e72bb9b980a602f7445a
Question: What was the previous best model?
Answer: Dr.QA
True answer: (c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 

Validation ID: 7ee5c45b127fb284a4a9e72bb9b980a602f7445a
Question: What was the previous best model?
Answer: Dr.QA
True answer: Unanswerable

Validation ID: ddf5e1f600b9ce2e8f63213982ef4209bab01fd8
Question: Which datasets did they use for evaluation?
Answer: Spoken-SQuAD
True answer: Spoken-SQuAD testing set

Validation ID: ddf5e1f600b9ce2e8f63213982ef4209bab01fd8
Question: Which datasets did they use for evaluation?
Answer: Spoken-SQuAD
True answer: Spoken-SQuAD

Validation ID: 27275fe9f6a9004639f9ac33c3a5767fea388a98
Question: What hyperparameters are explored?
Answer: dimension size, training epochs, window size and vocabulary size
True answer: Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.

Validation ID: 27275fe9f6a9004639f9ac33c3a5767fea388a98
Question: What hyperparameters are explored?
Answer: dimension size, training epochs, window size and vocabulary size
True answer: Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.

Validation ID: ef3567ce7301b28e34377e7b62c4ec9b496f00bf
Question: What Named Entity Recognition dataset is used?
Answer: GMB
True answer: Groningen Meaning Bank

Validation ID: ef3567ce7301b28e34377e7b62c4ec9b496f00bf
Question: What Named Entity Recognition dataset is used?
Answer: GMB
True answer: Groningen Meaning Bank (GMB)

Validation ID: 7595260c5747aede0b32b7414e13899869209506
Question: What sentiment analysis dataset is used?
Answer: IMDb
True answer: IMDb dataset of movie reviews

Validation ID: 7595260c5747aede0b32b7414e13899869209506
Question: What sentiment analysis dataset is used?
Answer: IMDb
True answer: IMDb

Validation ID: c2d1387e08cf25cb6b1f482178cca58030e85b70
Question: Do they test both skipgram and c-bow?
Answer: Yes
True answer: Yes

Validation ID: c2d1387e08cf25cb6b1f482178cca58030e85b70
Question: Do they test both skipgram and c-bow?
Answer: Yes
True answer: Yes

Validation ID: 5a22293b055f5775081d6acdc0450f7bd5f5de04
Question: What is the state-of-the-art model for the task?
Answer: Nie et al.
True answer: OpATT BIBREF6, Neural Content Planning with conditional copy (NCP+CC) BIBREF4

Validation ID: 5a22293b055f5775081d6acdc0450f7bd5f5de04
Question: What is the state-of-the-art model for the task?
Answer: Nie et al.
True answer: Unanswerable

Validation ID: 03c967763e51ef2537793db7902e2c9c17e43e95
Question: What is the strong baseline?
Answer: Conditional Copy (CC)
True answer: Conditional Copy (CC) model 

Validation ID: 03c967763e51ef2537793db7902e2c9c17e43e95
Question: What is the strong baseline?
Answer: Conditional Copy (CC)
True answer: delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)

Validation ID: 26327ccebc620a73ba37a95aabe968864e3392b2
Question: what aspects of conversation flow do they look at?
Answer: promoting one's own points and addressing the opponent's points
True answer: The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.

Validation ID: 26327ccebc620a73ba37a95aabe968864e3392b2
Question: what aspects of conversation flow do they look at?
Answer: promoting one's own points and addressing the opponent's points
True answer: —promoting one's own points and attacking the opponents' points

Validation ID: ababb79dd3c301f4541beafa181f6a6726839a10
Question: what debates dataset was used?
Answer: Intelligence Squared
True answer: Intelligence Squared Debates

Validation ID: ababb79dd3c301f4541beafa181f6a6726839a10
Question: what debates dataset was used?
Answer: Intelligence Squared
True answer: “Intelligence Squared Debates” (IQ2 for short)

Validation ID: c2b8ee872b99f698b3d2082d57f9408a91e1b4c1
Question: what is the state of the art?
Answer: 84–94%
True answer: Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
Answer: CoNLL2003
True answer: The GENIA Corpus , CoNLL2003

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
Answer: CoNLL2003
True answer: GENIA Corpus BIBREF3, CoNLL2003 BIBREF14, KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC

Validation ID: 8eefa116e3c3d3db751423cc4095d1c4153d3a5f
Question: what standard dataset were used?
Answer: CoNLL2003
True answer: CoNLL2003-testA, GENIA

Validation ID: 133eb4aa4394758be5f41744c60c99901b2bc01c
Question: Do they perform error analysis?
Answer: No
True answer: No

Validation ID: 133eb4aa4394758be5f41744c60c99901b2bc01c
Question: Do they perform error analysis?
Answer: No
True answer: No

Validation ID: 3fff37b9f68697d080dbd9d9008a63907137644e
Question: How do their results compare to state-of-the-art?
Answer: The results are not state-of-the-art.
True answer: Unanswerable

Validation ID: 3fff37b9f68697d080dbd9d9008a63907137644e
Question: How do their results compare to state-of-the-art?
Answer: The results are not state-of-the-art.
True answer: Unanswerable

Validation ID: a778b8204a415b295f73b93623d09599f242f202
Question: What is the Random Kitchen Sink approach?
Answer: Random Kitchen Sink approach is a method to explicitly map data vectors to a space where linear separation is possible.
True answer: Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.

Validation ID: a778b8204a415b295f73b93623d09599f242f202
Question: What is the Random Kitchen Sink approach?
Answer: Random Kitchen Sink approach is a method to explicitly map data vectors to a space where linear separation is possible.
True answer: explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping

Validation ID: 642e8cf1d39faa1cd985d16750cdc6696c52db2f
Question: what are the baseline systems?
Answer: attentional encoder-decoder networks
True answer: attentional encoder-decoder networks BIBREF0

Validation ID: 642e8cf1d39faa1cd985d16750cdc6696c52db2f
Question: what are the baseline systems?
Answer: attentional encoder-decoder networks
True answer:  the dl4mt-tutorial

Validation ID: 493e971ee3f57a821ef1f67ef3cd47ade154e7c4
Question: What word embeddings do they test?
Answer: CBOW, PV-DM, GloVe and EqEmb
True answer: Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model

Validation ID: 493e971ee3f57a821ef1f67ef3cd47ade154e7c4
Question: What word embeddings do they test?
Answer: CBOW, PV-DM, GloVe and EqEmb
True answer: Bernoulli embeddings, continuous bag-of-words, Distributed Memory version of Paragraph Vector, Global Vectors, equation embeddings, equation unit embeddings

Validation ID: 8dd8e5599fc56562f2acbc16dd8544689cddd938
Question: How do they define similar equations?
Answer: Euclidean distance computed between the context vector representations of the equations
True answer: By using Euclidean distance computed between the context vector representations of the equations

Validation ID: 8dd8e5599fc56562f2acbc16dd8544689cddd938
Question: How do they define similar equations?
Answer: Euclidean distance computed between the context vector representations of the equations
True answer: Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B.

Validation ID: abe2393415e533cb06311e74ed1c5674cff8571f
Question: What evaluation criteria and metrics were used to evaluate the generated text?
Answer: Word error rate (edit distance)
True answer: BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency

Validation ID: abe2393415e533cb06311e74ed1c5674cff8571f
Question: What evaluation criteria and metrics were used to evaluate the generated text?
Answer: Word error rate (edit distance)
True answer: BLEU, NIST, METEOR, ROUGE-L, CIDEr

Validation ID: 00c57e45ac6afbdfa67350a57e81b4fad0ed2885
Question: Do they evaluate only on English datasets?
Answer: No
True answer: Yes

Validation ID: 00c57e45ac6afbdfa67350a57e81b4fad0ed2885
Question: Do they evaluate only on English datasets?
Answer: No
True answer: Yes

Validation ID: 22714f6cad2d5c54c28823e7285dc85e8d6bc109
Question: What are the three steps to feature elimination?
Answer: Reduction, Selection, Rank
True answer: Reduction, Selection, Rank

Validation ID: 22714f6cad2d5c54c28823e7285dc85e8d6bc109
Question: What are the three steps to feature elimination?
Answer: Reduction, Selection, Rank
True answer: reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features.

Validation ID: 82642d3111287abf736b781043d49536fe48c350
Question: How is the dataset annotated?
Answer: Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression") or evidence of depression (e.g., “depressed over disappointment"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps"), disturbed sleep (e.g., “another restless night"), or fatigue or loss of energy (e.g., “the fatigue is unbearable") BIBREF10. For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0.
True answer: no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy

Validation ID: 82642d3111287abf736b781043d49536fe48c350
Question: How is the dataset annotated?
Answer: Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression") or evidence of depression (e.g., “depressed over disappointment"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps"), disturbed sleep (e.g., “another restless night"), or fatigue or loss of energy (e.g., “the fatigue is unbearable") BIBREF10. For each class, every annotation (9,473 tweets) is binarized as the positive class e.g., depressed mood=1 or negative class e.g., not depressed mood=0.
True answer: The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression

Validation ID: 5a81732d52f64e81f1f83e8fd3514251227efbc7
Question: What dataset is used for this study?
Answer: Twitter dataset
True answer: BIBREF12 , BIBREF13

Validation ID: 5a81732d52f64e81f1f83e8fd3514251227efbc7
Question: What dataset is used for this study?
Answer: Twitter dataset
True answer: an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13

Validation ID: 9a8b9ea3176d30da2453cac6e9347737c729a538
Question: what were their performance results?
Answer: The hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes.
True answer:  the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes

Validation ID: 9a8b9ea3176d30da2453cac6e9347737c729a538
Question: what were their performance results?
Answer: The hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes.
True answer: hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes

Validation ID: 4477bb513d56e57732fba126944073d414d1f75f
Question: where did they obtain the annotated clinical notes from?
Answer: 2010 i2b2/VA
True answer: clinical notes from the CE task in 2010 i2b2/VA

Validation ID: 4477bb513d56e57732fba126944073d414d1f75f
Question: where did they obtain the annotated clinical notes from?
Answer: 2010 i2b2/VA
True answer: clinical notes from the CE task in 2010 i2b2/VA 

Validation ID: 1b23c4535a6c10eb70bbc95313c465e4a547db5e
Question: Which architecture do they use for the encoder and decoder?
Answer: Convolutional, NIN and Bi-LSTM layers
True answer: we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part, On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)

Validation ID: 1b23c4535a6c10eb70bbc95313c465e4a547db5e
Question: Which architecture do they use for the encoder and decoder?
Answer: Convolutional, NIN and Bi-LSTM layers
True answer: In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM 

Validation ID: 0a75a52450ed866df3a304077769e1725a995bb7
Question: How does their decoder generate text?
Answer: Attention-based Encoder Decoder for Raw Speech Recognition
True answer: decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information

Validation ID: 0a75a52450ed866df3a304077769e1725a995bb7
Question: How does their decoder generate text?
Answer: Attention-based Encoder Decoder for Raw Speech Recognition
True answer: Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy

Validation ID: fd0a3e9c210163a55d3ed791e95ae3875184b8f8
Question: Which dataset do they use?
Answer: WSJ
True answer: WSJ

Validation ID: fd0a3e9c210163a55d3ed791e95ae3875184b8f8
Question: Which dataset do they use?
Answer: WSJ
True answer: WSJ-SI84, WSJ-SI284

Validation ID: c37f65c9f0d543a35c784263b79236ccf1c44fac
Question: What model is used to encode the images?
Answer: Inception V3
True answer: a Convolutional Neural Network (CNN)

Validation ID: c37f65c9f0d543a35c784263b79236ccf1c44fac
Question: What model is used to encode the images?
Answer: Inception V3
True answer: LSTM

Validation ID: 584af673429c7f8621c6bf83362a37048daa0e5d
Question: How is the sequential nature of the story captured?
Answer: The model's first component is a Recurrent Neural Network (RNN), more precisely an LSTM that summarizes the sequence of images. At every timestep $t$ the network takes as input an image $I_i$ where $i\in \lbrace 1,2,3,4,5\rbrace $ from the sequence. At time $t=5$, the LSTM has encoded the 5 images and provides the sequence's context through its last hidden state denoted by $h_e^{(t)}$. The representation of the images was obtained through Inception V3.
True answer: we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story

Validation ID: 584af673429c7f8621c6bf83362a37048daa0e5d
Question: How is the sequential nature of the story captured?
Answer: The model's first component is a Recurrent Neural Network (RNN), more precisely an LSTM that summarizes the sequence of images. At every timestep $t$ the network takes as input an image $I_i$ where $i\in \lbrace 1,2,3,4,5\rbrace $ from the sequence. At time $t=5$, the LSTM has encoded the 5 images and provides the sequence's context through its last hidden state denoted by $h_e^{(t)}$. The representation of the images was obtained through Inception V3.
True answer: The encoder takes the images in order, one at every timestep $t$ . At time $t=5$ , we obtain the context vector through $h_e^{(t)}$ (represented by $\mathbf {Z}$ ). This vector is used to initialize each decoder's hidden state while the first input to each decoder is its corresponding image embedding $e(I_i)$ . Each decoder generates a sequence of words $\lbrace p_1,...,p_{n}\rbrace $ for each image in the sequence. 

Validation ID: 1be54c5b3ea67d837ffba2290a40c1e720d9587f
Question: Is the position in the sequence part of the input?
Answer: Yes
True answer: No

Validation ID: 1be54c5b3ea67d837ffba2290a40c1e720d9587f
Question: Is the position in the sequence part of the input?
Answer: Yes
True answer: Yes

Validation ID: b08f88d1facefceb87e134ba2c1fa90035018e83
Question: Do the decoder LSTMs all have the same weights?
Answer: No
True answer: No

Validation ID: b08f88d1facefceb87e134ba2c1fa90035018e83
Question: Do the decoder LSTMs all have the same weights?
Answer: No
True answer: No

Validation ID: b06512c17d99f9339ffdab12cedbc63501ff527e
Question: Is fine-tuning required to incorporate these embeddings into existing models?
Answer: No. We can simply use these embeddings as a drop-in replacement for existing embedding layers.
True answer: No

Validation ID: b06512c17d99f9339ffdab12cedbc63501ff527e
Question: Is fine-tuning required to incorporate these embeddings into existing models?
Answer: No. We can simply use these embeddings as a drop-in replacement for existing embedding layers.
True answer: No

Validation ID: fd8e23947095fe2230ffe1a478945829b09c8c95
Question: How are meaningful chains in the graph selected?
Answer: Random walk
True answer: No

Validation ID: fd8e23947095fe2230ffe1a478945829b09c8c95
Question: How are meaningful chains in the graph selected?
Answer: Random walk
True answer: utilize the machinery of language modeling using deep neural networks to learn Dolores embeddings.

Validation ID: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0
Question: Do the authors also analyze transformer-based architectures?
Answer: No, we only analyze LSTM-based architectures.
True answer: No

Validation ID: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0
Question: Do the authors also analyze transformer-based architectures?
Answer: No.
True answer: No

Validation ID: 3611a72f754de1e256fbd25b012197e1c24e8470
Question: Do they remove seasonality from the time series?
Answer: No
True answer: Unanswerable

Validation ID: 3611a72f754de1e256fbd25b012197e1c24e8470
Question: Do they remove seasonality from the time series?
Answer: No
True answer: No

Validation ID: 4c07c33dfaf4f3e6db55e377da6fa69825d0ba15
Question: What is the dimension of the embeddings?
Answer: 300
True answer: 300

Validation ID: 4c07c33dfaf4f3e6db55e377da6fa69825d0ba15
Question: What is the dimension of the embeddings?
Answer: 300
True answer: 300

Validation ID: b1ce129678e37070e69f01332f1a8587e18e06b0
Question: What dataset is used to train the model?
Answer: 3,216 tweets
True answer: 2,50,000 tweets, Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016

Validation ID: b1ce129678e37070e69f01332f1a8587e18e06b0
Question: What dataset is used to train the model?
Answer: 3,216 tweets
True answer: Collected tweets and opening and closing stock prices of Microsoft.

Validation ID: 7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1
Question: What is the previous state of the art?
Answer: RoBERTa
True answer: RoBERTa

Validation ID: 7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1
Question: What is the previous state of the art?
Answer: RoBERTa
True answer: RoBERTa

Validation ID: 0689904db9b00a814e3109fb1698086370a28fa2
Question: Which text embedding methodologies are used?
Answer: Distributed Bag of Words (DBOW)
True answer: Document to Vector (Doc2Vec)

Validation ID: 0689904db9b00a814e3109fb1698086370a28fa2
Question: Which text embedding methodologies are used?
Answer: Distributed Bag of Words (DBOW)
True answer: Doc2Vec, PV-DBOW model

Validation ID: cc354c952b5aaed2d4d1e932175e008ff2d801dd
Question: Which race and gender are given higher sentiment intensity predictions?
Answer: African American and female
True answer: Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.
African American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task.

Validation ID: cc354c952b5aaed2d4d1e932175e008ff2d801dd
Question: Which race and gender are given higher sentiment intensity predictions?
Answer: African American and female
True answer:  the number of systems consistently giving higher scores to sentences with female noun phrases, higher scores to sentences with African American names on the tasks of anger, fear, and sadness,  joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names

Validation ID: 0f12dc077fe8e5b95ca9163cea1dd17195c96929
Question: What criteria are used to select the 8,640 English sentences?
Answer: The sentences are selected based on the following criteria:
True answer: Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.

Validation ID: 0f12dc077fe8e5b95ca9163cea1dd17195c96929
Question: What criteria are used to select the 8,640 English sentences?
Answer: The sentences are selected based on the following criteria:
True answer: generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates, differ only in one word corresponding to gender or race

Validation ID: 2ddb51b03163d309434ee403fef42d6b9aecc458
Question: what were the baselines?
Answer: wav2letter
True answer: Unanswerable

Validation ID: 2ddb51b03163d309434ee403fef42d6b9aecc458
Question: what were the baselines?
Answer: wav2letter
True answer: LF-MMI Attention
Seq2Seq 
RNN-T 
Char E2E LF-MMI 
Phone E2E LF-MMI 
CTC + Gram-CTC

Validation ID: e587559f5ab6e42f7d981372ee34aebdc92b646e
Question: what competitive results did they obtain?
Answer: on other benchmarks
True answer: In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.
In case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. 

Validation ID: e587559f5ab6e42f7d981372ee34aebdc92b646e
Question: what competitive results did they obtain?
Answer: on other benchmarks
True answer: On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.
On Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.

Validation ID: bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a
Question: How much is performance improved on NLI?
Answer: 4%
True answer:  improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase

Validation ID: bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a
Question: How much is performance improved on NLI?
Answer: 4%
True answer: The average score improved by 1.4 points over the previous best result.

Validation ID: 7b4fb6da74e6bd1baea556788a02969134cf0800
Question: Do they train their model starting from a checkpoint?
Answer: Yes
True answer: No

Validation ID: 7b4fb6da74e6bd1baea556788a02969134cf0800
Question: Do they train their model starting from a checkpoint?
Answer: Yes
True answer: No

Validation ID: bc31a3d2f7c608df8c019a64d64cb0ccc5669210
Question: What BERT model do they test?
Answer: BERTBase
True answer: BERTbase

Validation ID: bc31a3d2f7c608df8c019a64d64cb0ccc5669210
Question: What BERT model do they test?
Answer: BERTBase
True answer: BERTbase

Validation ID: 761de1610e934189850e8fda707dc5239dd58092
Question: what was the baseline?
Answer: the baseline was the M2M Transformer model trained from scratch on the mixture of six-way pseudo-parallel data generated by VII and the original parallel data.
True answer: pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17

Validation ID: 761de1610e934189850e8fda707dc5239dd58092
Question: what was the baseline?
Answer: the baseline was the M2M Transformer model trained from scratch on the mixture of six-way pseudo-parallel data generated by VII and the original parallel data.
True answer: M2M Transformer

Validation ID: f8da63df16c4c42093e5778c01a8e7e9b270142e
Question: How is segmentation quality evaluated?
Answer: The evaluation is based on the correspondence of word pairs representing a border.
True answer: Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.

Validation ID: f8da63df16c4c42093e5778c01a8e7e9b270142e
Question: How is segmentation quality evaluated?
Answer: The evaluation is based on the correspondence of word pairs representing a border.
True answer: we compare the Annodis segmentation with the automatically produced segmentation

Validation ID: c09a92e25e6a81369fcc4ae6045491f2690ccc10
Question: How do they compare lexicons?
Answer: They compare lexicons by using the same data set.
True answer: Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.

Validation ID: c09a92e25e6a81369fcc4ae6045491f2690ccc10
Question: How do they compare lexicons?
Answer: They compare lexicons by using the same data set.
True answer: 1000 term groups based on the number of total annotations(200 term groups with 2 total annotations, 200 term groups with 3 total annotations, and so on up to term groups with 6 total annotations)

Validation ID: 63c3550c6fb42f41a0c93133e9fca12ac00df9b3
Question: Is it possible to convert a cloze-style questions to a naturally-looking questions?
Answer: Yes
True answer: Unanswerable

Validation ID: 63c3550c6fb42f41a0c93133e9fca12ac00df9b3
Question: Is it possible to convert a cloze-style questions to a naturally-looking questions?
Answer: Yes
True answer: Unanswerable

Validation ID: 603fee7314fa65261812157ddfc2c544277fcf90
Question: How larger are the training sets of these versions of ELMo compared to the previous ones?
Answer: The training sets are much larger.
True answer: By 14 times.

Validation ID: 603fee7314fa65261812157ddfc2c544277fcf90
Question: How larger are the training sets of these versions of ELMo compared to the previous ones?
Answer: The training sets are much larger.
True answer: up to 1.95 times larger

Validation ID: 09a1173e971e0fcdbf2fbecb1b077158ab08f497
Question: What is the improvement in performance for Estonian in the NER task?
Answer: 0.01
True answer: 5 percent points.

Validation ID: 09a1173e971e0fcdbf2fbecb1b077158ab08f497
Question: What is the improvement in performance for Estonian in the NER task?
Answer: 0.01
True answer: 0.05 F1

Validation ID: 70e9210fe64f8d71334e5107732d764332a81cb1
Question: what is the state of the art on WSJ?
Answer: INLINEFORM6
True answer: CNN-DNN-BLSTM-HMM

Validation ID: 70e9210fe64f8d71334e5107732d764332a81cb1
Question: what is the state of the art on WSJ?
Answer: INLINEFORM6
True answer: HMM-based system

Validation ID: 051df74dc643498e95d16e58851701628fdfd43e
Question: How did they obtain the OSG dataset?
Answer: They crawled and pre-processed an OSG web forum.
True answer: crawling and pre-processing an OSG web forum

Validation ID: 051df74dc643498e95d16e58851701628fdfd43e
Question: How did they obtain the OSG dataset?
Answer: They crawled and pre-processed an OSG web forum.
True answer: data has been developed by crawling and pre-processing an OSG web forum

Validation ID: 33554065284110859a8ea3ca7346474ab2cab100
Question: How large is the Twitter dataset?
Answer: 1.873
True answer: 1,873 Twitter conversation threads, roughly 14k tweets

Validation ID: 33554065284110859a8ea3ca7346474ab2cab100
Question: How large is the Twitter dataset?
Answer: 1.873
True answer: 1,873 Twitter conversation threads, roughly 14k tweets

Validation ID: 57f23dfc264feb62f45d9a9e24c60bd73d7fe563
Question: what is the size of the augmented dataset?
Answer: The augmented dataset is the same size as the original dataset.
True answer: Unanswerable

Validation ID: 57f23dfc264feb62f45d9a9e24c60bd73d7fe563
Question: what is the size of the augmented dataset?
Answer: The augmented dataset is the same size as the original dataset.
True answer: 609

Validation ID: 54830abe73fef4e629a36866ceeeca10214bd2c8
Question: How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?
Answer: They applied LDA algorithm and Gibbs sampling on ISWC and WWW conference's publications from 2013-2017.
True answer: the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter

Validation ID: 54830abe73fef4e629a36866ceeeca10214bd2c8
Question: How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?
Answer: They applied LDA algorithm and Gibbs sampling on ISWC and WWW conference's publications from 2013-2017.
True answer: discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\alpha =0.01$, $\beta = 0.01$ and using Gibbs sampling as a parameter estimation

Validation ID: 2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6
Question: What dataset do they use to evaluate their method?
Answer: WordNet
True answer: antonym and synonym pairs, collected from WordNet BIBREF9 and Wordnik

Validation ID: 2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6
Question: What dataset do they use to evaluate their method?
Answer: WordNet
True answer: English Wikipedia dump from June 2016

Validation ID: ef7212075e80bf35b7889dc8dd52fcbae0d1400a
Question: Why are current ELS's not sufficiently effective?
Answer: The extracted entities may be too ambiguous and coarse to be considered relevant to the summary.
True answer: Linked entities may be ambiguous or too common

Validation ID: ef7212075e80bf35b7889dc8dd52fcbae0d1400a
Question: Why are current ELS's not sufficiently effective?
Answer: The extracted entities may be too ambiguous and coarse to be considered relevant to the summary.
True answer: linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . These issues can be summarized into two parts: ambiguity and coarseness., the linked entities may also be too common to be considered an entity.

Validation ID: 567dc9bad8428ea9a2658c88203a0ed0f8da0dc3
Question: What is the best model?
Answer: BiLSTM+CNN(grapheme-level)
True answer: BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS 

Validation ID: d51dc36fbf6518226b8e45d4c817e07e8f642003
Question: How many sentences does the dataset contain?
Answer: 6946
True answer: 3606

Validation ID: d51dc36fbf6518226b8e45d4c817e07e8f642003
Question: How many sentences does the dataset contain?
Answer: 6946
True answer: 6946

Validation ID: d8627ba08b7342e473b8a2b560baa8cdbae3c7fd
Question: Do the authors train a Naive Bayes classifier on their dataset?
Answer: No
True answer: No

Validation ID: d8627ba08b7342e473b8a2b560baa8cdbae3c7fd
Question: Do the authors train a Naive Bayes classifier on their dataset?
Answer: No
True answer: No

Validation ID: cb77d6a74065cb05318faf57e7ceca05e126a80d
Question: What is the baseline?
Answer: BiLSTM+CNN(grapheme-level)
True answer: CNN modelBIBREF0, Stanford CRF modelBIBREF21

Validation ID: cb77d6a74065cb05318faf57e7ceca05e126a80d
Question: What is the baseline?
Answer: BiLSTM+CNN(grapheme-level)
True answer: Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec

Validation ID: 8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d
Question: Which machine learning models do they explore?
Answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF
True answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF

Validation ID: 8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d
Question: Which machine learning models do they explore?
Answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF
True answer: BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21

Validation ID: a1b3e2107302c5a993baafbe177684ae88d6f505
Question: What is the size of the dataset?
Answer: 72782
True answer: Dataset contains 3606 total sentences and 79087 total entities.

Validation ID: a1b3e2107302c5a993baafbe177684ae88d6f505
Question: What is the size of the dataset?
Answer: 72782
True answer: ILPRL contains 548 sentences, OurNepali contains 3606 sentences

Validation ID: bb2de20ee5937da7e3e6230e942bec7b6e8f61ee
Question: What is the source of their dataset?
Answer: ILPRL Lab, KU and KEIV Technologies
True answer: daily newspaper of the year 2015-2016

Validation ID: bb2de20ee5937da7e3e6230e942bec7b6e8f61ee
Question: What is the source of their dataset?
Answer: ILPRL Lab, KU and KEIV Technologies
True answer: daily newspaper of the year 2015-2016

Validation ID: 1170e4ee76fa202cabac9f621e8fbeb4a6c5f094
Question: Do they try to use byte-pair encoding representations?
Answer: No
True answer: No

Validation ID: 1170e4ee76fa202cabac9f621e8fbeb4a6c5f094
Question: Do they try to use byte-pair encoding representations?
Answer: No
True answer: No

Validation ID: 1462eb312944926469e7cee067dfc7f1267a2a8c
Question: How many different types of entities exist in the dataset?
Answer: 4
True answer: OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities

Validation ID: 1462eb312944926469e7cee067dfc7f1267a2a8c
Question: How many different types of entities exist in the dataset?
Answer: 4
True answer: three

Validation ID: f59f1f5b528a2eec5cfb1e49c87699e0c536cc45
Question: How big is the new Nepali NER dataset?
Answer: The new Nepali NER dataset is almost ten times bigger compared to ILPRL dataset in terms of entities.
True answer: 3606 sentences

Validation ID: f59f1f5b528a2eec5cfb1e49c87699e0c536cc45
Question: How big is the new Nepali NER dataset?
Answer: It contains around 10,000 sentences
True answer: Dataset contains 3606 total sentences and 79087 total entities.

Validation ID: 9bd080bb2a089410fd7ace82e91711136116af6c
Question: What is the performance improvement of the grapheme-level representation model over the character-level model?
Answer: 10%
True answer: On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement

Validation ID: 9bd080bb2a089410fd7ace82e91711136116af6c
Question: What is the performance improvement of the grapheme-level representation model over the character-level model?
Answer: 10%
True answer: BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration

Validation ID: 6d1217b3d9cfb04be7fcd2238666fa02855ce9c5
Question: Which models are used to solve NER for Nepali?
Answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF
True answer: BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21

Validation ID: 6d1217b3d9cfb04be7fcd2238666fa02855ce9c5
Question: Which models are used to solve NER for Nepali?
Answer: BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF
True answer: BiLSTM, BiLSTM+CNN, BiLSTM+CRF, BiLSTM+CNN+CRF, CNN, Stanford CRF

Validation ID: 1e775cf30784e6b1c2b573294a82e145a3f959bb
Question: What language(s) is/are represented in the dataset?
Answer: English
True answer: english

Validation ID: 1e775cf30784e6b1c2b573294a82e145a3f959bb
Question: What language(s) is/are represented in the dataset?
Answer: English
True answer: english

Validation ID: 392fb87564c4f45d0d8d491a9bb217c4fce87f03
Question: What baseline model is used?
Answer: RNN
True answer:  LastStateRNN, AvgRNN, AttentionRNN

Validation ID: 392fb87564c4f45d0d8d491a9bb217c4fce87f03
Question: What baseline model is used?
Answer: RNN
True answer: LastStateRNN, AvgRNN, AttentionRNN 

Validation ID: 203337c15bd1ee05763c748391d295a1f6415b9b
Question: Which variation provides the best results on this dataset?
Answer: MultiAttentionRNN with Projected Layer
True answer: the model with multi-attention mechanism and a projected layer

Validation ID: 203337c15bd1ee05763c748391d295a1f6415b9b
Question: Which variation provides the best results on this dataset?
Answer: MultiAttentionRNN with Projected Layer
True answer: Projected Layer

Validation ID: d004ca2e999940ac5c1576046e30efa3059832fa
Question: What are the different variations of the attention-based approach which are examined?
Answer: Multi-attention and single attention models
True answer: classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer

Validation ID: d004ca2e999940ac5c1576046e30efa3059832fa
Question: What are the different variations of the attention-based approach which are examined?
Answer: Multi-attention and single attention models
True answer:  four attention mechanisms instead of one, a projection layer for the word embeddings

Validation ID: 21548433abd21346659505296fb0576e78287a74
Question: What dataset is used for this work?
Answer: The dataset from Twitter
True answer: Twitter dataset provided by the organizers

Validation ID: 21548433abd21346659505296fb0576e78287a74
Question: What dataset is used for this work?
Answer: The dataset from Twitter
True answer: The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.

Validation ID: f0b2289cb887740f9255909018f400f028b1ef26
Question: What types of online harassment are studied?
Answer: indirect harassment, information threat, sexual harassment, physical harassment and not sexist
True answer: indirect harassment, sexual and physical harassment

Validation ID: f0b2289cb887740f9255909018f400f028b1ef26
Question: What types of online harassment are studied?
Answer: indirect harassment, information threat, sexual harassment, physical harassment and not sexist
True answer: indirect, physical, sexual

Validation ID: 51b1142c1d23420dbf6d49446730b0e82b32137c
Question: What was the baseline?
Answer: RNN
True answer: LastStateRNN, AvgRNN, AttentionRNN

Validation ID: 58355e2a782bf145b61ee2a3e0e426119985c179
Question: What were the datasets used in this paper?
Answer: Twitter
True answer: The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. 

Validation ID: 58355e2a782bf145b61ee2a3e0e426119985c179
Question: What were the datasets used in this paper?
Answer: Twitter
True answer: Twitter dataset provided by organizers containing harassment and non-harassment tweets

Validation ID: 25c1c4a91f5dedd4e06d14121af3b5921db125e9
Question: Is car-speak language collection of abstract features that classifier is later trained on?
Answer: Yes
True answer: No

Validation ID: 25c1c4a91f5dedd4e06d14121af3b5921db125e9
Question: Is car-speak language collection of abstract features that classifier is later trained on?
Answer: Yes
True answer: No

Validation ID: f88036174b4a0dbf4fe70ddad884d16082c5748d
Question: Is order of "words" important in car speak language?
Answer: Yes
True answer: Unanswerable

Validation ID: f88036174b4a0dbf4fe70ddad884d16082c5748d
Question: Is order of "words" important in car speak language?
Answer: Yes
True answer: No

Validation ID: a267d620af319b48e56c191aa4c433ea3870f6fb
Question: What are labels in car speak language dataset?
Answer: car models
True answer: car 

Validation ID: a267d620af319b48e56c191aa4c433ea3870f6fb
Question: What are labels in car speak language dataset?
Answer: car models
True answer: the car

Validation ID: 899ed05c460bf2aa0aa65101cad1986d4f622652
Question: How big is dataset of car-speak language?
Answer: 3,209 reviews
True answer: $3,209$ reviews 

Validation ID: 899ed05c460bf2aa0aa65101cad1986d4f622652
Question: How big is dataset of car-speak language?
Answer: 3,209 reviews
True answer: $3,209$ reviews about 553 different cars from 49 different car manufacturers

Validation ID: d53299fac8c94bd0179968eb868506124af407d1
Question: What is the performance of classifiers?
Answer: KNN
True answer: Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject

Validation ID: d53299fac8c94bd0179968eb868506124af407d1
Question: What is the performance of classifiers?
Answer: KNN
True answer: Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.

Validation ID: 29f2954098f055fb19d9502572f085862d75bf61
Question: What classifiers have been trained?
Answer: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)
True answer: KNN
RF
SVM
MLP

Validation ID: 29f2954098f055fb19d9502572f085862d75bf61
Question: What classifiers have been trained?
Answer: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)
True answer:  K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)

Validation ID: 6bf93968110c6e3e3640360440607744007a5228
Question: How does car speak pertains to a car's physical attributes?
Answer: It could be the horsepower, or it could be the car's form factor (how the car looks).
True answer: we do not know exactly

Validation ID: 37a79be0148e1751ffb2daabe4c8ec6680036106
Question: What topic is covered in the Chinese Facebook data??
Answer: Nuclear power plants
True answer: anti-nuclear-power

Validation ID: 37a79be0148e1751ffb2daabe4c8ec6680036106
Question: What topic is covered in the Chinese Facebook data??
Answer: Nuclear power plants
True answer: anti-nuclear-power

Validation ID: 518dae6f936882152c162058895db4eca815e649
Question: How many layers does the UTCNN model have?
Answer: 3
True answer: eight layers

Validation ID: e44a6bf67ce3fde0c6608b150030e44d87eb25e3
Question: What topics are included in the debate data?
Answer: Abortion, gay rights, Obama, and marijuana
True answer: abortion, gay rights, Obama, marijuana

Validation ID: e44a6bf67ce3fde0c6608b150030e44d87eb25e3
Question: What topics are included in the debate data?
Answer: Abortion, gay rights, Obama, and marijuana
True answer: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)

Validation ID: 6a31db1aca57a818f36bba9002561724655372a7
Question: What is the size of the Chinese data?
Answer: 505,412 unique users
True answer: 32,595 posts

Validation ID: 6a31db1aca57a818f36bba9002561724655372a7
Question: What is the size of the Chinese data?
Answer: 505,412 unique users
True answer: 32,595

Validation ID: e330e162ec29722f5ec9f83853d129c9e0693d65
Question: Did they collected the two datasets?
Answer: Yes
True answer: No

Validation ID: e330e162ec29722f5ec9f83853d129c9e0693d65
Question: Did they collected the two datasets?
Answer: Yes
True answer: No

Validation ID: d3093062aebff475b4deab90815004051e802aa6
Question: What are the baselines?
Answer: SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM with n-gram or average word embedding feature with comment information, UTCNN without user information, UTCNN without the LDA model, UTCNN without comments
True answer: SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information

Validation ID: d3093062aebff475b4deab90815004051e802aa6
Question: What are the baselines?
Answer: SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, Recurrent Convolutional Neural Networks (RCNN), SVM with comment information, UTCNN without user information, UTCNN without the LDA model, UTCNN without comments
True answer: SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
Answer: SentEval
True answer: MR, CR, SUBJ, MPQA, SST, TREC, MRPC

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
Answer: SentEval
True answer: MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.

CR: Sentiment prediction of customer product reviews BIBREF26.

SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.

MPQA: Phrase level opinion polarity classification from newswire BIBREF28.

SST: Stanford Sentiment Treebank with binary labels BIBREF29.

TREC: Fine grained question-type classification from TREC BIBREF30.

MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.

Validation ID: 4944cd597b836b62616a4e37c045ce48de8c82ca
Question: What transfer learning tasks are evaluated?
Answer: SentEval
True answer: Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification.

Validation ID: a29c071065d26e5ee3c3bcd877e7f215c59d1d33
Question: What metrics are used for the STS tasks?
Answer: Spearman's rank correlation
True answer:  Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels

Validation ID: a29c071065d26e5ee3c3bcd877e7f215c59d1d33
Question: What metrics are used for the STS tasks?
Answer: Spearman's rank correlation
True answer: Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels

Validation ID: 7f207549c75f5c4388efc15ed28822672b845663
Question: How much time takes its training?
Answer: 20 minutes
True answer: 20 minutes

Validation ID: 596aede2b311deb8cb0a82d2e7de314ef6e83e4e
Question: How many GPUs are used for the training of SBERT?
Answer: 1
True answer: Unanswerable

Validation ID: 2e89ebd2e4008c67bb2413699589ee55f59c4f36
Question: How are the siamese networks trained?
Answer: The siamese networks are trained using the classification objective function.
True answer: Unanswerable

Validation ID: 2e89ebd2e4008c67bb2413699589ee55f59c4f36
Question: How are the siamese networks trained?
Answer: The siamese networks are trained using the classification objective function.
True answer: update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity., Classification Objective Function, Regression Objective Function, Triplet Objective Function

Validation ID: e2db361ae9ad9dbaa9a85736c5593eb3a471983d
Question: What other sentence embeddings methods are evaluated?
Answer: InferSent, Universal Sentence Encoder, average GloVe embeddings
True answer: GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent

Validation ID: e2db361ae9ad9dbaa9a85736c5593eb3a471983d
Question: What other sentence embeddings methods are evaluated?
Answer: InferSent, Universal Sentence Encoder, average GloVe embeddings
True answer: Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.

Validation ID: 252a645af9876241fb166e5822992ce17fec6eb6
Question: What is the average length of the title text?
Answer: 20
True answer: Unanswerable

Validation ID: ed67359889cf61fa11ee291d6c378cccf83d599d
Question: Which pretrained word vectors did they use?
Answer: GloVe
True answer:  pre-trained GloVe word vectors 

Validation ID: ed67359889cf61fa11ee291d6c378cccf83d599d
Question: Which pretrained word vectors did they use?
Answer: GloVe
True answer: GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)

Validation ID: 425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82
Question: What evaluation metrics are used?
Answer: Accuracy
True answer: standard accuracy metric

Validation ID: 425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82
Question: What evaluation metrics are used?
Answer: Accuracy
True answer: accuracy

Validation ID: 955de9f7412ba98a0c91998919fa048d339b1d48
Question: Which shallow approaches did they experiment with?
Answer: Support Vector Machine with linear kernel
True answer: SVM

Validation ID: 955de9f7412ba98a0c91998919fa048d339b1d48
Question: Which shallow approaches did they experiment with?
Answer: Support Vector Machine with linear kernel
True answer: SVM with linear kernel using bag-of-words features

Validation ID: 3b371ea554fa6639c76a364060258454e4b931d4
Question: Where do they obtain the news videos from?
Answer: Facebook
True answer: NowThisNews Facebook page

Validation ID: 3b371ea554fa6639c76a364060258454e4b931d4
Question: Where do they obtain the news videos from?
Answer: Facebook
True answer: NowThisNews Facebook page

Validation ID: ddb23a71113cbc092cbc158066d891cae261e2c6
Question: What is the source of the news articles?
Answer: Social media platform
True answer: main news channels, such as Yahoo News, The Guardian or The Washington Post

Validation ID: ddb23a71113cbc092cbc158066d891cae261e2c6
Question: What is the source of the news articles?
Answer: Social media platform
True answer: The BreakingNews dataset

Validation ID: e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38
Question: which non-english language had the best performance?
Answer: Dutch
True answer: Russian

Validation ID: e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38
Question: which non-english language had the best performance?
Answer: Dutch
True answer: Russsian

Validation ID: c7486d039304ca9d50d0571236429f4f6fbcfcf7
Question: which non-english language was the had the worst results?
Answer: Russian
True answer: Turkish

Validation ID: f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9
Question: what datasets were used in evaluation?
Answer: SemEval-2016 Task 5
True answer: SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28

Validation ID: f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9
Question: what datasets were used in evaluation?
Answer: SemEval-2016 Challenge Task 5
True answer:  English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)

Validation ID: a103636c8d1dbfa53341133aeb751ffec269415c
Question: what are the baselines?
Answer: majority baseline and lexicon-based baseline
True answer: majority baseline, lexicon-based approach

Validation ID: a103636c8d1dbfa53341133aeb751ffec269415c
Question: what are the baselines?
Answer: majority baseline and lexicon-based baseline
True answer: majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset, lexicon-based approach

Validation ID: 55139fcfe04ce90aad407e2e5a0067a45f31e07e
Question: how did the authors translate the reviews to other languages?
Answer: Google translation API
True answer: Using Google translation API.

Validation ID: 55139fcfe04ce90aad407e2e5a0067a45f31e07e
Question: how did the authors translate the reviews to other languages?
Answer: Google translation API
True answer: Google translation API

Validation ID: fbaf060004f196a286fef67593d2d76826f0304e
Question: what dataset was used for training?
Answer: Amazon reviews
True answer: Amazon reviews, Yelp restaurant reviews, restaurant reviews

Validation ID: fbaf060004f196a286fef67593d2d76826f0304e
Question: what dataset was used for training?
Answer: Amazon reviews
True answer: Amazon reviews BIBREF23 , BIBREF24, Yelp restaurant reviews dataset,  restaurant reviews dataset as part of a Kaggle competition BIBREF26

Validation ID: 7ae38f51243cb80b16a1df14872b72a1f8a2048f
Question: How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech?
Answer: They show that their model outperforms the baseline methods by a large average margin of 22.51%.
True answer: we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 .

Validation ID: deb89bca0925657e0f91ab5daca78b9e548de2bd
Question: What are the five different binary classification tasks?
Answer: presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels
True answer:  presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.

Validation ID: deb89bca0925657e0f91ab5daca78b9e548de2bd
Question: What are the five different binary classification tasks?
Answer: presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels
True answer: presence/absence of consonants, presence/absence of phonemic nasal, presence/absence of bilabial, presence/absence of high-front vowels, and presence/absence of high-back vowels

Validation ID: 9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570
Question: How was the spatial aspect of the EEG signal computed?
Answer: By computing the channel cross-covariance
True answer: we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.

Validation ID: 9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570
Question: How was the spatial aspect of the EEG signal computed?
Answer: By computing the channel cross-covariance
True answer: They use four-layered 2D CNN and two fully connected hidden layers on the channel covariance matrix to compute the spatial aspect.

Validation ID: e6583c60b13b87fc37af75ffc975e7e316d4f4e0
Question: What data was presented to the subjects to elicit event-related responses?
Answer: Phonemes
True answer: 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)

Validation ID: e6583c60b13b87fc37af75ffc975e7e316d4f4e0
Question: What data was presented to the subjects to elicit event-related responses?
Answer: Phonemes
True answer: KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)

Validation ID: c7b6e6cb997de1660fd24d31759fe6bb21c7863f
Question: How many electrodes were used on the subject in EEG sessions?
Answer: 64
True answer: 1913 signals

Validation ID: c7b6e6cb997de1660fd24d31759fe6bb21c7863f
Question: How many electrodes were used on the subject in EEG sessions?
Answer: 64
True answer: Unanswerable

Validation ID: f9f59c171531c452bd2767dc332dc74cadee5120
Question: How many subjects does the EEG data come from?
Answer: 14
True answer: 14

Validation ID: f9f59c171531c452bd2767dc332dc74cadee5120
Question: How many subjects does the EEG data come from?
Answer: 14
True answer: 14 participants

Validation ID: 4ac2c3c259024d7cd8e449600b499f93332dab60
Question: Do they report results only on English data?
Answer: Yes
True answer: Yes

Validation ID: 4ac2c3c259024d7cd8e449600b499f93332dab60
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: bc730e4d964b6a66656078e2da130310142ab641
Question: What type of classifiers are used?
Answer: Logistic Regression and Multilayer Perceptron
True answer: probabilistic model

Validation ID: bc730e4d964b6a66656078e2da130310142ab641
Question: What type of classifiers are used?
Answer: Logistic Regression and Multilayer Perceptron
True answer: Logistic Regression, Multilayer Perceptron

Validation ID: 3941401a182a3d6234894a5c8a75d48c6116c45c
Question: Which real-world datasets are used?
Answer: CyberAttack and PoliticianDeath
True answer: Tweets related to CyberAttack and tweets related to PoliticianDeath

Validation ID: 3941401a182a3d6234894a5c8a75d48c6116c45c
Question: Which real-world datasets are used?
Answer: CyberAttack and PoliticianDeath
True answer: cyber security (CyberAttack), death of politicians (PoliticianDeath)

Validation ID: 67e9e147b2cab5ba43572ce8a17fc863690172f0
Question: How are the interpretability merits of the approach demonstrated?
Answer: The approach is demonstrated to be interpretable by the fact that it provides human-understandable explanations for the improved model.
True answer: By involving humans for post-hoc evaluation of model's interpretability

Validation ID: 67e9e147b2cab5ba43572ce8a17fc863690172f0
Question: How are the interpretability merits of the approach demonstrated?
Answer: The approach is demonstrated to be interpretable by the fact that it provides human-understandable explanations for the improved model.
True answer: directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model

Validation ID: a74190189a6ced2a2d5b781e445e36f4e527e82a
Question: How are the accuracy merits of the approach demonstrated?
Answer: The accuracy merits of the approach are demonstrated by the fact that the approach significantly outperforms the state of the art.
True answer: significant improvements clearly demonstrate that our approach is effective at improving model performance

Validation ID: a74190189a6ced2a2d5b781e445e36f4e527e82a
Question: How are the accuracy merits of the approach demonstrated?
Answer: The accuracy merits of the approach are demonstrated by the fact that the approach significantly outperforms the state of the art.
True answer: By evaluating the performance of the approach using accuracy and AUC

Validation ID: 43f074bacabd0a355b4e0f91a1afd538c0a6244f
Question: How is the keyword specific expectation elicited from the crowd?
Answer: The keyword specific expectation is elicited from the crowd by asking them to label a set of tweets containing the keyword.
True answer: workers are first asked to find those microposts where the model predictions are deemed correct,  asked to find the keyword that best indicates the class of the microposts

Validation ID: 58ef2442450c392bfc55c4dc35f216542f5f2dbb
Question: Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?
Answer: Yes, the paper provides a case study in Section 5.
True answer: No

Validation ID: 58ef2442450c392bfc55c4dc35f216542f5f2dbb
Question: Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?
Answer: Yes, the paper provides a case study in Section 5.
True answer: No

Validation ID: 78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d
Question: What functionality does Macaw provide?
Answer: Macaw supports multi-turn, multi-modal, and mixed-initiative interactions.
True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d
Question: What functionality does Macaw provide?
Answer: Macaw supports multi-turn, multi-modal, and mixed-initiative interactions.
True answer: conversational search, conversational question answering, conversational recommendation, conversational natural language interface to structured and semi-structured data

Validation ID: 375b281e7441547ba284068326dd834216e55c07
Question: What is a wizard of oz setup?
Answer: A wizard of oz setup is a setup in which a real user interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.
True answer: seeker interacts with a real conversational interface, intermediary (or the wizard) receives the seeker's message and performs different information seeking actions

Validation ID: 375b281e7441547ba284068326dd834216e55c07
Question: What is a wizard of oz setup?
Answer: A wizard of oz setup is a setup in which a real user interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. This setup can simulate an ideal CIS system and thus is useful for collecting high-quality data from real users for CIS research.
True answer: a setup where the seeker interacts with a real conversational interface and the wizard, an intermediary, performs actions related to the seeker's message

Validation ID: 05c49b9f84772e6df41f530d86c1f7a1da6aa489
Question: What interface does Macaw currently have?
Answer: Telegram
True answer: File IO, Standard IO, Telegram

Validation ID: 05c49b9f84772e6df41f530d86c1f7a1da6aa489
Question: What interface does Macaw currently have?
Answer: Telegram
True answer: The current implementation of Macaw supports a command line interface as well as mobile, desktop, and web apps.

Validation ID: 6ecb69360449bb9915ac73c0a816c8ac479cbbfc
Question: What modalities are supported by Macaw?
Answer: multi-modal
True answer: text, speech, image, click, etc

Validation ID: 68df324e5fa697baed25c761d0be4c528f7f5cf7
Question: What are the different modules in Macaw?
Answer: The different modules in Macaw are:
True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 68df324e5fa697baed25c761d0be4c528f7f5cf7
Question: What are the different modules in Macaw?
Answer: The different modules in Macaw are:
True answer: Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation

Validation ID: 77c34f1033702278f7f044806c1eba0c6ecb8b04
Question: Do they report results only on English data?
Answer: Yes, they do.
True answer: Yes

Validation ID: 77c34f1033702278f7f044806c1eba0c6ecb8b04
Question: Do they report results only on English data?
Answer: Yes, they do.
True answer: Yes

Validation ID: 2ee715c7c6289669f11a79743a6b2b696073805d
Question: What baseline model is used?
Answer: B1
True answer: For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. 

For Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.

Validation ID: 2ee715c7c6289669f11a79743a6b2b696073805d
Question: What baseline model is used?
Answer: B1
True answer: B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .

, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0

Validation ID: 61a9ea36ddc37c60d1a51dabcfff9445a2225725
Question: What news article sources are used?
Answer: The news article sources are the news external references in Wikipedia.
True answer: Unanswerable

Validation ID: 61a9ea36ddc37c60d1a51dabcfff9445a2225725
Question: What news article sources are used?
Answer: The news article sources are the news external references in Wikipedia.
True answer:  the news external references in Wikipedia

Validation ID: cc850bc8245a7ae790e1f59014371d4f35cd46d7
Question: How do they determine the exact section to use the input article?
Answer: They use the class of the entity to generate section templates.
True answer: They use a multi-class classifier to determine the section it should be cited

Validation ID: 984fc3e726848f8f13dfe72b89e3770d00c3a1af
Question: What features are used to represent the novelty of news articles to entity pages?
Answer: The novelty of news articles to entity pages is measured by the KL divergence between the language models of the news article and the entity profile.
True answer: KL-divergences of language models for the news article and the already added news references

Validation ID: 984fc3e726848f8f13dfe72b89e3770d00c3a1af
Question: What features are used to represent the novelty of news articles to entity pages?
Answer: The novelty of news articles to entity pages is measured by the KL divergence between the language models of the news article and the entity profile.
True answer: KL divergence between the language model of INLINEFORM5 and articles in INLINEFORM6

Validation ID: fb1227b3681c69f60eb0539e16c5a8cd784177a7
Question: What features are used to represent the salience and relative authority of entities?
Answer: The features used to represent the salience and relative authority of entities are:
True answer: Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.
The relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it.

Validation ID: fb1227b3681c69f60eb0539e16c5a8cd784177a7
Question: What features are used to represent the salience and relative authority of entities?
Answer: The features used to represent the salience and relative authority of entities are:
True answer: positional features, occurrence frequency, internal POS structure of the entity and the sentence it occurs in, relative entity frequency, centrality measures like PageRank 

Validation ID: 8df35c24af9efc3348d3b8d746df116480dfe661
Question: Do they experiment with other tasks?
Answer: Yes
True answer: No

Validation ID: 277a7e916e65dfefd44d2d05774f95257ac946ae
Question: What baselines do they introduce?
Answer: CRF, BiLSTM-CRF, Multi-Task Learning, BioBERT
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT


Validation ID: 277a7e916e65dfefd44d2d05774f95257ac946ae
Question: What baselines do they introduce?
Answer: CRF, BiLSTM-CRF, Multi-Task Learning, BioBERT
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: 2916bbdb95ef31ab26527ba67961cf5ec94d6afe
Question: How large is the corpus?
Answer: 53 documents
True answer: 8,275 sentences and 167,739 words in total

Validation ID: 2916bbdb95ef31ab26527ba67961cf5ec94d6afe
Question: How large is the corpus?
Answer: 53 documents
True answer: The corpus comprises 8,275 sentences and 167,739 words in total.

Validation ID: f2e8497aa16327aa297a7f9f7d156e485fe33945
Question: How was annotation performed?
Answer: Using WebAnno
True answer: Experienced medical doctors used a linguistic annotation tool to annotate entities.

Validation ID: f2e8497aa16327aa297a7f9f7d156e485fe33945
Question: How was annotation performed?
Answer: Using WebAnno
True answer: WebAnno

Validation ID: 9b76f428b7c8c9fc930aa88ee585a03478bff9b3
Question: How many documents are in the new corpus?
Answer: 53
True answer: 53 documents

Validation ID: 9b76f428b7c8c9fc930aa88ee585a03478bff9b3
Question: How many documents are in the new corpus?
Answer: 53
True answer: 53 documents

Validation ID: dd6b378d89c05058e8f49e48fd48f5c458ea2ebc
Question: What baseline systems are proposed?
Answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: dd6b378d89c05058e8f49e48fd48f5c458ea2ebc
Question: What baseline systems are proposed?
Answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT
True answer: Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT

Validation ID: e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851
Question: How did they obtain the dataset?
Answer: They used a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). They discarded the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, they used a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. They discarded some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, they removed the duplicate, media-based, and link-only tweets. For each account, they collected the maximum amount of tweets allowed by Twitter API.
True answer: For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy

Validation ID: e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851
Question: How did they obtain the dataset?
Answer: They used a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources where suspicious Twitter accounts were annotated with the main fake news types (clickbait, propaganda, satire, and hoax). They discarded the satire labeled accounts since their intention is not to mislead or deceive. On the other hand, for the factual accounts, they used a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. They discarded some accounts that publish news in languages other than English (e.g., Russian or Arabic). Moreover, to ensure the quality of the data, they removed the duplicate, media-based, and link-only tweets. For each account, they collected the maximum amount of tweets allowed by Twitter API.
True answer: public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy

Validation ID: c00ce1e3be14610fb4e1f0614005911bb5ff0302
Question: What activation function do they use in their model?
Answer: relu
True answer: relu, selu, tanh

Validation ID: c00ce1e3be14610fb4e1f0614005911bb5ff0302
Question: What activation function do they use in their model?
Answer: relu
True answer: Activation function is hyperparameter. Possible values: relu, selu, tanh.

Validation ID: 71fe5822d9fccb1cb391c11283b223dc8aa1640c
Question: What baselines do they compare to?
Answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 71fe5822d9fccb1cb391c11283b223dc8aa1640c
Question: What baselines do they compare to?
Answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets
True answer: Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words

Validation ID: 97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3
Question: How are chunks defined?
Answer: Chunks are defined as a sequence of tweets labeled by the label of its corresponding account.
True answer: Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.

Validation ID: 97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3
Question: How are chunks defined?
Answer: Chunks are defined as a sequence of tweets labeled by the label of its corresponding account.
True answer: sequence of $s$ tweets

Validation ID: 1062a0506c3691a93bb914171c2701d2ae9621cb
Question: What features are extracted?
Answer: Emotion, sentiment, morality, style, and words embeddings
True answer: Sentiment, Morality, Style, Words embeddings

Validation ID: 1062a0506c3691a93bb914171c2701d2ae9621cb
Question: What features are extracted?
Answer: Emotion, sentiment, morality, style, and words embeddings
True answer: 15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings

Validation ID: 8e12b5c459fa963b3e549deadb864c244879fe82
Question: How many layers does their model have?
Answer: 2
True answer: Unanswerable

Validation ID: 483a699563efcb8804e1861b18809279f21c7610
Question: Was the approach used in this work to detect fake news fully supervised?
Answer: No
True answer: Yes

Validation ID: d3ff2986ca8cb85a9a5cec039c266df756947b43
Question: Based on this paper, what is the more predictive set of features to detect fake news?
Answer: Words embeddings, style, and morality features
True answer: words embeddings, style, and morality features

Validation ID: d3ff2986ca8cb85a9a5cec039c266df756947b43
Question: Based on this paper, what is the more predictive set of features to detect fake news?
Answer: Words embeddings, style, and morality features
True answer: words embeddings, style, and morality features

Validation ID: 3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4
Question: How big is the dataset used in this work?
Answer: 180 Twitter accounts
True answer: Total dataset size: 171 account (522967 tweets)

Validation ID: 3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4
Question: How big is the dataset used in this work?
Answer: 180 Twitter accounts
True answer: 212 accounts

Validation ID: 2317ca8d475b01f6632537b95895608dc40c4415
Question: How is a "chunk of posts" defined in this work?
Answer: A chunk of posts is a sequence of tweets labeled by the label of its corresponding account.
True answer: chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account

Validation ID: 2317ca8d475b01f6632537b95895608dc40c4415
Question: How is a "chunk of posts" defined in this work?
Answer: A chunk of posts is a sequence of tweets labeled by the label of its corresponding account.
True answer: sequence of $s$ tweets

Validation ID: 3e88fb3d28593309a307eb97e875575644a01463
Question: What baselines were used in this work?
Answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 3e88fb3d28593309a307eb97e875575644a01463
Question: What baselines were used in this work?
Answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets
True answer: LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets

Validation ID: 0767ca8ff1424f7a811222ca108a33b6411aaa8a
Question: What is the performance of their method?
Answer: We provide strong baseline for text summarization using AMR for possible future works.
True answer: Unanswerable

Validation ID: e8f969ffd637b82d04d3be28c51f0f3ca6b3883e
Question: Which evaluation methods are used?
Answer: ROGUE metric
True answer: Quantitative evaluation methods using ROUGE, Recall, Precision and F1.

Validation ID: e8f969ffd637b82d04d3be28c51f0f3ca6b3883e
Question: Which evaluation methods are used?
Answer: ROGUE metric
True answer: standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L

Validation ID: 46227b4265f1d300a5ed71bf40822829de662bc2
Question: What dataset is used in this paper?
Answer: CNN-Dailymail
True answer: AMR Bank, CNN-Dailymail

Validation ID: 46227b4265f1d300a5ed71bf40822829de662bc2
Question: What dataset is used in this paper?
Answer: CNN-Dailymail
True answer: AMR Bank BIBREF10, CNN-Dailymail ( BIBREF11 BIBREF12 )

Validation ID: a6a48de63c1928238b37c2a01c924b852fe752f8
Question: Which other methods do they compare with?
Answer: SummaRunNer
True answer: Lead-3, Lead-1-AMR

Validation ID: a6a48de63c1928238b37c2a01c924b852fe752f8
Question: Which other methods do they compare with?
Answer: SummaRunNer
True answer: Lead-3 model,  Lead-1-AMR, BIBREF0 

Validation ID: b65a83a24fc66728451bb063cf6ec50134c8bfb0
Question: How are sentences selected from the summary graph?
Answer: We select the sentences from the story that contain the most information of the summary sentence.
True answer:  finding the important sentences from the story, extracting the key information from those sentences using their AMR graphs

Validation ID: b65a83a24fc66728451bb063cf6ec50134c8bfb0
Question: How are sentences selected from the summary graph?
Answer: We select the first sentence from the story.
True answer:  Two methods: first is to simply pick initial few sentences,  second is to capture the relation between the two most important entities  (select the first sentence which contains both these entities).

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
Answer: SVM, BiLSTM, CNN
True answer: linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
Answer: SVM, BiLSTM, CNN
True answer: linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)

Validation ID: 8c852fc29bda014d28c3ee5b5a7e449ab9152d35
Question: What models are used in the experiment?
Answer: SVM, BiLSTM, CNN
True answer: linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) 

Validation ID: 682e26262abba473412f68cbeb5f69aa3b9968d7
Question: What are the differences between this dataset and pre-existing ones?
Answer: This is the first dataset to contain annotation of type and target of offenses in social media.
True answer: no prior work has explored the target of the offensive language

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
Answer: English
True answer: English

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
Answer: English
True answer: English 

Validation ID: 5daeb8d4d6f3b8543ec6309a7a35523e160437eb
Question: In what language are the tweets?
Answer: English
True answer: English

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
Answer: 14,100 tweets
True answer: Unanswerable

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
Answer: 14,100 tweets
True answer: 14,100 tweets

Validation ID: 74fb77a624ea9f1821f58935a52cca3086bb0981
Question: What is the size of the new dataset?
Answer: 14,100 tweets
True answer: Dataset contains total of 14100 annotations.

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
Answer: abusive language, (cyber-)aggression, (cyber-)bullying, toxic comments, hate speech, offensive language
True answer: non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
Answer: abusive language, (cyber-)aggression, (cyber-)bullying, toxic comments, hate speech, offensive language
True answer: Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others , Untargeted (UNT): Posts containing non-targeted profanity and swearing.

Validation ID: d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751
Question: What kinds of offensive content are explored?
Answer: abusive language, (cyber-)aggression, (cyber-)bullying, toxic comments, hate speech, offensive language
True answer: offensive (OFF) and non-offensive (NOT), targeted (TIN) and untargeted (INT) insults, targets of insults and threats as individual (IND), group (GRP), and other (OTH)

Validation ID: 55bd59076a49b19d3283af41c5e3ccb875f3eb0c
Question: What is the best performing model?
Answer: CNN
True answer: CNN 

Validation ID: 521280a87c43fcdf9f577da235e7072a23f0673e
Question: How many annotators participated?
Answer: 6
True answer: five annotators

Validation ID: 5a8cc8f80509ea77d8213ed28c5ead501c68c725
Question: What is the definition of offensive language?
Answer: Posts that do not contain offense or profanity
True answer:  Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .

Validation ID: 290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30
Question: What are the three layers of the annotation scheme?
Answer: Level A, Level B, Level C
True answer: Level A: Offensive language Detection
, Level B: Categorization of Offensive Language
, Level C: Offensive Language Target Identification


Validation ID: 1b72aa2ec3ce02131e60626639f0cf2056ec23ca
Question: How long is the dataset for each step of hierarchy?
Answer: 14,100 tweets
True answer: Level A: 14100 Tweets
Level B: 4640 Tweets
Level C: 4089 Tweets

Validation ID: c49ee6ac4dc812ff84d255886fd5aff794f53c39
Question: Do the authors report results only on English data?
Answer: Yes
True answer: Yes

Validation ID: c49ee6ac4dc812ff84d255886fd5aff794f53c39
Question: Do the authors report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: 3f856097be2246bde8244add838e83a2c793bd17
Question: In the proposed metric, how is content relevance measured?
Answer: By comparing the overlaps of the retrieved results.
True answer: The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. 

Validation ID: 3f856097be2246bde8244add838e83a2c793bd17
Question: In the proposed metric, how is content relevance measured?
Answer: By comparing the overlaps of the retrieved results.
True answer: On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval.

Validation ID: bf52c01bf82612d0c7bbf2e6a5bb2570c322936f
Question: What different correlations result when using different variants of ROUGE scores?
Answer: Pearson, Spearman and Kendall
True answer: we observe that many variants of Rouge scores do not have high correlations with human pyramid scores

Validation ID: bf52c01bf82612d0c7bbf2e6a5bb2570c322936f
Question: What different correlations result when using different variants of ROUGE scores?
Answer: Pearson, Spearman and Kendall
True answer: Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.

Validation ID: 74e866137b3452ec50fb6feaf5753c8637459e62
Question: What manual Pyramid scores are used?
Answer: semi-manual Pyramid evaluation framework
True answer:  higher tiers of the pyramid

Validation ID: 74e866137b3452ec50fb6feaf5753c8637459e62
Question: What manual Pyramid scores are used?
Answer: semi-manual Pyramid evaluation framework
True answer: following the pyramid framework, we design an annotation scheme

Validation ID: 184b0082e10ce191940c1d24785b631828a9f714
Question: What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'?
Answer: Rouge is the best metric for all summarization tasks
True answer: correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization

Validation ID: c59078efa7249acfb9043717237c96ae762c0a8c
Question: which existing strategies are compared?
Answer: CDA and REG
True answer: CDA, REG

Validation ID: c59078efa7249acfb9043717237c96ae762c0a8c
Question: which existing strategies are compared?
Answer: CDA and REG
True answer: Unanswerable

Validation ID: 73bddaaf601a4f944a3182ca0f4de85a19cdc1d2
Question: what dataset was used?
Answer: Daily Mail news articles
True answer: Daily Mail news articles released by BIBREF9 

Validation ID: 73bddaaf601a4f944a3182ca0f4de85a19cdc1d2
Question: what dataset was used?
Answer: Daily Mail news articles
True answer: Daily Mail news articles

Validation ID: d4e5e3f37679ff68914b55334e822ea18e60a6cf
Question: what kinds of male and female words are looked at?
Answer: gender pairs
True answer: gendered word pairs like he and she

Validation ID: 5f60defb546f35d25a094ff34781cddd4119e400
Question: how is mitigation of gender bias evaluated?
Answer: by measuring the difference between the occurrences of all gender-neutral words with female and male words.
True answer: Using INLINEFORM0 and INLINEFORM1

Validation ID: 5f60defb546f35d25a094ff34781cddd4119e400
Question: how is mitigation of gender bias evaluated?
Answer: by measuring the difference between the occurrences of all gender-neutral words with female and male words.
True answer: Unanswerable

Validation ID: 90d946ccc3abf494890e147dd85bd489b8f3f0e8
Question: what bias evaluation metrics are used?
Answer: Co-occurrence bias, Conditional co-occurrence bias, Causal occupation bias, Embedding bias
True answer: Unanswerable

Validation ID: 90d946ccc3abf494890e147dd85bd489b8f3f0e8
Question: what bias evaluation metrics are used?
Answer: Co-occurrence bias, Conditional co-occurrence bias, Causal occupation bias, Embedding bias
True answer: gender bias, normalized version of INLINEFORM0, ratio of occurrence of male and female words in the model generated text, Causal occupation bias conditioned on occupation, causal occupation bias conditioned on gender, INLINEFORM1

Validation ID: b962cc817a4baf6c56150f0d97097f18ad6cd9ed
Question: What kind of questions are present in the dataset?
Answer: Yes/No questions, rating questions, aspect questions, best aspect questions, worst aspect questions, aspect comparison questions, aspect-aspect comparison questions, aspect-aspect-aspect comparison questions
True answer: These 8 tasks require different competencies and a different level of understanding of the document to be well answered

Validation ID: fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f
Question: What baselines are presented?
Answer: Logistic regression, LSTM, MemN2N, Deep projective reader
True answer: Logistic regression, LSTM, End-to-end memory networks, Deep projective reader

Validation ID: fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f
Question: What baselines are presented?
Answer: Logistic regression, LSTM, MemN2N, Deep projective reader
True answer: Logistic regression, LSTM, End-to-end memory networks, Deep projective reader

Validation ID: 52f8a3e3cd5d42126b5307adc740b71510a6bdf5
Question: What tasks were evaluated?
Answer: 8 tasks
True answer: ReviewQA's test set

Validation ID: 52f8a3e3cd5d42126b5307adc740b71510a6bdf5
Question: What tasks were evaluated?
Answer: 8 tasks
True answer: Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review

Validation ID: 2236386729105f5cf42f73cc055ce3acdea2d452
Question: What language are the reviews in?
Answer: English
True answer: Unanswerable

Validation ID: 2236386729105f5cf42f73cc055ce3acdea2d452
Question: What language are the reviews in?
Answer: English
True answer: English

Validation ID: 18942ab8c365955da3fd8fc901dfb1a3b65c1be1
Question: Where are the hotel reviews from?
Answer: TripAdvisor
True answer: TripAdvisor

Validation ID: 18942ab8c365955da3fd8fc901dfb1a3b65c1be1
Question: Where are the hotel reviews from?
Answer: TripAdvisor
True answer: TripAdvisor

Validation ID: 7b4992e2d26577246a16ac0d1efc995ab4695d24
Question: What was the baseline used?
Answer: Felice2014a
True answer: error detection system by Rei2016

Validation ID: 7b4992e2d26577246a16ac0d1efc995ab4695d24
Question: What was the baseline used?
Answer: Felice2014a
True answer: error detection system by Rei2016

Validation ID: ab9b0bde6113ffef8eb1c39919d21e5913a05081
Question: What are their results on both datasets?
Answer: Making use of artificial data provided improvements for all data generation methods.
True answer: Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. 

Validation ID: 9a9d225f9ac35ed35ea02f554f6056af3b42471d
Question: What textual patterns are extracted?
Answer: (incorrect phrase, correct phrase)
True answer: (VVD shop_VV0 II, VVD shopping_VVG II)

Validation ID: 9a9d225f9ac35ed35ea02f554f6056af3b42471d
Question: What textual patterns are extracted?
Answer: (incorrect phrase, correct phrase)
True answer: patterns for generating all types of errors

Validation ID: ea56148a8356a1918bedcf0a99ae667c27792cfe
Question: Which annotated corpus did they use?
Answer: FCE
True answer:  FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) 

Validation ID: ea56148a8356a1918bedcf0a99ae667c27792cfe
Question: Which annotated corpus did they use?
Answer: FCE
True answer: FCE ,  two alternative annotations of the CoNLL 2014 Shared Task dataset

Validation ID: cd32a38e0f33b137ab590e1677e8fb073724df7f
Question: Which languages are explored in this paper?
Answer: English
True answer: English 

Validation ID: cd32a38e0f33b137ab590e1677e8fb073724df7f
Question: Which languages are explored in this paper?
Answer: English
True answer: English 

Validation ID: 2c6b50877133a499502feb79a682f4023ddab63e
Question: what language does this paper focus on?
Answer: English
True answer: English

Validation ID: 2c6b50877133a499502feb79a682f4023ddab63e
Question: what language does this paper focus on?
Answer: English
True answer: Simple English

Validation ID: f651cd144b7749e82aa1374779700812f64c8799
Question: what evaluation metrics did they use?
Answer: BLEU, FKGL, SARI
True answer: BLEU , FKGL , SARI 

Validation ID: f651cd144b7749e82aa1374779700812f64c8799
Question: what evaluation metrics did they use?
Answer: BLEU, FKGL, SARI
True answer: BLEU, FKGL, SARI, Simplicity

Validation ID: 4625cfba3083346a96e573af5464bc26c34ec943
Question: by how much did their model improve?
Answer: 6.37 BLEU
True answer: For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.
For the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.

Validation ID: 4625cfba3083346a96e573af5464bc26c34ec943
Question: by how much did their model improve?
Answer: 6.37 BLEU
True answer: 6.37 BLEU

Validation ID: 326588b1de9ba0fd049ab37c907e6e5413e14acd
Question: what state of the art methods did they compare with?
Answer: PBMT-R, Hybrid, SBMT-SARI, NMT, Dress
True answer: OpenNMT, PBMT-R, Hybrid, SBMT-SARI, Dress

Validation ID: ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb
Question: what are the sizes of both datasets?
Answer: WikiLarge has 296,402 sentence pairs, and WikiSmall has 89,042 sentence pairs.
True answer: training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing

Validation ID: ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb
Question: what are the sizes of both datasets?
Answer: WikiLarge has 296,402 sentence pairs, and WikiSmall has 89,042 sentence pairs.
True answer: WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. 

Validation ID: 55507f066073b29c1736b684c09c045064053ba9
Question: What are the distinctive characteristics of how Arabic speakers use offensive language?
Answer: Direct name calling, simile and metaphor, indirect speech, wishing evil, name alteration, societal stratification, and sexually related.
True answer: Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.

Validation ID: 55507f066073b29c1736b684c09c045064053ba9
Question: What are the distinctive characteristics of how Arabic speakers use offensive language?
Answer: Direct name calling, simile and metaphor, indirect speech, wishing evil, name alteration, societal stratification, and sexually related.
True answer: Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related

Validation ID: e838275bb0673fba0d67ac00e4307944a2c17be3
Question: How did they analyze which topics, dialects and gender are most associated with tweets?
Answer: They used the annotated tweets to ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language.
True answer: ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language

Validation ID: 8dda1ef371933811e2a25a286529c31623cca0c6
Question: How many annotators tagged each tweet?
Answer: 1
True answer: One

Validation ID: 8dda1ef371933811e2a25a286529c31623cca0c6
Question: How many annotators tagged each tweet?
Answer: 1
True answer: One experienced annotator tagged all tweets

Validation ID: b3de9357c569fb1454be8f2ac5fcecaea295b967
Question: How many tweets are in the dataset?
Answer: 10,000
True answer: 10,000 Arabic tweet dataset 

Validation ID: b3de9357c569fb1454be8f2ac5fcecaea295b967
Question: How many tweets are in the dataset?
Answer: 10,000
True answer: 10,000

Validation ID: 59e58c6fc63cf5b54b632462465bfbd85b1bf3dd
Question: In what way is the offensive dataset not biased by topic, dialect or target?
Answer: The dataset is not biased by topic, dialect or target because it does not use a seed list of offensive words.
True answer: It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets.

Validation ID: 59e58c6fc63cf5b54b632462465bfbd85b1bf3dd
Question: In what way is the offensive dataset not biased by topic, dialect or target?
Answer: The dataset is not biased by topic, dialect or target because the dataset is built using a pattern that is not associated with any specific topic or genre, and it appears in all Arabic dialects.
True answer: our methodology does not use a seed list of offensive words

Validation ID: 5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83
Question: What experiments are conducted?
Answer: We first systematically define irony generation based on style transfer. Because of the lack of irony data, we make use of twitter and build a large-scale dataset. In order to control irony accuracy, sentiment preservation and content preservation at the same time, we also design a combination of rewards for reinforcement learning and incorporate reinforcement learning with a pre-training process. Experimental results demonstrate that our model outperforms other generative models and our rewards are effective. Although our model design is effective, there are still many errors and we systematically analyze them. In the future, we are interested in exploring these directions and our work may extend to other kinds of ironies which are more difficult to model.
True answer: Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences

Validation ID: 3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f
Question: What is the combination of rewards for reinforcement learning?
Answer: irony reward and sentiment reward
True answer: irony accuracy, sentiment preservation

Validation ID: 3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f
Question: What is the combination of rewards for reinforcement learning?
Answer: irony reward and sentiment reward
True answer:  irony accuracy and sentiment preservation

Validation ID: 14b8ae5656e7d4ee02237288372d9e682b24fdb8
Question: What are the difficulties in modelling the ironic pattern?
Answer: The ironic pattern is hard to model because it is a complicated style and hard to describe with some specific style attribute words.
True answer: obscure and hard to understand,  lack of previous work and baselines on irony generation

Validation ID: 14b8ae5656e7d4ee02237288372d9e682b24fdb8
Question: What are the difficulties in modelling the ironic pattern?
Answer: The ironic pattern is hard to model because it is a complicated style and hard to describe with some specific style attribute words.
True answer: ironies are often obscure and hard to understand

Validation ID: e3a2d8886f03e78ed5e138df870f48635875727e
Question: How did the authors find ironic data on twitter?
Answer: They crawled over 2M tweets from twitter using GetOldTweets-python.
True answer: They developed a classifier to find ironic sentences in twitter data

Validation ID: e3a2d8886f03e78ed5e138df870f48635875727e
Question: How did the authors find ironic data on twitter?
Answer: They crawled over 2M tweets from twitter using GetOldTweets-python.
True answer: by crawling

Validation ID: 62f27fe08ddb67f16857fab2a8a721926ecbb6fb
Question: Who judged the irony accuracy, sentiment preservation and content preservation?
Answer: Annotators
True answer: Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).

Validation ID: 62f27fe08ddb67f16857fab2a8a721926ecbb6fb
Question: Who judged the irony accuracy, sentiment preservation and content preservation?
Answer: Annotators
True answer: four annotators who are proficient in English

Validation ID: 9ca447c8959a693a3f7bdd0a2c516f4b86f95718
Question: How were the tweets annotated?
Answer: The tweets were annotated with the target and stance information.
True answer: tweets are annotated with only Favor or Against for two targets - Galatasaray and Fenerbahçe

Validation ID: 9ca447c8959a693a3f7bdd0a2c516f4b86f95718
Question: How were the tweets annotated?
Answer: The tweets were annotated with the target and stance information.
True answer: Unanswerable

Validation ID: 05887a8466e0a2f0df4d6a5ffc5815acd7d9066a
Question: Which SVM approach resulted in the best performance?
Answer: SVM classifiers using unigram and bigram features in addition to using the existence of hashtags as another feature.
True answer: Target-1

Validation ID: c87fcc98625e82fdb494ff0f5309319620d69040
Question: What are hashtag features?
Answer: Hashtag features are the features that are used in the SVM classifiers.
True answer: hashtag features contain whether there is any hashtag in the tweet

Validation ID: c87fcc98625e82fdb494ff0f5309319620d69040
Question: What are hashtag features?
Answer: Hashtag features are the features that are used in the SVM classifiers.
True answer: Unanswerable

Validation ID: 500a8ec1c56502529d6e59ba6424331f797f31f0
Question: How many tweets did they collect?
Answer: 700
True answer: 700 

Validation ID: 500a8ec1c56502529d6e59ba6424331f797f31f0
Question: How many tweets did they collect?
Answer: 700
True answer: 700

Validation ID: ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc
Question: Which sports clubs are the targets?
Answer: Galatasaray and Fenerbahçe
True answer: Galatasaray, Fenerbahçe

Validation ID: ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc
Question: Which sports clubs are the targets?
Answer: Galatasaray and Fenerbahçe
True answer: Galatasaray , Fenerbahçe 

Validation ID: f2155dc4aeab86bf31a838c8ff388c85440fce6e
Question: Does this method help in sentiment classification task improvement?
Answer: No
True answer: Yes

Validation ID: f2155dc4aeab86bf31a838c8ff388c85440fce6e
Question: Does this method help in sentiment classification task improvement?
Answer: No
True answer: No

Validation ID: ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc
Question: For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding?
Answer: 3
True answer: performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks

Validation ID: ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc
Question: For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding?
Answer: 3
True answer: 3

Validation ID: 4d706ce5bde82caf40241f5b78338ea5ee5eb01e
Question: What are the black-box probes used?
Answer: BIBREF1
True answer: CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,
Chunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection

Validation ID: 4d706ce5bde82caf40241f5b78338ea5ee5eb01e
Question: What are the black-box probes used?
Answer: BIBREF1
True answer: Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.

Validation ID: 86bf75245358f17e35fc133e46a92439ac86d472
Question: What are improvements for these two approaches relative to ELMo-only baselines?
Answer: modest gains
True answer: only modest gains on three of the four downstream tasks

Validation ID: 86bf75245358f17e35fc133e46a92439ac86d472
Question: What are improvements for these two approaches relative to ELMo-only baselines?
Answer: modest gains
True answer:  the performance differences across all tasks are small enough 

Validation ID: 9132d56e26844dc13b3355448d0f14b95bd2178a
Question: Which syntactic features are obtained automatically on downstream task data?
Answer: Chunk labels
True answer: token-level chunk label embeddings,  chunk boundary information is passed into the task model via BIOUL encoding of the labels

Validation ID: f3c204723da53c7c8ef4dc1018ffbee545e81056
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: f3c204723da53c7c8ef4dc1018ffbee545e81056
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: 0602a974a879e6eae223cdf048410b5a0111665e
Question: What baseline approaches does this approach out-perform?
Answer: K-means, LEM, DPEMM
True answer: K-means, LEM BIBREF13, DPEMM BIBREF14

Validation ID: 0602a974a879e6eae223cdf048410b5a0111665e
Question: What baseline approaches does this approach out-perform?
Answer: K-means, LEM, DPEMM
True answer: K-means, LEM, DPEMM

Validation ID: 56b034c303983b2e276ed6518d6b080f7b8abe6a
Question: What datasets are used?
Answer: FSD, Twitter, Google
True answer: FSD BIBREF12 , Twitter, and Google datasets

Validation ID: 56b034c303983b2e276ed6518d6b080f7b8abe6a
Question: What datasets are used?
Answer: FSD, Twitter, Google
True answer: FSD dataset, Twitter dataset, Google dataset

Validation ID: 15e481e668114e4afe0c78eefb716ffe1646b494
Question: What alternative to Gibbs sampling is used?
Answer: CUDA acceleration
True answer: generator network to capture the event-related patterns

Validation ID: 3d7a982c718ea6bc7e770d8c5da564fbb9d11951
Question: How does this model overcome the assumption that all words in a document are generated from a single event?
Answer: The generator network is designed to learn the projection function between the document-event distribution and the four document-level word distributions (entity distribution, location distribution, keyword distribution and date distribution).
True answer: flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions, supervision signal provided by the discriminator will help generator to capture the event-related patterns

Validation ID: 3d7a982c718ea6bc7e770d8c5da564fbb9d11951
Question: How does this model overcome the assumption that all words in a document are generated from a single event?
Answer: The generator network is designed to learn the projection function between the document-event distribution and the four document-level word distributions (entity distribution, location distribution, keyword distribution and date distribution).
True answer: by learning a projection function between the document-event distribution and four event related word distributions 

Validation ID: 692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1
Question: How many users do they look at?
Answer: 20,000
True answer: 22,880 users

Validation ID: 692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1
Question: How many users do they look at?
Answer: 20,000
True answer: 20,000

Validation ID: 935d6a6187e6a0c9c0da8e53a42697f853f5c248
Question: What do they mean by a person's industry?
Answer: A person's industry is the aggregate of enterprises in a particular field.
True answer: the aggregate of enterprises in a particular field

Validation ID: 935d6a6187e6a0c9c0da8e53a42697f853f5c248
Question: What do they mean by a person's industry?
Answer: A person's industry is the aggregate of enterprises in a particular field.
True answer: the aggregate of enterprises in a particular field

Validation ID: 3b77b4defc8a139992bd0b07b5cf718382cb1a5f
Question: What model did they use for their system?
Answer: Naive Bayes
True answer: AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier

Validation ID: 01a41c0a4a7365cd37d28690735114f2ff5229f2
Question: What social media platform did they look at?
Answer: Sina Weibo
True answer:  http://www.blogger.com

Validation ID: 01a41c0a4a7365cd37d28690735114f2ff5229f2
Question: What social media platform did they look at?
Answer: Sina Weibo
True answer: http://www.blogger.com

Validation ID: cd2878c5a52542ddf080b20bec005d9a74f2d916
Question: What are the industry classes defined in this paper?
Answer: Technology, Environment, Real Estate, Tourism, Religion, Media, Finance, Education, Healthcare, Government, Law, and Automotive
True answer: technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive

Validation ID: cd2878c5a52542ddf080b20bec005d9a74f2d916
Question: What are the industry classes defined in this paper?
Answer: Technology, Environment, Real Estate, Tourism, Religion, Media, Finance, Education, Healthcare, Government, Law, and Automotive
True answer: Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.

Validation ID: fd2c6c26fd0ab3c10aae4f2550c5391576a77491
Question: Do they report results only on English data?
Answer: Yes
True answer: Yes

Validation ID: fd2c6c26fd0ab3c10aae4f2550c5391576a77491
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: 6b6d498546f856ac20958f666fc3fd55811347e2
Question: What are the hyperparameters of the bi-GRU?
Answer: batch size 250, epochs 20, binary cross-entropy as the objective function and Adam optimizer
True answer: They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%.

Validation ID: de3b1145cb4111ea2d4e113f816b537d052d9814
Question: What baseline is used?
Answer: BOW model and maximum entropy classifier
True answer:  Wang et al. BIBREF21, paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets

Validation ID: de3b1145cb4111ea2d4e113f816b537d052d9814
Question: What baseline is used?
Answer: BOW model and maximum entropy classifier
True answer: Wang et al. , maximum entropy classifier with bag of words model

Validation ID: 132f752169adf6dc5ade3e4ca773c11044985da4
Question: What data is used in experiments?
Answer: CrowdFlower emotional tweets dataset
True answer: Wang et al., CrowdFlower dataset 

Validation ID: 132f752169adf6dc5ade3e4ca773c11044985da4
Question: What data is used in experiments?
Answer: CrowdFlower emotional tweets dataset
True answer:  tweet dataset created by Wang et al. , CrowdFlower dataset

Validation ID: 1d9aeeaa6efa1367c22be0718f5a5635a73844bd
Question: What meaningful information does the GRU model capture, which traditional ML models do not?
Answer: The sequential nature of the data
True answer:  the context and sequential nature of the text

Validation ID: 1d9aeeaa6efa1367c22be0718f5a5635a73844bd
Question: What meaningful information does the GRU model capture, which traditional ML models do not?
Answer: The sequential nature of the data
True answer: information about the context and sequential nature of the text

Validation ID: 012b8a89aea27485797373adbcda32f16f9d7b54
Question: What is the approach of previous work?
Answer: Hierarchical stacked classifiers
True answer: 'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks

Validation ID: 012b8a89aea27485797373adbcda32f16f9d7b54
Question: What is the approach of previous work?
Answer: Hierarchical stacked classifiers
True answer: BIBREF11 that uses a character level n-gram language model, 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15, BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features, The winning approach for DSL 2015 used an ensemble naive Bayes classifier, The fasttext classifier BIBREF17, hierarchical stacked classifiers (including lexicons), bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24

Validation ID: c598028815066089cc1e131b96d6966d2610467a
Question: Is the lexicon the same for all languages?
Answer: No
True answer: Yes

Validation ID: c598028815066089cc1e131b96d6966d2610467a
Question: Is the lexicon the same for all languages?
Answer: No
True answer: Yes

Validation ID: ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed
Question: How do they obtain the lexicon?
Answer: They use a naive Bayesian classifier with character (2, 4 & 6)-grams, word unigram and word bigram features with a hierarchical lexicon based classifier.
True answer: Unanswerable

Validation ID: ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed
Question: How do they obtain the lexicon?
Answer: They use a naive Bayesian classifier with character (2, 4 & 6)-grams, word unigram and word bigram features with a hierarchical lexicon based classifier.
True answer: built over all the data and therefore includes the vocabulary from both the training and testing sets

Validation ID: 0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50
Question: What evaluation metric is used?
Answer: Accuracy
True answer: average classification accuracy

Validation ID: 0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50
Question: What evaluation metric is used?
Answer: Accuracy
True answer: average classification accuracy, execution performance

Validation ID: 92dfacbbfa732ecea006e251be415a6f89fb4ec6
Question: Which languages are similar to each other?
Answer: Nguni languages
True answer: Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)

Validation ID: 92dfacbbfa732ecea006e251be415a6f89fb4ec6
Question: Which languages are similar to each other?
Answer: Nguni languages
True answer: The Nguni languages are similar to each other, The same is true of the Sotho languages

Validation ID: c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9
Question: Which datasets are employed for South African languages LID?
Answer: NCHLT text corpora
True answer: DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora

Validation ID: 307e8ab37b67202fe22aedd9a98d9d06aaa169c5
Question: Does the paper report the performance of a baseline model on South African languages LID?
Answer: Yes
True answer: Yes

Validation ID: 307e8ab37b67202fe22aedd9a98d9d06aaa169c5
Question: Does the paper report the performance of a baseline model on South African languages LID?
Answer: Yes
True answer: Yes

Validation ID: 6415f38a06c2f99e8627e8ba6251aa4b364ade2d
Question: What are the languages represented in the DSL datasets??
Answer: 14 languages over 6 language groups
True answer: Unanswerable

Validation ID: e5c8e9e54e77960c8c26e8e238168a603fcdfcc6
Question: Does the algorithm improve on the state-of-the-art methods?
Answer: Yes
True answer: Yes

Validation ID: e5c8e9e54e77960c8c26e8e238168a603fcdfcc6
Question: Does the algorithm improve on the state-of-the-art methods?
Answer: Yes
True answer: From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.

Validation ID: 50be4a737dc0951b35d139f51075011095d77f2a
Question: What background knowledge do they leverage?
Answer: Prior knowledge
True answer: labeled features

Validation ID: 50be4a737dc0951b35d139f51075011095d77f2a
Question: What background knowledge do they leverage?
Answer: Prior knowledge
True answer: labelled features, which are words whose presence strongly indicates a specific class or topic

Validation ID: 6becff2967fe7c5256fe0b00231765be5b9db9f1
Question: What are the three regularization terms?
Answer: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.
True answer: a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution

Validation ID: 6becff2967fe7c5256fe0b00231765be5b9db9f1
Question: What are the three regularization terms?
Answer: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.
True answer: a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution

Validation ID: 76121e359dfe3f16c2a352bd35f28005f2a40da3
Question: What NLP tasks do they consider?
Answer: text categorization, sentiment classification
True answer: text classification for themes including sentiment, web-page, science, medical and healthcare

Validation ID: 02428a8fec9788f6dc3a86b5d5f3aa679935678d
Question: How do they define robustness of a model?
Answer: The model is robust if it is insensitive to the prior knowledge.
True answer: ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced

Validation ID: 02428a8fec9788f6dc3a86b5d5f3aa679935678d
Question: How do they define robustness of a model?
Answer: The model is robust if it can perform well even when the prior knowledge is biased.
True answer: Low sensitivity to bias in prior knowledge

Validation ID: 7793805982354947ea9fc742411bec314a6998f6
Question: Are the annotations automatic or manually created?
Answer: automatic
True answer: Automatic

Validation ID: 7793805982354947ea9fc742411bec314a6998f6
Question: Are the annotations automatic or manually created?
Answer: automatic
True answer: We performed the annotation with freely available tools for the Portuguese language.

Validation ID: 007b13f05d234d37966d1aa7d85b5fd78564ff45
Question: Do the errors of the model reflect linguistic similarity between different L1s?
Answer: Yes
True answer: Unanswerable

Validation ID: 2ceced87af4c8fdebf2dc959aa700a5c95bd518f
Question: Is the dataset balanced between speakers of different L1s?
Answer: No
True answer: No

Validation ID: 2ceced87af4c8fdebf2dc959aa700a5c95bd518f
Question: Is the dataset balanced between speakers of different L1s?
Answer: No
True answer: No

Validation ID: 72ed5fed07ace5e3ffe9de6c313625705bc8f0c7
Question: How long are the essays on average?
Answer: 150 to 250 tokens
True answer: 204 tokens

Validation ID: 72ed5fed07ace5e3ffe9de6c313625705bc8f0c7
Question: How long are the essays on average?
Answer: 150 to 250 tokens
True answer: Most texts, however, range roughly from 150 to 250 tokens.

Validation ID: 2e37e681942e28b5b05639baaff4cd5129adb5fb
Question: How large are the textual descriptions of entities?
Answer: The text descriptions of entities are variable-length sentences.
True answer: Unanswerable

Validation ID: 2e37e681942e28b5b05639baaff4cd5129adb5fb
Question: How large are the textual descriptions of entities?
Answer: The text descriptions of entities are variable-length sentences.
True answer: Unanswerable

Validation ID: b49598b05358117ab1471b8ebd0b042d2f04b2a4
Question: What neural models are used to encode the text?
Answer: NBOW, LSTM and attentive LSTM
True answer: NBOW, LSTM, attentive LSTM

Validation ID: b49598b05358117ab1471b8ebd0b042d2f04b2a4
Question: What neural models are used to encode the text?
Answer: NBOW, LSTM and attentive LSTM
True answer: neural bag-of-words (NBOW) model, bidirectional long short-term memory network (LSTM), attention-based encoder

Validation ID: 932b39fd6c47c6a880621a62e6a978491d881d60
Question: What baselines are used for comparison?
Answer: TransE
True answer: TransE

Validation ID: 932b39fd6c47c6a880621a62e6a978491d881d60
Question: What baselines are used for comparison?
Answer: TransE
True answer: TransE

Validation ID: b36f867fcda5ad62c46d23513369337352aa01d2
Question: What datasets are used to evaluate this paper?
Answer: WN18 and FB15K
True answer: WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2

Validation ID: c6a0b9b5dabcefda0233320dd1548518a0ae758e
Question: Which approach out of two proposed in the paper performed better in experiments?
Answer: CJFA (contextual joint factor analysis) encoder
True answer: CJFA encoder 

Validation ID: c6a0b9b5dabcefda0233320dd1548518a0ae758e
Question: Which approach out of two proposed in the paper performed better in experiments?
Answer: CJFA (contextual joint factor analysis) encoder
True answer: CJFA encoder

Validation ID: 1e185a3b8cac1da939427b55bf1ba7e768c5dae4
Question: What classification baselines are used for comparison?
Answer: VAE baseline
True answer: VAE

Validation ID: 1e185a3b8cac1da939427b55bf1ba7e768c5dae4
Question: What classification baselines are used for comparison?
Answer: VAE baseline
True answer: VAE based phone classification

Validation ID: 26e2d4d0e482e6963a76760323b8e1c26b6eee91
Question: What TIMIT datasets are used for testing?
Answer: Test set
True answer: Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H

Validation ID: 26e2d4d0e482e6963a76760323b8e1c26b6eee91
Question: What TIMIT datasets are used for testing?
Answer: Test set
True answer:  this paper makes use of the official training and test sets, covering in total 630 speakers with 8 utterances each

Validation ID: b80a3fbeb49a8968e149955bdcf199556478eeff
Question: How does this approach compares to the state-of-the-art results on these tasks?
Answer: The proposed unsupervised approaches obtain embeddings and can be improved with unlabelled out-of-domain data, the classification tasks benefits even though the labelled data remains the same.
True answer: Unanswerable

Validation ID: badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9
Question: What state-of-the-art results are achieved?
Answer: Empirical results on the benchmark datasets prove the effectiveness of the proposed approach that the two tasks of pun detection and location can be addressed by a single model from a sequence labeling perspective.
True answer: F1 score of 92.19 on homographic pun detection, 80.19 on homographic pun location, 89.76 on heterographic pun detection.

Validation ID: badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9
Question: What state-of-the-art results are achieved?
Answer: Empirical results on the benchmark datasets prove the effectiveness of the proposed approach that the two tasks of pun detection and location can be addressed by a single model from a sequence labeling perspective.
True answer: for the homographic dataset F1 score of 92.19 and 80.19 on detection and location and for the heterographic dataset F1 score of 89.76 on detection

Validation ID: 67b66fe67a3cb2ce043070513664203e564bdcbd
Question: What baselines do they compare with?
Answer: CRF
True answer: They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.

Validation ID: f56d07f73b31a9c72ea737b40103d7004ef6a079
Question: What datasets are used in evaluation?
Answer: homographic dataset and heterographic dataset
True answer: The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.

Validation ID: f56d07f73b31a9c72ea737b40103d7004ef6a079
Question: What datasets are used in evaluation?
Answer: homographic dataset and heterographic dataset
True answer: A homographic and heterographic benchmark datasets by BIBREF9.

Validation ID: 38e4aaeabf06a63a067b272f8950116733a7895c
Question: What is the tagging scheme employed?
Answer: INLINEFORM0
True answer: A new tagging scheme that tags the words before and after the pun as well as the pun words.

Validation ID: 38e4aaeabf06a63a067b272f8950116733a7895c
Question: What is the tagging scheme employed?
Answer: INLINEFORM0
True answer: a new tagging scheme consisting of three tags, namely { INLINEFORM0 }

Validation ID: 1d197cbcac7b3f4015416f0152a6692e881ada6c
Question: How they extract "structured answer-relevant relation"?
Answer: They use OpenIE to extract relations from sentences.
True answer: Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation.

Validation ID: 1d197cbcac7b3f4015416f0152a6692e881ada6c
Question: How they extract "structured answer-relevant relation"?
Answer: They use OpenIE to extract relations from sentences.
True answer: off-the-shelf toolbox of OpenIE

Validation ID: 92294820ac0d9421f086139e816354970f066d8a
Question: How big are significant improvements?
Answer: significant and consistent
True answer: Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1

Validation ID: 477d9d3376af4d938bb01280fe48d9ae7c9cf7f7
Question: What metrics do they use?
Answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19.
True answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19

Validation ID: 477d9d3376af4d938bb01280fe48d9ae7c9cf7f7
Question: What metrics do they use?
Answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19.
True answer: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET), ROUGE-L (R-L)

Validation ID: f225a9f923e4cdd836dd8fe097848da06ec3e0cc
Question: On what datasets are experiments performed?
Answer: SQuAD
True answer: SQuAD

Validation ID: f225a9f923e4cdd836dd8fe097848da06ec3e0cc
Question: On what datasets are experiments performed?
Answer: SQuAD
True answer: SQuAD

Validation ID: ff338921e34c15baf1eae0074938bf79ee65fdd2
Question: What was the baseline model?
Answer: The baseline model was a simple rule-based system that answered always YES to Yes/No questions and returned the first sentence of the snippet as the answer to factoid questions.
True answer: by answering always YES (in batch 2 and 3) 

Validation ID: e807d347742b2799bc347c0eff19b4c270449fee
Question: What dataset did they use?
Answer: BioASQ
True answer: BioASQ  dataset

Validation ID: e807d347742b2799bc347c0eff19b4c270449fee
Question: What dataset did they use?
Answer: BioASQ
True answer: A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers.

Validation ID: 31b92c03d5b9be96abcc1d588d10651703aff716
Question: What was their highest recall score?
Answer: 0.7033
True answer: 0.7033

Validation ID: 31b92c03d5b9be96abcc1d588d10651703aff716
Question: What was their highest recall score?
Answer: 0.7033
True answer: 0.7033

Validation ID: 9ec1f88ceec84a10dc070ba70e90a792fba8ce71
Question: What was their highest MRR score?
Answer: 0.6103
True answer: 0.5115

Validation ID: 9ec1f88ceec84a10dc070ba70e90a792fba8ce71
Question: What was their highest MRR score?
Answer: 0.6103
True answer: 0.6103

Validation ID: 384bf1f55c34b36cb03f916f50bbefade6c86a75
Question: Does their model suffer exhibit performance drops when incorporating word importance?
Answer: Yes, they do.
True answer: No

Validation ID: aef607d2ac46024be17b1ddd0ed3f13378c563a6
Question: How do they measure which words are under-translated by NMT models?
Answer: They use the gradient-based method to estimate the word importance, and then perturb the input sentence by masking the most important words.
True answer: They measured the under-translated words with low word importance score as calculated by Attribution.
method

Validation ID: aef607d2ac46024be17b1ddd0ed3f13378c563a6
Question: How do they measure which words are under-translated by NMT models?
Answer: They use the gradient-based method to estimate the word importance, and then perturb the input sentence by masking the most important words.
True answer: we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair

Validation ID: 93beae291b455e5d3ecea6ac73b83632a3ae7ec7
Question: How do their models decide how much improtance to give to the output words?
Answer: The model decides how much importance to give to the output words by using the gradient-based method.
True answer: Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. 

Validation ID: 93beae291b455e5d3ecea6ac73b83632a3ae7ec7
Question: How do their models decide how much improtance to give to the output words?
Answer: The model decides how much importance to give to the output words by using the gradient-based method.
True answer: They compute the gradient of the output at each time step with respect to the input words to decide the importance.

Validation ID: 6c91d44d5334a4ac80100eead4e105d34e99a284
Question: Which model architectures do they test their word importance approach on?
Answer: Transformer and RNN-Search
True answer:  Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0

Validation ID: 6c91d44d5334a4ac80100eead4e105d34e99a284
Question: Which model architectures do they test their word importance approach on?
Answer: Transformer and RNN-Search
True answer: Transformer, RNN-Search model

Validation ID: a69a59b6c0ab27bcee1a780d6867df21e30aec08
Question: Do they compare human-level performance to model performance for their dataset?
Answer: No
True answer: No

Validation ID: a69a59b6c0ab27bcee1a780d6867df21e30aec08
Question: Do they compare human-level performance to model performance for their dataset?
Answer: No
True answer: No

Validation ID: b3d01ac226ee979e188a4141877a6d2a5482de98
Question: What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models?
Answer: The weaknesses found by non-expert annotators of current state-of-the-art NLI models are:
True answer: state-of-the-art models learn to exploit spurious statistical patterns in datasets, human annotators—be they seasoned NLP researchers or non-experts—might easily be able to construct examples that expose model brittleness

Validation ID: af5730d82535464cedfa707a03415ac2e7a21295
Question: What data sources do they use for creating their dataset?
Answer: Wikipedia, News, Fiction, Formal spoken text, and causal or procedural text
True answer: Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), RTE5

Validation ID: af5730d82535464cedfa707a03415ac2e7a21295
Question: What data sources do they use for creating their dataset?
Answer: Wikipedia, News, Fiction, Formal spoken text, and causal or procedural text
True answer: Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, formal spoken text (excerpted from court and presidential debate transcripts in the Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), causal or procedural text, which describes sequences of events or actions, extracted from WikiHow, annotations using the longer contexts present in the GLUE RTE training data, which came from the RTE5 dataset

Validation ID: ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2
Question: Do they use active learning to create their dataset?
Answer: Yes
True answer: Yes

Validation ID: ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2
Question: Do they use active learning to create their dataset?
Answer: Yes
True answer: No

Validation ID: b249b60a8c94d0e40d65f1ffdfcac527dab57516
Question: Do the hashtag and SemEval datasets contain only English data?
Answer: Yes
True answer: Yes

Validation ID: b249b60a8c94d0e40d65f1ffdfcac527dab57516
Question: Do the hashtag and SemEval datasets contain only English data?
Answer: Yes
True answer: Yes

Validation ID: 0f567251a6566f65170a1329eeeb5105932036b2
Question: What current state of the art method was used for comparison?
Answer: Maximum entropy classifier
True answer: current state-of-the-art approach BIBREF14 , BIBREF15

Validation ID: 0f567251a6566f65170a1329eeeb5105932036b2
Question: What current state of the art method was used for comparison?
Answer: Maximum entropy classifier
True answer:  BIBREF14, BIBREF15 

Validation ID: 4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f
Question: What set of approaches to hashtag segmentation are proposed?
Answer: (a) gazeteer and rule based, (b) word boundary detection, and (c) ranking with language model and other features
True answer: Adaptive Multi-task Learning
, Margin Ranking (MR) Loss
, Pairwise Neural Ranking Model


Validation ID: 60ce4868af45753c9e124e64e518c32376f12694
Question: How is the dataset of hashtags sourced?
Answer: The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.
True answer: 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset

Validation ID: 60ce4868af45753c9e124e64e518c32376f12694
Question: How is the dataset of hashtags sourced?
Answer: The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.
True answer: Stanford Sentiment Analysis Dataset BIBREF36

Validation ID: 1b1a30e9e68a9ae76af467e60cefb180d135e285
Question: How big is their created dataset?
Answer: 1,200 turns
True answer: 353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers), we build templates and expression pools using linguistic analysis

Validation ID: 2c85865a65acd429508f50b5e4db9674813d67f2
Question: Which data do they use as a starting point for the dialogue dataset?
Answer: Seed data
True answer: A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital

Validation ID: 2c85865a65acd429508f50b5e4db9674813d67f2
Question: Which data do they use as a starting point for the dialogue dataset?
Answer: Seed data
True answer: recordings of nurse-initiated telephone conversations for congestive heart failure patients

Validation ID: 73a7acf33b26f5e9475ee975ba00d14fd06f170f
Question: What labels do they create on their dataset?
Answer: symptom, attribute, entity
True answer: (1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer

Validation ID: 73a7acf33b26f5e9475ee975ba00d14fd06f170f
Question: What labels do they create on their dataset?
Answer: symptom, attribute, entity
True answer: the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms

Validation ID: dd53baf26dad3d74872f2d8956c9119a27269bd5
Question: How do they select instances to their hold-out test set?
Answer: They randomly sample 1,200 turns from the seed dataset and manually categorize them to different types.
True answer: 1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues

Validation ID: dd53baf26dad3d74872f2d8956c9119a27269bd5
Question: How do they select instances to their hold-out test set?
Answer: They randomly sample 1,200 turns from the seed dataset and manually categorize them to different types.
True answer: held out from the simulated data

Validation ID: 218bc82796eb8d91611996979a4a42500131a936
Question: Which models/frameworks do they compare to?
Answer: MLP
True answer: MLP

Validation ID: 218bc82796eb8d91611996979a4a42500131a936
Question: Which models/frameworks do they compare to?
Answer: MLP
True answer: Eusboost, MWMOTE

Validation ID: b21bc09193699dc9cfad523f3d5542b0b2ff1b8e
Question: Which classification algorithm do they use for s2sL?
Answer: MLP
True answer: MLP

Validation ID: b21bc09193699dc9cfad523f3d5542b0b2ff1b8e
Question: Which classification algorithm do they use for s2sL?
Answer: MLP
True answer: MLP

Validation ID: 352bc6de5c5068c6c19062bad1b8f644919b1145
Question: Up to how many samples do they experiment with?
Answer: 4
True answer: 535

Validation ID: 352bc6de5c5068c6c19062bad1b8f644919b1145
Question: Up to how many samples do they experiment with?
Answer: 4
True answer: we considered 4 different proportions i.e., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 of the training data to train the classifier

Validation ID: d667731ea20605580c398a1224a0094d1155ebbb
Question: Do they use pretrained models?
Answer: No
True answer: No

Validation ID: 8bb0011ad1d63996d5650770f3be18abdd9f7fc6
Question: Do they report results only on English datasets?
Answer: Yes.
True answer: Unanswerable

Validation ID: 8bb0011ad1d63996d5650770f3be18abdd9f7fc6
Question: Do they report results only on English datasets?
Answer: Yes.
True answer: Yes

Validation ID: b0dbe75047310fec4d4ce787be5c32935fc4e37b
Question: How do the authors examine whether a model is robust to noise or not?
Answer: They evaluate the performance of the model on the adversarial sets.
True answer: By evaluating their model on adversarial sets containing misleading sentences

Validation ID: b0dbe75047310fec4d4ce787be5c32935fc4e37b
Question: How do the authors examine whether a model is robust to noise or not?
Answer: They evaluate the performance of the model on the adversarial sets.
True answer: we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise

Validation ID: d64383e39357bd4177b49c02eb48e12ba7ffd4fb
Question: What type of model is KAR?
Answer: End-to-end MRC model
True answer: Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer

Validation ID: 52f9cd05d8312ae3c7a43689804bac63f7cac34b
Question: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?
Answer: Yes.
True answer: Yes

Validation ID: 52f9cd05d8312ae3c7a43689804bac63f7cac34b
Question: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?
Answer: Yes.
True answer: Yes

Validation ID: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596
Question: What type of system does the baseline classification use?
Answer: Support vector machine
True answer: support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19

Validation ID: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596
Question: What type of system does the baseline classification use?
Answer: Support vector machine
True answer: Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers.

Validation ID: 955cbea7e5ead36fb89cd6229a97ccb3febcf8bc
Question: What experiments were carried out on the corpus?
Answer: 10-fold cross validation on 3543 code-mixed tweets was carried out by dividing the corpus into 10 equal parts with nine parts as training corpus and rest one for testing. Mean accuracy is calculated by taking the average of the accuracy obtained in each iteration of the testing process.
True answer: task of humor identification in social media texts is analyzed as a classification problem

Validation ID: 04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39
Question: How many annotators tagged each text?
Answer: 3
True answer: three 

Validation ID: 04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39
Question: How many annotators tagged each text?
Answer: 3
True answer: three annotators

Validation ID: 15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8
Question: Where did the texts in the corpus come from?
Answer: Twitter
True answer: tweets from the past two years from domains like `sports', `politics', `entertainment'

Validation ID: 15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8
Question: Where did the texts in the corpus come from?
Answer: Twitter
True answer: twitter

Validation ID: ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797
Question: What is the previous state-of-the-art in summarization?
Answer: BIBREF26
True answer: BIBREF26 

Validation ID: ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797
Question: What is the previous state-of-the-art in summarization?
Answer: BIBREF26
True answer: BIBREF26

Validation ID: 6ca938324dc7e1742a840d0a54dc13cc207394a1
Question: What dataset do they use?
Answer: newscrawl
True answer: German newscrawl distributed by WMT'18 , English newscrawl data, WMT'18 English-German (en-de) news translation task , WMT'18 English-Turkish (en-tr) news task

Validation ID: 6ca938324dc7e1742a840d0a54dc13cc207394a1
Question: What dataset do they use?
Answer: newscrawl
True answer: German newscrawl, English newscrawl, WMT'18 English-German (en-de) news, WMT'18 English-Turkish (en-tr) news task, WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task

Validation ID: 4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802
Question: What other models do they compare to?
Answer: BIBREF26
True answer: BIBREF11 , BIBREF26 

Validation ID: 4d47bef19afd70c10bbceafd1846516546641a2f
Question: What language model architectures are used?
Answer: bi-directional language model and uni-directional model
True answer: uni-directional model to augment the decoder

Validation ID: 4d47bef19afd70c10bbceafd1846516546641a2f
Question: What language model architectures are used?
Answer: bi-directional language model and uni-directional model
True answer: bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder

Validation ID: 506d21501d54a12d0c9fd3dbbf19067802439a04
Question: What are the user-defined keywords?
Answer: subway, manhattan
True answer: Words that a user wants them to appear in the generated output.

Validation ID: 506d21501d54a12d0c9fd3dbbf19067802439a04
Question: What are the user-defined keywords?
Answer: subway, manhattan
True answer: terms common to hosts' descriptions of popular Airbnb properties, like 'subway', 'manhattan', or 'parking'

Validation ID: 701571680724c05ca70c11bc267fb1160ea1460a
Question: Does the method achieve sota performance on this dataset?
Answer: No
True answer: No

Validation ID: 701571680724c05ca70c11bc267fb1160ea1460a
Question: Does the method achieve sota performance on this dataset?
Answer: No
True answer: Unanswerable

Validation ID: 600b097475b30480407ce1de81c28c54a0b3b2f8
Question: What are the baselines used in the paper?
Answer: The baseline is the standard binary cross-entropy loss.
True answer: GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling

Validation ID: 600b097475b30480407ce1de81c28c54a0b3b2f8
Question: What are the baselines used in the paper?
Answer: The baseline is the standard GAN model.
True answer: Unanswerable

Validation ID: ee7e9a948ee6888aa5830b1a3d0d148ff656d864
Question: What is the size of the Airbnb?
Answer: 160-acre
True answer: roughly 40,000 Manhattan listings

Validation ID: 5fda8539a97828e188ba26aad5cda1b9dd642bc8
Question: How better is performance compared to previous state-of-the-art models?
Answer: Our model gives new higher or comparable segmentation performance against previous state-of-the-art models.
True answer: F1 score of 97.5 on MSR and 95.7 on AS

Validation ID: 5fda8539a97828e188ba26aad5cda1b9dd642bc8
Question: How better is performance compared to previous state-of-the-art models?
Answer: Our model gives new higher or comparable segmentation performance against previous state-of-the-art models.
True answer: MSR: 97.7 compared to 97.5 of baseline
AS: 95.7 compared to 95.6 of baseline

Validation ID: 709feae853ec0362d4e883db8af41620da0677fe
Question: How does Gaussian-masked directional multi-head attention works?
Answer: Gaussian-masked directional multi-head attention is a variant of multi-head attention. It uses a Gaussian mask to adjust the weight between characters and their adjacent characters to a larger value which stands for the effect of adjacent characters.
True answer: pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters

Validation ID: 709feae853ec0362d4e883db8af41620da0677fe
Question: How does Gaussian-masked directional multi-head attention works?
Answer: Gaussian-masked directional multi-head attention is a variant of multi-head attention. It uses a Gaussian mask to adjust the weight between characters and their adjacent characters to a larger value which stands for the effect of adjacent characters.
True answer: Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters

Validation ID: 186b7978ee33b563a37139adff1da7d51a60f581
Question: What is meant by closed test setting?
Answer: Closed test limits all the data for learning should not be beyond the given training set.
True answer: closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation

Validation ID: 186b7978ee33b563a37139adff1da7d51a60f581
Question: What is meant by closed test setting?
Answer: Closed test limits all the data for learning should not be beyond the given training set.
True answer: closed test limits all the data for learning should not be beyond the given training set

Validation ID: fabcd71644bb63559d34b38d78f6ef87c256d475
Question: What are strong baselines model is compared to?
Answer: BIBREF4, BIBREF6, BIBREF7, BIBREF9, BIBREF11, BIBREF17, BIBREF18, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34, BIBREF35, BIBREF36, BIBREF37, BIBREF38, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF43, BIBREF44, BIBREF45, BIBREF46, BIBREF47, BIBREF48, BIBREF49, BIBREF50, BIBREF51, BIBREF52, BIBREF53, BIBREF54, BIBREF55, BIBREF56, BIBREF57, BIBREF58, BIBREF59, BIBREF60, BIBREF61, BIBREF62, BIBREF63, B
True answer: Baseline models are:
- Chen et al., 2015a
- Chen et al., 2015b
- Liu et al., 2016
- Cai and Zhao, 2016
- Cai et al., 2017
- Zhou et al., 2017
- Ma et al., 2018
- Wang et al., 2019

Validation ID: da9c0637623885afaf023a319beee87898948fe9
Question: Does the dataset feature only English language data?
Answer: Yes
True answer: Yes

Validation ID: da9c0637623885afaf023a319beee87898948fe9
Question: Does the dataset feature only English language data?
Answer: Yes
True answer: Unanswerable

Validation ID: 8a1c0ef69b6022a0642ca131a8eacb5c97016640
Question: What additional features and context are proposed?
Answer: Context tweets
True answer: using tweets that one has replied or quoted to as contextual information

Validation ID: 8a1c0ef69b6022a0642ca131a8eacb5c97016640
Question: What additional features and context are proposed?
Answer: Context tweets
True answer: text sequences of context tweets

Validation ID: 48088a842f7a433d3290eb45eb0d4c6ab1d8f13c
Question: What learning models are used on the dataset?
Answer: CNN, RNN, LTC
True answer: Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)

Validation ID: 48088a842f7a433d3290eb45eb0d4c6ab1d8f13c
Question: What learning models are used on the dataset?
Answer: CNN, RNN, LTC
True answer: Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN

Validation ID: 4907096cf16d506937e592c50ae63b642da49052
Question: What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give?
Answer: The authors give two examples of the difficulties presented by the context-dependent nature of online aggression. The first example is a tweet that uses vulgar language, but the intention of the user can be better understood with its context tweet. The second example is a tweet that is abusive, but the target of the malice can be better understood with its context tweet.
True answer: detecting abusive language extremely laborious, it is difficult to build a large and reliable dataset

Validation ID: 8748e8f64af57560d124c7b518b853bf2711c13e
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: 8748e8f64af57560d124c7b518b853bf2711c13e
Question: Do they report results only on English data?
Answer: Yes
True answer: Yes

Validation ID: 893ec40b678a72760b6802f6abf73b8f487ae639
Question: What evidence do the authors present that the model can capture some biases in data annotation and collection?
Answer: The evaluation results indicate that our model outperforms previous works by profiting the syntactical and contextual information embedded in different transformer encoder layers of the BERT model using a CNN-based fine-tuning strategy. Furthermore, examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data.
True answer: The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate

Validation ID: c81f215d457bdb913a5bade2b4283f19c4ee826c
Question: Which publicly available datasets are used?
Answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9
True answer: Waseem-dataset, Davidson-dataset,

Validation ID: c81f215d457bdb913a5bade2b4283f19c4ee826c
Question: Which publicly available datasets are used?
Answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9
True answer: Waseem and Hovey BIBREF5, Davidson et al. BIBREF9

Validation ID: e101e38efaa4b931f7dd75757caacdc945bb32b4
Question: What baseline is used?
Answer: BERT
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10

Validation ID: e101e38efaa4b931f7dd75757caacdc945bb32b4
Question: What baseline is used?
Answer: BERT
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10

Validation ID: afb77b11da41cd0edcaa496d3f634d18e48d7168
Question: What new fine-tuning methods are presented?
Answer: Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer
True answer: BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer

Validation ID: afb77b11da41cd0edcaa496d3f634d18e48d7168
Question: What new fine-tuning methods are presented?
Answer: Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer
True answer: BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer

Validation ID: 41b2355766a4260f41b477419d44c3fd37f3547d
Question: What are the existing biases?
Answer: Bias in the process of collecting or annotating datasets
True answer: systematic and substantial racial biases, biases from data collection, rules of annotation

Validation ID: 41b2355766a4260f41b477419d44c3fd37f3547d
Question: What are the existing biases?
Answer: Bias in the process of collecting or annotating datasets
True answer: sampling tweets from specific keywords create systematic and substancial racial biases in datasets

Validation ID: 96a4091f681872e6d98d0efee777d9e820cb8dae
Question: What biases does their model capture?
Answer: Bias in the process of collecting or annotating datasets
True answer: Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters

Validation ID: 81a35b9572c9d574a30cc2164f47750716157fc8
Question: What existing approaches do they compare to?
Answer: They compare to the following approaches:
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10

Validation ID: 81a35b9572c9d574a30cc2164f47750716157fc8
Question: What existing approaches do they compare to?
Answer: They compare to the following approaches:
True answer: Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10

Validation ID: f4496316ddd35ee2f0ccc6475d73a66abf87b611
Question: What is the benchmark dataset?
Answer: 20 Newsgroups
True answer: a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data

Validation ID: f4496316ddd35ee2f0ccc6475d73a66abf87b611
Question: What is the benchmark dataset?
Answer: 20 Newsgroups
True answer: dataset created by ceccarelli2013learning from the CoNLL 2003 data

Validation ID: e8a32460fba149003566969f92ab5dd94a8754a4
Question: What are the two neural embedding models?
Answer: Concept Raw Context model (CRC) and Concept-Concept Context model (3C)
True answer: Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only)

Validation ID: e8a32460fba149003566969f92ab5dd94a8754a4
Question: What are the two neural embedding models?
Answer: Concept Raw Context model (CRC) and Concept-Concept Context model (3C)
True answer: Concept Raw Context model, Concept-Concept Context model

Validation ID: 2a6003a74d051d0ebbe62e8883533a5f5e55078b
Question: which neural embedding model works better?
Answer: Concept Raw Context model (CRC)
True answer: the CRX model

Validation ID: 2a6003a74d051d0ebbe62e8883533a5f5e55078b
Question: which neural embedding model works better?
Answer: Concept Raw Context model (CRC)
True answer: 3C model

Validation ID: 1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b
Question: What is the degree of dimension reduction of the efficient aggregation method?
Answer: It scales linearly with the # of the BOC dimensions.
True answer: The number of dimensions can be reduced by up to 212 times.

Validation ID: 9c44df7503720709eac933a15569e5761b378046
Question: For which languages do they build word embeddings for?
Answer: English
True answer: Unanswerable

Validation ID: 9c44df7503720709eac933a15569e5761b378046
Question: For which languages do they build word embeddings for?
Answer: English
True answer: English

Validation ID: b7381927764536bd97b099b6a172708125364954
Question: How do they evaluate their resulting word embeddings?
Answer: They evaluate the resulting word embeddings by using intrinsic evaluation via word similarity and word analogy tasks, as well as downstream tasks.
True answer: We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.

Validation ID: df95b3cb6aa0187655fd4856ae2b1f503d533583
Question: What types of subwords do they incorporate in their model?
Answer: simple n-grams and unsupervised morphemes
True answer: n-gram subwords, unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords 

Validation ID: df95b3cb6aa0187655fd4856ae2b1f503d533583
Question: What types of subwords do they incorporate in their model?
Answer: simple n-grams and unsupervised morphemes
True answer: simple n-grams (like fastText) and unsupervised morphemes

Validation ID: f7ed3b9ed469ed34f46acde86b8a066c52ecf430
Question: Which matrix factorization methods do they use?
Answer: stochastic gradient descent
True answer: weighted factorization of a word-context co-occurrence matrix 

Validation ID: f7ed3b9ed469ed34f46acde86b8a066c52ecf430
Question: Which matrix factorization methods do they use?
Answer: stochastic gradient descent
True answer: The LexVec BIBREF7

Validation ID: c7eb71683f53ab7acffd691a36cad6edc7f5522e
Question: Do they report results only on English data?
Answer: Yes
True answer: Yes

Validation ID: c7eb71683f53ab7acffd691a36cad6edc7f5522e
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: 17a1eff7993c47c54eddc7344e7454fbe64191cd
Question: What experiments do they use to quantify the extent of interpretability?
Answer: Word similarity test
True answer: Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT).

Validation ID: 17a1eff7993c47c54eddc7344e7454fbe64191cd
Question: What experiments do they use to quantify the extent of interpretability?
Answer: Word similarity test
True answer: semantic category-based approach

Validation ID: a5e5cda1f6195ab1336855f1e39a609d61326d62
Question: Along which dimension do the semantically related words take larger values?
Answer: 32nd dimension
True answer: dimension corresponding to the concept that the particular word belongs to

Validation ID: 32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9
Question: What is the additive modification to the objective function?
Answer: INLINEFORM0
True answer: The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,

Validation ID: 32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9
Question: What is the additive modification to the objective function?
Answer: INLINEFORM0
True answer: An additive term added to the cost function for any one of the words of concept word-groups

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
Answer: PPDB
True answer: Unanswerable

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
Answer: PPDB
True answer:  Paraphrase Database (PPDB) ,  book corpus

Validation ID: eda4869c67fe8bbf83db632275f053e7e0241e8c
Question: Which dataset do they use?
Answer: PPDB
True answer: Unanswerable

Validation ID: 2c7494d47b2a69f182e83455fe4c75ae3b2893e9
Question: Do they evaluate their learned representations on downstream tasks?
Answer: Yes
True answer: No

Validation ID: 2c7494d47b2a69f182e83455fe4c75ae3b2893e9
Question: Do they evaluate their learned representations on downstream tasks?
Answer: Yes
True answer: No

Validation ID: 4d7ff4e5d06902de85b0e9a364dc455196d06a7d
Question: Which representation learning architecture do they adopt?
Answer: CNN-LSTM
True answer: Unanswerable

Validation ID: ecc63972b2783ee39b3e522653cfb6dc5917d522
Question: How do they encourage understanding of literature as part of their objective function?
Answer: They group the models based on the objective function it optimizes.
True answer: They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.

Validation ID: 8d074aabf4f51c8455618c5bf7689d3f62c4da1d
Question: What are the limitations of existing Vietnamese word segmentation systems?
Answer: The limitations of existing Vietnamese word segmentation systems are:
True answer:  ambiguous words, unknown words

Validation ID: 8d074aabf4f51c8455618c5bf7689d3f62c4da1d
Question: What are the limitations of existing Vietnamese word segmentation systems?
Answer: The limitations of existing Vietnamese word segmentation systems are:
True answer: lacks of complete review approaches, datasets and toolkits 

Validation ID: fe2666ace293b4bfac3182db6d0c6f03ea799277
Question: Why challenges does word segmentation in Vietnamese pose?
Answer: Because Vietnamese is an isolating language and one branch of Mon-Khmer language group.
True answer: Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback.

Validation ID: fe2666ace293b4bfac3182db6d0c6f03ea799277
Question: Why challenges does word segmentation in Vietnamese pose?
Answer: Because Vietnamese is an isolating language and one branch of Mon-Khmer language group.
True answer: to acquire very large Vietnamese corpus and to use them in building a classifier,  design and development of big data warehouse and analytic framework for Vietnamese documents, to building a system, which is able to incrementally learn new corpora and interactively process feedback

Validation ID: 70a1b0f9f26f1b82c14783f1b76dfb5400444aa4
Question: How successful are the approaches used to solve word segmentation in Vietnamese?
Answer: The accuracy of the approaches used to solve word segmentation in Vietnamese is around 94-97%.
True answer: Their accuracy in word segmentation is about 94%-97%.

Validation ID: d3ca5f1814860a88ff30761fec3d860d35e39167
Question: Which approaches have been applied to solve word segmentation in Vietnamese?
Answer: Maximum Matching, Maximum Entropy, Hidden Markov Model, Conditional Random Fields, and Support Vector Machines
True answer: Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)

Validation ID: d3ca5f1814860a88ff30761fec3d860d35e39167
Question: Which approaches have been applied to solve word segmentation in Vietnamese?
Answer: Maximum Matching, Maximum Entropy, Hidden Markov Model, Conditional Random Fields, and Support Vector Machines
True answer: Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines

Validation ID: dd20d93166c14f1e57644cd7fa7b5e5738025cd0
Question: Which two news domains are country-independent?
Answer: Mainstream and disinformation news
True answer: mainstream news and disinformation

Validation ID: dd20d93166c14f1e57644cd7fa7b5e5738025cd0
Question: Which two news domains are country-independent?
Answer: Mainstream and disinformation news
True answer: mainstream and disinformation news

Validation ID: dc2a2c177cd5df6da5d03e6e74262bf424850ec9
Question: How is the political bias of different sources included in the model?
Answer: We used the procedure described in BIBREF2 to label different outlets.
True answer: By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains

Validation ID: dc2a2c177cd5df6da5d03e6e74262bf424850ec9
Question: How is the political bias of different sources included in the model?
Answer: We used the procedure described in BIBREF2 to label different outlets.
True answer: we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries.

Validation ID: ae90c5567746fe25af2fcea0cc5f355751e05c71
Question: What are the two large-scale datasets used?
Answer: US dataset and Italian dataset
True answer: US dataset, Italian dataset

Validation ID: ae90c5567746fe25af2fcea0cc5f355751e05c71
Question: What are the two large-scale datasets used?
Answer: US dataset and Italian dataset
True answer: US dataset, Italian dataset

Validation ID: d7644c674887ca9708eb12107acd964ae53b216d
Question: What are the global network features which quantify different aspects of the sharing process?
Answer: Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d), Structural virality of the largest weakly connected component (SV)
True answer: Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)

Validation ID: a3bb9a936f61bafb509fa12ac0a61f91abcc5106
Question: Which datasets are used for evaluation?
Answer: We evaluate on the ARC dataset, the TREC question classification dataset, and the MLBioMedLAT and GARD datasets.
True answer: ARC , TREC, GARD , MLBioMedLAT 

Validation ID: a3bb9a936f61bafb509fa12ac0a61f91abcc5106
Question: Which datasets are used for evaluation?
Answer: We evaluate on the ARC dataset, the TREC question classification dataset, and the MLBioMedLAT and GARD datasets.
True answer: ARC, TREC, GARD, MLBioMedLAT

Validation ID: df6d327e176740da9edcc111a06374c54c8e809c
Question: What previous methods is their model compared to?
Answer: They compare their model to the following methods:
True answer: bag-of-words model, CNN

Validation ID: 49764eee7fb523a6a28375cc699f5e0220b81766
Question: Did they use a crowdsourcing platform?
Answer: No
True answer: No

Validation ID: 49764eee7fb523a6a28375cc699f5e0220b81766
Question: Did they use a crowdsourcing platform?
Answer: No
True answer: No

Validation ID: 3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1
Question: How was the dataset collected?
Answer: The dataset was collected from the ARC corpus.
True answer: from 3rd to 9th grade science questions collected from 12 US states

Validation ID: 3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1
Question: How was the dataset collected?
Answer: The dataset was collected from the ARC corpus.
True answer: Used from  science exam questions of the Aristo Reasoning Challenge (ARC) corpus.

Validation ID: bb3267c3f0a12d8014d51105de5d81686afe5f1b
Question: Which datasets do they use?
Answer: CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW
True answer: CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW

Validation ID: bb3267c3f0a12d8014d51105de5d81686afe5f1b
Question: Which datasets do they use?
Answer: CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW
True answer: CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW

Validation ID: 114934e1a1e818630ff33ac5c4cd4be6c6f75bb2
Question: How effective is their NCEL approach overall?
Answer: NCEL outperforms the state-of-the-art collective methods across five different datasets.
True answer: NCEL consistently outperforms various baselines with a favorable generalization ability

Validation ID: 2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72
Question: How do they verify generalization ability?
Answer: We train NCEL using collected Wikipedia hyperlinks instead of specific annotated data. We then evaluate the trained model on five different benchmarks to verify the linking precision as well as the generalization ability.
True answer: By calculating Macro F1 metric at the document level.

Validation ID: 2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72
Question: How do they verify generalization ability?
Answer: We train NCEL using collected Wikipedia hyperlinks instead of specific annotated data. We then evaluate the trained model on five different benchmarks to verify the linking precision as well as the generalization ability.
True answer: by evaluating their model on five different benchmarks

Validation ID: b8d0e4e0e820753ffc107c1847fe1dfd48883989
Question: Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?
Answer: They only use adjacent entity mentions.
True answer: NCEL considers only adjacent mentions.

Validation ID: b8d0e4e0e820753ffc107c1847fe1dfd48883989
Question: Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?
Answer: They only use adjacent entity mentions.
True answer: More than that in some cases (next to adjacent) 

Validation ID: 5aa12b4063d6182a71870c98e4e1815ff3dc8a72
Question: Do the authors mention any downside of lemmatizing input before training ELMo?
Answer: Yes, they do.
True answer: Yes

Validation ID: 5aa12b4063d6182a71870c98e4e1815ff3dc8a72
Question: Do the authors mention any downside of lemmatizing input before training ELMo?
Answer: Yes, they do.
True answer: Yes

Validation ID: 22815878083ebd2f9e08bc33a5e733063dac7a0f
Question: What other examples of morphologically-rich languages do the authors give?
Answer: Russian
True answer: Russian

Validation ID: 22815878083ebd2f9e08bc33a5e733063dac7a0f
Question: What other examples of morphologically-rich languages do the authors give?
Answer: Russian
True answer: Russian

Validation ID: 220d11a03897d85af91ec88a9b502815c7d2b6f3
Question: Why is lemmatization not necessary in English?
Answer: Because English has simple morphology.
True answer: Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations.

Validation ID: d509081673f5667060400eb325a8050fa5db7cc8
Question: How big was the corpora they trained ELMo on?
Answer: English Wikipedia dump from February 2017 and Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC).
True answer: 2174000000, 989000000

Validation ID: d509081673f5667060400eb325a8050fa5db7cc8
Question: How big was the corpora they trained ELMo on?
Answer: English Wikipedia dump from February 2017 and Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC).
True answer: 2174 million tokens for English and 989 million tokens for Russian

Validation ID: c2e475adeddcdc4d637ef0d4f5065b6a9b299827
Question: What metrics are used for evaluation?
Answer: BLEU-4, NIST-4 and ROUGE-4
True answer: BLEU-4, NIST-4, ROUGE-4

Validation ID: c2e475adeddcdc4d637ef0d4f5065b6a9b299827
Question: What metrics are used for evaluation?
Answer: BLEU-4, NIST-4 and ROUGE-4
True answer: BLEU-4, NIST-4, ROUGE-4

Validation ID: cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5
Question: Do they use pretrained embeddings?
Answer: Yes, they use Glove embeddings.
True answer: Yes

Validation ID: cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5
Question: Do they use pretrained embeddings?
Answer: Yes, they use Glove embeddings.
True answer: Yes

Validation ID: 6cd25c637c6b772ce29e8ee81571e8694549c5ab
Question: What dataset is used?
Answer: WikiBio dataset
True answer: English WIKIBIO, French WIKIBIO , German WIKIBIO 

Validation ID: 6cd25c637c6b772ce29e8ee81571e8694549c5ab
Question: What dataset is used?
Answer: WikiBio dataset
True answer: WikiBio dataset,  introduce two new biography datasets, one in French and one in German

Validation ID: 1088255980541382a2aa2c0319427702172bbf84
Question: What is a bifocal attention mechanism?
Answer: A bifocal attention mechanism is a mechanism that computes an attention over fields at a macro level and over values at a micro level.
True answer: At the macro level, it is important to decide which is the appropriate field to attend to next, micro level (i.e., within a field) it is important to know which values to attend to next, fuse the attention weights at the two levels

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
Answer: The expected number of unique outputs it assigns to a set of adversarial perturbations.
True answer: the number of distinct word recognition outputs that an attacker can induce

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
Answer: The expected number of unique outputs it assigns to a set of adversarial perturbations.
True answer: The expected number of unique outputs a word recognition system assigns to a set of adversarial perturbations 

Validation ID: 0d9fcc715dee0ec85132b3f4a730d7687b6a06f4
Question: What does the "sensitivity" quantity denote?
Answer: The expected number of unique outputs it assigns to a set of adversarial perturbations.
True answer: the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is “fooled”

Validation ID: 8910ee2236a497c92324bbbc77c596dba39efe46
Question: What end tasks do they evaluate on?
Answer: Sentiment analysis and paraphrase detection
True answer: Sentiment analysis and paraphrase detection under adversarial attacks

Validation ID: 2c59528b6bc5b5dc28a7b69b33594b274908cca6
Question: What is a semicharacter architecture?
Answer: A semicharacter architecture is a neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step.
True answer: A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters

Validation ID: 2c59528b6bc5b5dc28a7b69b33594b274908cca6
Question: What is a semicharacter architecture?
Answer: A semicharacter architecture is a neural network that processes a sentence of words with misspelled characters, predicting the correct words at each step.
True answer: processes a sentence of words with misspelled characters, predicting the correct words at each step

Validation ID: 6b367775a081f4d2423dc756c9b65b6eef350345
Question: Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?
Answer: We did not experiment with this, but it is a good idea.
True answer: No

Validation ID: bc01853512eb3c11528e33003ceb233d7c1d7038
Question: Why is the adversarial setting appropriate for misspelling recognition?
Answer: Because spammers continually bombard email servers, subtly misspelling words in efforts to evade spam detection while preserving the emails' intended meaning.
True answer: Adversarial misspellings are a real-world problem

Validation ID: 67ec8ef85844e01746c13627090dc2706bb2a4f3
Question: Why do they experiment with RNNs instead of transformers for this task?
Answer: RNNs are more efficient than transformers.
True answer: Unanswerable

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
Answer: Backoff strategies are used when the word recognizer predicts UNK. The backoff strategies are:
True answer: In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
Answer: Backoff strategies are used when the word recognizer predicts UNK. The backoff strategies are:
True answer: Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.

Validation ID: ba539cab80d25c3e20f39644415ed48b9e4e4185
Question: How do the backoff strategies work?
Answer: Backoff strategies are used when the word recognizer predicts UNK. The backoff strategies are:
True answer: Backoff to "a" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK

Validation ID: 6bf5620f295b5243230bc97b340fae6e92304595
Question: What baseline model is used?
Answer: The baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.
True answer: same baseline as used by lang2011unsupervised

Validation ID: 6bf5620f295b5243230bc97b340fae6e92304595
Question: What baseline model is used?
Answer: The baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.
True answer: We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.

Validation ID: 4986f420884f917d1f60d3cea04dc8e64d3b5bf1
Question: Which additional latent variables are used in the model?
Answer: Crosslingual latent variables
True answer: CLV as a parent of the two corresponding role variables

Validation ID: 4986f420884f917d1f60d3cea04dc8e64d3b5bf1
Question: Which additional latent variables are used in the model?
Answer: Crosslingual latent variables
True answer: crosslingual latent variables

Validation ID: 747b847d687f703cc20a87877c5b138f26ff137d
Question: Which parallel corpora are used?
Answer: Europarl corpus
True answer: English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14

Validation ID: 747b847d687f703cc20a87877c5b138f26ff137d
Question: Which parallel corpora are used?
Answer: Europarl corpus
True answer: the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 , and EN-DE section of the Europarl corpus BIBREF14 

Validation ID: 111afb77cfbf4c98e0458606378fa63a0e965e36
Question: Overall, does having parallel data improve semantic role induction across multiple languages?
Answer: No
True answer: No

Validation ID: 111afb77cfbf4c98e0458606378fa63a0e965e36
Question: Overall, does having parallel data improve semantic role induction across multiple languages?
Answer: No
True answer: No

Validation ID: 6568a31241167f618ef5ede939053feaa2fb0d7e
Question: Do they add one latent variable for each language pair in their Bayesian model?
Answer: No, they add one latent variable for each aligned argument.
True answer: Yes

Validation ID: 50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed
Question: What does an individual model consist of?
Answer: Bayesian model
True answer: Bayesian model of garg2012unsupervised as our base monolingual model

Validation ID: 0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c
Question: Do they improve on state-of-the-art semantic role induction?
Answer: Yes
True answer: Unanswerable

Validation ID: 0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c
Question: Do they improve on state-of-the-art semantic role induction?
Answer: Yes
True answer: Yes

Validation ID: 4dc268e3d482e504ca80d2ab514e68fd9b1c3af1
Question: how many tags do they look at?
Answer: 19
True answer: Unanswerable

Validation ID: 4dc268e3d482e504ca80d2ab514e68fd9b1c3af1
Question: how many tags do they look at?
Answer: 19
True answer: 48,705

Validation ID: ab54cd2dc83141bad3cb3628b3f0feee9169a556
Question: which algorithm was the highest performer?
Answer: INLINEFORM7
True answer: A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach

Validation ID: 249c805ee6f2ebe4dbc972126b3d82fb09fa3556
Question: how is diversity measured?
Answer: We calculate recommendation diversity as the average dissimilarity of all pairs of tags in the list of recommended tags.
True answer: average dissimilarity of all pairs of tags in the list of recommended tags

Validation ID: 249c805ee6f2ebe4dbc972126b3d82fb09fa3556
Question: how is diversity measured?
Answer: We calculate recommendation diversity as the average dissimilarity of all pairs of tags in the list of recommended tags.
True answer:  the average dissimilarity of all pairs of tags in the list of recommended tags

Validation ID: b4f881331b975e6e4cab1868267211ed729d782d
Question: how large is the vocabulary?
Answer: 48,705
True answer: 33,663

Validation ID: b4f881331b975e6e4cab1868267211ed729d782d
Question: how large is the vocabulary?
Answer: 48,705
True answer: 33,663 distinct review keywords 

Validation ID: 79413ff5d98957c31866f22179283902650b5bb6
Question: what dataset was used?
Answer: Amazon review keywords
True answer: 48,705 e-books from 13 publishers, search query logs of 21,243 e-books for 12 months

Validation ID: 79413ff5d98957c31866f22179283902650b5bb6
Question: what dataset was used?
Answer: Amazon review keywords
True answer:  E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.

Validation ID: 29c014baf99fb9f40b5171aab3e2c7f12a748f79
Question: what algorithms did they use?
Answer: Doc2Vec
True answer: popularity-based, similarity-based, hybrid

Validation ID: 09c86ef78e567033b725fc56b85c5d2602c1a7c3
Question: How does their ensemble method work?
Answer: They start with the best performing model according to validation performance. Then in each step they try adding the best performing model that had not been previously tried. They keep it in the ensemble if it does improve its validation performance and discard it otherwise. This way they gradually try each model once. They call the resulting model a greedy ensemble. They use the 20k BookTest validation dataset for this procedure.
True answer: simply averaging the predictions from the constituent single models

Validation ID: d67c01d9b689c052045f3de1b0918bab18c3f174
Question: How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?
Answer: The Attention-Sum Reader model trained on the BookTest dataset improves over the best Attention-Sum Reader model trained on the CBT dataset by INLINEFORM1 absolute on the CBT NE validation dataset and INLINEFORM2 absolute on the CBT CN validation dataset.
True answer: INLINEFORM2 

Validation ID: d67c01d9b689c052045f3de1b0918bab18c3f174
Question: How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?
Answer: The Attention-Sum Reader model trained on the BookTest dataset improves over the best Attention-Sum Reader model trained on the CBT dataset by INLINEFORM1 absolute on the CBT NE validation dataset and INLINEFORM2 absolute on the CBT CN validation dataset.
True answer: Answer with content missing: (Table 2) Accuracy of best AS reader results including ensembles are 78.4 and 83.7 when trained on BookTest compared to 71.0 and 68.9 when trained on CBT for Named endity and Common noun respectively.

Validation ID: e5bc73974c79d96eee2b688e578a9de1d0eb38fd
Question: How do they show there is space for further improvement?
Answer: They show that humans can answer questions that the model cannot.
True answer:  by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly

Validation ID: e5bc73974c79d96eee2b688e578a9de1d0eb38fd
Question: How do they show there is space for further improvement?
Answer: They show that humans can answer questions that the model cannot.
True answer: majority of questions that our system could not answer so far are in fact answerable

Validation ID: 2cd37743bcc7ea3bd405ce6d91e79e5339d7642e
Question: Do they report results only on English data?
Answer: Yes
True answer: Yes

Validation ID: 2cd37743bcc7ea3bd405ce6d91e79e5339d7642e
Question: Do they report results only on English data?
Answer: Yes
True answer: Yes

Validation ID: eac9dae3492e17bc49c842fb566f464ff18c049b
Question: What argument components do the ML methods aim to identify?
Answer: claim, premise, conclusion
True answer: claim, premise, backing, rebuttal, and refutation

Validation ID: eac9dae3492e17bc49c842fb566f464ff18c049b
Question: What argument components do the ML methods aim to identify?
Answer: claim, premise, conclusion
True answer: claim, premise, backing, rebuttal, refutation

Validation ID: 7697baf8d8d582c1f664a614f6332121061f87db
Question: Which machine learning methods are used in experiments?
Answer: SVM, CRF, and Naive Bayes
True answer: Structural Support Vector Machine

Validation ID: 7697baf8d8d582c1f664a614f6332121061f87db
Question: Which machine learning methods are used in experiments?
Answer: SVM, CRF, and Naive Bayes
True answer: SVMhmm 

Validation ID: 1cb100182508cf55b3509283c0e2bbcd527d625e
Question: How is the data in the new corpus come sourced?
Answer: The data in the new corpus come sourced from the Web.
True answer: user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles

Validation ID: 1cb100182508cf55b3509283c0e2bbcd527d625e
Question: How is the data in the new corpus come sourced?
Answer: The data in the new corpus come sourced from the Web.
True answer: refer to each article, blog post, comment, or forum posts as a document

Validation ID: 206739417251064b910ae9e5ff096e867ee10fb8
Question: What argumentation phenomena encounter in actual data are now accounted for by this work?
Answer: The phenomena that are accounted for by this work are:
True answer: Unanswerable

Validation ID: d6401cece55a14d2a35ba797a0878dfe2deabedc
Question: What challenges do different registers and domains pose to this task?
Answer: The linguistic variability of the education topics represents a challenge for NLP.
True answer: linguistic variability

Validation ID: ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff
Question: who transcribed the corpus?
Answer: the transcriber
True answer: Unanswerable

Validation ID: ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff
Question: who transcribed the corpus?
Answer: the transcriber
True answer: Unanswerable

Validation ID: fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c
Question: how was the speech collected?
Answer: crowdsourcing
True answer: The speech was collected from respondents using an android application.

Validation ID: fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c
Question: how was the speech collected?
Answer: crowdsourcing
True answer: Android application

Validation ID: b1a068c1050e2bed12d5c9550c73e59cd5b1f78d
Question: what accents are present in the corpus?
Answer: Persian and English
True answer: Unanswerable

Validation ID: b1a068c1050e2bed12d5c9550c73e59cd5b1f78d
Question: what accents are present in the corpus?
Answer: Persian and English
True answer: Unanswerable

Validation ID: f9edd8f9c13b54d8b1253ed30e7decc1999602da
Question: what evaluation protocols are provided?
Answer: We provided several evaluation protocols for each part of the database. The protocols allow researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration and phrase text on the performance.
True answer: three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set

Validation ID: d93c0e78a3fe890cd534a11276e934be68583f4b
Question: what age range is in the data?
Answer: 18-60
True answer: Unanswerable

Validation ID: d93c0e78a3fe890cd534a11276e934be68583f4b
Question: what age range is in the data?
Answer: 18-60
True answer: Unanswerable

Validation ID: 30af1926559079f59b0df055da76a3a34df8336f
Question: what is the source of the data?
Answer: The data collection project was mainly supported by Sharif DeepMine company.
True answer: Android application

Validation ID: ceb767e33fde4b927e730f893db5ece947ffb0d8
Question: what topics did they label?
Answer: demographics, diagnosis history, and symptoms/signs
True answer: Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.

Validation ID: ceb767e33fde4b927e730f893db5ece947ffb0d8
Question: what topics did they label?
Answer: demographics, diagnosis history, and symptoms/signs
True answer: Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others

Validation ID: c2cb6c4500d9e02fc9a1bdffd22c3df69655189f
Question: did they compare with other extractive summarization methods?
Answer: No
True answer: No

Validation ID: c571deefe93f0a41b60f9886db119947648e967c
Question: what datasets were used?
Answer: MIMIC-III
True answer: MIMIC-III

Validation ID: c571deefe93f0a41b60f9886db119947648e967c
Question: what datasets were used?
Answer: MIMIC-III
True answer: MIMIC-III

Validation ID: 06eb9f2320451df83e27362c22eb02f4a426a018
Question: what levels of document preprocessing are looked at?
Answer: four levels of preprocessing
True answer: raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document

Validation ID: 06eb9f2320451df83e27362c22eb02f4a426a018
Question: what levels of document preprocessing are looked at?
Answer: four levels of preprocessing
True answer: Level 1, Level 2 and Level 3.

Validation ID: e54257585cc75564341eb02bdc63ff8111992f82
Question: what keyphrase extraction models were reassessed?
Answer: TopicRank, TF-IDF, KP-Miner, WINGNUS, and TextRank
True answer: Answer with content missing: (LVL1, LVL2, LVL3) 
- Stanford CoreNLP
- Optical Character Recognition (OCR) system, ParsCIT 
- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.

Validation ID: 2a3e36c220e7b47c1b652511a4fdd7238a74a68f
Question: how many articles are in the dataset?
Answer: 244
True answer: 244

Validation ID: 2a3e36c220e7b47c1b652511a4fdd7238a74a68f
Question: how many articles are in the dataset?
Answer: 244
True answer: 244 

Validation ID: 9658b5ffb5c56e5a48a3fea0342ad8fc99741908
Question: Is this dataset publicly available for commercial use?
Answer: No
True answer: No

Validation ID: 9658b5ffb5c56e5a48a3fea0342ad8fc99741908
Question: Is this dataset publicly available for commercial use?
Answer: No
True answer: Yes

Validation ID: 46c9e5f335b2927db995a55a18b7c7621fd3d051
Question: How many different phenotypes are present in the dataset?
Answer: 15
True answer: 15 clinical patient phenotypes

Validation ID: 46c9e5f335b2927db995a55a18b7c7621fd3d051
Question: How many different phenotypes are present in the dataset?
Answer: 15
True answer: Thirteen different phenotypes are present in the dataset.

Validation ID: ce0e2a8675055a5468c4c54dbb099cfd743df8a7
Question: What are 10 other phenotypes that are annotated?
Answer: None, Unsure, Alcoholism, Depression, Diabetes, Drug Abuse, Hypertension, Obesity, Psychiatric Disorder, Pulmonary Disease
True answer: Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse

Validation ID: 3a6e843c6c81244c14730295cfb8b865cd7ede46
Question: What are the state of the art models?
Answer: BIBREF9, BIBREF8
True answer: BIBREF9 , BIBREF8 

Validation ID: 3a6e843c6c81244c14730295cfb8b865cd7ede46
Question: What are the state of the art models?
Answer: BIBREF9, BIBREF8
True answer: BIBREF9 , BIBREF8

Validation ID: fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf
Question: Which benchmark datasets are used?
Answer: Semeval 2014 Twitter Sentiment Analysis Dataset, corpus developed by BIBREF35, corpus developed by BIBREF36
True answer: Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset ,  dataset was created by BIBREF8,  English dataset from BIBREF8,  dataset from The Sarcasm Detector

Validation ID: fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf
Question: Which benchmark datasets are used?
Answer: Semeval 2014 Twitter Sentiment Analysis Dataset, corpus developed by BIBREF35, corpus developed by BIBREF36
True answer: This dataset was created by BIBREF8, another English dataset from BIBREF8 ,  dataset from The Sarcasm Detector

Validation ID: 1beb4a590fa6127a138f4ed1dd13d5d51cc96809
Question: What are the network's baseline features?
Answer: The network's baseline features are the features extracted from the fully-connected layer of the baseline CNN.
True answer:  The features extracted from CNN.

Validation ID: 5c5aeee83ea3b34f5936404f5855ccb9869356c1
Question: What tasks are used for evaluation?
Answer: machine translation
True answer: four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German

Validation ID: 5c5aeee83ea3b34f5936404f5855ccb9869356c1
Question: What tasks are used for evaluation?
Answer: machine translation
True answer:  four machine translation tasks, IWSLT 2017 German $\rightarrow $ English BIBREF27, KFTT Japanese $\rightarrow $ English BIBREF28, WMT 2016 Romanian $\rightarrow $ English BIBREF29, WMT 2014 English $\rightarrow $ German BIBREF30

Validation ID: f8c1b17d265a61502347c9a937269b38fc3fcab1
Question: HOw does the method perform compared with baselines?
Answer: The method performs better than the baselines.
True answer: On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The α-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.

Validation ID: 5913930ce597513299e4b630df5e5153f3618038
Question: How does their model improve interpretability compared to softmax transformers?
Answer: The softmax transformers assign non-zero attention weights to all context words, while the sparsemax transformers assign zero attention weights to irrelevant words.
True answer: the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence

Validation ID: 5913930ce597513299e4b630df5e5153f3618038
Question: How does their model improve interpretability compared to softmax transformers?
Answer: The softmax transformers assign non-zero attention weights to all context words, while the sparsemax transformers assign zero attention weights to irrelevant words.
True answer: We introduce sparse attention into the Transformer architecture

Validation ID: 81d193672090295e687bc4f4ac1b7a9c76ea35df
Question: What baseline method is used?
Answer: word2vec
True answer: using word2vec to create features that are used as input to the SVM

Validation ID: 81d193672090295e687bc4f4ac1b7a9c76ea35df
Question: What baseline method is used?
Answer: word2vec
True answer: use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach

Validation ID: cf171fad0bea5ab985c53d11e48e7883c23cdc44
Question: What details are given about the Twitter dataset?
Answer: It has nearly 1.6 million tweets annotated through a distant supervised method.
True answer: Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive.

Validation ID: cf171fad0bea5ab985c53d11e48e7883c23cdc44
Question: What details are given about the Twitter dataset?
Answer: It has nearly 1.6 million tweets annotated through a distant supervised method.
True answer: one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels

Validation ID: 2a564b092916f2fabbfe893cf13de169945ef2e1
Question: What details are given about the movie domain dataset?
Answer: The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39.
True answer: there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score

Validation ID: 2a564b092916f2fabbfe893cf13de169945ef2e1
Question: What details are given about the movie domain dataset?
Answer: The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39.
True answer: The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment.

Validation ID: 0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d
Question: Which hand-crafted features are combined with word2vec?
Answer: 3-feats
True answer: three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores

Validation ID: 0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d
Question: Which hand-crafted features are combined with word2vec?
Answer: 3-feats
True answer: polarity scores, which are minimum, mean, and maximum polarity scores, from each review

Validation ID: 73e83c54251f6a07744413ac8b8bed6480b2294f
Question: What word-based and dictionary-based feature are used?
Answer: word2vec and TDK
True answer: generate word embeddings specific to a domain, TDK (Türk Dil Kurumu - “Turkish Language Institution”) dictionary to obtain word polarities

Validation ID: 3355918bbdccac644afe441f085d0ffbbad565d7
Question: How are the supervised scores of the words calculated?
Answer: The supervised polarity score per word is calculated as in (DISPLAY_FORM4).
True answer: (+1 or -1), words of opposite polarities (e.g. “happy" and “unhappy") get far away from each other

Validation ID: e48e750743aef36529fbea4328b8253dbe928b4d
Question: what dataset was used?
Answer: Edinburgh corpus
True answer: WASSA-2017 Shared Task on Emotion Intensity

Validation ID: c08aab979dcdc8f4fe8ec1337c3c8290ab13414e
Question: how many total combined features were there?
Answer: 14
True answer: Fourteen 

Validation ID: c08aab979dcdc8f4fe8ec1337c3c8290ab13414e
Question: how many total combined features were there?
Answer: 14
True answer: Unanswerable

Validation ID: 8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f
Question: what pretrained word embeddings were used?
Answer: GloVe, Edinburgh embeddings, Emoji embeddings
True answer: Pretrained word embeddings  were not used

Validation ID: 8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f
Question: what pretrained word embeddings were used?
Answer: GloVe, Edinburgh embeddings, Emoji embeddings
True answer: GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16

Validation ID: cc608df2884e1e82679f663ed9d9d67a4b6c03f3
Question: What evaluation metrics did look at?
Answer: We looked at the following metrics:
True answer: precision, recall, F1 and accuracy

Validation ID: cc608df2884e1e82679f663ed9d9d67a4b6c03f3
Question: What evaluation metrics did look at?
Answer: We looked at the following metrics:
True answer: Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.

Validation ID: 3e432d71512ffbd790a482c716e7079ee78ce732
Question: What datasets are used?
Answer: The datasets used are the ones from the DSTC2 BIBREF38 and DSTC3 BIBREF39 challenges.
True answer: Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.

Validation ID: 3e432d71512ffbd790a482c716e7079ee78ce732
Question: What datasets are used?
Answer: The datasets used are the ones from the DSTC2 BIBREF38 and DSTC3 BIBREF39 challenges.
True answer: a self-collected financial intents dataset in Portuguese

Validation ID: dd76130ec5fac477123fe8880472d03fbafddef6
Question: What is the state of the art described in the paper?
Answer: The state of the art described in the paper is the lack of methods and tools to design and engineer the coordination and mediation among chatbots and humans.
True answer: ELIZA,  PARRY, A.L.I.C.E., Cleverbot

Validation ID: 43eecc576348411b0634611c81589f618cd4fddf
Question: What GAN models were used as baselines to compare against?
Answer: SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN
True answer: MLE, SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN

Validation ID: 43eecc576348411b0634611c81589f618cd4fddf
Question: What GAN models were used as baselines to compare against?
Answer: SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN
True answer: SeqGAN, LeakGAN, MaliGAN, DialogGAN, DPGAN

Validation ID: 79f9468e011670993fd162543d1a4b3dd811ac5d
Question: How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?
Answer: Better performance on three text generation tasks.
True answer: ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.

Validation ID: 79f9468e011670993fd162543d1a4b3dd811ac5d
Question: How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?
Answer: Better performance on three text generation tasks.
True answer: Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.

Validation ID: c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf
Question: Is the discriminator's reward made available at each step to the generator?
Answer: Yes
True answer: Unanswerable

Validation ID: 902b3123aec0f3a39319ffa9d05ab8e08a2eb567
Question: What is the algorithm used to create word embeddings?
Answer: Skip-Gram with negative sampling (SGNS)
True answer: Unanswerable

Validation ID: 1038542243efe5ab3e65c89385e53c4831cd9981
Question: What is the corpus used for the task?
Answer: DTA18 and DTA19
True answer: DTA18, DTA19

Validation ID: 1038542243efe5ab3e65c89385e53c4831cd9981
Question: What is the corpus used for the task?
Answer: DTA18 and DTA19
True answer: Diachronic Usage Relatedness (DURel) gold standard data set

Validation ID: e2b0cd30cf56a4b13f96426489367024310c3a05
Question: How is evaluation performed?
Answer: Spearman's $\rho $
True answer: As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used

Validation ID: e2b0cd30cf56a4b13f96426489367024310c3a05
Question: How is evaluation performed?
Answer: Spearman's $\rho $
True answer: Spearman's rank-order correlation

Validation ID: e831041d50f3922265330fcbee5a980d0e2586dd
Question: What is a normal reading paradigm?
Answer: Reading without any specific task other than comprehension
True answer: read the sentences normally without any special instructions

Validation ID: e831041d50f3922265330fcbee5a980d0e2586dd
Question: What is a normal reading paradigm?
Answer: Reading without any specific task other than comprehension
True answer: participants were instructed to read the sentences naturally, without any specific task other than comprehension

Validation ID: 7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93
Question: Did they experiment with this new dataset?
Answer: Yes
True answer: No

Validation ID: ac7f6497be4bcca64e75f28934b207c9e8097576
Question: What kind of sentences were read?
Answer: Wikipedia sentences
True answer: sentences that were selected from the Wikipedia corpus provided by culotta2006integrating

Validation ID: ac7f6497be4bcca64e75f28934b207c9e8097576
Question: What kind of sentences were read?
Answer: Wikipedia sentences
True answer: seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer

Validation ID: 87bb3105e03ed6ac5abfde0a7ca9b8de8985663c
Question: why are their techniques cheaper to implement?
Answer: because they do not require to train a backward translation system
True answer: They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper

Validation ID: 87bb3105e03ed6ac5abfde0a7ca9b8de8985663c
Question: why are their techniques cheaper to implement?
Answer: because they do not require to train a backward translation engine.
True answer: They do not require the availability of a backward translation engine.

Validation ID: d9980676a83295dda37c20cfd5d58e574d0a4859
Question: what data simulation techniques were introduced?
Answer: back-translation, forward-translation, copy, copy-marked, copy-dummies, copy+noise, copy+noise+GANs
True answer: copy, copy-marked, copy-dummies

Validation ID: d9980676a83295dda37c20cfd5d58e574d0a4859
Question: what data simulation techniques were introduced?
Answer: back-translation, forward-translation, copy, copy-marked, copy-dummies, copy+noise, copy+noise+GANs
True answer: copy, copy-marked, copy-dummies

Validation ID: 9225b651e0fed28d4b6261a9f6b443b52597e401
Question: what is their explanation for the effectiveness of back-translation?
Answer: monotonicity is a facilitating factor
True answer: when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources

Validation ID: 565189b672efee01d22f4fc6b73cd5287b2ee72c
Question: what dataset is used?
Answer: Europarl
True answer: Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)

Validation ID: 565189b672efee01d22f4fc6b73cd5287b2ee72c
Question: what dataset is used?
Answer: Europarl
True answer: Europarl tests from 2006, 2007, 2008; WMT newstest 2014.

Validation ID: b6f7fadaa1bb828530c2d6780289f12740229d84
Question: what language pairs are explored?
Answer: English-French and English-German
True answer: English-German, English-French.

Validation ID: b6f7fadaa1bb828530c2d6780289f12740229d84
Question: what language pairs are explored?
Answer: English-French and English-German
True answer: English-German, English-French

Validation ID: 7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613
Question: what language is the data in?
Answer: English
True answer: English , German, French

Validation ID: 4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f
Question: Does the experiments focus on a specific domain?
Answer: No
True answer: No

Validation ID: 4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f
Question: Does the experiments focus on a specific domain?
Answer: No
True answer: No

Validation ID: 6c96e910bd98c9fd58ba2050f99b9c9bac69840a
Question: how many training samples do you have for training?
Answer: We have a total of 822,040 questions in our dataset. We randomly split the dataset into training and test sets in the ratio of 80:20.
True answer: Unanswerable

Validation ID: 6c96e910bd98c9fd58ba2050f99b9c9bac69840a
Question: how many training samples do you have for training?
Answer: We have a total of 822,040 questions in our dataset. We randomly split the dataset into training and test sets in the ratio of 80:20.
True answer: Unanswerable

Validation ID: 9af3142630b350c93875441e1e1767312df76d17
Question: Do the answered questions measure for the usefulness of the answer?
Answer: Yes, they do.
True answer: No

Validation ID: e374169ee10f835f660ab8403a5701114586f167
Question: What profile metadata is used for this analysis?
Answer: Profile name, username, description, profile image, and location.
True answer: username, display name, profile image, location, description

Validation ID: e374169ee10f835f660ab8403a5701114586f167
Question: What profile metadata is used for this analysis?
Answer: We consider 5 major profile attributes for the analysis, including, Profile Name, Username, Description, Profile Image, and Location.
True answer: username, display name, profile image, location and description

Validation ID: 82595ca5d11e541ed0c3353b41e8698af40a479b
Question: What are the organic and inorganic ways to show political affiliation through profile changes?
Answer: Organic ways include mentioning party name in the profile attributes. Inorganic ways include adding election campaign related keywords to the profile attributes.
True answer: Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.
Inorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile.

Validation ID: 82595ca5d11e541ed0c3353b41e8698af40a479b
Question: What are the organic and inorganic ways to show political affiliation through profile changes?
Answer: Organic ways include mentioning party name in the profile attributes. Inorganic ways include adding election campaign related keywords to the profile attributes.
True answer: Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way.

Validation ID: d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f
Question: How do profile changes vary for influential leads and their followers over the social movement?
Answer: We find that the political handles made more changes to profile attributes than the follower handles. About $80\%$ changes made by the followers in the period of data collection was related to #LokSabhaElectios2019. While the profile handles made changes, the changes mostly included going back and forth to same profile attribute, with the change in attribute being active support to a political party or political leader.
True answer: Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.

Validation ID: 53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e
Question: What evaluation metrics do they use?
Answer: BLEU-1, Meteor, Rouge-L
True answer: BLEU-1, Meteor , Rouge-L 

Validation ID: 53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e
Question: What evaluation metrics do they use?
Answer: BLEU-1, Meteor, Rouge-L
True answer: BLEU-1, Meteor ,  Rouge-L 

Validation ID: 869feb7f47606105005efdb6bea1c549824baea0
Question: What is the size of this dataset?
Answer: The dataset includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.
True answer: 13,757

Validation ID: 869feb7f47606105005efdb6bea1c549824baea0
Question: What is the size of this dataset?
Answer: The dataset includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.
True answer: 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs

Validation ID: c497e8701060583d91bb64b9f9202d40047effc4
Question: How do they determine if tweets have been used by journalists?
Answer: We crawled thousands of news articles that include tweet quotations and then employed crowd-sourcing to elicit questions and answers based on these event-aligned tweets.
True answer:  we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles

Validation ID: 8060a773f6a136944f7b59758d08cc6f2a59693b
Question: how small of a dataset did they train on?
Answer: 14%
True answer: 1000 hours data

Validation ID: 8060a773f6a136944f7b59758d08cc6f2a59693b
Question: how small of a dataset did they train on?
Answer: 14%
True answer: 23085 hours of data

Validation ID: 1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306
Question: what was their character error rate?
Answer: 14%
True answer: 2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.

Validation ID: 1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306
Question: what was their character error rate?
Answer: 14%
True answer: Their best model achieved a 2.49% Character Error Rate.

Validation ID: c0af8b7bf52dc15e0b33704822c4a34077e09cd1
Question: which lstm models did they compare with?
Answer: 2-layers regular trained model
True answer: Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.

Validation ID: 9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e
Question: Do they use datasets with transcribed text or do they determine text from the audio?
Answer: They use datasets with transcribed text.
True answer: They use text transcription.

Validation ID: 9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e
Question: Do they use datasets with transcribed text or do they determine text from the audio?
Answer: They use datasets with transcribed text.
True answer: both

Validation ID: e0122fc7b0143d5cbcda2120be87a012fb987627
Question: By how much does their model outperform the state of the art results?
Answer: 68.8% to 71.8%
True answer: the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)

Validation ID: 5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4
Question: How do they combine audio and text sequences in their RNN?
Answer: They concatenate the final encoding vector of the audio-RNN with the final encoding vector of the text-RNN.
True answer: combines the information from these sources using a feed-forward neural model

Validation ID: 5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4
Question: How do they combine audio and text sequences in their RNN?
Answer: They concatenate the final encoding vector of the audio-RNN with the final encoding vector of the text-RNN.
True answer: encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model

Validation ID: 37edc25e39515ffc2d92115d2fcd9e6ceb18898b
Question: What was the baseline?
Answer: BIBREF2
True answer: SVMs, LR, BIBREF2

Validation ID: 37edc25e39515ffc2d92115d2fcd9e6ceb18898b
Question: What was the baseline?
Answer: BIBREF2
True answer: SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt

Validation ID: e431661f17347607c3d3d9764928385a8f3d9650
Question: By how much did they improve?
Answer: several points
True answer: They decrease MAE in 0.34

Validation ID: 876700622bd6811d903e65314ac75971bbe23dcc
Question: What dataset did they use?
Answer: SemEval-2016
True answer:  high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task

Validation ID: 876700622bd6811d903e65314ac75971bbe23dcc
Question: What dataset did they use?
Answer: SemEval-2016
True answer:  SemEval-2016 “Sentiment Analysis in Twitter”

Validation ID: 312e9cc11b9036a6324bdcb64eca6814053ffa17
Question: What is the reported agreement for the annotation?
Answer: 92%
True answer: Unanswerable

Validation ID: 312e9cc11b9036a6324bdcb64eca6814053ffa17
Question: What is the reported agreement for the annotation?
Answer: 92%
True answer: Unanswerable

Validation ID: 1c0ba6958da09411deded4a14dfea5be55687619
Question: How many annotators participated?
Answer: 3
True answer: Unanswerable

Validation ID: 1c0ba6958da09411deded4a14dfea5be55687619
Question: How many annotators participated?
Answer: 3
True answer: Unanswerable

Validation ID: 1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b
Question: What features are used?
Answer: TF-IDF features
True answer: Unanswerable

Validation ID: d915b401bb96c9f104a0353bef9254672e6f5a47
Question: What future possible improvements are listed?
Answer: 1. The model could be improved by using a more complex decoder, such as the one proposed in BIBREF12.
True answer: rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions

Validation ID: d915b401bb96c9f104a0353bef9254672e6f5a47
Question: What future possible improvements are listed?
Answer: 1. The model could be improved by using a more complex decoder, such as the one proposed in BIBREF12.
True answer: to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions

Validation ID: 79a44a68bb57b375d8a57a0a7f522d33476d9f33
Question: Which qualitative metric are used for evaluation?
Answer: BLEU score
True answer:  Relation Generation (RG) , Content Selection (CS),  Content Ordering (CO)

Validation ID: 79a44a68bb57b375d8a57a0a7f522d33476d9f33
Question: Which qualitative metric are used for evaluation?
Answer: BLEU score
True answer: Relation Generation (RG), Content Selection (CS), Content Ordering (CO)

Validation ID: 664db503509b8236bc4d3dc39cebb74498365750
Question: What is quantitative improvement of proposed method (the best variant) w.r.t. baseline (the best variant)?
Answer: BLEU score: 16.7 vs. 14.5
True answer: Hierarchical-k

Validation ID: 64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8
Question: How is "propaganda" defined for the purposes of this study?
Answer: Propaganda is a social phenomenon and takes place as an act of communication.
True answer: an intentional and potentially multicast communication, “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends"

Validation ID: 64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8
Question: How is "propaganda" defined for the purposes of this study?
Answer: Propaganda is a social phenomenon and takes place as an act of communication.
True answer: First, propaganda is a social phenomenon and takes place as an act of communication BIBREF19, and so it is more than a simple information-theoretic message of zeros and ones—it also incorporates an addresser and addressee(s), each in phatic contact (typically via broadcast media), ideally with a shared denotational code and contextual surround(s) BIBREF20.

Validation ID: b0a18628289146472aa42f992d0db85c200ec64b
Question: What metrics are used in evaluation?
Answer: F1 score
True answer: precision, recall , F1 score

Validation ID: 72ce05546c81ada05885026470f4c8c218805055
Question: Which natural language(s) are studied in this paper?
Answer: English
True answer: Unanswerable

Validation ID: 72ce05546c81ada05885026470f4c8c218805055
Question: Which natural language(s) are studied in this paper?
Answer: English
True answer: English

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
Answer: We use BLEU-1/4, ROUGE-L, Distinct-1/2, and perplexity.
True answer: Byte-Pair Encoding perplexity  (BPE PPL),
BLEU-1,
BLEU-4,
ROUGE-L,
percentage of distinct unigram (D-1),
percentage of distinct bigrams(D-2),
user matching accuracy(UMA),
Mean Reciprocal Rank(MRR)
Pairwise preference over baseline(PP)

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
Answer: We use BLEU-1/4, ROUGE-L, Distinct-1/2, and perplexity.
True answer: BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence

Validation ID: 5b551ba47d582f2e6467b1b91a8d4d6a30c343ec
Question: What metrics are used for evaluation?
Answer: We use BLEU-1/4, ROUGE-L, Distinct-1/2, and perplexity.
True answer:  Distinct-1/2, UMA = User Matching Accuracy, MRR
= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
Answer: English
True answer: English

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
Answer: English
True answer: English

Validation ID: 3cf1edfa6d53a236cf4258afd87c87c0a477e243
Question: What natural language(s) are the recipes written in?
Answer: English
True answer: Unanswerable

Validation ID: 9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa
Question: What were their results on the new dataset?
Answer: They achieved an average recipe-level coherence score of 1.78-1.82, surpassing the baseline at 1.77.
True answer: average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time

Validation ID: 34dc0838632d643f33c8dbfe7bd4b656586582a2
Question: What are the baseline models?
Answer: The baseline models are the encoder-decoder model and the nearest-neighbor model.
True answer: name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)

Validation ID: c77359fb9d3ef96965a9af0396b101f82a0a9de6
Question: How did they obtain the interactions?
Answer: They scraped the data from Food.com.
True answer: from Food.com

Validation ID: 1bdc990c7e948724ab04e70867675a334fdd3051
Question: Where do they get the recipes from?
Answer: They are scraped from Food.com.
True answer: from Food.com

Validation ID: 78536da059b884d6ad04680baeb894895458055c
Question: What were the baselines?
Answer: Maximum Entropy Classifiers, Support Vector Machines, Recursive Neural Networks, Convolutional Neural Networks, Bidirectional Encoder Representation from Transformers (BERT)
True answer: varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)

Validation ID: 96b07373756d7854bccc3c12e8d41454ab8741f5
Question: Does RoBERTa outperform BERT?
Answer: No
True answer: Unanswerable

Validation ID: 96b07373756d7854bccc3c12e8d41454ab8741f5
Question: Does RoBERTa outperform BERT?
Answer: No
True answer: No

Validation ID: 511517efc96edcd3e91e7783821c9d6d5a6562af
Question: Which multiple datasets did they train on during joint training?
Answer: BF, BA, SFU
True answer: BF, BA, SFU and Sherlock

Validation ID: 511517efc96edcd3e91e7783821c9d6d5a6562af
Question: Which multiple datasets did they train on during joint training?
Answer: BF, BA, SFU
True answer: BioScope Abstracts, SFU, and BioScope Full Papers

Validation ID: 9122de265577e8f6b5160cd7d28be9e22da752b2
Question: What were the previously reported results?
Answer: We obtained a gain of 0.42 F1 points on BF, 1.98 F1 points on BA and 0.29 F1 points on SFU, while on the scope resolution task, we obtained a gain of 8.06 F1 points on BF, 4.27 F1 points on BA and 11.87 F1 points on SFU, when trained on a single dataset. While training on multiple datasets, we observed a gain of 10.6 F1 points on BF and 1.94 F1 points on BA on the speculation detection task and 2.16 F1 points on BF and 0.25 F1 points on SFU on the scope resolution task over the single dataset training approach. We thus significantly advance the state-of-the-art for speculation detection and scope resolution. On the negation scope resolution task, we applied the XLNet and RoBERTa and obtained a gain of 3.16 F1 points on BF, 0.06 F1 points on BA and 0.3 F1 points on SFU. Thus, we demonstrated the usefulness of transformer-based architectures in the field of negation and speculation detection and scope resolution.
True answer: Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution

Validation ID: e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d
Question: What is the size of SFU Review corpus?
Answer: 2670 sentences
True answer: Unanswerable

Validation ID: e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d
Question: What is the size of SFU Review corpus?
Answer: 2670 sentences
True answer: Unanswerable

Validation ID: 45be665a4504f0c7f458cf3f75a95d5a75eefd42
Question: What is the size of bioScope corpus?
Answer: 11871 sentences
True answer: Unanswerable

Validation ID: 45be665a4504f0c7f458cf3f75a95d5a75eefd42
Question: What is the size of bioScope corpus?
Answer: 11871 sentences
True answer: Unanswerable

Validation ID: 22b740cc3c8598247ee102279f96575bdb10d53f
Question: Do they study numerical properties of their obtained vectors (such as orthogonality)?
Answer: No
True answer: No

Validation ID: 22b740cc3c8598247ee102279f96575bdb10d53f
Question: Do they study numerical properties of their obtained vectors (such as orthogonality)?
Answer: No
True answer: No

Validation ID: 74b4779de437c697fe702e51f23e2b0538b0f631
Question: How do they score phrasal compositionality?
Answer: context-dependent
True answer: Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators

Validation ID: 435570723b37ee1f5898c1a34ef86a0b2e8701bb
Question: Which translation systems do they compare against?
Answer: phrase-based MT system
True answer: hierarchical phrase-based system BIBREF29, appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn)

Validation ID: 435570723b37ee1f5898c1a34ef86a0b2e8701bb
Question: Which translation systems do they compare against?
Answer: phrase-based MT system
True answer:  English-Spanish MT system 

Validation ID: aa2948209cc33b071dbf294822e72bb136678345
Question: what are their results on the constructed dataset?
Answer: The results are shown in Table TABREF37.
True answer: AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement

Validation ID: d9412dda3279729e95fcb35cbed09e61577a896e
Question: what evaluation metrics are reported?
Answer: precision, recall, F1 and accuracy
True answer: precision, recall, F1 and accuracy

Validation ID: d9412dda3279729e95fcb35cbed09e61577a896e
Question: what evaluation metrics are reported?
Answer: precision, recall, F1 and accuracy
True answer: precision, recall, F1 , accuracy 

Validation ID: 41b70699514703820435b00efbc3aac4dd67560a
Question: what civil field is the dataset about?
Answer: divorce proceedings
True answer: divorce 

Validation ID: 41b70699514703820435b00efbc3aac4dd67560a
Question: what civil field is the dataset about?
Answer: divorce proceedings
True answer: divorce

Validation ID: e3c9e4bc7bb93461856e1f4354f33010bc7d28d5
Question: what are the state-of-the-art models?
Answer: CNN/GRU+law
True answer: SVM , CNN , GRU , CNN/GRU+law, r-net , AoA 

Validation ID: e3c9e4bc7bb93461856e1f4354f33010bc7d28d5
Question: what are the state-of-the-art models?
Answer: CNN/GRU+law
True answer: SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard

Validation ID: 06cc8fcafc0880cf69a2514bb7341642b9833041
Question: what is the size of the real-world civil case dataset?
Answer: INLINEFORM0
True answer: 100 000 documents

Validation ID: 06cc8fcafc0880cf69a2514bb7341642b9833041
Question: what is the size of the real-world civil case dataset?
Answer: INLINEFORM0
True answer:  INLINEFORM1 cases

Validation ID: d650101712e36594bd77b45930a990402a455222
Question: what datasets are used in the experiment?
Answer: divorce proceedings
True answer: build a new one, collect INLINEFORM0 cases from China Judgments Online

Validation ID: cb384dc5366b693f28680374d31ff45356af0461
Question: Do they model semantics?
Answer: Yes
True answer: Yes

Validation ID: cb384dc5366b693f28680374d31ff45356af0461
Question: Do they model semantics?
Answer: Yes
True answer: Yes

Validation ID: d41e20ec716b5904a272938e5a8f5f3f15a7779e
Question: How do they identify discussions of LGBTQ people in the New York Times?
Answer: They use a predetermined list of LGBTQ terms.
True answer: act paragraphs containing any word from a predetermined list of LGTBQ terms 

Validation ID: 0682bf049f96fa603d50f0fdad0b79a5c55f6c97
Question: Do they analyze specific derogatory words?
Answer: Yes
True answer: Yes

Validation ID: 0682bf049f96fa603d50f0fdad0b79a5c55f6c97
Question: Do they analyze specific derogatory words?
Answer: Yes
True answer: Yes

Validation ID: 97d1ac71eed13d4f51f29aac0e1a554007907df8
Question: What is novel about their document-level encoder?
Answer: It is able to encode a document and obtain representations for its sentences.
True answer: Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically

Validation ID: c17b609b0b090d7e8f99de1445be04f8f66367d4
Question: What rouge score do they achieve?
Answer: 0.9
True answer: Best results on unigram:
CNN/Daily Mail: Rogue F1 43.85
NYT: Rogue Recall 49.02
XSum: Rogue F1 38.81

Validation ID: c17b609b0b090d7e8f99de1445be04f8f66367d4
Question: What rouge score do they achieve?
Answer: 0.9
True answer: Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55

Validation ID: 53014cfb506f6fffb22577bf580ae6f4d5317ce5
Question: What are the datasets used for evaluation?
Answer: CNN/DailyMail, NYT, XSum
True answer: CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum

Validation ID: 53014cfb506f6fffb22577bf580ae6f4d5317ce5
Question: What are the datasets used for evaluation?
Answer: CNN/DailyMail, NYT, XSum
True answer: the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22

Validation ID: fa30a938b58fc05131c3854f12efe376cbad887f
Question: What was their performance on emotion detection?
Answer: competitive results
True answer: Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. 

Validation ID: f875337f2ecd686cd7789e111174d0f14972638d
Question: Which existing benchmarks did they compare to?
Answer: Affective Text dataset, Fairy Tales dataset, and the ISEAR dataset.
True answer: Affective Text, Fairy Tales, ISEAR

Validation ID: f875337f2ecd686cd7789e111174d0f14972638d
Question: Which existing benchmarks did they compare to?
Answer: Affective Text dataset, Fairy Tales dataset, and the ISEAR dataset.
True answer:  Affective Text dataset, Fairy Tales dataset, ISEAR dataset

Validation ID: de53af4eddbc30c808d90b8a11a29217d377569e
Question: Which Facebook pages did they look at?
Answer: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney
True answer: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney

Validation ID: de53af4eddbc30c808d90b8a11a29217d377569e
Question: Which Facebook pages did they look at?
Answer: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney
True answer: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.

Validation ID: dac087e1328e65ca08f66d8b5307d6624bf3943f
Question: LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?
Answer: No, this paper is introducing a supervised approach to spam detection.
True answer: No

Validation ID: dac087e1328e65ca08f66d8b5307d6624bf3943f
Question: LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?
Answer: Yes
True answer: No

Validation ID: a1645d0ba50e4c29f0feb806521093e7b1459081
Question: What is the benchmark dataset and is its quality high?
Answer: Social Honeypot dataset and yes
True answer: Social Honeypot dataset (public) and Weibo dataset (self-collected); yes

Validation ID: a1645d0ba50e4c29f0feb806521093e7b1459081
Question: What is the benchmark dataset and is its quality high?
Answer: Social Honeypot dataset and yes
True answer: Social Honeypot, which is not of high quality

Validation ID: 3cd185b7adc835e1c4449eff81222f5fc15c8500
Question: How do they detect spammers?
Answer: By using the LDA model, we obtain the topic probability for each Twitter user. By utilizing the topic probability result, we extract our two topic-based features: GOSS and LOSS which represent the account with global and local information.
True answer: Extract features from the LDA model and use them in a binary classification task

Validation ID: f03112b868b658c954db62fc64430bebbaa7d9e0
Question: Do they use other evaluation metrics besides ROUGE?
Answer: Yes, they use BLEU.
True answer: Yes

Validation ID: f03112b868b658c954db62fc64430bebbaa7d9e0
Question: Do they use other evaluation metrics besides ROUGE?
Answer: Yes, they use BLEU.
True answer: No

Validation ID: 5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4
Question: What is their ROUGE score?
Answer: 0.57
True answer: Unanswerable

Validation ID: a6d3e57de796172c236e33a6ceb4cca793dc2315
Question: What are the baselines?
Answer: ROUGE-1, ROUGE-2, ROUGE-L
True answer: Unanswerable

Validation ID: a6d3e57de796172c236e33a6ceb4cca793dc2315
Question: What are the baselines?
Answer: ROUGE-1, ROUGE-2, ROUGE-L
True answer: Answer with content missing: (Experimental Setup missing subsections)
To be selected: We compared REFRESH against a baseline which simply selects the first m leading sentences from each document (LEAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss.
Answer: LEAD

Validation ID: 395b61d368e8766014aa960fde0192e4196bcb85
Question: What datasets do they use?
Answer: IMDB, Yelp
True answer: three datasets based on IMDB reviews and Yelp reviews

Validation ID: 395b61d368e8766014aa960fde0192e4196bcb85
Question: What datasets do they use?
Answer: IMDB, Yelp
True answer: 1 IMDB dataset and 2 Yelp datasets

Validation ID: 92bb41cf7bd1f7886784796a8220ed5aa07bc49b
Question: What other factors affect the performance?
Answer: dataset features such as length of text, training data size (for target classifiers) and input domains
True answer: architecture of the classifier, sentence length,  input domain

Validation ID: 4ef11518b40cc55d86c485f14e24732123b0d907
Question: What are the benchmark attacking methods?
Answer: FGM, FGVM, DeepFool, HotFlip and TYC
True answer: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4

Validation ID: 4ef11518b40cc55d86c485f14e24732123b0d907
Question: What are the benchmark attacking methods?
Answer: FGM, FGVM, DeepFool, HotFlip and TYC
True answer: FGM, FGVM, DeepFool, HotFlip, TYC

Validation ID: 6a219d7c58451842aa5d6819a7cdf51c55e9fc0f
Question: What domains are covered in the corpus?
Answer: Tourism, Bible readings
True answer: No specific domain is covered in the corpus.

Validation ID: cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad
Question: What is the architecture of their model?
Answer: Transformer
True answer: follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing

Validation ID: f8f4e4a50d2b3fbd193327e79ea32d8d057e1414
Question: How was the dataset collected?
Answer: Crowdsourcing
True answer: Contributors record voice clips by reading from a bank of donated sentences.

Validation ID: f8f4e4a50d2b3fbd193327e79ea32d8d057e1414
Question: How was the dataset collected?
Answer: Crowdsourcing
True answer: crowdsourcing

Validation ID: bc84c5a58c57038910f7720d7a784560054d3e1a
Question: Which languages are part of the corpus?
Answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese
True answer: French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)

Validation ID: bc84c5a58c57038910f7720d7a784560054d3e1a
Question: Which languages are part of the corpus?
Answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese
True answer: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese

Validation ID: 29923a824c98b3ba85ced964a0e6a2af35758abe
Question: How is the quality of the data empirically evaluated??
Answer: We compute sentence-level BLEU with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system.
True answer: Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets

Validation ID: 29923a824c98b3ba85ced964a0e6a2af35758abe
Question: How is the quality of the data empirically evaluated??
Answer: We compute sentence-level BLEU with the NLTK implementation between the human translations and the automatic translations produced by a state-of-the-art system.
True answer: computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations

Validation ID: 559c68802ee2bb8b11e2188127418ca3a6155ba7
Question: Is the data in CoVoST annotated for dialect?
Answer: No
True answer: No

Validation ID: 8dc707a0daf7bff61a97d9d854283e65c0c85064
Question: Is Arabic one of the 11 languages in CoVost?
Answer: No
True answer: No

Validation ID: 8dc707a0daf7bff61a97d9d854283e65c0c85064
Question: Is Arabic one of the 11 languages in CoVost?
Answer: No
True answer: No

Validation ID: ffde866b1203a01580eb33237a0bb9da71c75ecf
Question: How big is Augmented LibriSpeech dataset?
Answer: 1000 hours
True answer: Unanswerable

Validation ID: ffde866b1203a01580eb33237a0bb9da71c75ecf
Question: How big is Augmented LibriSpeech dataset?
Answer: 1000 hours
True answer: Unanswerable

Validation ID: 6cd8bad8a031ce6d802ded90f9754088e0c8d653
Question: By how much does their best model outperform the state-of-the-art?
Answer: 1.3
True answer: 0.8% F1 better than the best state-of-the-art

Validation ID: 6cd8bad8a031ce6d802ded90f9754088e0c8d653
Question: By how much does their best model outperform the state-of-the-art?
Answer: 1.3
True answer: Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.

Validation ID: 30eacb4595014c9c0e5ee9669103d003cfdfe1e5
Question: Which dataset do they train their models on?
Answer: SemEval 2010 task 8
True answer: relation classification dataset of the SemEval 2010 task 8

Validation ID: 30eacb4595014c9c0e5ee9669103d003cfdfe1e5
Question: Which dataset do they train their models on?
Answer: SemEval 2010 task 8
True answer: SemEval 2010 task 8 BIBREF8

Validation ID: 0f7867f888109b9e000ef68965df4dde2511a55f
Question: How does their simple voting scheme work?
Answer: We apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly.
True answer: we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes, In case of a tie, we pick one of the most frequent classes randomly

Validation ID: 0f7867f888109b9e000ef68965df4dde2511a55f
Question: How does their simple voting scheme work?
Answer: We apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly.
True answer: Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly.

Validation ID: e2e977d7222654ee8d983fd8ba63b930e9a5a691
Question: Which variant of the recurrent neural network do they use?
Answer: Connectionist Bi-directional RNNs
True answer: uni-directional RNN

Validation ID: 0cfe0e33fbb100751fc0916001a5a19498ae8cb5
Question: How do they obtain the new context represetation?
Answer: By splitting the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context.
True answer: They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
Answer: Yes.
True answer: Yes

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
Answer: Yes.
True answer: Unanswerable

Validation ID: 35b3ce3a7499070e9b280f52e2cb0c29b0745380
Question: Does the paper report the performance of the model for each individual language?
Answer: Yes.
True answer: Yes

Validation ID: 71ba1b09bb03f5977d790d91702481cc406b3767
Question: What is the performance of the baseline?
Answer: The baseline achieves a macro-average F1-score of 0.845 on the supervised test set.
True answer: M-Bert had 76.6 F1 macro score.

Validation ID: 71ba1b09bb03f5977d790d91702481cc406b3767
Question: What is the performance of the baseline?
Answer: The baseline achieves a macro-average F1-score of 0.845 on the supervised test set.
True answer: 75.1% and 75.6% accuracy

Validation ID: 612c3675b6c55b60ae6d24265ed8e20f62cb117e
Question: Did they pefrorm any cross-lingual vs single language evaluation?
Answer: Yes, they did.
True answer: Yes

Validation ID: bd40f33452da7711b65faaa248aca359b27fddb6
Question: What was the performance of multilingual BERT?
Answer: The performance of multilingual BERT was good.
True answer: BERT had 76.6 F1 macro score on x-stance dataset.

Validation ID: 787c4d4628eac00dbceb1c96020bff0090edca46
Question: What annotations are present in dataset?
Answer: Stance.
True answer: answer each question with either `yes', `rather yes', `rather no', or `no'., can supplement each answer with a comment of at most 500 characters

Validation ID: 3c3807f226ba72fc41f59f0338f12a49a0c35605
Question: Are the experts comparable to real-world users?
Answer: No. The experts are legal experts, and are not representative of the general public.
True answer: No

Validation ID: c70bafc35e27be9d1efae60596bc0dd390c124c0
Question: Are the answers double (and not triple) annotated?
Answer: Yes, the answers are double annotated.
True answer: Yes

Validation ID: 81d607fc206198162faa54a796717c2805282d9b
Question: Who were the experts used for annotation?
Answer: The experts were recruited from the University of Michigan Law School.
True answer: Individuals with legal training

Validation ID: 81d607fc206198162faa54a796717c2805282d9b
Question: Who were the experts used for annotation?
Answer: The experts were recruited from the University of Michigan Law School.
True answer: Yes

Validation ID: 51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad
Question: What type of neural model was used?
Answer: BERT
True answer: Bert + Unanswerable

Validation ID: 51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad
Question: What type of neural model was used?
Answer: BERT
True answer: CNN, BERT

Validation ID: f0848e7a339da0828278f6803ed7990366c975f0
Question: Were other baselines tested to compare with the neural baseline?
Answer: Yes, we tested a number of baselines, including SVM and CNN.
True answer: SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance

Validation ID: f0848e7a339da0828278f6803ed7990366c975f0
Question: Were other baselines tested to compare with the neural baseline?
Answer: Yes, we tested a number of baselines, including SVM and CNN.
True answer: No-Answer Baseline (NA), Word Count Baseline, Human Performance

Validation ID: b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8
Question: Does the paper clearly establish that the challenges listed here exist in this dataset and task?
Answer: Yes
True answer: Yes

Validation ID: 792f6d76d2befba2af07198584aac1b189583ae4
Question: Is this hashtag prediction task an established task, or something new?
Answer: It is a new task.
True answer: established task

Validation ID: 792f6d76d2befba2af07198584aac1b189583ae4
Question: Is this hashtag prediction task an established task, or something new?
Answer: It is a new task.
True answer: Hashtag prediction for social media has been addressed earlier

Validation ID: 127d5ddfabec5c58832e5865cbd8ed0978c25a13
Question: What is the word-level baseline?
Answer: Word-level baseline is a simple word-level encoder for tweets.
True answer: a simple word-level encoder, The encoder is essentially the same as tweet2vec, with the input as words instead of characters.

Validation ID: 127d5ddfabec5c58832e5865cbd8ed0978c25a13
Question: What is the word-level baseline?
Answer: Word-level baseline is a simple word-level encoder for tweets.
True answer: The encoder is essentially the same as tweet2vec, with the input as words instead of characters.

Validation ID: b91671715ad4fad56c67c28ce6f29e180fe08595
Question: What other tasks do they test their method on?
Answer: Tracking infectious diseases
True answer: None

Validation ID: a6d37b5975050da0b1959232ae756fc09e5f87e8
Question: what is the word level baseline they compare to?
Answer: word level baseline is a simple word-level encoder for tweets. The input tweet is first split into tokens along white-spaces. A more sophisticated tokenizer may be used, but for a fair comparison we wanted to keep language specific preprocessing to a minimum. The encoder is essentially the same as tweet2vec, with the input as words instead of characters. A lookup table stores word vectors for the $V$ (20K here) most common words, and the rest are grouped together under the `UNK' token.
True answer: a simple word-level encoder, with the input as words instead of characters

Validation ID: a6d37b5975050da0b1959232ae756fc09e5f87e8
Question: what is the word level baseline they compare to?
Answer: word level baseline is a simple word-level encoder for tweets. The input tweet is first split into tokens along white-spaces. A more sophisticated tokenizer may be used, but for a fair comparison we wanted to keep language specific preprocessing to a minimum. The encoder is essentially the same as tweet2vec, with the input as words instead of characters. A lookup table stores word vectors for the $V$ (20K here) most common words, and the rest are grouped together under the `UNK' token.
True answer: The encoder is essentially the same as tweet2vec, with the input as words instead of characters

Validation ID: e82fa03f1638a8c59ceb62bb9a6b41b498950e1f
Question: What is the state of the art system mentioned?
Answer: BERT
True answer: Two knowledge-based systems,
two traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.

Validation ID: 7ab9c0b4ceca1c142ff068f85015a249b14282d0
Question: Do they incoprorate WordNet into the model?
Answer: Yes
True answer: construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem

Validation ID: 7ab9c0b4ceca1c142ff068f85015a249b14282d0
Question: Do they incoprorate WordNet into the model?
Answer: Yes
True answer: construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word

Validation ID: 00050f7365e317dc0487e282a4c33804b58b1fb3
Question: Is SemCor3.0 reflective of English language data in general?
Answer: No
True answer: Yes

Validation ID: 00050f7365e317dc0487e282a4c33804b58b1fb3
Question: Is SemCor3.0 reflective of English language data in general?
Answer: No
True answer: Unanswerable

Validation ID: c5b0ed5db65051eebd858beaf303809aa815e8e5
Question: Do they use large or small BERT?
Answer: small
True answer: small BERT

Validation ID: c5b0ed5db65051eebd858beaf303809aa815e8e5
Question: Do they use large or small BERT?
Answer: small
True answer: small BERT

Validation ID: 10fb7dc031075946153baf0a0599e126de29e3a4
Question: How does the neural network architecture accomodate an unknown amount of senses per word?
Answer: It doesn't.
True answer: converts WSD to a sequence learning task,  leverage gloss knowledge, by extending gloss knowledge

Validation ID: e438445cf823893c841b2bc26cdce32ccc3f5cbe
Question: Which fonts are the best indicators of high quality?
Answer: Fonts that are easy to read
True answer: Unanswerable

Validation ID: 12f7fac818f0006cf33269c9eafd41bbb8979a48
Question: What kind of model do they use?
Answer: Inception V3
True answer: visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models

Validation ID: 12f7fac818f0006cf33269c9eafd41bbb8979a48
Question: What kind of model do they use?
Answer: Inception V3
True answer: Inception V3, biLSTM

Validation ID: d5a8fd8bb48dd1f75927e874bdea582b4732a0cd
Question: Did they release their data set of academic papers?
Answer: Yes
True answer: No

Validation ID: d5a8fd8bb48dd1f75927e874bdea582b4732a0cd
Question: Did they release their data set of academic papers?
Answer: Yes
True answer: Unanswerable

Validation ID: 1097768b89f8bd28d6ef6443c94feb04c1a1318e
Question: Do the methods that work best on academic papers also work best on Wikipedia?
Answer: No
True answer: Yes

Validation ID: 1097768b89f8bd28d6ef6443c94feb04c1a1318e
Question: Do the methods that work best on academic papers also work best on Wikipedia?
Answer: No
True answer: No

Validation ID: fc1679c714eab822431bbe96f0e9cf4079cd8b8d
Question: What is their system's absolute accuracy?
Answer: 59.4%
True answer: 59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers

Validation ID: 23e2971c962bb6486bc0a66ff04242170dd22a1d
Question: Which is more useful, visual or textual features?
Answer: Both are useful, but visual features are more useful.
True answer: It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. 

Validation ID: c9bc6f53b941863e801280343afa14248521ce43
Question: Which languages do they use?
Answer: English
True answer: English

Validation ID: c9bc6f53b941863e801280343afa14248521ce43
Question: Which languages do they use?
Answer: English
True answer: English

Validation ID: 07b70b2b799b9efa630e8737df8b1dd1284f032c
Question: How large is their data set?
Answer: 29,794
True answer: a sample of  29,794 wikipedia articles and 2,794 arXiv papers 

Validation ID: 71a0c4f19be4ce1b1bae58a6e8f2a586e125d074
Question: Where do they get their ground truth quality judgments?
Answer: Wikipedia
True answer: Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”)., The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus., The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). 

Validation ID: 71a0c4f19be4ce1b1bae58a6e8f2a586e125d074
Question: Where do they get their ground truth quality judgments?
Answer: Wikipedia
True answer: quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI

Validation ID: c2eb743c9d0baf1781c3c0df9533fab588250af3
Question: Which models did they experiment with?
Answer: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, and (iv) models that integrate lower contexts via peephole connections.
True answer: Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers

Validation ID: c35806cf68220b2b9bb082b62f493393b9bdff86
Question: What were their best results on the benchmark datasets?
Answer: SNLI: 87.0%, MultiNLI: 86.7%, Quora Question Pairs: 91.2%, SST-2: 90.0%, SST-5: 47.0%
True answer: In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5

Validation ID: c35806cf68220b2b9bb082b62f493393b9bdff86
Question: What were their best results on the benchmark datasets?
Answer: SNLI: 87.0%, MultiNLI: 86.7%, Quora Question Pairs: 91.2%, SST-2: 90.0%, SST-5: 47.0%
True answer: accuracy of 87.0%

Validation ID: f7d0fa52017a642a9f70091a252857fccca31f12
Question: What were the baselines?
Answer: The baselines were the models that used plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrated lower contexts via peephole connections.
True answer: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections

Validation ID: 01209a3bead7c87bcdc628be2a5a26b41abde9d1
Question: Which datasets were used?
Answer: SNLI, MultiNLI, Quora Question Pairs, SST
True answer: SNLI BIBREF22 and MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24,  Stanford Sentiment Treebank (SST) BIBREF25

Validation ID: 01209a3bead7c87bcdc628be2a5a26b41abde9d1
Question: Which datasets were used?
Answer: SNLI, MultiNLI, Quora Question Pairs, SST
True answer: SNLI BIBREF22 and MultiNLI BIBREF23 datasets, Quora Question Pairs dataset BIBREF24, Stanford Sentiment Treebank (SST) BIBREF25

Validation ID: 2740e3d7d33173664c1c5ab292c7ec75ff6e0802
Question: what datasets were used?
Answer: WikiNews dataset
True answer: diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31, WikiNews test set BIBREF31,  large collection of fully diacritized classical texts (2.7M tokens) from a book publisher

Validation ID: 2740e3d7d33173664c1c5ab292c7ec75ff6e0802
Question: what datasets were used?
Answer: WikiNews dataset
True answer: the diacritized corpus that was used to train the RDI BIBREF7 diacritizer , WikiNews , a large collection of fully diacritized classical texts

Validation ID: db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21
Question: what are the previous state of the art?
Answer: Farasa, MADAMIRA, RDI, MIT, Microsoft ATKS
True answer: Farasa, RDI

Validation ID: db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21
Question: what are the previous state of the art?
Answer: Farasa, MADAMIRA, RDI, MIT, Microsoft ATKS
True answer: Farasa BIBREF31, MADAMIRA BIBREF29, RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), Microsoft ATKS BIBREF28

Validation ID: 48bd71477d5f89333fa7ce5c4556e4d950fb16ed
Question: what surface-level features are used?
Answer: word surface forms, stems, prefixes, and suffixes
True answer: affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities

Validation ID: 76ed74788e3eb3321e646c48ae8bf6cdfe46dca1
Question: what linguistics features are used?
Answer: CHAR, SEG, PRIOR, CASE
True answer: POS, gender/number and stem POS

Validation ID: ad1be65c4f0655ac5c902d17f05454c0d4c4a15d
Question: what dataset statistics are provided?
Answer: The number of questions, the number of questions that require commonsense knowledge, the number of questions that can be answered from the text, the number of questions that can be answered from the text and the number of questions that can be answered from the text.
True answer: More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).

Validation ID: ad1be65c4f0655ac5c902d17f05454c0d4c4a15d
Question: what dataset statistics are provided?
Answer: The number of questions, the number of questions that require commonsense knowledge, the number of questions that can be answered from the text, the number of questions that can be answered from the text and the number of questions that can be answered from the text.
True answer: Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text

Validation ID: 2eb9280d72cde9de3aabbed993009a98a5fe0990
Question: what is the size of their dataset?
Answer: 2,100 texts and a total of approx. 14,000 questions
True answer: 13,939

Validation ID: 154a721ccc1d425688942e22e75af711b423e086
Question: what crowdsourcing platform was used?
Answer: Amazon Mechanical Turk
True answer: Amazon Mechanical Turk

Validation ID: 154a721ccc1d425688942e22e75af711b423e086
Question: what crowdsourcing platform was used?
Answer: Amazon Mechanical Turk
True answer: Amazon Mechanical Turk

Validation ID: 84bad9a821917cb96584cf5383c6d2a035358d7c
Question: how was the data collected?
Answer: We collected the data using Amazon Mechanical Turk.
True answer: The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.

Validation ID: c9305e5794b65b33399c22ac8e4e024f6b757a30
Question: What is best performing model among author's submissions, what performance it had?
Answer: BERT
True answer: For SLC task, the "ltuorp" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the "newspeak" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).

Validation ID: 56b7319be68197727baa7d498fa38af0a8440fe4
Question: What extracted features were most influencial on performance?
Answer: linguistic, layout and topical features
True answer: Linguistic

Validation ID: 56b7319be68197727baa7d498fa38af0a8440fe4
Question: What extracted features were most influencial on performance?
Answer: linguistic, layout and topical features
True answer: BERT

Validation ID: 2268c9044e868ba0a16e92d2063ada87f68b5d03
Question: Did ensemble schemes help in boosting peformance, by how much?
Answer: Yes, ensemble schemes helped in boosting performance by 0.02 F1.
True answer: The best ensemble topped the best single model by 0.029 in F1 score on dev (external).

Validation ID: 2268c9044e868ba0a16e92d2063ada87f68b5d03
Question: Did ensemble schemes help in boosting peformance, by how much?
Answer: Yes, ensemble schemes helped in boosting performance by 0.02 F1.
True answer: They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification

Validation ID: 6b7354d7d715bad83183296ce2f3ddf2357cb449
Question: Which basic neural architecture perform best by itself?
Answer: LSTM-CRF
True answer: BERT

Validation ID: e949b28f6d1f20e18e82742e04d68158415dc61e
Question: What participating systems had better results than ones authors submitted?
Answer: None
True answer: For SLC task : Ituorp, ProperGander and YMJA  teams had better results.
For FLC task: newspeak and Antiganda teams had better results.

Validation ID: a1ac2a152710335519c9a907eec60d9f468b19db
Question: What is specific to multi-granularity and multi-tasking neural arhiteture design?
Answer: multi-tasking with another auxiliary task of PFD
True answer: An output layer for each task

Validation ID: a1ac2a152710335519c9a907eec60d9f468b19db
Question: What is specific to multi-granularity and multi-tasking neural arhiteture design?
Answer: multi-tasking with another auxiliary task of PFD
True answer: Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT.

Validation ID: ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25
Question: Do they report results only on English data?
Answer: Yes
True answer: Unanswerable

Validation ID: 49eb52b3ec0647e165a5e41488088c80a20cc78f
Question: What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?
Answer: The instructor's reply is based on the context of the previous post.
True answer: context inference

Validation ID: 9bb7ae50bff91571a945c1af025ed2e67714a788
Question: What was the previous state of the art for this task?
Answer: BIBREF7
True answer: hLSTM

Validation ID: 9bb7ae50bff91571a945c1af025ed2e67714a788
Question: What was the previous state of the art for this task?
Answer: BIBREF7
True answer: hLSTM

Validation ID: 81dbe9a9ddaa5d02b02e01a306d898015a56ffb6
Question: What type of latent context is used to predict instructor intervention?
Answer: A series of linear contiguous posts
True answer: the series of posts that trigger an intervention

Validation ID: 348886b4762db063711ef8b7a10952375fbdcb57
Question: Do they report results only on English dataset?
Answer: No
True answer: No

Validation ID: 348886b4762db063711ef8b7a10952375fbdcb57
Question: Do they report results only on English dataset?
Answer: No
True answer: No

Validation ID: 1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b
Question: What dataset does this approach achieve state of the art results on?
Answer: Multi30K
True answer: the English-German dataset

Validation ID: f9aa055bf73185ba939dfb03454384810eb17ad1
Question: How much training data from the non-English language is used by the system?
Answer: None
True answer: No data. Pretrained model is used.

Validation ID: d571e0b0f402a3d36fb30d70cdcd2911df883bc7
Question: Is the system tested on low-resource languages?
Answer: Yes
True answer: Yes

Validation ID: d571e0b0f402a3d36fb30d70cdcd2911df883bc7
Question: Is the system tested on low-resource languages?
Answer: Yes
True answer: Yes

Validation ID: ce2b921e4442a21555d65d8ce4ef7e3bde931dfc
Question: What languages are the model transferred to?
Answer: French, Russian, Arabic, Chinese, Hindi, and Vietnamese
True answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)

Validation ID: ce2b921e4442a21555d65d8ce4ef7e3bde931dfc
Question: What languages are the model transferred to?
Answer: French, Russian, Arabic, Chinese, Hindi, and Vietnamese
True answer: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)

Validation ID: 2275b0e195cd9cb25f50c5c570da97a4cce5dca8
Question: How is the model transferred to other languages?
Answer: We develop a technique to rapidly transfer an existing pre-trained model from English to other languages in an energy efficient way.
True answer: Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM.

Validation ID: 37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6
Question: What metrics are used for evaluation?
Answer: LAS
True answer: translation probabilities, Labeled Attachment Scores (LAS)

Validation ID: 37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6
Question: What metrics are used for evaluation?
Answer: LAS
True answer: accuracy, Labeled Attachment Scores (LAS)

Validation ID: d01c51155e4719bf587d114bcd403b273c77246f
Question: What datasets are used for evaluation?
Answer: XNLI and Universal Dependencies v2.4
True answer: United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018

Validation ID: 9b4dc790e4ff49562992aae4fad3a38621fadd8b
Question: what are the existing approaches?
Answer: 
True answer: BOW-Tags, BOW-KL(Tags), BOW-All, GloVe

Validation ID: a1dac888f63c9efaf159d9bdfde7c938636f07b1
Question: what dataset is used in this paper?
Answer: Flickr tags
True answer:  the same datasets as BIBREF7

Validation ID: a1dac888f63c9efaf159d9bdfde7c938636f07b1
Question: what dataset is used in this paper?
Answer: Flickr tags
True answer: same datasets as BIBREF7

Validation ID: 1e4dbfc556cf237accb8b370de2f164fa723687b
Question: How is keyphrase diversity measured?
Answer: We use two metrics, one quantitative and one qualitative.
True answer: average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set

Validation ID: fff5c24dca92bc7d5435a2600e6764f039551787
Question: How was the StackExchange dataset collected?
Answer: We use the public StackExchange data dump.
True answer: they obtained computer science related topics by looking at titles and user-assigned tags

Validation ID: fff5c24dca92bc7d5435a2600e6764f039551787
Question: How was the StackExchange dataset collected?
Answer: We use the public StackExchange data dump.
True answer: Unanswerable

Validation ID: b2ecfd5480a2a4be98730e2d646dfb84daedab17
Question: What does the TextWorld ACG dataset contain?
Answer: TextWorld is a new benchmark for text-based games. It contains a set of procedurally generated text-based games, which are used to train and evaluate reinforcement learning agents.
True answer: Unanswerable

Validation ID: a3efe43a72b76b8f5e5111b54393d00e6a5c97ab
Question: What is the size of the StackExchange dataset?
Answer: 6.8 million
True answer: Unanswerable

Validation ID: a3efe43a72b76b8f5e5111b54393d00e6a5c97ab
Question: What is the size of the StackExchange dataset?
Answer: 6.8 million
True answer: around 332k questions

Validation ID: f1e90a553a4185a4b0299bd179f4f156df798bce
Question: What were the baselines?
Answer: CopyRNN, KEA, Maui, TextRank, SingleRank, PositionRank, TopicRank, Topical PageRank, MultipartiteRank, TF-IDF, TextTeaser, LexRank, SumBasic, LSA, LDA, PLSA, PLSA-LDA, PLSA-LDA-LSA, PLSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-PLSA, PLSA-LDA-LSA-LDA-PLSA-LDA, PLSA-LDA-LSA-LDA-PLSA-LDA-LSA, PLSA-LDA-LSA-LDA-PLSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-PLSA-LDA-LSA-LDA-PLSA, PLSA-LDA-LSA-LDA-PLSA-LDA-LSA-LDA-PLSA-LDA, PLSA-LDA-LSA-LDA-PLSA-LDA-LSA-LDA-PLSA-LDA-LSA, PLSA-L
True answer: CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)

Validation ID: f1e90a553a4185a4b0299bd179f4f156df798bce
Question: What were the baselines?
Answer: CopyRNN, KEA, Maui, TextRank, SingleRank, PositionRank, TopicRank, Topical PageRank, MultipartiteRank, TF-IDF, TextTeaser, LexRank, SumBasic, LSA, LDA, PLSA, PLSA-LDA, PLSA-LDA-LSA, PLSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-LSA, PLSA-LDA-LSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA-LSA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA, PLSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA-LSA-LDA-LSA, PLSA-L
True answer: CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*

Validation ID: 19b7312cfdddb02c3d4eaa40301a67143a72a35a
Question: What two metrics are proposed?
Answer: F INLINEFORM0 @ INLINEFORM1 and F INLINEFORM2 @ INLINEFORM3
True answer: average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )

Validation ID: 22744c3bc68f120669fc69490f8e539b09e34b94
Question: Can the findings of this paper be generalized to a general-purpose task?
Answer: Yes
True answer: Yes

Validation ID: 22744c3bc68f120669fc69490f8e539b09e34b94
Question: Can the findings of this paper be generalized to a general-purpose task?
Answer: Yes
True answer: Yes

Validation ID: dcea88698949da4a1bd00277c06df06c33f6a5ff
Question: Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks?
Answer: The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.
True answer: The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.

Validation ID: d7b60abb0091246e29d1a9c28467de598e090c20
Question: What was the baseline?
Answer: 7.8%
True answer: stochastic gradient descent, naive bayes, decision tree

Validation ID: d7b60abb0091246e29d1a9c28467de598e090c20
Question: What was the baseline?
Answer: 7.8%
True answer: Unanswerable

Validation ID: bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e
Question: What was their system's performance?
Answer: 89.6%
True answer: accuracy and F1-score of 89.6% and 89.2%, respectively

Validation ID: bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e
Question: What was their system's performance?
Answer: 89.6%
True answer: accuracy and F1-score of 89.6% and 89.2%, respectively

Validation ID: 5a6926de13a8cc25ce687c22741ba97a6e63d4ee
Question: What other political events are included in the database?
Answer: US presidential primaries, Democratic and Republican National Conventions
True answer: US presidential primaries, Democratic and Republican National Conventions

Validation ID: dcc1115aeaf87118736e86f3e3eb85bf5541281c
Question: What classifier did they use?
Answer: Random Forest
True answer: Random Forest

Validation ID: c74185bced810449c5f438f11ed6a578d1e359b4
Question: What labels for antisocial events are available in datasets?
Answer: Wikipedia: personal attack, CMV: moderator action
True answer: The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: "Don't be rude or hostile to others users."

Validation ID: 88e5d37617e14d6976cc602a168332fc23644f19
Question: What are two datasets model is applied to?
Answer: Wikipedia and CMV
True answer:  `Conversations Gone Awry' dataset, subreddit ChangeMyView

Validation ID: 88e5d37617e14d6976cc602a168332fc23644f19
Question: What are two datasets model is applied to?
Answer: Wikipedia and CMV
True answer: An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. 

Validation ID: 45f7c03a686b68179cadb1413c5f3c1d373328bd
Question: What is the CORD-19 dataset?
Answer: CORD-19 dataset is a collection of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses.
True answer: which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses

Validation ID: 45f7c03a686b68179cadb1413c5f3c1d373328bd
Question: What is the CORD-19 dataset?
Answer: CORD-19 dataset is a collection of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses.
True answer: contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses

Validation ID: a2015f02dfb376bf9b218d1c897018f4e70424d7
Question: How large is the collection of COVID-19 literature?
Answer: 45,000 scholarly articles
True answer: 45,000 scholarly articles, including over 33,000 with full text

Validation ID: f697d00a82750b14376fe20a5a2b249e98bebe9b
Question: Which deep learning architecture do they use for sentence segmentation?
Answer: Bi-LSTM-CRF
True answer: Bi-LSTM-CRF

Validation ID: f697d00a82750b14376fe20a5a2b249e98bebe9b
Question: Which deep learning architecture do they use for sentence segmentation?
Answer: Bi-LSTM-CRF
True answer: Bi-LSTM-CRF

Validation ID: e0e379e546f1da9da874a2e90c79b41c60feb817
Question: How do they utilize unlabeled data to improve model representations?
Answer: Cross-View Training
True answer: During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data.

Validation ID: 70148c8d0f345ea36200d5ba19d021924d98e759
Question: What is the McGurk effect?
Answer: The McGurk effect is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see.
True answer: a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard

Validation ID: 70148c8d0f345ea36200d5ba19d021924d98e759
Question: What is the McGurk effect?
Answer: The McGurk effect is the well-studied phenomenon by which the perception of what we hear can be influenced by what we see.
True answer: When the perception of what we hear is influenced by what we see.

Validation ID: 27cf16bc9ef71761b9df6217f00f39f21130ce15
Question: Are humans and machine learning systems fooled by the same kinds of illusions?
Answer: Yes
True answer: Unanswerable

Validation ID: 627b8d7b5b985394428c974aca5ba0c1bbbba377
Question: how many humans evaluated the results?
Answer: 700
True answer: Unanswerable

Validation ID: 627b8d7b5b985394428c974aca5ba0c1bbbba377
Question: how many humans evaluated the results?
Answer: 700
True answer: Unanswerable

Validation ID: 126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0
Question: what was the baseline?
Answer: the baseline was the model used for back-translation
True answer:  MT system on the data released by BIBREF11

Validation ID: 126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0
Question: what was the baseline?
Answer: the baseline was the model used for back-translation
True answer: Transformer base, two-pass CADec model

Validation ID: 7e4ef0a4debc048b244b61b4f7dc2518b5b466c0
Question: what phenomena do they mention is hard to capture?
Answer: VP ellipsis
True answer: Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection.

Validation ID: b68f72aed961d5ba152e9dc50345e1e832196a76
Question: by how much did the BLEU score improve?
Answer: 0.7
True answer: On average 0.64 

Validation ID: cf874cd9023d901e10aa8664b813d32501e7e4d2
Question: What is NER?
Answer: Named Entity Recognition
True answer: Named Entity Recognition

Validation ID: cf874cd9023d901e10aa8664b813d32501e7e4d2
Question: What is NER?
Answer: Named Entity Recognition
True answer: Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain

Validation ID: 42084c41343e5a6ae58a22e5bfc5ce987b5173de
Question: Does the paper explore extraction from electronic health records?
Answer: Yes
True answer: Yes

Validation ID: b637d6393ef3af7462917b81861531022b291933
Question: Does jiant involve datasets for the 50 NLU tasks?
Answer: Yes
True answer: Yes

Validation ID: 8b9c12df9f89040f1485b3847a29f11b5c9262e0
Question: Is jiant compatible with models in any programming language?
Answer: No
True answer: Yes

Validation ID: 8b9c12df9f89040f1485b3847a29f11b5c9262e0
Question: Is jiant compatible with models in any programming language?
Answer: No
True answer: Unanswerable

Validation ID: 72e4e26d0dd79c590c28b10938952a9f9497ff1e
Question: What models are used for painting embedding and what for language style transfer?
Answer: CNN-RNN based image-to-poem net and seq2seq model with parallel text corpus
True answer: generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models

Validation ID: 63b92dcc701ec77fdb3355ede5d37d2fbf057bcc
Question: What applicability of their approach is demonstrated by the authors?
Answer: Generating Shakespearean prose for a given painting
True answer: Unanswerable

Validation ID: 58ee0cbf1d8e3711c617b1cd3d7aca8620e26187
Question: What limitations do the authors demnostrate of their model?
Answer: The generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score.
True answer: Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer

Validation ID: 58ee0cbf1d8e3711c617b1cd3d7aca8620e26187
Question: What limitations do the authors demnostrate of their model?
Answer: The generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score.
True answer: we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for "Starry Night" with a low average content score

Validation ID: f71b52e00e0be80c926f153b9fe0a06dd93af11e
Question: How does final model rate on Likert scale?
Answer: 3.9
True answer: average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9 

Validation ID: f71b52e00e0be80c926f153b9fe0a06dd93af11e
Question: How does final model rate on Likert scale?
Answer: 3.9
True answer: average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9

Validation ID: 8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe
Question: How big is English poem description of the painting dataset?
Answer: 18,395 sentences
True answer: Unanswerable

Validation ID: 54e945ea4b014e11ed4e1e61abc2aa9e68fea310
Question: What is best BLEU score of language style transfer authors got?
Answer: 29.65
True answer: seq2seq model with global attention gives the best results with an average target BLEU score of 29.65

Validation ID: 54e945ea4b014e11ed4e1e61abc2aa9e68fea310
Question: What is best BLEU score of language style transfer authors got?
Answer: 29.65
True answer: average target BLEU score of 29.65

Validation ID: df0257ab04686ddf1c6c4d9b0529a7632330b98e
Question: How better does new approach behave than existing solutions?
Answer: Our experiments show promising results in three settings, with improved generalization and sample efficiency compared to existing methods.
True answer:  On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model

Validation ID: df0257ab04686ddf1c6c4d9b0529a7632330b98e
Question: How better does new approach behave than existing solutions?
Answer: Our experiments show promising results in three settings, with improved generalization and sample efficiency compared to existing methods.
True answer: On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.
On Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.

Validation ID: 568fb7989a133564d84911e7cb58e4d8748243ef
Question: How is trajectory with how rewards extracted?
Answer: Go-Explore
True answer: explores the state space through keeping track of previously visited states by maintaining an archive

Validation ID: 2c947447d81252397839d58c75ebcc71b34379b5
Question: On what Text-Based Games are experiments performed?
Answer: CoinCollector, CookingWorld
True answer: CoinCollector , CookingWorld 

Validation ID: 2c947447d81252397839d58c75ebcc71b34379b5
Question: On what Text-Based Games are experiments performed?
Answer: CoinCollector, CookingWorld
True answer: CoinCollector, CookingWorld

Validation ID: c01784b995f6594fdb23d7b62f20a35ae73eaa77
Question: How do the authors show that their learned policy generalize better than existing solutions to unseen games?
Answer: They show that their learned policy generalize better than existing solutions to unseen games by comparing the performance of their learned policy on unseen games with the performance of existing solutions on unseen games.
True answer: promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game

Validation ID: 3415762847ed13acc3c90de60e3ef42612bc49af
Question: How much is classification performance improved in experiments for low data regime and class-imbalance problems?
Answer: 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000
True answer: Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline
Imbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000

Validation ID: 223dc2b9ea34addc0f502003c2e1c1141f6b36a7
Question: What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?
Answer: BIBREF7
True answer: BIBREF7

Validation ID: 223dc2b9ea34addc0f502003c2e1c1141f6b36a7
Question: What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?
Answer: BIBREF7
True answer:  reward learning algorithm BIBREF7

Validation ID: e1ab11885f72b4658263a60751d956ba661c1d61
Question: What subtasks did they participate in?
Answer: Four Spanish subtasks
True answer: Answer with content missing: (Subscript 1: "We did not participate in subtask 5 (E-c)") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks.

Validation ID: c85b6f9bafc4c64fc538108ab40a0590a2f5768e
Question: What were the scores of their system?
Answer: second, second, fourth and fifth place
True answer: column Ens Test in Table TABREF19

Validation ID: 8e52637026bee9061f9558178eaec08279bf7ac6
Question: How was the training data translated?
Answer: Machine translation
True answer: using the machine translation platform Apertium 

Validation ID: 8e52637026bee9061f9558178eaec08279bf7ac6
Question: How was the training data translated?
Answer: Machine translation
True answer: machine translation platform Apertium BIBREF5

Validation ID: 0f6216b9e4e59252b0c1adfd1a848635437dfcdc
Question: What dataset did they use?
Answer: DISC
True answer:  Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish.

Validation ID: 0f6216b9e4e59252b0c1adfd1a848635437dfcdc
Question: What dataset did they use?
Answer: DISC
True answer: Spanish tweets were scraped between November 8, 2017 and January 12, 2018, Affect in Tweets Distant Supervision Corpus (DISC)

Validation ID: 22ccee453e37536ddb0c1c1d17b0dbac04c6c607
Question: What other languages did they translate the data from?
Answer: English
True answer: English 

Validation ID: 22ccee453e37536ddb0c1c1d17b0dbac04c6c607
Question: What other languages did they translate the data from?
Answer: English
True answer: English

Validation ID: d00bbeda2a45495e6261548710afa6b21ea32870
Question: What semi-supervised learning is applied?
Answer: The DISC corpus was used.
True answer: first a model is trained on the training set and then this model is used to predict the labels of the silver data, This silver data is then simply added to our training set, after which the model is retrained

Validation ID: 71b1af123fe292fd9950b8439db834212f0b0e32
Question: How were the datasets annotated?
Answer: The translated pairs must preserve the semantic relations between the two words when possible. This means that, when multiple translations are possible, the translation that best conveys the semantic relation between the two words found in the original English pair is selected.
True answer: 1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process

Validation ID: a616a3f0d244368ec588f04dfbc37d77fda01b4c
Question: What are the 12 languages covered?
Answer: English, German, French, Spanish, Italian, Russian, Mandarin, Kiswahili, Welsh, Turkish, Farsi, and Yue Chinese
True answer: Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese

Validation ID: a616a3f0d244368ec588f04dfbc37d77fda01b4c
Question: What are the 12 languages covered?
Answer: English, German, Spanish, French, Italian, Russian, Mandarin, Kiswahili, Welsh, Turkish, Farsi, and Yue Chinese
True answer: Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
Answer: No
True answer: Yes

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
Answer: No
True answer: Unanswerable

Validation ID: 8e44c02c2d9fa56fb74ace35ee70a5add50b52ae
Question: Does the corpus contain only English documents?
Answer: Yes
True answer: Unanswerable

Validation ID: 1522ccedbb1f668958f24cca070f640274bc2549
Question: What type of evaluation is proposed for this task?
Answer: extrinsic
True answer: Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2

Validation ID: 97466a37525536086ed5d6e5ed143df085682318
Question: What baseline system is proposed?
Answer: We use the same approach as in BIBREF40, which is based on the idea that important concepts are mentioned more often than unimportant ones. We first extract all concepts from the documents and then count their occurrences. The most frequent concepts are then selected as the summary.
True answer: Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction.

Validation ID: e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a
Question: How were crowd workers instructed to identify important elements in large document collections?
Answer: They were asked to read a short description of the topic and then select important elements from a list of propositions.
True answer: provide only a description of the document cluster's topic along with the propositions

Validation ID: e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a
Question: How were crowd workers instructed to identify important elements in large document collections?
Answer: They were asked to read a short description of the topic and then select important elements from a list of propositions.
True answer: They break down the task of importance annotation to the level of single propositions and obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary.

Validation ID: d6191c4643201262a770947fc95a613f57bedb6b
Question: Which collections of web documents are included in the corpus?
Answer: DIP corpus
True answer: DIP corpus BIBREF37

Validation ID: ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8
Question: How do the authors define a concept map?
Answer: A concept map is a labeled graph showing concepts as nodes and relationships between them as edges.
True answer: concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges

Validation ID: fc4ae12576ea3a85ea6d150b46938890d63a7d18
Question: Is the LSTM baseline a sub-word model?
Answer: No
True answer: Yes

Validation ID: fc4ae12576ea3a85ea6d150b46938890d63a7d18
Question: Is the LSTM baseline a sub-word model?
Answer: No
True answer: Yes

Validation ID: 19cf7884c0c509c189b1e74fe92c149ff59e444b
Question: How is pseudo-perplexity defined?
Answer: Pseudo-perplexity is defined as the inverse probability of the set of the tokens and taking the $T^{th}$ root were $T$ is the number of tokens
True answer: Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated.

Validation ID: ecd5770cf8cb12cb34285e26ab834301c17c53e1
Question: What is the model architecture used?
Answer: LSTM-CNN
True answer: LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.

Validation ID: ecd5770cf8cb12cb34285e26ab834301c17c53e1
Question: What is the model architecture used?
Answer: LSTM-CNN
True answer: random forest, The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.

Validation ID: 4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729
Question: How is the data used for training annotated?
Answer: The data is annotated by humans.
True answer: The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions.

Validation ID: 5529f26f72ce47440c2a64248063a6d5892b9fde
Question: what quantitative analysis is done?
Answer: We show the loss function analysis of transformer and our model. Figure FIGREF23 shows the validation performance of transformer against global training steps. Figure FIGREF21 show the validation performance of our model for the same number of global steps. Figure FIGREF22 shows that NMT loss is going down with the number of steps, while Figure FIGREF20 shows the degradation of the loss of our proposed RAT task.
True answer: Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP).

Validation ID: f85ca6135b101736f5c16c5b5d40895280016023
Question: what are the baselines?
Answer: transformer
True answer: the baseline transformer BIBREF8

Validation ID: f85ca6135b101736f5c16c5b5d40895280016023
Question: what are the baselines?
Answer: transformer
True answer: baseline transformer BIBREF8

Validation ID: 5fa36dc8f7c4e65acb962fc484989d20b8fdaeec
Question: Do they report results only on English data?
Answer: Yes, they report results only on English data.
True answer: Unanswerable

Validation ID: 5fa36dc8f7c4e65acb962fc484989d20b8fdaeec
Question: Do they report results only on English data?
Answer: Yes, they report results only on English data.
True answer: Yes

Validation ID: d98847340e46ffe381992f1a594e75d3fb8d385e
Question: What machine learning and deep learning methods are used for RQE?
Answer: Logistic Regression and Deep Learning
True answer: Logistic Regression, neural networks

Validation ID: 7006c66a15477b917656f435d66f63760d33a304
Question: by how much did nus outperform abus?
Answer: 4.4%
True answer: Average success rate is higher by 2.6 percent points.

Validation ID: a15bc19674d48cd9919ad1cf152bf49c88f4417d
Question: what corpus is used to learn behavior?
Answer: DSTC2
True answer: DSTC2

Validation ID: a15bc19674d48cd9919ad1cf152bf49c88f4417d
Question: what corpus is used to learn behavior?
Answer: DSTC2
True answer: The manual transcriptions of the DSTC2 training set 

Validation ID: 440faf8d0af8291d324977ad0f68c8d661fe365e
Question: Which dataset has been used in this work?
Answer: Reuters-8
True answer: Reuters-8 dataset without stop words

Validation ID: 440faf8d0af8291d324977ad0f68c8d661fe365e
Question: Which dataset has been used in this work?
Answer: Reuters-8
True answer: The Reuters-8 dataset (with stop words removed)

Validation ID: 0ec56e15005a627d0b478a67fd627a9d85c3920e
Question: What can word subspace represent?
Answer: Text data
True answer: Word vectors, usually in the context of others within the same class

Validation ID: a712718e6596ba946f29a99838d82f95b9ebb1ce
Question: How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?
Answer: We have an improvement of 12.27% on the accuracy and 14.86% on F1 score.
True answer: 7.36% on accuracy and 9.69% on F1 score

Validation ID: a712718e6596ba946f29a99838d82f95b9ebb1ce
Question: How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?
Answer: We have an improvement of 12.27% on the accuracy and 14.86% on F1 score.
True answer: it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too

Validation ID: 3116453e35352a3a90ee5b12246dc7f2e60cfc59
Question: To what baseline models is proposed model compared?
Answer: CNN, LSTM, LSTM-soft, LSTM-self
True answer: support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self

Validation ID: 3116453e35352a3a90ee5b12246dc7f2e60cfc59
Question: To what baseline models is proposed model compared?
Answer: CNN, LSTM, LSTM-soft, LSTM-self
True answer: support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self

Validation ID: dfca00be3284cc555a6a4eac4831471fb1f5875b
Question: How big is dataset for testing?
Answer: 14.56 samples
True answer: 30 terms, each term-sanse pair has around 15 samples for testing

Validation ID: a9a532399237b514c1227f2d6be8601474e669be
Question: What existing dataset is re-examined and corrected for training?
Answer: UM Inventory
True answer:  UM Inventory 

Validation ID: 26126068d72408555bcb52977cd669faf660bdf7
Question: What are the qualitative experiments performed on benchmark datasets?
Answer: Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.
True answer: Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.
Evaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.

Validation ID: 26126068d72408555bcb52977cd669faf660bdf7
Question: What are the qualitative experiments performed on benchmark datasets?
Answer: Table TABREF9 shows the qualitative results of GM$\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses.
True answer: Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed

Validation ID: 660284b0a21fe3801e64dc9e0e51da5400223fe3
Question: How does this approach compare to other WSD approaches employing word embeddings?
Answer: The proposed approach is different from other WSD approaches employing word embeddings in the following ways:
True answer: GM$\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.

Validation ID: c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd
Question: What tasks did they use to evaluate performance for male and female speakers?
Answer: They used the same tasks for both male and female speakers.
True answer: ASR

Validation ID: c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd
Question: What tasks did they use to evaluate performance for male and female speakers?
Answer: They used the same tasks for both male and female speakers.
True answer: Unanswerable

Validation ID: f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3
Question: What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?
Answer: The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to understand how women under-representation in broadcast data can lead to bias in ASR performances.
True answer: create fair systems

Validation ID: f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3
Question: What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?
Answer: The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to understand how women under-representation in broadcast data can lead to bias in ASR performances.
True answer:  broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools

Validation ID: a253749e3b4c4f340778235f640ce694642a4555
Question: Which corpora does this paper analyse?
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: a253749e3b4c4f340778235f640ce694642a4555
Question: Which corpora does this paper analyse?
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: 1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891
Question: How many categories do authors define for speaker role?
Answer: 2
True answer:  two salient roles called Anchors and Punctual speakers

Validation ID: 777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47
Question: How big is imbalance in analyzed corpora?
Answer: 65% of the speakers are men, speaking more than 75% of the time.
True answer: Women represent 33.16% of the speakers

Validation ID: 2da4c3679111dd92a1d0869dae353ebe5989dfd2
Question: What are four major corpora of French broadcast?
Answer: ESTER1, ESTER2, ETAPE and REPERE
True answer: ESTER1, ESTER2, ETAPE, REPERE

Validation ID: b7c3f3942a07c118e57130bc4c3ec4adc431d725
Question: What did the best systems use for their model?
Answer: ULMFiT
True answer: Unanswerable

Validation ID: a5505e25ee9ae84090e1442034ddbb3cedabcf04
Question: What were their results on the classification and regression tasks?
Answer: 3rd and 2nd
True answer: F1 of 0.8099

Validation ID: a5505e25ee9ae84090e1442034ddbb3cedabcf04
Question: What were their results on the classification and regression tasks?
Answer: 3rd and 2nd
True answer: F1 score result of 0.8099

Validation ID: 1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf
Question: Do the authors conduct experiments on the tasks mentioned?
Answer: Yes
True answer: Yes

Validation ID: 1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf
Question: Do the authors conduct experiments on the tasks mentioned?
Answer: Yes
True answer: No

Validation ID: 7fa3c2c0cf7f559d43e84076a9113a390c5ba03a
Question: Did they collect their own datasets?
Answer: No
True answer: No

Validation ID: 9a7ba5ed1779c664d2cac92494a43517d3e87c96
Question: What data do they look at?
Answer: A collection of sentences
True answer: WSC collection

Validation ID: 662870a90890c620a964720b2ca122a1139410ea
Question: What language do they explore?
Answer: French
True answer: English, French, German 

Validation ID: 662870a90890c620a964720b2ca122a1139410ea
Question: What language do they explore?
Answer: French
True answer: French, English, Spanish, Italian, Portuguese, Hebrew, Arabic

Validation ID: 92d1a6df3041667dc662376938bc65527a5a1c3c
Question: Do they report results only on English datasets?
Answer: Yes
True answer: Yes

Validation ID: 92d1a6df3041667dc662376938bc65527a5a1c3c
Question: Do they report results only on English datasets?
Answer: Yes
True answer: Yes

Validation ID: 12159f04e0427fe33fa05af6ba8c950f1a5ce5ea
Question: Which hyperparameters were varied in the experiments on the four tasks?
Answer: The number of clusters
True answer: number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding

Validation ID: 12159f04e0427fe33fa05af6ba8c950f1a5ce5ea
Question: Which hyperparameters were varied in the experiments on the four tasks?
Answer: The number of clusters
True answer: different number of clusters, different embeddings

Validation ID: a4a1fcef760b133e9aa876ac28145ad98a609927
Question: Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?
Answer: Dimensionality of the word vectors
True answer: selection of word vectors

Validation ID: 63bb2040fa107c5296351c2b5f0312336dad2863
Question: How were the cluster extracted??
Answer: We cluster the embeddings with INLINEFORM0 -Means. The k-means clusters are initialized using “k-means++” as proposed in BIBREF9, while the algorithm is run for 300 iterations. We try different values for INLINEFORM1. For each INLINEFORM2, we repeat the clustering experiment with different seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia.
True answer: Word clusters are extracted using k-means on word embeddings

Validation ID: 01f4a0a19467947a8f3bdd7ec9fac75b5222d710
Question: what were the evaluation metrics?
Answer: perplexity, syntactic evaluation, grammaticality judgment
True answer: INLINEFORM0 scores

Validation ID: 01f4a0a19467947a8f3bdd7ec9fac75b5222d710
Question: what were the evaluation metrics?
Answer: perplexity, syntactic evaluation, grammaticality judgment
True answer: Unlabeled sentence-level F1, perplexity, grammatically judgment performance

Validation ID: 7784d321ccc64db5141113b6783e4ba92fdd4b20
Question: what are the state of the art methods?
Answer: neural network-based approaches
True answer: Unanswerable

Validation ID: 7784d321ccc64db5141113b6783e4ba92fdd4b20
Question: what are the state of the art methods?
Answer: neural network-based approaches
True answer: Unanswerable

Validation ID: 218615a005f7f00606223005fef22c07057d9d77
Question: what english datasets were used?
Answer: PTB
True answer: Answer with content missing: (Data section) Penn Treebank (PTB)

Validation ID: 867290103f762e1ddfa6f2ea30dd0a327f595182
Question: which chinese datasets were used?
Answer: PTB
True answer: Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)

Validation ID: 907b3af3cfaf68fe188de9467ed1260e52ec6cf1
Question: What were their distribution results?
Answer: The distribution of the number of followers, the number of URLs on tweets, and the verification of the users.
True answer: Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different

Validation ID: 56a8826cbee49560592b2d4b47b18ada236a12b9
Question: How did they determine fake news tweets?
Answer: They manually inspected the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before).
True answer: an expert annotator determined if the tweet fell under a specific category

Validation ID: 56a8826cbee49560592b2d4b47b18ada236a12b9
Question: How did they determine fake news tweets?
Answer: They manually inspected the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before).
True answer: Exposure, Characterization, Polarization

Validation ID: 968b7c3553a668ba88da105eff067d57f393c63f
Question: What is their definition of tweets going viral?
Answer: Retweeted more than 1000 times
True answer: Viral tweets are the ones that are retweeted more than 1000 times

Validation ID: 968b7c3553a668ba88da105eff067d57f393c63f
Question: What is their definition of tweets going viral?
Answer: Retweeted more than 1000 times
True answer: those that contain a high number of retweets

Validation ID: f03df5d99b753dc4833ef27b32bb95ba53d790ee
Question: What are the characteristics of the accounts that spread fake news?
Answer: Accounts spreading fake news appear to have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only. Together with the fact that, on average, tweets containing fake news have more URLs than those spreading viral content, it is possible to hypothesize that, both, the ratio of friends/followers of the account producing a viral tweet and number of URLs contained in such a tweet could be useful to single-out fake news in Twitter. Not only that, but our finding related to the number of URLs is in line with intuitions behind the incentives to create fake news commonly found in the literature BIBREF9 (in particular that of obtaining revenue through click-through advertising).
True answer: Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio

Validation ID: f03df5d99b753dc4833ef27b32bb95ba53d790ee
Question: What are the characteristics of the accounts that spread fake news?
Answer: Accounts spreading fake news appear to have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only. Together with the fact that, on average, tweets containing fake news have more URLs than those spreading viral content, it is possible to hypothesize that, both, the ratio of friends/followers of the account producing a viral tweet and number of URLs contained in such a tweet could be useful to single-out fake news in Twitter. Not only that, but our finding related to the number of URLs is in line with intuitions behind the incentives to create fake news commonly found in the literature BIBREF9 (in particular that of obtaining revenue through click-through advertising).
True answer: have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only

Validation ID: a8f51b4e334a917702422782329d97304a2fe139
Question: What is the threshold for determining that a tweet has gone viral?
Answer: 1000 retweets
True answer: 1000

Validation ID: dca86fbe1d57b44986055b282a03c15ef7882e51
Question: How is the ground truth for fake news established?
Answer: The dataset BIBREF8, manually labelled by an expert, has been publicly released and is available to researchers and interested parties.
True answer: Ground truth is not established in the paper

Validation ID: 27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa
Question: what NMT models did they compare with?
Answer: RNN-based NMT, Transformer-NMT
True answer: RNN-based NMT model, Transformer-NMT

Validation ID: b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72
Question: Where does the ancient Chinese dataset come from?
Answer: The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.
True answer: ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era

Validation ID: b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72
Question: Where does the ancient Chinese dataset come from?
Answer: The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.
True answer: Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet 

Validation ID: 808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc
Question: How many different characters were in dataset?
Answer: 327
True answer: 45,821 characters

Validation ID: 808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc
Question: How many different characters were in dataset?
Answer: 327
True answer: 45,821 characters

Validation ID: 36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd
Question: How does dataset model character's profiles?
Answer: HLAs
True answer: attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics

Validation ID: f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6
Question: How big is the difference in performance between proposed model and baselines?
Answer: Significant
True answer: Metric difference between Aloha and best baseline score:
Hits@1/20: +0.061 (0.3642 vs 0.3032)
MRR: +0.0572(0.5114 vs 0.4542)
F1: -0.0484 (0.3901 vs 0.4385)
BLEU: +0.0474 (0.2867 vs 0.2393)

Validation ID: 357eb9f0c07fa45e482d998a8268bd737beb827f
Question: What baseline models are used?
Answer: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset.
True answer: the Poly-encoder from BIBREF7 humeau2019real, Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously, We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20., a BERT bi-ranker

Validation ID: 357eb9f0c07fa45e482d998a8268bd737beb827f
Question: What baseline models are used?
Answer: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset.
True answer: Kvmemnn,  Feed Yourself, Poly-encoder, BERT bi-ranker

Validation ID: ad08b215dca538930ef1f50b4e49cd25527028ad
Question: Was PolyReponse evaluated against some baseline?
Answer: Yes, it was evaluated against a baseline.
True answer: No

Validation ID: ad08b215dca538930ef1f50b4e49cd25527028ad
Question: Was PolyReponse evaluated against some baseline?
Answer: Yes, it was evaluated against a baseline.
True answer: No

Validation ID: 31101dc9937f108e27e08a5f34be44f0090b8b6b
Question: What metric is used to evaluate PolyReponse system?
Answer: Cosine similarity
True answer: Unanswerable

Validation ID: e4a315e9c190cf96493eefe04ce4ba6ae6894550
Question: How does PolyResponse architecture look like?
Answer: PolyResponse: Conversational Search
True answer: Henderson:2017, MobileNet model

Validation ID: 6263b2cba18207474786b303852d2f0d7068d4b6
Question: In what 8 languages is PolyResponse engine used for restourant search and booking system?
Answer: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian
True answer: English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian

Validation ID: 6263b2cba18207474786b303852d2f0d7068d4b6
Question: In what 8 languages is PolyResponse engine used for restourant search and booking system?
Answer: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian
True answer: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)

Validation ID: c1c44fd96c3fa6e16949ae8fa453e511c6435c68
Question: Why masking words in the decoder is helpful?
Answer: Because the decoder can only see the left context, so it is hard to learn the right context. Masking words in the decoder can help the decoder learn the right context.
True answer: ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.

Validation ID: d28d86524292506d4b24ae2d486725a6d57a3db3
Question: What is the ROUGE score of the highest performing model?
Answer: 33.33
True answer: 33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L 

Validation ID: d28d86524292506d4b24ae2d486725a6d57a3db3
Question: What is the ROUGE score of the highest performing model?
Answer: 33.33
True answer: 33.33

Validation ID: feafcc1c4026d7f55a2c8ce7850d7e12030b5c22
Question: How are the different components of the model trained? Is it trained end-to-end?
Answer: Yes, it is trained end-to-end.
True answer: the objective of our model is sum of the two processes, jointly trained using "teacher-forcing" algorithm, we feed the ground-truth summary to each decoder and minimize the objective, At test time, each time step we choose the predicted word by $\hat{y} = argmax_{y^{\prime }} P(y^{\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries., the model can be trained end-to-end

Validation ID: feafcc1c4026d7f55a2c8ce7850d7e12030b5c22
Question: How are the different components of the model trained? Is it trained end-to-end?
Answer: Yes, it is trained end-to-end.
True answer: the model can be trained end-to-end

Validation ID: 63488da6c7aff9e374561a24ba224e9ce7f65e40
Question: When is this paper published?
Answer: 2019
True answer: Unanswerable

Validation ID: c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c
Question: Can their indexing-based method be applied to create other QA datasets in other domains, and not just Wikipedia?
Answer: Yes, it can be applied to other domains.
True answer: Unanswerable

Validation ID: a9337636b52de375c852682a2561af2c1db5ec63
Question: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?
Answer: Yes
True answer: Yes

Validation ID: a9337636b52de375c852682a2561af2c1db5ec63
Question: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?
Answer: Yes
True answer: No

Validation ID: 45a5961a4e1d1c22874c4918e5c98bd3c0a670b3
Question: How many question types do they find in the datasets analyzed?
Answer: 7
True answer: seven 

Validation ID: 45a5961a4e1d1c22874c4918e5c98bd3c0a670b3
Question: How many question types do they find in the datasets analyzed?
Answer: 7
True answer: 7

Validation ID: 30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1
Question: How do they analyze contextual similaries across datasets?
Answer: They use a CNN model to classify the question types.
True answer: They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.

Validation ID: 5c6fa86757410aee6f5a0762328637de03a569e9
Question: What were their performance results?
Answer: F1 score of 0.94 for both these datasets using BLSTM with attention and feature level transfer learning.
True answer: best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset

Validation ID: 7e38e0279a620d3df05ab9b5e2795044f18d4471
Question: What cyberbulling topics did they address?
Answer: Personal attack, racism, and sexism
True answer: personal attack, racism, and sexism

Validation ID: 7e38e0279a620d3df05ab9b5e2795044f18d4471
Question: What cyberbulling topics did they address?
Answer: Personal attack, racism, and sexism
True answer: racism, sexism, personal attack, not specifically about any single topic

Validation ID: 8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92
Question: Were any of the pipeline components based on deep learning models?
Answer: No
True answer: No

Validation ID: 8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92
Question: Were any of the pipeline components based on deep learning models?
Answer: No
True answer: No

Validation ID: 03ebb29c08375afc42a957c7b2dc1a42bed7b713
Question: How is the effectiveness of this pipeline approach evaluated?
Answer: The obtained results support our claim that the proposed system can be used as a base tool for information extraction for the Portuguese language.
True answer: proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents.

Validation ID: 9cf070d6671ee4a6353f79a165aa648309e01295
Question: What is the size of the parallel corpus used to train the model constraints?
Answer: 1500
True answer: 1500 sentences

Validation ID: 9cf070d6671ee4a6353f79a165aa648309e01295
Question: What is the size of the parallel corpus used to train the model constraints?
Answer: 1500
True answer: 1500 sentences

Validation ID: 87bc6f83f7f90df3c6c37659139b92657c3f7a38
Question: How does enforcing agreement between parse trees work across different languages?
Answer: We make use of the parallel Hindi sentences to disambiguate the PP attachments for English sentences.
True answer: we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints

Validation ID: 01e2d10178347d177519f792f86f25575106ddc7
Question: What datasets are used to assess the performance of the system?
Answer: Switchboard, LORELEI
True answer: Switchboard Telephone Speech Corpus BIBREF21, LORELEI (Low Resource Languages for Emergent Incidents) Program

Validation ID: 01e2d10178347d177519f792f86f25575106ddc7
Question: What datasets are used to assess the performance of the system?
Answer: Switchboard, LORELEI
True answer: LORELEI datasets of Uzbek, Mandarin and Turkish

Validation ID: 021bfb7e180d67112b74f05ecb3fa13acc036c86
Question: How is the vocabulary of word-like or phoneme-like units automatically discovered?
Answer: Unsupervised term discovery (UTD)
True answer: Zero Resource Toolkit (ZRTools) BIBREF7

Validation ID: d201b9992809142fe59ae74508bc576f8ca538ff
Question: IS the graph representation supervised?
Answer: No
True answer: The graph representation appears to be semi-supervised. It is included in the learning pipeline for the medical recommendation, where the attention model is learned. (There is some additional evidence that is unavailable in parsed text)

Validation ID: c4628d965983934d7a2a9797a2de6a411629d5bc
Question: Is the G-BERT model useful beyond the task considered?
Answer: Yes
True answer: There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding.

Validation ID: c4628d965983934d7a2a9797a2de6a411629d5bc
Question: Is the G-BERT model useful beyond the task considered?
Answer: Yes
True answer: It learns a representation of medical records. The learned representation (embeddings) can be used for other predictive tasks involving information from electronic health records.

Validation ID: bd419f4094186a5ce74ba6ac1622b24e29e553f4
Question: How well did the baseline perform?
Answer: 30.3%
True answer: accuracy of 30.3% on single sentences and 0.3 on complete paragraphs

Validation ID: 11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af
Question: What is the baseline?
Answer: The baseline is the lowest level of performance that can be achieved by a system.
True answer: NO-MOVE, RANDOM, JUMP

Validation ID: 11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af
Question: What is the baseline?
Answer: The baseline is the lowest level of performance that can be achieved by a system.
True answer: NO-MOVE: the only position considered is the starting point, RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route, JUMP: at each sentence, extract entities from the map and move between them in the order they appear

Validation ID: 1269c5d8f61e821ee0029080c5ba2500421d5fa6
Question: what methods were used to reduce data sparsity effects?
Answer: Back Translation and Mix-Source
True answer: Back Translation, Mix-Source Approach

Validation ID: 1269c5d8f61e821ee0029080c5ba2500421d5fa6
Question: what methods were used to reduce data sparsity effects?
Answer: Back Translation and Mix-Source
True answer: data augmentation

Validation ID: e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb
Question: what was the baseline?
Answer: phrase-based and tree-to-string models
True answer: traditional phrase-based statistical machine translation (SMT), NMT system

Validation ID: e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb
Question: what was the baseline?
Answer: phrase-based and tree-to-string models
True answer: traditional phrase-based statistical machine translation (SMT), NMT system

Validation ID: b9ea841b817ba23281c95c7a769873b840dee8d5
Question: did they collect their own data?
Answer: Yes
True answer: No

Validation ID: 219af68afeaecabdfd279f439f10ba7c231736e4
Question: what japanese-vietnamese dataset do they use?
Answer: TED talks
True answer: WIT3's corpus

Validation ID: a66a275a817f980c36e0b67d2e00bd823f63abf8
Question: How do they measure style transfer success?
Answer: We evaluated the style transfer itself in the following way. For each pair of sentences, the average score given by three evaluators was calculated, in which the answer that the translated sentence is more formal counts as +1, more informal as -1, and neither as 0.
True answer: Unanswerable

Validation ID: b6f466e0fdcb310ecd212fd90396d9d13e0c0504
Question: Do they introduce errors in the data or does the data already contain them?
Answer: The data already contains them.
True answer:  all three languages have error-corrected corpora for testing purposes

Validation ID: b6f466e0fdcb310ecd212fd90396d9d13e0c0504
Question: Do they introduce errors in the data or does the data already contain them?
Answer: The data already contains them.
True answer: Data already contain errors

Validation ID: 62ea141d0fb342dfb97c69b49d1c978665b93b3c
Question: What error types is their model more reliable for?
Answer: spelling, word order and grammatical errors
True answer: grammatical, spelling and word order errors

Validation ID: 62ea141d0fb342dfb97c69b49d1c978665b93b3c
Question: What error types is their model more reliable for?
Answer: spelling, word order and grammatical errors
True answer: spelling, word order and grammatical errors

Validation ID: a32c792a0cef03218bf66322245677fc2d5e5a31
Question: How does their parallel data differ in terms of style?
Answer: The parallel data is different in terms of style.
True answer: Unanswerable

Validation ID: 0101ebfbaba75fd47868ad0c796ac44ebc19c566
Question: How do they split text to obtain sentence levels?
Answer: They split text to obtain sentence levels by using the sentence tokenizer in NLTK.
True answer: Unanswerable

Validation ID: 50cb50657572e315fd452a89f3e0be465094b66f
Question: Do they experiment with their proposed model on any other dataset other than MovieQA?
Answer: No
True answer: Yes

Validation ID: 50cb50657572e315fd452a89f3e0be465094b66f
Question: Do they experiment with their proposed model on any other dataset other than MovieQA?
Answer: No
True answer: Yes

Validation ID: 981fd79dd69581659cb1d4e2b29178e82681eb4d
Question: What is the difference of the proposed model with a standard RNN encoder-decoder?
Answer: The proposed model is an extension of the RNN encoder-decoder model by introducing a Refinement cell and an Adjustment cell.
True answer: Introduce a "Refinement Adjustment LSTM-based component" to the decoder

Validation ID: 03e9ac1a2d90152cd041342a11293a1ebd33bcc3
Question: Does the model evaluated on NLG datasets or dialog datasets?
Answer: NLG datasets
True answer: NLG datasets

Validation ID: 03e9ac1a2d90152cd041342a11293a1ebd33bcc3
Question: Does the model evaluated on NLG datasets or dialog datasets?
Answer: NLG datasets
True answer: NLG datasets

Validation ID: ef396a34436072cb3c40b0c9bc9179fee4a168ae
Question: What tasks do they experiment with?
Answer: Text classification and text semantic matching
True answer: text classification and text semantic matching

Validation ID: ef396a34436072cb3c40b0c9bc9179fee4a168ae
Question: What tasks do they experiment with?
Answer: Text classification and text semantic matching
True answer: text classification and text semantic matching

Validation ID: 04bde1d2b445f971e97bb46ade2d0290981c7a32
Question: What is the meta knowledge specifically?
Answer: The parameters of compositional function vary from position to position and from sample to sample, allowing for more sophisticated operations on the input.
True answer: Unanswerable

Validation ID: bfbd6040cb95b179118557352e8e3899ef25c525
Question: Are there elements, other than pitch, that can potentially result in out of key converted singing?
Answer: Yes
True answer: Unanswerable

Validation ID: d6e353e0231d09fd5dcba493544d53706f3fe1ab
Question: How is the quality of singing voice measured?
Answer: Mean Opinion Score (MOS)
True answer: To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score.

Validation ID: d6e353e0231d09fd5dcba493544d53706f3fe1ab
Question: How is the quality of singing voice measured?
Answer: Mean Opinion Score (MOS)
True answer: Automatic: Normalized cross correlation (NCC)
Manual: Mean Opinion Score (MOS)

Validation ID: 7bd6a6ec230e1efb27d691762cc0674237dc7967
Question: what data did they use?
Answer: Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20
True answer:  Penn Treebank, WikiText2

Validation ID: 7bd6a6ec230e1efb27d691762cc0674237dc7967
Question: what data did they use?
Answer: Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20
True answer: Penn Treebank (PTB) , WikiText2 (WT-2)

Validation ID: 6aaf12505add25dd133c7b0dafe8f4fe966d1f1d
Question: what previous RNN models do they compare with?
Answer: LSTM
True answer: Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM

Validation ID: 73906462bd3415f23d6378590a5ba28709b17605
Question: What are examples of these artificats?
Answer: lexical overlap, sentence length, negation words
True answer: the degree of lexical overlap between them, presence of negation words

Validation ID: 73906462bd3415f23d6378590a5ba28709b17605
Question: What are examples of these artificats?
Answer: lexical overlap, sentence length, negation words
True answer: hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap

Validation ID: 5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11
Question: What are the languages they use in their experiment?
Answer: English, Spanish, Finnish
True answer: English
French
Spanish
German
Greek
Bulgarian
Russian
Turkish
Arabic
Vietnamese
Thai
Chinese
Hindi
Swahili
Urdu
Finnish

Validation ID: 5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11
Question: What are the languages they use in their experiment?
Answer: English, Spanish, Finnish
True answer: English, Spanish, Finnish

Validation ID: 88bf368491f9613767f696f84b4bb1f5a7d7cb48
Question: Does the professional translation or the machine translation introduce the artifacts?
Answer: Machine translation
True answer: Yes

Validation ID: 0737954caf66f2b4c898b356d2a3c43748b9706b
Question: Do they recommend translating the premise and hypothesis together?
Answer: No
True answer: No

Validation ID: 0737954caf66f2b4c898b356d2a3c43748b9706b
Question: Do they recommend translating the premise and hypothesis together?
Answer: No
True answer: No

Validation ID: 664b3eadc12c8dde309e8bbd59e9af961a433cde
Question: Is the improvement over state-of-the-art statistically significant?
Answer: Yes.
True answer: Yes

Validation ID: b3307d5b68c57a074c483636affee41054be06d1
Question: What are examples of these artifacts?
Answer: lexical overlap, sentence length, negation words
True answer: hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap

Validation ID: bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31
Question: What languages do they use in their experiments?
Answer: English, Spanish, Finnish
True answer: English, Spanish, Finnish

Validation ID: 12d7055baf5bffb6e9e95e977c000ef2e77a4362
Question: How much higher quality is the resulting annotated data?
Answer: $\sim $ 10%
True answer: improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added

Validation ID: 498c0229f831c82a5eb494cdb3547452112a66a0
Question: How do they match annotators to instances?
Answer: They use a routing strategy in which difficult examples are routed to experts and other instances to crowd annotators.
True answer: Annotations from experts are used if they have already been collected.

Validation ID: 8c48c726bb17a17d70ab29db4d65a93030dd5382
Question: How much data is needed to train the task-specific encoder?
Answer: We used the large version of the universal sentence encoder with a transformer. We did not update the pretrained sentence encoder parameters during training.
True answer: 57,505 sentences

Validation ID: 8c48c726bb17a17d70ab29db4d65a93030dd5382
Question: How much data is needed to train the task-specific encoder?
Answer: We used the large version of the universal sentence encoder with a transformer. We did not update the pretrained sentence encoder parameters during training.
True answer: 57,505 sentences

Validation ID: 89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f
Question: What kind of out-of-domain data?
Answer: common crawl data
True answer: Unanswerable

Validation ID: 89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f
Question: What kind of out-of-domain data?
Answer: common crawl data
True answer: Unanswerable

Validation ID: 06b5272774ec43ee5facfa7111033386f06cf448
Question: Is an instance a sentence or an IE tuple?
Answer: Sentence
True answer: sentence

Validation ID: 08b57deb237f15061e4029b6718f1393fa26acce
Question: Who are the crowdworkers?
Answer: Crowdworkers were located in the US and hired on the BIBREF22 platform.
True answer: people in the US that use Amazon Mechanical Turk

Validation ID: 08b57deb237f15061e4029b6718f1393fa26acce
Question: Who are the crowdworkers?
Answer: Crowdworkers were located in the US and hired on the BIBREF22 platform.
True answer: located in the US, hired on the BIBREF22 platform

Validation ID: 9b7655d39c7a19a23eb8944568eb5618042b9026
Question: Which toolkits do they use?
Answer: NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, Rosette Text Analytics, Google Cloud, spaCy, CogComp-NLP, Stanford NLP NER
True answer: BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21

Validation ID: 9b7655d39c7a19a23eb8944568eb5618042b9026
Question: Which toolkits do they use?
Answer: NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, Rosette Text Analytics, Google Cloud, spaCy, CogComp-NLP, Stanford NLP NER
True answer: BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26

Validation ID: cd06d775f491b4a17c9d616a8729fd45aa2e79bf
Question: Which sentiment class is the most accurately predicted by ELS systems?
Answer: Neutral
True answer: neutral sentiment

Validation ID: 1329280df5ee9e902b2742bde4a97bc3e6573ff3
Question: Is datasets for sentiment analysis balanced?
Answer: No
True answer: No

Validation ID: 58c6737070ef559e9220a8d08adc481fdcd53a24
Question: What measures are used for evaluation?
Answer: CCR
True answer: correct classification rate (CCR)

Validation ID: 0af16b164db20d8569df4ce688d5a62c861ace0b
Question: what were the baselines?
Answer: BOW, TFIDF, TextCNN, C-TextCNN
True answer: BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN

Validation ID: 0af16b164db20d8569df4ce688d5a62c861ace0b
Question: what were the baselines?
Answer: BOW, TFIDF, TextCNN, C-TextCNN
True answer: bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe

Validation ID: 78a4ec72d76f0a736a4a01369a42b092922203b6
Question: what datasets were used?
Answer: Friends and EmotionPush
True answer: Friends, EmotionPush

Validation ID: 78a4ec72d76f0a736a4a01369a42b092922203b6
Question: what datasets were used?
Answer: Friends and EmotionPush
True answer: EmotionLines BIBREF6

Validation ID: 6a14379fee26a39631aebd0e14511ce3756e42ad
Question: What BERT models are used?
Answer: FriendsBERT and ChatBERT
True answer: BERT-base, BERT-large, BERT-uncased, BERT-cased

Validation ID: 81588e0e207303c2867c896f3911a54a1ef7c874
Question: What are the sources of the datasets?
Answer: Friends is a speech-based dataset and EmotionPush is a chat-based dataset.
True answer: Friends TV sitcom, Facebook messenger chats

Validation ID: dd09db5eb321083dba16c2550676e60682f9a0cd
Question: What labels does the dataset have?
Answer: Joy, Sadness, Anger, Neutral
True answer: Ekman’s six basic emotions,  neutral

Validation ID: 40c0f97c3547232d6aa039fcb330f142668dea4b
Question: Do they evaluate on English only datasets?
Answer: Yes
True answer: Unanswerable

Validation ID: 40c0f97c3547232d6aa039fcb330f142668dea4b
Question: Do they evaluate on English only datasets?
Answer: Yes
True answer: No

Validation ID: 777217e025132ddc173cf33747ee590628a8f62f
Question: What experiments are used to demonstrate the benefits of this approach?
Answer: We compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34. In particular, we compare the following methods:
True answer: On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:

Validation ID: 777217e025132ddc173cf33747ee590628a8f62f
Question: What experiments are used to demonstrate the benefits of this approach?
Answer: We compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34. In particular, we compare the following methods:
True answer: Calculate test log-likelihood on the three considered datasets

Validation ID: 2dbf6fe095cd879a9bf40f110b7b72c8bdde9475
Question: What hierarchical modelling approach is used?
Answer: Hierarchical embedding structure
True answer: the group-specific embedding representations are tied through a global embedding

Validation ID: 7d483077ed7f2f504d59f4fc2f162741fa5ac23b
Question: How do co-purchase patterns vary across seasons?
Answer: We fit a Poisson sefe model to the shopping data, grouped by season. We use the same context as in the efe model, i.e., the set of items purchased in the same shopping trip, excluding the item of interest. We use the same vocabulary as in the efe model, i.e., the 15k most frequent items. We use the same hyperparameters as in the efe model, i.e., INLINEFORM0 and INLINEFORM1.
True answer: Unanswerable

Validation ID: de830c534c23f103288c198eb19174c76bfd38a1
Question: Which words are used differently across ArXiv?
Answer: artificial, ai, abilities, consciousness
True answer: intelligence

Validation ID: b0d66760829f111b8fad0bd81ca331ddd943ef41
Question: What is future work planed?
Answer: Improving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in textGAN.
True answer: ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE

Validation ID: ae7c93646aa5f3206cd759904965b4d484d12f83
Question: What is this method improvement over the best performing state-of-the-art?
Answer: It is a novel loss function, ARL, to automatically balance RL with MLE.
True answer: absolute improvement of 18.2% over the Pointer-Gen baseline

Validation ID: d1ec42b2b5a3c956ff528543636e024bfde5e5ba
Question: Which baselines are used for evaluation?
Answer: Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set
True answer: Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN

Validation ID: 1dac4bc5af239024566fcb0f43bb9ff1c248ecec
Question: Did they used dataset from another domain for evaluation?
Answer: No
True answer: No

Validation ID: 1dac4bc5af239024566fcb0f43bb9ff1c248ecec
Question: Did they used dataset from another domain for evaluation?
Answer: No
True answer: No

Validation ID: 3bf0306e9bd044f723e38170c13455877b2aeec3
Question: How is sensationalism scorer trained?
Answer: We regard headlines receiving lots of comments as sensational samples and the headlines generated by a summarization model as non-sensational samples.
True answer: by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$

Validation ID: 3bf0306e9bd044f723e38170c13455877b2aeec3
Question: How is sensationalism scorer trained?
Answer: We regard headlines receiving lots of comments as sensational samples and the headlines generated by a summarization model as non-sensational samples.
True answer: classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss

Validation ID: 2858620e0498db2f2224bfbed5263432f0570832
Question: Which component is the least impactful?
Answer: No master node
True answer: Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.

Validation ID: 545e92833b0ad4ba32eac5997edecf97a366a244
Question: Which component has the greatest impact on performance?
Answer: Number of MP iterations
True answer: Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations

Validation ID: 545e92833b0ad4ba32eac5997edecf97a366a244
Question: Which component has the greatest impact on performance?
Answer: Number of MP iterations
True answer: Removing the master node deteriorates performance across all datasets

Validation ID: cb12c19f9d14bef7b2f778892d9071eea2d6c63d
Question: What is the state-of-the-art system?
Answer: MPAD
True answer: doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT

Validation ID: 9193006f359c53eb937deff1248ee3317978e576
Question: Which datasets are used?
Answer: Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013
True answer: Reuters,  BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013

Validation ID: 9193006f359c53eb937deff1248ee3317978e576
Question: Which datasets are used?
Answer: Reuters, BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013
True answer:  Reuters, BBCSport BIBREF30, Polarity BIBREF31, Subjectivity BIBREF32, MPQA BIBREF33, IMDB BIBREF34, TREC BIBREF35, SST-1 BIBREF36, SST-2 BIBREF36, Yelp2013 BIBREF26

Validation ID: bc67b91dd73acded2d52fd4fee732b7a9722ea8b
Question: What is the message passing framework?
Answer: The message passing framework is a framework under which many of the recently introduced GNNs can be reformulated.
True answer: It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors.

Validation ID: 49c32a2a64eb41381e5f12ccea4150cac9f3303d
Question: What other evaluation metrics are looked at?
Answer: Kappa statistics
True answer: F-score, Kappa

Validation ID: 49c32a2a64eb41381e5f12ccea4150cac9f3303d
Question: What other evaluation metrics are looked at?
Answer: Kappa statistics
True answer: Unanswerable

Validation ID: bbb77f2d6685c9257763ca38afaaef29044b4018
Question: What is the best reported system?
Answer: joshi2015harnessing
True answer: Gaze Sarcasm using Multi Instance Logistic Regression.

Validation ID: bbb77f2d6685c9257763ca38afaaef29044b4018
Question: What is the best reported system?
Answer: joshi2015harnessing
True answer: the MILR classifier

Validation ID: 22732cb9476e521452bf0538f3fdb94cf3867651
Question: What kind of stylistic features are obtained?
Answer: Emoticons, laughter expressions such as “lol” etc
True answer: Unanswerable

Validation ID: 4e748cb2b5e74d905d9b24b53be6cfdf326e8054
Question: What traditional linguistics features did they use?
Answer: Unigrams and Pragmatic features
True answer: Unanswerable

Validation ID: 74b338d5352fe1a6fd592e38269a4c81fe79b866
Question: What cognitive features are used?
Answer: Simple Gaze Based Features and Complex Gaze Based Features
True answer: Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half
to first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze
graph (ED),  Fixation Duration at Left/Source
(F1H, F1S),  Fixation Duration at Right/Target
(F2H, F2S),  Forward Saccade Word Count of
Source (PSH, PSS),  Forward SaccadeWord Count of Destination
(PSDH, PSDS), Regressive Saccade Word Count of
Source (RSH, RSS),  Regressive Saccade Word Count of
Destination (RSDH, RSDS)

Validation ID: d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf
Question: What approaches do they use towards text analysis?
Answer: Computational approaches
True answer: Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use

Validation ID: d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf
Question: What approaches do they use towards text analysis?
Answer: Computational approaches
True answer: Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis.

Validation ID: f903396d943541a8cc65edefb04ca37814ed30dd
Question: What dataset do they use for analysis?
Answer: Reddit
True answer: Unanswerable

Validation ID: ba28ce9a2f7e8524243adf288cc3f11055e667bb
Question: Do they demonstrate why interdisciplinary insights are important?
Answer: Yes
True answer: No

Validation ID: ba28ce9a2f7e8524243adf288cc3f11055e667bb
Question: Do they demonstrate why interdisciplinary insights are important?
Answer: Yes
True answer: No

Validation ID: 975e60535724f4149c7488699a199ba2920a062c
Question: What background do they have?
Answer: They are all from the same school.
True answer: Unanswerable

Validation ID: b970f48d30775d3468952795bc72976baab3438e
Question: What kind of issues (that are not on the forefront of computational text analysis) do they tackle?
Answer: 
True answer: identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines

